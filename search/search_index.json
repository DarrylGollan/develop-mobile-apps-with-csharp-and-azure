{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nWelcome to my first book.  It's free, it's open source, and it's comprehensive.\nThose attributes also describe two of my favorite technologies, and I'm basing\nmy first book on them.  The first is \nXamarin Forms\n , a technology that\nallows you to develop cross-platform mobile applications using C# and the .NET\nframework.  The second is \nAzure Mobile Apps\n, a technology that allows\nyou to connect your mobile app to  resources that are important in cloud\nconnected mobile applications such as table data, authentication, and push\nnotifications.\n\n\nThis book does not tell you everything there is to know about either topic.  It\nfocuses on the topics necessary to get your mobile apps connected to the cloud.\n\n\nWhat are Cloud Connected Mobile Apps?\n\n\nI guess I should define some of the terminology that I am going to use.  When I\nrefer to a \nmobile application\n or \nmobile app\n, I mean every piece of\nsoftware that is related to the application you want to use.  This includes, for\nexample, the \nmobile client\n. This is the piece of code you run on your iPhone\nor Android phone.  It also includes the \nmobile backend\n which is the service\nthat you run in the cloud to provide important services to your mobile client.\n\n\nA \ncloud connected mobile application\n is a mobile client that connects to a\nmobile backend for shared services.  Quite a few of the apps on your phone are\ncloud connected already.  For example, Instagram uses the cloud for photo\nstorage, and Facebook uses the cloud to store the news feeds of you and your\nfriends.\n\n\nWhy Cross-Platform Native Development is important?\n\n\nIt should come as no surprise that Apple and Google have pretty much won the\nmobile OS wars.  Over 90% of the smart phones being sold today run either iOS\nor Android.  However, these are two very different mobile operating systems,\nwith different programming models.  iOS is based on either \nSwift\n or\nObjective-C.  Android is based on \nJava\n.  If you want to develop for the\n80% case (and you should), then you need to know both Swift and Java.  That's\na tall order even for the most dedicated mobile developer.\n\n\nHowever, there are alternatives out there.  Most notable, you can write your\nmobile application with one code-base and just write exceptions for when the\nplatforms diverge.  You have to pick a single language and a tool set that\nsupports cross-platform development to do this.  Not all cross-platform tool\nsets are created equal, however.  Some do not compile your code to native\nbinaries, which means that you do not get access to all the functionality of\nthe mobile platforms you are targeting.\n\n\nXamarin, recently acquired by Microsoft, allows you to target all major\nplatforms - iOS, Android and Windows - to gain greater than 95% coverage of\nthe mobile smart phone market.  It does this by leveraging the .NET framework\nand compiling your code to a native binary for each platform.\n\n\nXamarin.Forms is a cross-platform framework, based on XAML and .NET, that\nallows you to use common UI pages to develop your apps.\n\n\nWhy Azure Mobile Apps?\n\n\nWhen you think about the major apps in the marketplace for each mobile\nplatform, the thing that they have in common is that they have some sort\nof cloud infrastructure driving them.  It might be as simple as storing\nyour task list, or as complex as your Facebook news feed.  It could be a\ngaming leader board, or the social sharing of your photos.  Whatever it\nis, cloud connectivity is a must.\n\n\nNot all clouds are created equal.  There are some common features that you\nshould think about including irrespective of the application.  I like to use\nAzure Mobile Apps for these features because they are all included and you\ncan get started with most of the features for zero cost.  Even the features\nthat cannot be obtained without spending a little money are relatively cheap.\n\n\n\n\nInfo\n\n\nAzure Mobile Apps\n is a feature of Azure App Service.  Azure App Service\nis a collection of services that commonly are used together to develop modern\nInternet Apps.  This includes web hosting, API hosting and Mobile SDKs.\n\n\n\n\nFeatures of Cloud Connected Mobile Apps\n\n\nA cloud connected mobile application will use one or more services in the\nfollowing areas:\n\n\n\n\nAuthentication\n\n\nStorage of structured data (like a task list)\n\n\nStorage of unstructured data (like photographs)\n\n\nPush notifications\n\n\nInvocation of Custom Code\n\n\n\n\nI am going to cover each of these in great detail.  In addition, I will also\ncover some common issues and solutions that developers run into while developing\ncloud connected mobile applications such as testing and going to production.\n\n\nAside from the actual features of mobile apps, there are other things to\nconsider while developing your mobile application.  Here is my list, in no\nparticular order:\n\n\n\n\nContinuous Deployment\n\n\nSlots or Staging Sites\n\n\nAutomatic Scalability\n\n\nDatabase Backups\n\n\nCombined Web\n\n\n\n\nThe point here is that my intent is to write a production quality application.\nI need to be able to deploy my site with confidence without resorting to jumping\nthrough hoops.  I want to run multiple versions of the backend so that I can run\na staging site for testing purposes.  I want to be able to roll back my\nproduction site to a previous version at a moments notice.  I want to be able to\nhandle the load when my app is successful, and I want things to be backed up\n(since bad things inevitably happen when I am least prepared).\n\n\nAll of these features are available in Azure App Service, and the Mobile Apps\nSDK that I will use throughout the book is supported only on Azure App Service.\n\n\nWho is This Book For?\n\n\nThis book is for intermediate to experienced C# developers who have already\nbuilt a mobile app with Xamarin and want to take their mobile apps to the next\nlevel by utilizing cloud services.\n\n\nThis book is not for the beginner.  Explicitly, I already expect you to know how\nto develop mobile applications with C# and Xamarin technologies.  If you are\nunfamiliar with the C# language, you can get started with a free course on the\nInternet.  The basics of the language can be learned at \nwww.learncs.org\n.\nOnce you have the language basics under your belt, you can move on to building\nmobile applications with Xamarin. You can learn more about developing cross-platform\nmobile development with Xamarin at the \nXamarin\n website.  Although you do\nnot need to understand ASP.NET to get value out of this book, be aware that the\nmobile back ends that I will be covering are written in C# and ASP.NET.  A good\nunderstanding of ASP.NET will assist you.\n\n\nThings You Should Know!\n\n\nBefore you get started with development, spend some time learning the tools of\nthe trade.  The command prompt on the Mac is \nbash\n and the command prompt\non the PC is \nPowerShell\n.  You should be proficient in the shell on the\nplatforms that you use.\n\n\nAdditionally, you should become familiar with the source code control system\nthat you will use.  For most, this means becoming familiar with\n\ngit\n.  Don't even think of developing without using source control.\n\n\nWhat You Will Need\n\n\nThe list of hardware and software for mobile development is longer than your\ntypical development projects.  It is still, thankfully, relatively short and\neasy to acquire.\n\n\nHardware\n\n\nYou will want a computer on which to develop code.  If you develop iOS\napplications, then you \nMUST\n have a Mac running the latest version of Mac\nOSX.  If you develop Universal Windows applications, then you \nMUST\n have a\nPC running Windows 10.  Android applications can be developed on either platform.\n\n\nMy own experience has taught me that the tooling for developing mobile backends\nin C# and ASP.NET (our primary languages during the course of this book) are\nbetter on a PC running Windows 10.  Thus, my hardware choice is a rather beefy\n\nWindows 10 PC\n for my main development system.  In addition, I have a\n\nMac Mini\n underneath my desk that I use to build the iOS portions of the\napplications.\n\n\nSoftware\n\n\nAll of the following software are freely available.  You should install each\npackage and update it (if appropriate) so that it is fully patched.\n\n\nOn your Mac\n\n\n\n\nXCode\n (available on the Mac App Store)\n\n\nVisual Studio for Mac\n\n\nAndroid Studio and Tools\n (if you intend to build Android apps on the Mac)\n\n\n\n\nYou must run XCode at least once after installation so that you can accept the\nlicense agreement.\n\n\nOn your Windows PC\n\n\n\n\nAndroid Studio and Tools\n\n\nVisual Studio 2017 Community\n\n\n\n\nWhen installing Visual Studio, you will want to install the following workloads:\n\n\n\n\nUniversal Windows Platform development.\n\n\nASP.NET and web development.\n\n\nAzure development.\n\n\nData storage and processing.\n\n\nMobile development with .NET.\n\n\n\n\nIf you have already installed Visual Studio and did not install these components, \nrun the installer again to add the components.\n\n\nYou can also use earlier versions of Visual Studio.  If you do use an earlier\nversion, then the screen shots I provide will not match up.  You will also need\nthe latest version of the \nAzure SDK\n installed.  \n\n\n\n\nTip\n\n\nDevelopment Tools are big, multi-gigabyte installers.  If you are on a slow or\nrestricted link, you may want to download the installers onto a thumb drive for\nlocal installation.\n\n\n\n\nCloud Services\n\n\nYou will need an Azure account to complete most of the tutorials in this book.\nIn fact, you won't be able to get very far without one. If you have an MSDN\naccount, you already have access to free Azure resources.  You just need to log\ninto your \nMSDN account\n and activate your Azure benefit.  Students may be\nable to get access to \nMicrosoft Imagine\n from school resources, but this \nis not suitable for developing mobile applications.  This is because storage \ncosts money.  If you don't have MSDN, then there is a \nfree trial\n available.\nOnce the trial period ends, you can move to a Pay-As-You-Go account and continue\nto use free services without incurring a charge. I'll point out when you are\ngoing to incur charges on your Azure account, but I will be using free resources\nmost of the time.\n\n\nAside from Azure resources, you will want some place to store your code.  This\ndoesn't have to be in the cloud.  If you want to use the cloud, you can use\nGitHub or Visual Studio Team Services.  Both are free to use.  GitHub provides\npublic repositories for free.  Visual Studio Team Services provides private\nrespositories for free.  Visual Studio Team Services also includes other\nservices that I will talk about during the course of the book, some of which may\nincur cost.  I will be publishing all my samples and tutorial code on GitHub so\nthat you can easily download it.  You don't have to use one of these resources,\nbut I won't be covering other service usage.\n\n\nYou will need a \nDeveloper Account\n for the appropriate app store if you\nintend to distribute your mobile clients or if you intend to use specific cloud\nservices.  Apple is specific - if you intend to use push notifications or\ndistribute iOS apps, then you need an \nApple Developer Account\n,\n\nGoogle Developer Account\n and/or \nWindows Store Developer Account\n.\nThe terms of the accounts are changed constantly, so review the current terms\nwhen you sign up.  My recommendation is to defer signing up for these programs\nuntil you need something they offer.\n\n\nNow, let's get developing!  Our next section is dependent on where you are developing:\n\n\n\n\nOn a Mac, skip ahead to the \nMac section\n.\n\n\nOn a PC, the \nnext section\n covers Windows.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to my first book.  It's free, it's open source, and it's comprehensive.\nThose attributes also describe two of my favorite technologies, and I'm basing\nmy first book on them.  The first is  Xamarin Forms  , a technology that\nallows you to develop cross-platform mobile applications using C# and the .NET\nframework.  The second is  Azure Mobile Apps , a technology that allows\nyou to connect your mobile app to  resources that are important in cloud\nconnected mobile applications such as table data, authentication, and push\nnotifications.  This book does not tell you everything there is to know about either topic.  It\nfocuses on the topics necessary to get your mobile apps connected to the cloud.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#what-are-cloud-connected-mobile-apps", 
            "text": "I guess I should define some of the terminology that I am going to use.  When I\nrefer to a  mobile application  or  mobile app , I mean every piece of\nsoftware that is related to the application you want to use.  This includes, for\nexample, the  mobile client . This is the piece of code you run on your iPhone\nor Android phone.  It also includes the  mobile backend  which is the service\nthat you run in the cloud to provide important services to your mobile client.  A  cloud connected mobile application  is a mobile client that connects to a\nmobile backend for shared services.  Quite a few of the apps on your phone are\ncloud connected already.  For example, Instagram uses the cloud for photo\nstorage, and Facebook uses the cloud to store the news feeds of you and your\nfriends.", 
            "title": "What are Cloud Connected Mobile Apps?"
        }, 
        {
            "location": "/#why-cross-platform-native-development-is-important", 
            "text": "It should come as no surprise that Apple and Google have pretty much won the\nmobile OS wars.  Over 90% of the smart phones being sold today run either iOS\nor Android.  However, these are two very different mobile operating systems,\nwith different programming models.  iOS is based on either  Swift  or\nObjective-C.  Android is based on  Java .  If you want to develop for the\n80% case (and you should), then you need to know both Swift and Java.  That's\na tall order even for the most dedicated mobile developer.  However, there are alternatives out there.  Most notable, you can write your\nmobile application with one code-base and just write exceptions for when the\nplatforms diverge.  You have to pick a single language and a tool set that\nsupports cross-platform development to do this.  Not all cross-platform tool\nsets are created equal, however.  Some do not compile your code to native\nbinaries, which means that you do not get access to all the functionality of\nthe mobile platforms you are targeting.  Xamarin, recently acquired by Microsoft, allows you to target all major\nplatforms - iOS, Android and Windows - to gain greater than 95% coverage of\nthe mobile smart phone market.  It does this by leveraging the .NET framework\nand compiling your code to a native binary for each platform.  Xamarin.Forms is a cross-platform framework, based on XAML and .NET, that\nallows you to use common UI pages to develop your apps.", 
            "title": "Why Cross-Platform Native Development is important?"
        }, 
        {
            "location": "/#why-azure-mobile-apps", 
            "text": "When you think about the major apps in the marketplace for each mobile\nplatform, the thing that they have in common is that they have some sort\nof cloud infrastructure driving them.  It might be as simple as storing\nyour task list, or as complex as your Facebook news feed.  It could be a\ngaming leader board, or the social sharing of your photos.  Whatever it\nis, cloud connectivity is a must.  Not all clouds are created equal.  There are some common features that you\nshould think about including irrespective of the application.  I like to use\nAzure Mobile Apps for these features because they are all included and you\ncan get started with most of the features for zero cost.  Even the features\nthat cannot be obtained without spending a little money are relatively cheap.   Info  Azure Mobile Apps  is a feature of Azure App Service.  Azure App Service\nis a collection of services that commonly are used together to develop modern\nInternet Apps.  This includes web hosting, API hosting and Mobile SDKs.", 
            "title": "Why Azure Mobile Apps?"
        }, 
        {
            "location": "/#features-of-cloud-connected-mobile-apps", 
            "text": "A cloud connected mobile application will use one or more services in the\nfollowing areas:   Authentication  Storage of structured data (like a task list)  Storage of unstructured data (like photographs)  Push notifications  Invocation of Custom Code   I am going to cover each of these in great detail.  In addition, I will also\ncover some common issues and solutions that developers run into while developing\ncloud connected mobile applications such as testing and going to production.  Aside from the actual features of mobile apps, there are other things to\nconsider while developing your mobile application.  Here is my list, in no\nparticular order:   Continuous Deployment  Slots or Staging Sites  Automatic Scalability  Database Backups  Combined Web   The point here is that my intent is to write a production quality application.\nI need to be able to deploy my site with confidence without resorting to jumping\nthrough hoops.  I want to run multiple versions of the backend so that I can run\na staging site for testing purposes.  I want to be able to roll back my\nproduction site to a previous version at a moments notice.  I want to be able to\nhandle the load when my app is successful, and I want things to be backed up\n(since bad things inevitably happen when I am least prepared).  All of these features are available in Azure App Service, and the Mobile Apps\nSDK that I will use throughout the book is supported only on Azure App Service.", 
            "title": "Features of Cloud Connected Mobile Apps"
        }, 
        {
            "location": "/#who-is-this-book-for", 
            "text": "This book is for intermediate to experienced C# developers who have already\nbuilt a mobile app with Xamarin and want to take their mobile apps to the next\nlevel by utilizing cloud services.  This book is not for the beginner.  Explicitly, I already expect you to know how\nto develop mobile applications with C# and Xamarin technologies.  If you are\nunfamiliar with the C# language, you can get started with a free course on the\nInternet.  The basics of the language can be learned at  www.learncs.org .\nOnce you have the language basics under your belt, you can move on to building\nmobile applications with Xamarin. You can learn more about developing cross-platform\nmobile development with Xamarin at the  Xamarin  website.  Although you do\nnot need to understand ASP.NET to get value out of this book, be aware that the\nmobile back ends that I will be covering are written in C# and ASP.NET.  A good\nunderstanding of ASP.NET will assist you.", 
            "title": "Who is This Book For?"
        }, 
        {
            "location": "/#things-you-should-know", 
            "text": "Before you get started with development, spend some time learning the tools of\nthe trade.  The command prompt on the Mac is  bash  and the command prompt\non the PC is  PowerShell .  You should be proficient in the shell on the\nplatforms that you use.  Additionally, you should become familiar with the source code control system\nthat you will use.  For most, this means becoming familiar with git .  Don't even think of developing without using source control.", 
            "title": "Things You Should Know!"
        }, 
        {
            "location": "/#what-you-will-need", 
            "text": "The list of hardware and software for mobile development is longer than your\ntypical development projects.  It is still, thankfully, relatively short and\neasy to acquire.", 
            "title": "What You Will Need"
        }, 
        {
            "location": "/#hardware", 
            "text": "You will want a computer on which to develop code.  If you develop iOS\napplications, then you  MUST  have a Mac running the latest version of Mac\nOSX.  If you develop Universal Windows applications, then you  MUST  have a\nPC running Windows 10.  Android applications can be developed on either platform.  My own experience has taught me that the tooling for developing mobile backends\nin C# and ASP.NET (our primary languages during the course of this book) are\nbetter on a PC running Windows 10.  Thus, my hardware choice is a rather beefy Windows 10 PC  for my main development system.  In addition, I have a Mac Mini  underneath my desk that I use to build the iOS portions of the\napplications.", 
            "title": "Hardware"
        }, 
        {
            "location": "/#software", 
            "text": "All of the following software are freely available.  You should install each\npackage and update it (if appropriate) so that it is fully patched.", 
            "title": "Software"
        }, 
        {
            "location": "/#on-your-mac", 
            "text": "XCode  (available on the Mac App Store)  Visual Studio for Mac  Android Studio and Tools  (if you intend to build Android apps on the Mac)   You must run XCode at least once after installation so that you can accept the\nlicense agreement.", 
            "title": "On your Mac"
        }, 
        {
            "location": "/#on-your-windows-pc", 
            "text": "Android Studio and Tools  Visual Studio 2017 Community   When installing Visual Studio, you will want to install the following workloads:   Universal Windows Platform development.  ASP.NET and web development.  Azure development.  Data storage and processing.  Mobile development with .NET.   If you have already installed Visual Studio and did not install these components, \nrun the installer again to add the components.  You can also use earlier versions of Visual Studio.  If you do use an earlier\nversion, then the screen shots I provide will not match up.  You will also need\nthe latest version of the  Azure SDK  installed.     Tip  Development Tools are big, multi-gigabyte installers.  If you are on a slow or\nrestricted link, you may want to download the installers onto a thumb drive for\nlocal installation.", 
            "title": "On your Windows PC"
        }, 
        {
            "location": "/#cloud-services", 
            "text": "You will need an Azure account to complete most of the tutorials in this book.\nIn fact, you won't be able to get very far without one. If you have an MSDN\naccount, you already have access to free Azure resources.  You just need to log\ninto your  MSDN account  and activate your Azure benefit.  Students may be\nable to get access to  Microsoft Imagine  from school resources, but this \nis not suitable for developing mobile applications.  This is because storage \ncosts money.  If you don't have MSDN, then there is a  free trial  available.\nOnce the trial period ends, you can move to a Pay-As-You-Go account and continue\nto use free services without incurring a charge. I'll point out when you are\ngoing to incur charges on your Azure account, but I will be using free resources\nmost of the time.  Aside from Azure resources, you will want some place to store your code.  This\ndoesn't have to be in the cloud.  If you want to use the cloud, you can use\nGitHub or Visual Studio Team Services.  Both are free to use.  GitHub provides\npublic repositories for free.  Visual Studio Team Services provides private\nrespositories for free.  Visual Studio Team Services also includes other\nservices that I will talk about during the course of the book, some of which may\nincur cost.  I will be publishing all my samples and tutorial code on GitHub so\nthat you can easily download it.  You don't have to use one of these resources,\nbut I won't be covering other service usage.  You will need a  Developer Account  for the appropriate app store if you\nintend to distribute your mobile clients or if you intend to use specific cloud\nservices.  Apple is specific - if you intend to use push notifications or\ndistribute iOS apps, then you need an  Apple Developer Account , Google Developer Account  and/or  Windows Store Developer Account .\nThe terms of the accounts are changed constantly, so review the current terms\nwhen you sign up.  My recommendation is to defer signing up for these programs\nuntil you need something they offer.  Now, let's get developing!  Our next section is dependent on where you are developing:   On a Mac, skip ahead to the  Mac section .  On a PC, the  next section  covers Windows.", 
            "title": "Cloud Services"
        }, 
        {
            "location": "/chapter1/firstapp_pc/", 
            "text": "Your First Mobile App\n\n\nThere is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working?  Microsoft Azure has a great \nfirst-steps tutorial\n that takes you via the quickest possible route from creating a mobile backend to having a functional backend.  I would like to take things a little slower so that we can understand what is going on while we are doing the process.  We will have practically the same application at the end.  The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly.  If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up.  This chapter covers the set up required for a Windows PC.  If you wish to develop your applications on a Mac, then skip to the \nnext section\n.\n\n\nThe application we are going to build together is a simple task list.  The mobile client will have three screens - an entry screen, a task list and a task details page.  I have mocked these pages up using \nMockingBot\n.\n\n\n\n\nTip\n\n\nMocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like \nMockingBot\n.  Doing mockups before you start coding is a good way to prevent wasted time later on.\n\n\n\n\n\n\n\n\nTip\n\n\nIf you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one.  Other platforms will need it though, so it's best to start with the least common denominator.  It's the same reason I add a refresh button even though it's only valid on Windows Phone!\n\n\n\n\nMy ideas for this app include:\n\n\n\n\nTapping on a task title in the task list will bring up the details page.\n\n\nToggling the completed link in the task list will set the completed flag.\n\n\nTapping the spinner will initiate a network refresh.\n\n\n\n\nNow that we have our client screens planned out, we can move onto the thinking about the mobile backend.\n\n\nThe Mobile Backend\n\n\nThe mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python).  Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.\n\n\nCreating a Simple Azure Mobile Apps Backend\n\n\nMicrosoft Azure has included a comprehensive starter kit template in the Azure SDK.  To get started:\n\n\n\n\nFire up Visual Studio.\n\n\nAdd a new project with File -\n New -\n Project...\n\n\n\n\nIn the \nNew Project\n window:\n\n\n\n\nOpen up Templates -\n Visual C# -\n Web and select \nASP.NET Web Application (.NET Framework)\n.\n\n\nEnter \nBackend\n for the Name and \nChapter1\n for the Solution name.\n\n\nSelect \n.NET Framework 4.6\n in the framework dropdown at the top.\n\n\nPick a suitable directory for the Location field.\n\n\nClick OK.\n\n\n\n\n\n\n\n\n\n\nIn the \nNew ASP.NET Web Application\n window:\n\n\n\n\nClick \nAzure Mobile App\n.\n\n\nDo \nNOT\n check \"Host in the cloud\" or any other checkboxes.\n\n\nClick OK.\n\n\n\n\n\n\n\n\nAt this point, Visual Studio will create your backend project.\n\n\nThere are a few files of which you should take note.  The Mobile Apps SDK is initialized within \nApp_Start\\Startup.MobileApp.cs\n (with the call to the configuration routine happening within \nStartup.cs\n).  The default startup routine is reasonable but it hides what it is doing behind extension methods.  This technique is fairly common in ASP.NET programs.  Let's expand the configuration routine to only include what we need:\n\n\npublic static void ConfigureMobileApp(IAppBuilder app)\n{\n    var config = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(config);\n\n    Database.SetInitializer(new MobileServiceInitializer());\n\n    app.UseWebApi(config);\n}\n\n\n\n\nThe minimal version of the mobile backend initialization is actually shorter than the original.  It also only includes a data access layer.  Other services like authentication, storage and push notifications are not configured.\n\n\nThere is another method in the \nApp_Start\\Startup.MobileApp.cs\n file for seeding data into the database for us.  We can leave that alone for now, but remember it is there in case you need to seed data into a new database for your own backend.\n\n\n\n\nInfo\n\n\nWe refer to \"seeding data\" into a database.  This means that we are going to introduce some data into the database so that we aren't operating on an empty database. The data will be there when we query the database later on.\n\n\n\n\nThe next important file is the \nDbContext\n - located in \nModels\\MobileServiceContext.cs\n. Azure Mobile Apps is heavily dependent on \nEntity Framework v6.x\n and the \nDbContext\n is a central part of that library.  Fortunately, we don't need to do anything to this file right now.\n\n\nFinally, we get to the meat of the backend.  The whole point of this demonstration is to project a single database table - the TodoItem table - into the mobile realm with the aid of an opinionated \nOData v3\n feed.  To that end, we need three items:\n\n\n\n\nA \nDbSet\n within the \nDbContext\n\n\nA Data Transfer Object (or DTO)\n\n\nA Table Controller\n\n\n\n\nWhen we create the project, a sample of each one of these for the TodoItem table is added for us.  You can see the \nDbSet\n in the \nModels\\MobileServiceContext.cs\n file, for example.  Let's take a look at the DTO and Table Controller for this example table as well.  The DTO for the TodoItem table is located within the \nDataObjects\n directory:\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nNote that the model uses \nEntityData\n as a base class.  The \nEntityData\n class adds five additional properties to the class - we'll discuss those in more details during the \nData Access and Offline Sync\n chapter.\n\n\nFinally, let's look at the table controller for the example TodoItem table.  This is located in \nControllers\\TodoItemController.cs\n:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems() =\n Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id) =\n Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch) =\n UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =\n DeleteAsync(id);\n    }\n}\n\n\n\n\nThe \nTableController\n is the central processing for the database access layer.  It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI.  This means that the actual code for this controller is tiny - just 12 lines of code.\n\n\n\n\nInfo\n\n\nOData\n is a specification for accessing table data on the Internet.  It provides a mechanism for querying and manipulating data within a table.  Entity Framework is a common data access layer for ASP.NET applications.\n\n\n\n\nWe can build the project at this point.  If Visual Studio hasn't done so already, the missing NuGet packages for Azure Mobile Apps will be downloaded.  There should not be any errors.  If there are, check the typing for any changes you made.\n\n\nBuilding an Azure App Service for Mobile Apps\n\n\nThe next step in the process is to build the resources on Azure that will run your mobile backend.  Start by logging into the \nAzure Portal\n, then follow these instructions:\n\n\n\n\nClick the big \n+ New\n button in the top-left corner.\n\n\nClick \nWeb + Mobile\n, then \nMobile App\n.\n\n\n\n\nEnter a unique name in the \nApp name\n box.\n\n\n\n\nTip\n\n\nSince the name doesn't matter and it has to be unique, you can use \na GUID generator\n to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID  (example: chapter1-GUID) to aid in discovery later on.  Generally, the first four digits of a GUID are enough to identify individual resources.\n\n\n\n\n\n\n\n\nIf you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the \nSubscription\n drop-down.\n\n\n\n\n\n\nSelect \nCreate new\n under resource group and enter a name for this mobile application.\n\n\n\n\nResource Groups\n\n\nResource groups are great for grouping all the resources associated with a mobile application together.  During development, it means you can delete all the resources in one operation.  For production, it means you can see how much the service is costing you and how the resources are being used.\n\n\n\n\n\n\n\n\nFinally, select or create a new \nApp Service Plan\n.\n\n\n\n\nApp Service Plan\n\n\nThe App Service Plan is the thing that actually bills you - not the web or mobile backend.  You can run a number of web or mobile backends on the same App Service Plan.\n\n\n\n\nI tend to create a new App Service Plan for each mobile application.  This is because the App Service Plan lives inside the Resource Group that you create.  The process for creating an App Service Plan is straight forward.  You have two decisions to make.  The first decision is where is the service going to run.  In a production environment, the correct choice is \"near your customers\".  \"Close to the developers\" is a good choice during development.  Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location.  With 16 regions to choose from, you have a lot of choice.\n\n\nThe second decision you have to make is what to run the service on; also known as the Pricing tier.   If you Click \nView all\n, you will see you have lots of choices.  F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota.  That leaves Basic, Standard and Premium.  Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks.  Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example.  Finally, there is a number after the plan.  This tells you how big the virtual machine is that the plan is running on.  The numbers differ by number of cores and memory.\n\n\nFor our purposes, an F1 Free site is enough to run this small demonstration project.  More complex development projects should use something in the Basic range of pricing plans.  Production apps should be set up in Standard or Premium pricing plans.\n\n\n\n\n\n\nOnce you have created your app service plan and saved it, Click \nCreate\n.\n\n\n\n\n\n\nThe creation of the service can take a couple of minutes.  You can monitor the process of deployment by clicking on the Notifications icon.  This is in the top bar on the right-hand side and looks like a Bell.  Clicking on a specific notification will provide more information about the activity.  Once you have created your app service, the App Service blade will open.\n\n\nWe will also want a place to store our data.  This role is taken on by a SQL Azure instance.  We could link an existing database if we had one defined.  However, we can also create a test database.\n\n\n\n\nTip\n\n\nCreating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database.  This option is not normally available through other SQL database creation flows.\n\n\n\n\nBefore we can create a database, we need to create a logical server for the database.  The SQL Server (the logical server) sets the region and the login credentials for all the contained databases:\n\n\n\n\nClick \nResource groups\n in the left hand side menu.\n\n\nClick the resource group you created.\n\n\nClick \nAdd\n at the top of the blade.\n\n\nEnter \nSQL Server\n into the search box, then press Enter.\n\n\nClick \nSQL Server (logical server)\n.\n\n\nClick \nCreate\n.\n\n\nEnter the information required by the form:\n\n\nA server name (which must be unique in the world - this is a great place to use a GUID).\n\n\nA username and password for accessing the databases on the server.\n\n\nSelect the existing resource group.\n\n\nPick the same Location as you did for the App Service Plan.\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nOnce the deployment has completed, you can move on to creating and linking a database.  You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner.\n\n\nTo create and link the database:\n\n\n\n\nClick \nResource groups\n in the left hand side menu.\n\n\nClick the resource group you created.\n\n\n\n\nClick the App Service your created.\n\n\n\n\nTip\n\n\nIf you pinned your App Service to the dashboard, you can Click the pinned App Service instead.  It will bring you to the same place.\n\n\n\n\n\n\n\n\nClick \nData connections\n in the \nMOBILE\n menu.  You can also search for Data connections in the left hand menu.\n\n\n\n\n\n\nClick \nAdd\n.\n\n\n\n\nIn the \nType\n box, select \nSQL Database\n.\n\n\nClick the unconfigured \nSQL Database\n link:\n\n\n\n\n\n\n\n\nIn the \nDatabase\n blade, select \nCreate a new database\n.\n\n\nEnter a name for the database (like \nchapter1-db\n).\n\n\nClick the Target server box and select the logical server you created earlier.\n\n\nSelect a Pricing Tier, then click \nApply\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n to close the SQL Database blade.\n\n\nClick the \nConnection string\n box.\n\n\nEnter the username and password you set up for the SQL logical server.\n\n\nClick \nOK\n.  The username and password will be validated before proceeding.\n\n\nClick \nOK\n to close the Add data connection blade.\n\n\n\n\n\n\n\n\nThis produces another deployment step that creates a SQL database with your settings and binds it to the App Service.  Once complete, the connection \nMS_TableConnectionString\n will be listed in Data Connections blade.\n\n\n\n\nDeploying the Azure Mobile Apps Backend\n\n\nDeploying to Azure as a developer can be accomplished while entirely within Visual Studio:\n\n\n\n\nRight-Click the \nBackend\n project, then select \nPublish...\n.\n\n\n\n\nThe following will be shown:\n\n\n\n\nIf you have an earlier version of Visual Studio, a different screen will be shown.  If Azure App Service is not listed, ensure you have the latest version of Azure SDK installed - at least v2.9.\n\n\n\n\n\n\nClick \nMicrosoft Azure App Service\n.\n\n\n\n\nClick \nSelect Existing\n, then click \nPublish\n.\n\n\nYou may be prompted to enter your Azure credentials here.  Enter the same information that you enter to access the Azure Portal.\n\n\nIn the lower box, expand the resource group that you created and select the app service you created in the portal.\n\n\nClick \nOK\n.\n\n\n\n\nVisual Studio will open a browser pointing to the root of your Azure App Service.  Add \n/tables/todoitem?ZUMO-API-VERSION=2.0.0\n to the end of the URL.  This will show the JSON contents of the table that was defined as a table controller in the backend.\n\n\n\n\nInfo\n\n\nYou will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code.  ZUMO was the original code name within Microsoft for A\nZU\nre \nMO\nbile.\n\n\n\n\nBuilding The Mobile Client\n\n\n\n\nInfo\n\n\nWhen you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows\n\n\n\n\nNow that the mobile backend is created and deployed, we can move onto the client side of things.  Right-Click the solution and select \nAdd\n -\n \nNew Project...\n. This will bring up the familiar New Project dialog.  Select \nVisual C#\n -\n \nCross-Platform\n -\n \nCross Platform App (Xamarin.Forms or Native)\n. Give the project a name, then Click \nOK\n.\n\n\n\n\nIn the \nNew Cross Platform App\n window, select \nBlank App\n, and use \nXamarin.Forms\n as the UI technology, and a \nShared Project\n for the code sharing strategy.\n\n\n\n\nProject creation will take longer than you expect, but there is a lot going on.  If you have never created a mobile or UWP project before, you will be prompted to turn on Windows 10 Developer Mode:\n\n\n\n\nDeveloper mode in Windows 10 allows you to run unsigned binaries for development purposes and to turn on debugging so that you can step through your UWP programs within Visual Studio.  Visual Studio may also just bring up the appropriate Settings page where you can turn on Developer mode.\n\n\nWe will also get asked to choose what version of the Universal Windows platform we want to target:\n\n\n\n\nVersion 10240 was the first version of Windows 10 that was released to the general public, so that's a good minimum version to pick.  In general, the defaults for the Universal Windows Platform choice are good enough.\n\n\nXamarin allows us to build iOS applications directly from Visual Studio. For this to work, we must have access to a Mac. This could be anything from a MacBook Air/Pro, to a Mac Mini in a drawer or closet in the office, or maybe even a \nMac in the cloud\n.  The Xamarin tools use SSH to connect to the Mac, which must be \nconfigured to build iOS apps from Visual Studio\n.\n\n\n\n\nTip\n\n\nIf you don't have a Mac and are not interested in building iOS applications, don't give up now!  You can cancel through the Mac specific project setup and continue with building a great Android and Universal Windows app.  You can also use \nVisual Studio Mobile Center\n to build an iOS project.  You can delete the iOS specific project after it has been created.\n\n\n\n\nWhen prompted about the Xamarin Mac Agent, Click \nOK\n to get the list of local mac agents:\n\n\n\n\nHighlight your mac (in case there are multiples), then Click \nConnect...\n.  If your mac is not listed or you are using a Mac in the cloud, then you can always enter the IP address for your mac.\n\n\n\n\nTip\n\n\nFor more troubleshooting tips, visit \nThe Xamarin Troubleshooting Site\n.\n\n\n\n\nYou will be prompted for your username and password:\n\n\n\n\nJust enter the (real) username and password for your account on your mac and click on \nLogin\n.\n\n\n\n\nTip\n\n\nApple tries very hard to hide the real username of your account from you.  The easiest way to find your mac username is to open up the Finder.  The name next to your home icon is the name of your account.\n\n\n\n\nIf the connection is successful, you will see a green icon in the Xamarin Visual Studio toolbar area. It may take a minute or two to connect and verify that the mac can be used.\n\n\n\n\nOnce the project is created, you will see that four new projects have been created: a common library which you named plus one project for each platform that has been chosen.  Since we chose a project with three platforms, we get four projects:\n\n\n\n\nMost of our work will happen in the common library.  However, we can introduce platform-specific code at any point.  The platform-specific code is stored in the platform-specific project.\n\n\nThere is one final item we must do before we leave the set up of the project.  There are a number of platform upgrades that inevitably have to happen.  The Xamarin Platform is updated much more often than the Visual Studio plugin - the updates are released via NuGet: the standard method of distributing libraries for .NET applications.\n\n\n\n\nWarn\n\n\nAlthough it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services.  There are many differences between the wire protocols of the two products.\n\n\n\n\nYou can install the NuGet packages by right-clicking on the solution and selecting \nManage NuGet Packages for Solution...\n.\n\n\n\n\nYou can generally select all the updates.  However, do \nNOT\n update the Jwt package (System.IdentityModel.Tokens.Jwt) as this will break the server project.  You can update the System.IdentityModel.Tokens.Jwt to the latest v4.x release. Do \nNOT\n install a v5.x release.\n\n\n\n\nInfo\n\n\nAndroid generally has more updates than the other platforms.  Ensure that you update the main Xamarin.Forms package and then refresh the update list.  This will ensure the right list of packages is updated.\n\n\n\n\nYou should also install the \nMicrosoft.Azure.Mobile.Client\n library in all the client projects.\n\n\nBuilding the Common Library\n\n\nThere are two parts that we must concentrate on within the common library.  The first is the connection to Azure Mobile Apps and the second is in the pages that the user interacts with.  In both cases, there are best practices to observe.\n\n\nBuilding an Azure Mobile Apps Connection\n\n\nWe will rely on interfaces for defining the shape for the class for any service that we interact with.  This is really not important in small projects like this one.  This technique allows us to mock the backend service, as we shall see later on.  Mocking the backend service is a great technique to rapidly iterate on the front end mobile client without getting tied into what the backend is doing.\n\n\nLet's start with the cloud service - this is defined in \nAbstractions\\ICloudService.cs\n.  It is used for initializing the connection and getting a table definition:\n\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n    }\n}\n\n\n\n\nThe \nICloudTable\n generic interface represents a CRUD interface into a table and is defined in \nAbstractions\\ICloudTable.cs\n:\n\n\nusing System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable\nT\n where T : TableData\n    {\n        Task\nT\n CreateItemAsync(T item);\n        Task\nT\n ReadItemAsync(string id);\n        Task\nT\n UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n\n        Task\nICollection\nT\n ReadAllItemsAsync();\n    }\n}\n\n\n\n\nThe \nICloudTable\nT\n interface defines the normal CRUD operations: Create, Read, Update and Delete.  However, it does so asynchronously.  We are dealing with network operations in general so it is easy for those operations to tie up the UI thread for an appreciable amount of time.  Making them async provides the ability to respond to other events.  I also provide a \nReadAllItemsAsync()\n method that returns a collection of all the items.\n\n\nThere are some fields that every single record within an Azure Mobile Apps table provides.  These fields are required for offline sync capabilities like incremental sync and conflict resolution.  The fields are provided by an abstract base class on the client called \nTableData\n:\n\n\nusing System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public DateTimeOffset? CreatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}\n\n\n\n\nAs we will learn when we deal with \ntable data\n, these fields need to be defined with the same name and semantics as on the server.  Our model on the server was sub-classed from \nEntityData\n and the \nEntityData\n class on the server defines these fields.\n\n\nIt's tempting to call the client version of the class the same as the server version.  If we did that, the models on both the client and server would look the same.  However, I find that this confuses the issue.  The models on the client and server are not the same.  They are missing the \nDeleted\n flag and they do not contain any relationship information on the client.  I choose to deliberately call the base class something else on the client to avoid this confusion.\n\n\nWe will be adding to these interfaces in future chapters as we add more capabilities to the application.\n\n\nThe concrete implementations of these classes are similarly easily defined.  The Azure Mobile Apps Client SDK does most of the work for us.  Here is the concrete implementation of the \nICloudService\n (in \nServices\\AzureCloudService.cs\n):\n\n\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(\nhttps://my-backend.azurewebsites.net\n);\n        }\n\n        public ICloudTable\nT\n GetTable\nT\n() where T : TableData\n        {\n            return new AzureCloudTable\nT\n(client);\n        }\n    }\n}\n\n\n\n\n\n\nEnsure you use HTTPS\n\n\nIf you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint.  You must provide the https version of the endpoint when using App Service.  The http endpoint redirects to https and the standard HttpClient does not handle redirects.\n\n\n\n\nThe Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published.  Just swap out the name of your mobile backend and the rest is silently dealt with.\n\n\n\n\nWarn\n\n\nThe name \nMicrosoft.WindowsAzure.MobileServices\n is a hold-over from the old Azure Mobile Services code-base.  Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps.\n\n\n\n\nWe also need a concrete implementation of the \nICloudTable\nT\n interface (in \nServices\\AzureCloudTable.cs\n):\n\n\nusing System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable\nT\n : ICloudTable\nT\n where T : TableData\n    {\n        MobileServiceClient client;\n        IMobileServiceTable\nT\n table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.client = client;\n            this.table = client.GetTable\nT\n();\n        }\n\n        #region ICloudTable implementation\n        public async Task\nT\n CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            await table.DeleteAsync(item);\n        }\n\n        public async Task\nICollection\nT\n ReadAllItemsAsync()\n        {\n            return await table.ToListAsync();\n        }\n\n        public async Task\nT\n ReadItemAsync(string id)\n        {\n            return await table.LookupAsync(id);\n        }\n\n        public async Task\nT\n UpdateItemAsync(T item)\n        {\n          await table.UpdateAsync(item);\n          return item;\n        }\n        #endregion\n    }\n}\n\n\n\n\nIt's important to note here that the Azure Mobile Apps Client SDK does a lot of the work for us.  In fact, we are just wrapping the basic interface here.  This won't normally be the case, but you can see that the majority of the code for dealing with the remote server is done for us.\n\n\n\n\nTip\n\n\nYou can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as:\n\n\npublic async Task DeleteItemAsync(T item) =\n await table.DeleteAsync(item);\n\n\nYou may see this sort of short hand in samples.\n\n\n\n\nWe also need to create the model that we will use for the data.  This should look very similar to the model on the server - including having the same name and fields.  In this case, it's \nModels\\TodoItem.cs\n:\n\n\nusing TaskList.Abstractions\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nWe have a final piece of code to write before we move on to the views, but it's an important piece.  The \nICloudService\n must be a singleton in the client.  We will add authentication and offline sync capabilities in future versions of this code.  The singleton becomes critical when using those features.  For right now, it's good practice and saves on memory if you only have one copy of the \nICloudService\n in your mobile client.  Since there is only one copy of the \nApp.cs\n in any given app, I can place it there.  Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this.  Here is the \nApp.cs\n:\n\n\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n\n        // There are lifecycle methods here...\n    }\n}\n\n\n\n\nWe haven't written \nPages.EntryPage\n yet, but that's coming.  The original \nApp.cs\n class file had several methods for handling lifecycle events like starting, suspending or resuming the app.  I did not touch those methods for this example.\n\n\nBuilding the UI for the App\n\n\nEarlier, I showed the mockup for my UI.  It included three pages - an entry page, a list page and a detail page.  These pages have three elements - a XAML definition file, a (simple) code-behind file and a view model.\n\n\n\n\nInfo\n\n\nThis book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML.  If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold: \nCreating Mobile Apps with Xamarin.Forms\n.\n\n\n\n\nI tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications.  It's a nice clean pattern and is well understood and documented.  In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements.  In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed.\n\n\nTo do this, we will use a \nBaseViewModel\n class that implements the base functionality for each view.  Aside from the \nINotifyPropertyChanged\n interface, there are some common properties we need for each page.  Each page needs a title, for example, and each page needs an indicator of network activity.  These can be placed in the \nAbstractions\\BaseViewModel.cs\n class:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.ComponentModel;\n\nnamespace TaskList.Abstractions\n{\n    public class BaseViewModel : INotifyPropertyChanged\n    {\n        public event PropertyChangedEventHandler PropertyChanged;\n        string _propTitle = string.Empty;\n        bool _propIsBusy;\n\n        public string Title\n        {\n            get { return _propTitle; }\n            set { SetProperty(ref _propTitle, value, \nTitle\n); }\n        }\n\n        public bool IsBusy\n        {\n            get { return _propIsBusy; }\n            set { SetProperty(ref _propIsBusy, value, \nIsBusy\n); }\n        }\n\n        protected void SetProperty\nT\n(ref T store, T value, string propName, Action onChanged = null)\n        {\n            if (EqualityComparer\nT\n.Default.Equals(store, value))\n                return;\n            store = value;\n            if (onChanged != null)\n                onChanged();\n            OnPropertyChanged(propName);\n        }\n\n        public void OnPropertyChanged(string propName)\n        {\n            if (PropertyChanged == null)\n                return;\n            PropertyChanged(this, new PropertyChangedEventArgs(propName));\n        }\n    }\n}\n\n\n\n\nThis is a fairly common \nINotifyPropertyChanged\n interface implementation pattern.  Each property that we want to expose is a standard property, but the \nset\n operation is replaced by the \nSetProperty()\n call.  The \nSetProperty()\n call deals with the notification; calling the event emitter if the property has changed value.  We only need two properties on the \nBaseViewModel\n: the title and the network indicator.\n\n\nI tend to write my apps in two stages.  I concentrate on the functionality of the app in the first stage.  There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking.   The page is all about the functionality of the various interactions.  Once I have the functionality working, I work on the styling of the page.  We won't be doing any styling work in the demonstration apps that we write during the course of this book.\n\n\nThe EntryPage has just one thing to do.  It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend.  If you are looking at the perfect app, this is a great place to put the introductory screen.\n\n\nCreating a XAML file is relatively simple.  First, create a \nPages\n directory to hold the pages of our application.  Then right-Click the \nPages\n directory in the solution explorer and choose \nAdd\n -\n \nNew Item...\n.  In the \nAdd New Item\n dialog, pick \nVisual C#\n -\n \nCross-Platform\n -\n \nForms Blank Content Page Xaml\n.  Name the new page \nEntryPage.xaml\n.  This will create two files - \nEntryPage.xaml\n and \nEntryPage.xaml.cs\n.  Let's center a button on the page and wire it up with a command.  Here is the \nPages\\EntryPage.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.EntryPage\n\n             Title=\n{Binding Title}\n\n    \nContentPage.Content\n\n        \nStackLayout HorizontalOptions=\nCenter\n\n                     Orientation=\nVertical\n\n                     VerticalOptions=\nCenter\n\n            \nButton BackgroundColor=\nTeal\n\n                    BorderRadius=\n10\n\n                    Command=\n{Binding LoginCommand}\n\n                    Text=\nLogin\n\n                    TextColor=\nWhite\n /\n\n        \n/StackLayout\n\n    \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThere are a couple of interesting things to note here.  The \nStackLayout\n element is our layout element.  It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are.  The only contents are a button.\n\n\nThere are two bindings.  These are bound from the view-model.  We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command.  When the button is tapped, the login command will be run.  We'll get onto that in the view-model later.\n\n\nThe other part of the XAML is the code-behind file.  Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial:\n\n\nusing TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class EntryPage : ContentPage\n    {\n        public EntryPage ()\n        {\n            InitializeComponent ();\n            BindingContext = new EntryPageViewModel();\n        }\n    }\n}\n\n\n\n\nThis is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM.  We initialize the UI, then bind all the bindings to a new instantiation of the view model.\n\n\nTalking of which, the view-model needs just to handle the login click.  Note that the location or namespace is \nTaskList.ViewModels\n.  I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together.  However, just about all the sample code that I see has the view-models in a separate namespace.  Which one is correct? I'll go with copying the samples for now.  Here is the code for \nViewModels\\EntryPageViewModel.cs\n:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title = \nTask List\n;\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =\n loginCmd ?? (loginCmd = new Command(async () =\n await ExecuteLoginCommand().ConfigureAwait(false)));\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[Login] Error = {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}\n\n\n\n\nThis is a fairly simple view-model but there are some patterns here that are worth explaining.  Firstly, note the way we create the \nLoginCommand\n property. This is the property that is bound to the \nCommand\n parameter in the \nButton\n of our view.  This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity.\n\n\nThe second is the pattern for the \nExecuteLoginCommand\n method.  Firstly, I ensure nothing else is happening by checking the IsBusy flag.   If nothing is happening, I set the IsBusy flag.  Then I do what I need to do in a try/catch block.  If an exception is thrown, I deal with it.  Most of the time this involves displaying an error condition.  There are several cross-platform dialog packages to choose from or you can roll your own.  That is not covered here.  We just write a debug log statement so we can see the result in the debug log.  Once everything is done, we clear the IsBusy flag.\n\n\nThe only thing we are doing now is swapping out our main page for a new main page.  This is where we will attach authentication later on.\n\n\nThe next page is the Task List page, which is in \nPages\\TaskList.xaml\n:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.TaskList\n\n             Title=\n{Binding Title}\n\n  \nContentPage.Content\n\n    \nStackLayout\n\n      \nListView BackgroundColor=\n#7F7F7F\n\n                CachingStrategy=\nRecycleElement\n\n                IsPullToRefreshEnabled=\nTrue\n\n                IsRefreshing=\n{Binding IsBusy, Mode=OneWay}\n\n                ItemsSource=\n{Binding Items}\n\n                RefreshCommand=\n{Binding RefreshCommand}\n\n                RowHeight=\n50\n\n                SelectedItem=\n{Binding SelectedItem, Mode=TwoWay}\n\n        \nListView.ItemTemplate\n\n          \nDataTemplate\n\n            \nViewCell\n\n              \nStackLayout HorizontalOptions=\nFillAndExpand\n\n                           Orientation=\nHorizontal\n\n                           Padding=\n10\n\n                           VerticalOptions=\nCenterAndExpand\n\n                \nLabel HorizontalOptions=\nFillAndExpand\n\n                       Text=\n{Binding Text}\n\n                       TextColor=\n#272832\n /\n\n                \nSwitch IsToggled=\n{Binding Complete, Mode=OneWay}\n /\n\n              \n/StackLayout\n\n            \n/ViewCell\n\n          \n/DataTemplate\n\n        \n/ListView.ItemTemplate\n\n      \n/ListView\n\n      \nStackLayout HorizontalOptions=\nCenter\n\n                   Orientation=\nHorizontal\n\n        \nButton BackgroundColor=\nTeal\n\n                Command=\n{Binding AddNewItemCommand}\n\n                Text=\nAdd New Item\n\n                TextColor=\nWhite\n /\n\n      \n/StackLayout\n\n    \n/StackLayout\n\n  \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nNote that some bindings here are one-way.  This means that the value in the view-model drives the value in the UI.  There is nothing within the UI that you can do to alter the state of the underlying property.  Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property.\n\n\nThis view is a little more complex.  It can be split into two parts - the list at the top of the page and the button area at the bottom of the page.  The list area uses a template to help with the display of each item.\n\n\nNote that the \nListView\n object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand.  It also has an indicator that I have wired up to the IsBusy indicator.  Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does.\n\n\nThe code behind in \nPages\\TaskList.xaml.cs\n:\n\n\nusing TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class TaskList : ContentPage\n    {\n        public TaskList ()\n        {\n            InitializeComponent ();\n            BindingContext = new TaskListViewModel();\n        }\n    }\n}\n\n\n\n\n\nThere is a view-model that goes along with the view (in \nViewModels\\TaskListViewModel.cs\n):\n\n\nusing System;\nusing System.Collections.ObjectModel;\nusing System.Collections.Specialized;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskListViewModel : BaseViewModel\n    {\n        public TaskListViewModel()\n        {\n            Title = \nTask List\n;\n            RefreshList();\n        }\n\n        ObservableCollection\nTodoItem\n items = new ObservableCollection\nTodoItem\n();\n        public ObservableCollection\nTodoItem\n Items\n        {\n            get { return items; }\n            set { SetProperty(ref items, value, \nItems\n); }\n        }\n\n        TodoItem selectedItem;\n        public TodoItem SelectedItem\n        {\n            get { return selectedItem; }\n            set\n            {\n                SetProperty(ref selectedItem, value, \nSelectedItem\n);\n                if (selectedItem != null)\n                {\n                    Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem));\n                    SelectedItem = null;\n                }\n            }\n        }\n\n        Command refreshCmd;\n        public Command RefreshCommand =\n refreshCmd ?? (refreshCmd = new Command(async () =\n await ExecuteRefreshCommand()));\n\n        async Task ExecuteRefreshCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var table = App.CloudService.GetTable\nTodoItem\n();\n                var list = await table.ReadAllItemsAsync();\n                Items.Clear();\n                foreach (var item in list)\n                    Items.Add(item);\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskList] Error loading items: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command addNewCmd;\n        public Command AddNewItemCommand =\n addNewCmd ?? (addNewCmd = new Command(async () =\n await ExecuteAddNewItemCommand()));\n\n        async Task ExecuteAddNewItemCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskList] Error in AddNewItem: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        async Task RefreshList()\n        {\n            await ExecuteRefreshCommand();\n            MessagingCenter.Subscribe\nTaskDetailViewModel\n(this, \nItemsChanged\n, async (sender) =\n\n            {\n                await ExecuteRefreshCommand();\n            });\n        }\n    }\n}\n\n\n\n\nThis is a combination of the patterns we have seen earlier.  The Add New Item and Refresh commands should be fairly normal patterns now.  We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the \nSelectedItem\n property through a two-way binding) and when the user clicks on the Add New Item button.  When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed.  It is fairly common to use an \nObservableCollection\n or another class that uses the \nICollectionChanged\n event handler for the list storage.  Doing so allows the UI to react to changes in the items.\n\n\nNote the use of the \nICloudTable\n interface here.  We are using the \nReadAllItemsAsync()\n method to get a list of items, then we copy the items we received into the  ObservableCollection`.\n\n\nFinally, there is the TaskDetail page.  This is defined in the \nPages\\TaskDetail.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.TaskDetail\n\n             Title=\n{Binding Title}\n\n  \nContentPage.Content\n\n    \nStackLayout Padding=\n10\n Spacing=\n10\n\n      \nLabel Text=\nWhat should I be doing?\n/\n\n      \nEntry Text=\n{Binding Item.Text}\n/\n\n      \nLabel Text=\nCompleted?\n/\n\n      \nSwitch IsToggled=\n{Binding Item.Complete}\n/\n\n      \nStackLayout VerticalOptions=\nCenterAndExpand\n/\n\n      \nStackLayout Orientation=\nVertical\n VerticalOptions=\nEnd\n\n        \nStackLayout HorizontalOptions=\nCenter\n Orientation=\nHorizontal\n\n          \nButton BackgroundColor=\n#A6E55E\n\n                  Command=\n{Binding SaveCommand}\n\n                  Text=\nSave\n TextColor=\nWhite\n/\n\n          \nButton BackgroundColor=\nRed\n\n                  Command=\n{Binding DeleteCommand}\n\n                  Text=\nDelete\n TextColor=\nWhite\n/\n\n        \n/StackLayout\n\n      \n/StackLayout\n\n    \n/StackLayout\n\n  \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThis page is a simple form with just two buttons that need to have commands wired up.  However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture.  As a result of this, we need to handle the passing of the item to be edited.  This is done in the \nPages\\TaskDetail.xaml.cs\n code-behind file:\n\n\nusing TodoList.Models;\nusing TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class TaskDetail : ContentPage\n    {\n        public TaskDetail (TodoItem item = null)\n        {\n            InitializeComponent ();\n            BindingContext = new TaskDetailViewModel(item);\n        }\n    }\n}\n\n\n\n\n\nThe item that is passed in from the \nTaskList\n page is used to create a specific view-model for that item.  The view-model is similarly configured to use that item:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskDetailViewModel : BaseViewModel\n    {\n        ICloudTable\nTodoItem\n table = App.CloudService.GetTable\nTodoItem\n();\n\n        public TaskDetailViewModel(TodoItem item = null)\n        {\n            if (item != null)\n            {\n                Item = item;\n                Title = item.Text;\n            }\n            else\n            {\n                Item = new TodoItem { Text = \nNew Item\n, Complete = false };\n                Title = \nNew Item\n;\n            }\n        }\n\n        public TodoItem Item { get; set; }\n\n        Command cmdSave;\n        public Command SaveCommand =\n cmdSave ?? (cmdSave = new Command(async () =\n await ExecuteSaveCommand()));\n\n        async Task ExecuteSaveCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id == null)\n                {\n                    await table.CreateItemAsync(Item);\n                }\n                else\n                {\n                    await table.UpdateItemAsync(Item);\n                }\n                MessagingCenter.Send\nTaskDetailViewModel\n(this, \nItemsChanged\n);\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskDetail] Save error: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command cmdDelete;\n        public Command DeleteCommand =\n cmdDelete ?? (cmdDelete = new Command(async () =\n await ExecuteDeleteCommand()));\n\n        async Task ExecuteDeleteCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id != null)\n                {\n                    await table.DeleteItemAsync(Item);\n                }\n                MessagingCenter.Send\nTaskDetailViewModel\n(this, \nItemsChanged\n);\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskDetail] Save error: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}\n\n\n\n\nThe save command uses the \nICloudTable\n interface again - this time doing either \nCreateItemAsync()\n or \nUpdateItemAsync()\n to create or update the item.  The delete command, as you would expect, deletes the item with the \nDeleteItemAsync()\n method.\n\n\nThe final thing to note from our views is that I am using the \nMessagingCenter\n to communicate between the TaskDetail and TaskList views.  If I change the item in the \nTaskDetail\n list, then I want to update the list in the \nTaskList\n view.\n\n\nNote that all the code we have added to the solution thus far is in the common \nTaskList\n project.  Nothing is required for this simple example in a platform specific project.  That isn't normal, as we shall see in later chapters.\n\n\nBuilding the Client for Universal Windows\n\n\nI tend to start by building the Universal Windows mobile client.  I'm using Visual Studio, after all, and I don't need to use any emulator.  To build the clients:\n\n\n\n\nRight-Click the \nTaskList.UWP (Universal Windows)\n project, then select \nSet as StartUp Project\n.\n\n\nRight-Click the \nTaskList.UWP (Universal Windows)\n project again, then select \nBuild\n.\n\n\nOnce the build is complete, Right-Click the \nTaskList.UWP (Universal Windows)\n project again, then select \nDeploy\n.\n\n\nClick the \nLocal Machine\n button in your command bar to run the application.\n\n\n\n\nIgnore the warning \nAPPX0108\n produced during the build.  It warns that your certificate has expired (you don't have one yet).  You can still run the application because you have turned Developer mode on in the Windows Settings.\n\n\nHere are the three screen screens we generated on Windows:\n\n\n\n\nThere are some problems with the UWP version.  Most notably, the \"pull-to-refresh\" gesture does not exist, so we will need to set up an alternate gesture.  This could be as easy as adding a refresh button right next to the Add New Item button.  In addition, there is no indication of network activity - this manifests as a significant delay between the TaskList page appearing and the data appearing in the list.\n\n\nAside from this, I did do some styling work to ensure that the final version looked like my mock-ups (with the exception of the UI form of the switch, which is platform dependent).  If you want to see what I did to correct this, check out the final version of \nthe Chapter 1 sample\n on GitHub.\n\n\n\n\nBuilding for On-Premise\n\n\nIf you want to run your backend using on-premise resources (for example, Azure Stack or a local IIS service), your UWP application will need the \nPrivate Networks\n capability.  You can add this in the \nPackage.appxmanifest\n file.  Choose the \nCapabilities\n tab and add the required capability.\n\n\n\n\nIf you need to build the project, ensure you redeploy the project after building.  It's a step that is easy to miss and can cause some consternation as you change the code and it doesn't seem to have an effect on the application.  To aid you in this:\n\n\n\n\nSelect \nBuild\n -\n \nConfiguration Manager...\n\n\nSet the \nActive solution platform\n to \nAny CPU\n.\n\n\nUncheck all the boxes under \nBuild\n and \nDeploy\n.\n\n\nCheck the boxes for \nTodoList.UWP\n under to \nBuild\n and \nDeploy\n.\n\n\nClick \nClose\n.\n\n\n\n\n\n\nWhen you click the \nLocal Machine\n button to start the application, Visual Studio will automatically build and deploy your app.\n\n\nBuilding the Client for Android\n\n\nPrior to building the Android version of the application, we need to make two additional changes.  Go to your Android project and open the \nMainActivity.cs\n file.  In the \nOnCreate\n method we need to add an initalizer for our Mobile Apps SDK:\n\n\nprotected override void OnCreate(Bundle bundle)\n{\n    TabLayoutResource = Resource.Layout.Tabbar;\n    ToolbarResource = Resource.Layout.Toolbar;\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n    LoadApplication(new App());\n}\n\n\n\n\nWe also need to set the minimum API version.  Azure Mobile Apps only works with API level 19 or above and the provided template set the minimum SDK version to API level 15.  Open the \nProperties\\AndroidManifest.xml\n file in the \nTaskList.Android\n project and update the \nuses-sdk\n element:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n?\n\n\nmanifest xmlns:android=\nhttp://schemas.android.com/apk/res/android\n\n    \nuses-sdk android:minSdkVersion=\n19\n /\n\n    \napplication android:label=\nTodoList.Android\n/application\n\n\n/manifest\n\n\n\n\n\nOnce that is done:\n\n\n\n\nRight-Click the \nTaskList.Droid\n project, then select \nSet as StartUp Project\n.\n\n\nRight-Click the \nTaskList.Droid\n project again, then select \nBuild\n.\n\n\n\n\n\n\nUpdating Xamarin.Android Support Packages\n\n\nIf you use NuGet to update the referenced packages, ensure all the Xamarin Android support packages are the same version.  Your build will fail if this is not the case.\n\n\n\n\nYou may see build failures for a variety of reasons.  Generally, a search on the Xamarin Forums will turn up the root cause of the failure.  Xamarin Android builds seem to have more errors than either Windows or iOS, but they are easily correctable issues with the environment.\n\n\nIn Visual Studio 2017, we use the Google Emulator for Android to test our Android application.  Four devices will be defined for you by default:\n\n\n\n\nSelect the \nVisualStudio_android-23_x86_phone\n emulator.\n\n\n\n\nDisable Hyper-V before using the Google Emulator\n\n\nThe Google Android Emulator is incompatible with Hyper-V.  If you see deployment errors, check the output window.  You may see a message asking you to disable Hyper-V.  Open up a PowerShell prompt with \"Run as Administrator\" and type \nbcdedit /set hypervisorlaunchtype off\n, then restart your computer.  When the computer is restarted, Hyper-V will not be running.\n\n\n\n\nIf everything is working, you should see the Google Android Emulator display your mobile client:\n\n\n\n\nNote that the task list view is a \"light\" style and the rest of the app is a \"dark\" style.  This is because the default styling on an Android device is dark.  We are using the default styling on two of the pages and specifying colors on the list page.  Fortunately, Xamarin Forms allows for \nplatform-specific styling\n.  The \nfinal sample\n has platform-specific styling for the list page.\n\n\nBuilding the Client for iOS\n\n\nFinally, we get to the iOS platform.  You will need to ensure your Mac is turned on and accessible via ssh, that it has XCode installed (and you have run XCode once so that you can accept the license agreement), and it has Visual Studio for Mac installed.\n\n\n\n\nUse Visual Studio Mobile Center for Mac Builds\n\n\nIf you don't have a Mac handy, you can use \nVisual Studio Mobile Center\n to build your iOS version.  You will need a signing certificate for this as you will obviously not have access to the iOS Simulator that is available on a Mac.\n\n\n\n\nWhen you created the projects, you were asked to link Visual Studio to your mac.  This linkage is used for building the project.  In essence, the entire project is sent to the Mac and the build tools that are supplied with Visual Studio for Mac are used to build the project.\n\n\n\n\nRight-click the \nTaskList.iOS\n project and select \nSet as StartUp Project\n.\n\n\nRight-click the \nTaskList.iOS\n project and select \nBuild\n.\n\n\n\n\nYou knew it was not going to be that easy, right?  Here are the errors that I received when building for the first time:\n\n\n\n\nThe error about \nNo valid iOS code signing keys found in keychain\n is because we selected (by default) a package to deploy onto an iPhone and have not signed  up for an Apple Developer Account and linked it to our Mac development environment.  To fix this, select the \"iPhoneSimulator\" Configuration:\n\n\n\n\nOnce you have selected the iPhoneSimulator configuration, re-build the mobile application.  It should work this time.  Run the simulator using the normal Visual Studio method.  Visual Studio 2017 will connect to the Mac to run the simulator but display the simulator on your PC.\n\n\nBefore long, you should see the following:\n\n\n\n\nYou need a platform initialization call on most platforms for offline sync, and you always need a platform initializer for iOS. In TaskList.Droid, this should be added in \nMainActivity.cs\n and in TaskList.iOS, it's \nAppDelegate.cs\n.  In each of these files, there is a line that initializes the Xamarin Forms system.\n\n\n// Android Version\nglobal::Xamarin.Forms.Forms.Init(this, bundle);\n// iOS Version\nglobal::Xamarin.Forms.Forms.Init();\n\n\n\n\nImmediately before this line, you should add the  initializer for Azure Mobile Apps. It's important that the Azure Mobile Apps subsystem is initialized prior to Xamarin Forms being initialized and any UI code being called.\n\n\nMicrosoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n\n\n\nThis initializer is not required on the UWP project.  Once this is done and you have re-built the project, you can see the fruits of your labor.  The final product screens look like this:\n\n\n\n\nSome Final Thoughts\n\n\nIf you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations.  There are a lot of places where things can go wrong.  You are really integrating the build systems across Visual Studio, Android, Xamarin, and XCode.\n\n\nFortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work.\n\n\nIf you would rather develop code on your mac, the next sectiom is for you - it gets into the nitty gritty of developing the exact same app, but using Visual Studio for Mac instead.\n\n\nThe following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.", 
            "title": "Your First App - PC Edition"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#your-first-mobile-app", 
            "text": "There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects. First, wouldn't it be nice to write some code and get something working?  Microsoft Azure has a great  first-steps tutorial  that takes you via the quickest possible route from creating a mobile backend to having a functional backend.  I would like to take things a little slower so that we can understand what is going on while we are doing the process.  We will have practically the same application at the end.  The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly.  If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up.  This chapter covers the set up required for a Windows PC.  If you wish to develop your applications on a Mac, then skip to the  next section .  The application we are going to build together is a simple task list.  The mobile client will have three screens - an entry screen, a task list and a task details page.  I have mocked these pages up using  MockingBot .   Tip  Mocking your screens before you start coding is a great habit to get into. There are some great tools available including free tools like  MockingBot .  Doing mockups before you start coding is a good way to prevent wasted time later on.     Tip  If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one.  Other platforms will need it though, so it's best to start with the least common denominator.  It's the same reason I add a refresh button even though it's only valid on Windows Phone!   My ideas for this app include:   Tapping on a task title in the task list will bring up the details page.  Toggling the completed link in the task list will set the completed flag.  Tapping the spinner will initiate a network refresh.   Now that we have our client screens planned out, we can move onto the thinking about the mobile backend.", 
            "title": "Your First Mobile App"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#the-mobile-backend", 
            "text": "The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python).  Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.", 
            "title": "The Mobile Backend"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#creating-a-simple-azure-mobile-apps-backend", 
            "text": "Microsoft Azure has included a comprehensive starter kit template in the Azure SDK.  To get started:   Fire up Visual Studio.  Add a new project with File -  New -  Project...   In the  New Project  window:   Open up Templates -  Visual C# -  Web and select  ASP.NET Web Application (.NET Framework) .  Enter  Backend  for the Name and  Chapter1  for the Solution name.  Select  .NET Framework 4.6  in the framework dropdown at the top.  Pick a suitable directory for the Location field.  Click OK.      In the  New ASP.NET Web Application  window:   Click  Azure Mobile App .  Do  NOT  check \"Host in the cloud\" or any other checkboxes.  Click OK.     At this point, Visual Studio will create your backend project.  There are a few files of which you should take note.  The Mobile Apps SDK is initialized within  App_Start\\Startup.MobileApp.cs  (with the call to the configuration routine happening within  Startup.cs ).  The default startup routine is reasonable but it hides what it is doing behind extension methods.  This technique is fairly common in ASP.NET programs.  Let's expand the configuration routine to only include what we need:  public static void ConfigureMobileApp(IAppBuilder app)\n{\n    var config = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(config);\n\n    Database.SetInitializer(new MobileServiceInitializer());\n\n    app.UseWebApi(config);\n}  The minimal version of the mobile backend initialization is actually shorter than the original.  It also only includes a data access layer.  Other services like authentication, storage and push notifications are not configured.  There is another method in the  App_Start\\Startup.MobileApp.cs  file for seeding data into the database for us.  We can leave that alone for now, but remember it is there in case you need to seed data into a new database for your own backend.   Info  We refer to \"seeding data\" into a database.  This means that we are going to introduce some data into the database so that we aren't operating on an empty database. The data will be there when we query the database later on.   The next important file is the  DbContext  - located in  Models\\MobileServiceContext.cs . Azure Mobile Apps is heavily dependent on  Entity Framework v6.x  and the  DbContext  is a central part of that library.  Fortunately, we don't need to do anything to this file right now.  Finally, we get to the meat of the backend.  The whole point of this demonstration is to project a single database table - the TodoItem table - into the mobile realm with the aid of an opinionated  OData v3  feed.  To that end, we need three items:   A  DbSet  within the  DbContext  A Data Transfer Object (or DTO)  A Table Controller   When we create the project, a sample of each one of these for the TodoItem table is added for us.  You can see the  DbSet  in the  Models\\MobileServiceContext.cs  file, for example.  Let's take a look at the DTO and Table Controller for this example table as well.  The DTO for the TodoItem table is located within the  DataObjects  directory:  using Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}  Note that the model uses  EntityData  as a base class.  The  EntityData  class adds five additional properties to the class - we'll discuss those in more details during the  Data Access and Offline Sync  chapter.  Finally, let's look at the table controller for the example TodoItem table.  This is located in  Controllers\\TodoItemController.cs :  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems() =  Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id) =  Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch) =  UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =  DeleteAsync(id);\n    }\n}  The  TableController  is the central processing for the database access layer.  It handles all the OData capabilities for us and exposes these as REST endpoints within our WebAPI.  This means that the actual code for this controller is tiny - just 12 lines of code.   Info  OData  is a specification for accessing table data on the Internet.  It provides a mechanism for querying and manipulating data within a table.  Entity Framework is a common data access layer for ASP.NET applications.   We can build the project at this point.  If Visual Studio hasn't done so already, the missing NuGet packages for Azure Mobile Apps will be downloaded.  There should not be any errors.  If there are, check the typing for any changes you made.", 
            "title": "Creating a Simple Azure Mobile Apps Backend"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-an-azure-app-service-for-mobile-apps", 
            "text": "The next step in the process is to build the resources on Azure that will run your mobile backend.  Start by logging into the  Azure Portal , then follow these instructions:   Click the big  + New  button in the top-left corner.  Click  Web + Mobile , then  Mobile App .   Enter a unique name in the  App name  box.   Tip  Since the name doesn't matter and it has to be unique, you can use  a GUID generator  to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID  (example: chapter1-GUID) to aid in discovery later on.  Generally, the first four digits of a GUID are enough to identify individual resources.     If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the  Subscription  drop-down.    Select  Create new  under resource group and enter a name for this mobile application.   Resource Groups  Resource groups are great for grouping all the resources associated with a mobile application together.  During development, it means you can delete all the resources in one operation.  For production, it means you can see how much the service is costing you and how the resources are being used.     Finally, select or create a new  App Service Plan .   App Service Plan  The App Service Plan is the thing that actually bills you - not the web or mobile backend.  You can run a number of web or mobile backends on the same App Service Plan.   I tend to create a new App Service Plan for each mobile application.  This is because the App Service Plan lives inside the Resource Group that you create.  The process for creating an App Service Plan is straight forward.  You have two decisions to make.  The first decision is where is the service going to run.  In a production environment, the correct choice is \"near your customers\".  \"Close to the developers\" is a good choice during development.  Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location.  With 16 regions to choose from, you have a lot of choice.  The second decision you have to make is what to run the service on; also known as the Pricing tier.   If you Click  View all , you will see you have lots of choices.  F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota.  That leaves Basic, Standard and Premium.  Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks.  Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example.  Finally, there is a number after the plan.  This tells you how big the virtual machine is that the plan is running on.  The numbers differ by number of cores and memory.  For our purposes, an F1 Free site is enough to run this small demonstration project.  More complex development projects should use something in the Basic range of pricing plans.  Production apps should be set up in Standard or Premium pricing plans.    Once you have created your app service plan and saved it, Click  Create .    The creation of the service can take a couple of minutes.  You can monitor the process of deployment by clicking on the Notifications icon.  This is in the top bar on the right-hand side and looks like a Bell.  Clicking on a specific notification will provide more information about the activity.  Once you have created your app service, the App Service blade will open.  We will also want a place to store our data.  This role is taken on by a SQL Azure instance.  We could link an existing database if we had one defined.  However, we can also create a test database.   Tip  Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database.  This option is not normally available through other SQL database creation flows.   Before we can create a database, we need to create a logical server for the database.  The SQL Server (the logical server) sets the region and the login credentials for all the contained databases:   Click  Resource groups  in the left hand side menu.  Click the resource group you created.  Click  Add  at the top of the blade.  Enter  SQL Server  into the search box, then press Enter.  Click  SQL Server (logical server) .  Click  Create .  Enter the information required by the form:  A server name (which must be unique in the world - this is a great place to use a GUID).  A username and password for accessing the databases on the server.  Select the existing resource group.  Pick the same Location as you did for the App Service Plan.    Click  Create .   Once the deployment has completed, you can move on to creating and linking a database.  You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner.  To create and link the database:   Click  Resource groups  in the left hand side menu.  Click the resource group you created.   Click the App Service your created.   Tip  If you pinned your App Service to the dashboard, you can Click the pinned App Service instead.  It will bring you to the same place.     Click  Data connections  in the  MOBILE  menu.  You can also search for Data connections in the left hand menu.    Click  Add .   In the  Type  box, select  SQL Database .  Click the unconfigured  SQL Database  link:     In the  Database  blade, select  Create a new database .  Enter a name for the database (like  chapter1-db ).  Click the Target server box and select the logical server you created earlier.  Select a Pricing Tier, then click  Apply .     Click  Select  to close the SQL Database blade.  Click the  Connection string  box.  Enter the username and password you set up for the SQL logical server.  Click  OK .  The username and password will be validated before proceeding.  Click  OK  to close the Add data connection blade.     This produces another deployment step that creates a SQL database with your settings and binds it to the App Service.  Once complete, the connection  MS_TableConnectionString  will be listed in Data Connections blade.", 
            "title": "Building an Azure App Service for Mobile Apps"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#deploying-the-azure-mobile-apps-backend", 
            "text": "Deploying to Azure as a developer can be accomplished while entirely within Visual Studio:   Right-Click the  Backend  project, then select  Publish... .   The following will be shown:   If you have an earlier version of Visual Studio, a different screen will be shown.  If Azure App Service is not listed, ensure you have the latest version of Azure SDK installed - at least v2.9.    Click  Microsoft Azure App Service .   Click  Select Existing , then click  Publish .  You may be prompted to enter your Azure credentials here.  Enter the same information that you enter to access the Azure Portal.  In the lower box, expand the resource group that you created and select the app service you created in the portal.  Click  OK .   Visual Studio will open a browser pointing to the root of your Azure App Service.  Add  /tables/todoitem?ZUMO-API-VERSION=2.0.0  to the end of the URL.  This will show the JSON contents of the table that was defined as a table controller in the backend.   Info  You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code.  ZUMO was the original code name within Microsoft for A ZU re  MO bile.", 
            "title": "Deploying the Azure Mobile Apps Backend"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-mobile-client", 
            "text": "Info  When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows   Now that the mobile backend is created and deployed, we can move onto the client side of things.  Right-Click the solution and select  Add  -   New Project... . This will bring up the familiar New Project dialog.  Select  Visual C#  -   Cross-Platform  -   Cross Platform App (Xamarin.Forms or Native) . Give the project a name, then Click  OK .   In the  New Cross Platform App  window, select  Blank App , and use  Xamarin.Forms  as the UI technology, and a  Shared Project  for the code sharing strategy.   Project creation will take longer than you expect, but there is a lot going on.  If you have never created a mobile or UWP project before, you will be prompted to turn on Windows 10 Developer Mode:   Developer mode in Windows 10 allows you to run unsigned binaries for development purposes and to turn on debugging so that you can step through your UWP programs within Visual Studio.  Visual Studio may also just bring up the appropriate Settings page where you can turn on Developer mode.  We will also get asked to choose what version of the Universal Windows platform we want to target:   Version 10240 was the first version of Windows 10 that was released to the general public, so that's a good minimum version to pick.  In general, the defaults for the Universal Windows Platform choice are good enough.  Xamarin allows us to build iOS applications directly from Visual Studio. For this to work, we must have access to a Mac. This could be anything from a MacBook Air/Pro, to a Mac Mini in a drawer or closet in the office, or maybe even a  Mac in the cloud .  The Xamarin tools use SSH to connect to the Mac, which must be  configured to build iOS apps from Visual Studio .   Tip  If you don't have a Mac and are not interested in building iOS applications, don't give up now!  You can cancel through the Mac specific project setup and continue with building a great Android and Universal Windows app.  You can also use  Visual Studio Mobile Center  to build an iOS project.  You can delete the iOS specific project after it has been created.   When prompted about the Xamarin Mac Agent, Click  OK  to get the list of local mac agents:   Highlight your mac (in case there are multiples), then Click  Connect... .  If your mac is not listed or you are using a Mac in the cloud, then you can always enter the IP address for your mac.   Tip  For more troubleshooting tips, visit  The Xamarin Troubleshooting Site .   You will be prompted for your username and password:   Just enter the (real) username and password for your account on your mac and click on  Login .   Tip  Apple tries very hard to hide the real username of your account from you.  The easiest way to find your mac username is to open up the Finder.  The name next to your home icon is the name of your account.   If the connection is successful, you will see a green icon in the Xamarin Visual Studio toolbar area. It may take a minute or two to connect and verify that the mac can be used.   Once the project is created, you will see that four new projects have been created: a common library which you named plus one project for each platform that has been chosen.  Since we chose a project with three platforms, we get four projects:   Most of our work will happen in the common library.  However, we can introduce platform-specific code at any point.  The platform-specific code is stored in the platform-specific project.  There is one final item we must do before we leave the set up of the project.  There are a number of platform upgrades that inevitably have to happen.  The Xamarin Platform is updated much more often than the Visual Studio plugin - the updates are released via NuGet: the standard method of distributing libraries for .NET applications.   Warn  Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services.  There are many differences between the wire protocols of the two products.   You can install the NuGet packages by right-clicking on the solution and selecting  Manage NuGet Packages for Solution... .   You can generally select all the updates.  However, do  NOT  update the Jwt package (System.IdentityModel.Tokens.Jwt) as this will break the server project.  You can update the System.IdentityModel.Tokens.Jwt to the latest v4.x release. Do  NOT  install a v5.x release.   Info  Android generally has more updates than the other platforms.  Ensure that you update the main Xamarin.Forms package and then refresh the update list.  This will ensure the right list of packages is updated.   You should also install the  Microsoft.Azure.Mobile.Client  library in all the client projects.", 
            "title": "Building The Mobile Client"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-common-library", 
            "text": "There are two parts that we must concentrate on within the common library.  The first is the connection to Azure Mobile Apps and the second is in the pages that the user interacts with.  In both cases, there are best practices to observe.", 
            "title": "Building the Common Library"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-an-azure-mobile-apps-connection", 
            "text": "We will rely on interfaces for defining the shape for the class for any service that we interact with.  This is really not important in small projects like this one.  This technique allows us to mock the backend service, as we shall see later on.  Mocking the backend service is a great technique to rapidly iterate on the front end mobile client without getting tied into what the backend is doing.  Let's start with the cloud service - this is defined in  Abstractions\\ICloudService.cs .  It is used for initializing the connection and getting a table definition:  namespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n    }\n}  The  ICloudTable  generic interface represents a CRUD interface into a table and is defined in  Abstractions\\ICloudTable.cs :  using System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable T  where T : TableData\n    {\n        Task T  CreateItemAsync(T item);\n        Task T  ReadItemAsync(string id);\n        Task T  UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n\n        Task ICollection T  ReadAllItemsAsync();\n    }\n}  The  ICloudTable T  interface defines the normal CRUD operations: Create, Read, Update and Delete.  However, it does so asynchronously.  We are dealing with network operations in general so it is easy for those operations to tie up the UI thread for an appreciable amount of time.  Making them async provides the ability to respond to other events.  I also provide a  ReadAllItemsAsync()  method that returns a collection of all the items.  There are some fields that every single record within an Azure Mobile Apps table provides.  These fields are required for offline sync capabilities like incremental sync and conflict resolution.  The fields are provided by an abstract base class on the client called  TableData :  using System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public DateTimeOffset? CreatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}  As we will learn when we deal with  table data , these fields need to be defined with the same name and semantics as on the server.  Our model on the server was sub-classed from  EntityData  and the  EntityData  class on the server defines these fields.  It's tempting to call the client version of the class the same as the server version.  If we did that, the models on both the client and server would look the same.  However, I find that this confuses the issue.  The models on the client and server are not the same.  They are missing the  Deleted  flag and they do not contain any relationship information on the client.  I choose to deliberately call the base class something else on the client to avoid this confusion.  We will be adding to these interfaces in future chapters as we add more capabilities to the application.  The concrete implementations of these classes are similarly easily defined.  The Azure Mobile Apps Client SDK does most of the work for us.  Here is the concrete implementation of the  ICloudService  (in  Services\\AzureCloudService.cs ):  using Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient( https://my-backend.azurewebsites.net );\n        }\n\n        public ICloudTable T  GetTable T () where T : TableData\n        {\n            return new AzureCloudTable T (client);\n        }\n    }\n}   Ensure you use HTTPS  If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint.  You must provide the https version of the endpoint when using App Service.  The http endpoint redirects to https and the standard HttpClient does not handle redirects.   The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published.  Just swap out the name of your mobile backend and the rest is silently dealt with.   Warn  The name  Microsoft.WindowsAzure.MobileServices  is a hold-over from the old Azure Mobile Services code-base.  Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps.   We also need a concrete implementation of the  ICloudTable T  interface (in  Services\\AzureCloudTable.cs ):  using System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable T  : ICloudTable T  where T : TableData\n    {\n        MobileServiceClient client;\n        IMobileServiceTable T  table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.client = client;\n            this.table = client.GetTable T ();\n        }\n\n        #region ICloudTable implementation\n        public async Task T  CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            await table.DeleteAsync(item);\n        }\n\n        public async Task ICollection T  ReadAllItemsAsync()\n        {\n            return await table.ToListAsync();\n        }\n\n        public async Task T  ReadItemAsync(string id)\n        {\n            return await table.LookupAsync(id);\n        }\n\n        public async Task T  UpdateItemAsync(T item)\n        {\n          await table.UpdateAsync(item);\n          return item;\n        }\n        #endregion\n    }\n}  It's important to note here that the Azure Mobile Apps Client SDK does a lot of the work for us.  In fact, we are just wrapping the basic interface here.  This won't normally be the case, but you can see that the majority of the code for dealing with the remote server is done for us.   Tip  You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as:  public async Task DeleteItemAsync(T item) =  await table.DeleteAsync(item);  You may see this sort of short hand in samples.   We also need to create the model that we will use for the data.  This should look very similar to the model on the server - including having the same name and fields.  In this case, it's  Models\\TodoItem.cs :  using TaskList.Abstractions\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}  We have a final piece of code to write before we move on to the views, but it's an important piece.  The  ICloudService  must be a singleton in the client.  We will add authentication and offline sync capabilities in future versions of this code.  The singleton becomes critical when using those features.  For right now, it's good practice and saves on memory if you only have one copy of the  ICloudService  in your mobile client.  Since there is only one copy of the  App.cs  in any given app, I can place it there.  Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this.  Here is the  App.cs :  using TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n\n        // There are lifecycle methods here...\n    }\n}  We haven't written  Pages.EntryPage  yet, but that's coming.  The original  App.cs  class file had several methods for handling lifecycle events like starting, suspending or resuming the app.  I did not touch those methods for this example.", 
            "title": "Building an Azure Mobile Apps Connection"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-ui-for-the-app", 
            "text": "Earlier, I showed the mockup for my UI.  It included three pages - an entry page, a list page and a detail page.  These pages have three elements - a XAML definition file, a (simple) code-behind file and a view model.   Info  This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML.  If you wish to have that sort of introduction, then I recommend reading the excellent book by Charles Petzold:  Creating Mobile Apps with Xamarin.Forms .   I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications.  It's a nice clean pattern and is well understood and documented.  In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements.  In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed.  To do this, we will use a  BaseViewModel  class that implements the base functionality for each view.  Aside from the  INotifyPropertyChanged  interface, there are some common properties we need for each page.  Each page needs a title, for example, and each page needs an indicator of network activity.  These can be placed in the  Abstractions\\BaseViewModel.cs  class:  using System;\nusing System.Collections.Generic;\nusing System.ComponentModel;\n\nnamespace TaskList.Abstractions\n{\n    public class BaseViewModel : INotifyPropertyChanged\n    {\n        public event PropertyChangedEventHandler PropertyChanged;\n        string _propTitle = string.Empty;\n        bool _propIsBusy;\n\n        public string Title\n        {\n            get { return _propTitle; }\n            set { SetProperty(ref _propTitle, value,  Title ); }\n        }\n\n        public bool IsBusy\n        {\n            get { return _propIsBusy; }\n            set { SetProperty(ref _propIsBusy, value,  IsBusy ); }\n        }\n\n        protected void SetProperty T (ref T store, T value, string propName, Action onChanged = null)\n        {\n            if (EqualityComparer T .Default.Equals(store, value))\n                return;\n            store = value;\n            if (onChanged != null)\n                onChanged();\n            OnPropertyChanged(propName);\n        }\n\n        public void OnPropertyChanged(string propName)\n        {\n            if (PropertyChanged == null)\n                return;\n            PropertyChanged(this, new PropertyChangedEventArgs(propName));\n        }\n    }\n}  This is a fairly common  INotifyPropertyChanged  interface implementation pattern.  Each property that we want to expose is a standard property, but the  set  operation is replaced by the  SetProperty()  call.  The  SetProperty()  call deals with the notification; calling the event emitter if the property has changed value.  We only need two properties on the  BaseViewModel : the title and the network indicator.  I tend to write my apps in two stages.  I concentrate on the functionality of the app in the first stage.  There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking.   The page is all about the functionality of the various interactions.  Once I have the functionality working, I work on the styling of the page.  We won't be doing any styling work in the demonstration apps that we write during the course of this book.  The EntryPage has just one thing to do.  It provides a button that enters the app. When we cover authentication later on, we'll use this to log in to the backend.  If you are looking at the perfect app, this is a great place to put the introductory screen.  Creating a XAML file is relatively simple.  First, create a  Pages  directory to hold the pages of our application.  Then right-Click the  Pages  directory in the solution explorer and choose  Add  -   New Item... .  In the  Add New Item  dialog, pick  Visual C#  -   Cross-Platform  -   Forms Blank Content Page Xaml .  Name the new page  EntryPage.xaml .  This will create two files -  EntryPage.xaml  and  EntryPage.xaml.cs .  Let's center a button on the page and wire it up with a command.  Here is the  Pages\\EntryPage.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.EntryPage \n             Title= {Binding Title} \n     ContentPage.Content \n         StackLayout HorizontalOptions= Center \n                     Orientation= Vertical \n                     VerticalOptions= Center \n             Button BackgroundColor= Teal \n                    BorderRadius= 10 \n                    Command= {Binding LoginCommand} \n                    Text= Login \n                    TextColor= White  / \n         /StackLayout \n     /ContentPage.Content  /ContentPage   There are a couple of interesting things to note here.  The  StackLayout  element is our layout element.  It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are.  The only contents are a button.  There are two bindings.  These are bound from the view-model.  We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command.  When the button is tapped, the login command will be run.  We'll get onto that in the view-model later.  The other part of the XAML is the code-behind file.  Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial:  using TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class EntryPage : ContentPage\n    {\n        public EntryPage ()\n        {\n            InitializeComponent ();\n            BindingContext = new EntryPageViewModel();\n        }\n    }\n}  This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM.  We initialize the UI, then bind all the bindings to a new instantiation of the view model.  Talking of which, the view-model needs just to handle the login click.  Note that the location or namespace is  TaskList.ViewModels .  I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together.  However, just about all the sample code that I see has the view-models in a separate namespace.  Which one is correct? I'll go with copying the samples for now.  Here is the code for  ViewModels\\EntryPageViewModel.cs :  using System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title =  Task List ;\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =  loginCmd ?? (loginCmd = new Command(async () =  await ExecuteLoginCommand().ConfigureAwait(false)));\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [Login] Error = {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}  This is a fairly simple view-model but there are some patterns here that are worth explaining.  Firstly, note the way we create the  LoginCommand  property. This is the property that is bound to the  Command  parameter in the  Button  of our view.  This recipe is the method of invoking a UI action asynchronously. It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity.  The second is the pattern for the  ExecuteLoginCommand  method.  Firstly, I ensure nothing else is happening by checking the IsBusy flag.   If nothing is happening, I set the IsBusy flag.  Then I do what I need to do in a try/catch block.  If an exception is thrown, I deal with it.  Most of the time this involves displaying an error condition.  There are several cross-platform dialog packages to choose from or you can roll your own.  That is not covered here.  We just write a debug log statement so we can see the result in the debug log.  Once everything is done, we clear the IsBusy flag.  The only thing we are doing now is swapping out our main page for a new main page.  This is where we will attach authentication later on.  The next page is the Task List page, which is in  Pages\\TaskList.xaml :  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.TaskList \n             Title= {Binding Title} \n   ContentPage.Content \n     StackLayout \n       ListView BackgroundColor= #7F7F7F \n                CachingStrategy= RecycleElement \n                IsPullToRefreshEnabled= True \n                IsRefreshing= {Binding IsBusy, Mode=OneWay} \n                ItemsSource= {Binding Items} \n                RefreshCommand= {Binding RefreshCommand} \n                RowHeight= 50 \n                SelectedItem= {Binding SelectedItem, Mode=TwoWay} \n         ListView.ItemTemplate \n           DataTemplate \n             ViewCell \n               StackLayout HorizontalOptions= FillAndExpand \n                           Orientation= Horizontal \n                           Padding= 10 \n                           VerticalOptions= CenterAndExpand \n                 Label HorizontalOptions= FillAndExpand \n                       Text= {Binding Text} \n                       TextColor= #272832  / \n                 Switch IsToggled= {Binding Complete, Mode=OneWay}  / \n               /StackLayout \n             /ViewCell \n           /DataTemplate \n         /ListView.ItemTemplate \n       /ListView \n       StackLayout HorizontalOptions= Center \n                   Orientation= Horizontal \n         Button BackgroundColor= Teal \n                Command= {Binding AddNewItemCommand} \n                Text= Add New Item \n                TextColor= White  / \n       /StackLayout \n     /StackLayout \n   /ContentPage.Content  /ContentPage   Note that some bindings here are one-way.  This means that the value in the view-model drives the value in the UI.  There is nothing within the UI that you can do to alter the state of the underlying property.  Some bindings are two-way. Doing something in the UI (for example, toggling the switch) alters the underlying property.  This view is a little more complex.  It can be split into two parts - the list at the top of the page and the button area at the bottom of the page.  The list area uses a template to help with the display of each item.  Note that the  ListView  object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand.  It also has an indicator that I have wired up to the IsBusy indicator.  Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does.  The code behind in  Pages\\TaskList.xaml.cs :  using TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class TaskList : ContentPage\n    {\n        public TaskList ()\n        {\n            InitializeComponent ();\n            BindingContext = new TaskListViewModel();\n        }\n    }\n}  There is a view-model that goes along with the view (in  ViewModels\\TaskListViewModel.cs ):  using System;\nusing System.Collections.ObjectModel;\nusing System.Collections.Specialized;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskListViewModel : BaseViewModel\n    {\n        public TaskListViewModel()\n        {\n            Title =  Task List ;\n            RefreshList();\n        }\n\n        ObservableCollection TodoItem  items = new ObservableCollection TodoItem ();\n        public ObservableCollection TodoItem  Items\n        {\n            get { return items; }\n            set { SetProperty(ref items, value,  Items ); }\n        }\n\n        TodoItem selectedItem;\n        public TodoItem SelectedItem\n        {\n            get { return selectedItem; }\n            set\n            {\n                SetProperty(ref selectedItem, value,  SelectedItem );\n                if (selectedItem != null)\n                {\n                    Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem));\n                    SelectedItem = null;\n                }\n            }\n        }\n\n        Command refreshCmd;\n        public Command RefreshCommand =  refreshCmd ?? (refreshCmd = new Command(async () =  await ExecuteRefreshCommand()));\n\n        async Task ExecuteRefreshCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var table = App.CloudService.GetTable TodoItem ();\n                var list = await table.ReadAllItemsAsync();\n                Items.Clear();\n                foreach (var item in list)\n                    Items.Add(item);\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskList] Error loading items: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command addNewCmd;\n        public Command AddNewItemCommand =  addNewCmd ?? (addNewCmd = new Command(async () =  await ExecuteAddNewItemCommand()));\n\n        async Task ExecuteAddNewItemCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskList] Error in AddNewItem: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        async Task RefreshList()\n        {\n            await ExecuteRefreshCommand();\n            MessagingCenter.Subscribe TaskDetailViewModel (this,  ItemsChanged , async (sender) = \n            {\n                await ExecuteRefreshCommand();\n            });\n        }\n    }\n}  This is a combination of the patterns we have seen earlier.  The Add New Item and Refresh commands should be fairly normal patterns now.  We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the  SelectedItem  property through a two-way binding) and when the user clicks on the Add New Item button.  When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed.  It is fairly common to use an  ObservableCollection  or another class that uses the  ICollectionChanged  event handler for the list storage.  Doing so allows the UI to react to changes in the items.  Note the use of the  ICloudTable  interface here.  We are using the  ReadAllItemsAsync()  method to get a list of items, then we copy the items we received into the  ObservableCollection`.  Finally, there is the TaskDetail page.  This is defined in the  Pages\\TaskDetail.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.TaskDetail \n             Title= {Binding Title} \n   ContentPage.Content \n     StackLayout Padding= 10  Spacing= 10 \n       Label Text= What should I be doing? / \n       Entry Text= {Binding Item.Text} / \n       Label Text= Completed? / \n       Switch IsToggled= {Binding Item.Complete} / \n       StackLayout VerticalOptions= CenterAndExpand / \n       StackLayout Orientation= Vertical  VerticalOptions= End \n         StackLayout HorizontalOptions= Center  Orientation= Horizontal \n           Button BackgroundColor= #A6E55E \n                  Command= {Binding SaveCommand} \n                  Text= Save  TextColor= White / \n           Button BackgroundColor= Red \n                  Command= {Binding DeleteCommand} \n                  Text= Delete  TextColor= White / \n         /StackLayout \n       /StackLayout \n     /StackLayout \n   /ContentPage.Content  /ContentPage   This page is a simple form with just two buttons that need to have commands wired up.  However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture.  As a result of this, we need to handle the passing of the item to be edited.  This is done in the  Pages\\TaskDetail.xaml.cs  code-behind file:  using TodoList.Models;\nusing TodoList.ViewModels;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TodoList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class TaskDetail : ContentPage\n    {\n        public TaskDetail (TodoItem item = null)\n        {\n            InitializeComponent ();\n            BindingContext = new TaskDetailViewModel(item);\n        }\n    }\n}  The item that is passed in from the  TaskList  page is used to create a specific view-model for that item.  The view-model is similarly configured to use that item:  using System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskDetailViewModel : BaseViewModel\n    {\n        ICloudTable TodoItem  table = App.CloudService.GetTable TodoItem ();\n\n        public TaskDetailViewModel(TodoItem item = null)\n        {\n            if (item != null)\n            {\n                Item = item;\n                Title = item.Text;\n            }\n            else\n            {\n                Item = new TodoItem { Text =  New Item , Complete = false };\n                Title =  New Item ;\n            }\n        }\n\n        public TodoItem Item { get; set; }\n\n        Command cmdSave;\n        public Command SaveCommand =  cmdSave ?? (cmdSave = new Command(async () =  await ExecuteSaveCommand()));\n\n        async Task ExecuteSaveCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id == null)\n                {\n                    await table.CreateItemAsync(Item);\n                }\n                else\n                {\n                    await table.UpdateItemAsync(Item);\n                }\n                MessagingCenter.Send TaskDetailViewModel (this,  ItemsChanged );\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskDetail] Save error: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command cmdDelete;\n        public Command DeleteCommand =  cmdDelete ?? (cmdDelete = new Command(async () =  await ExecuteDeleteCommand()));\n\n        async Task ExecuteDeleteCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id != null)\n                {\n                    await table.DeleteItemAsync(Item);\n                }\n                MessagingCenter.Send TaskDetailViewModel (this,  ItemsChanged );\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskDetail] Save error: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}  The save command uses the  ICloudTable  interface again - this time doing either  CreateItemAsync()  or  UpdateItemAsync()  to create or update the item.  The delete command, as you would expect, deletes the item with the  DeleteItemAsync()  method.  The final thing to note from our views is that I am using the  MessagingCenter  to communicate between the TaskDetail and TaskList views.  If I change the item in the  TaskDetail  list, then I want to update the list in the  TaskList  view.  Note that all the code we have added to the solution thus far is in the common  TaskList  project.  Nothing is required for this simple example in a platform specific project.  That isn't normal, as we shall see in later chapters.", 
            "title": "Building the UI for the App"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-client-for-universal-windows", 
            "text": "I tend to start by building the Universal Windows mobile client.  I'm using Visual Studio, after all, and I don't need to use any emulator.  To build the clients:   Right-Click the  TaskList.UWP (Universal Windows)  project, then select  Set as StartUp Project .  Right-Click the  TaskList.UWP (Universal Windows)  project again, then select  Build .  Once the build is complete, Right-Click the  TaskList.UWP (Universal Windows)  project again, then select  Deploy .  Click the  Local Machine  button in your command bar to run the application.   Ignore the warning  APPX0108  produced during the build.  It warns that your certificate has expired (you don't have one yet).  You can still run the application because you have turned Developer mode on in the Windows Settings.  Here are the three screen screens we generated on Windows:   There are some problems with the UWP version.  Most notably, the \"pull-to-refresh\" gesture does not exist, so we will need to set up an alternate gesture.  This could be as easy as adding a refresh button right next to the Add New Item button.  In addition, there is no indication of network activity - this manifests as a significant delay between the TaskList page appearing and the data appearing in the list.  Aside from this, I did do some styling work to ensure that the final version looked like my mock-ups (with the exception of the UI form of the switch, which is platform dependent).  If you want to see what I did to correct this, check out the final version of  the Chapter 1 sample  on GitHub.   Building for On-Premise  If you want to run your backend using on-premise resources (for example, Azure Stack or a local IIS service), your UWP application will need the  Private Networks  capability.  You can add this in the  Package.appxmanifest  file.  Choose the  Capabilities  tab and add the required capability.   If you need to build the project, ensure you redeploy the project after building.  It's a step that is easy to miss and can cause some consternation as you change the code and it doesn't seem to have an effect on the application.  To aid you in this:   Select  Build  -   Configuration Manager...  Set the  Active solution platform  to  Any CPU .  Uncheck all the boxes under  Build  and  Deploy .  Check the boxes for  TodoList.UWP  under to  Build  and  Deploy .  Click  Close .    When you click the  Local Machine  button to start the application, Visual Studio will automatically build and deploy your app.", 
            "title": "Building the Client for Universal Windows"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-client-for-android", 
            "text": "Prior to building the Android version of the application, we need to make two additional changes.  Go to your Android project and open the  MainActivity.cs  file.  In the  OnCreate  method we need to add an initalizer for our Mobile Apps SDK:  protected override void OnCreate(Bundle bundle)\n{\n    TabLayoutResource = Resource.Layout.Tabbar;\n    ToolbarResource = Resource.Layout.Toolbar;\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n    LoadApplication(new App());\n}  We also need to set the minimum API version.  Azure Mobile Apps only works with API level 19 or above and the provided template set the minimum SDK version to API level 15.  Open the  Properties\\AndroidManifest.xml  file in the  TaskList.Android  project and update the  uses-sdk  element:  ?xml version= 1.0  encoding= utf-8 ?  manifest xmlns:android= http://schemas.android.com/apk/res/android \n     uses-sdk android:minSdkVersion= 19  / \n     application android:label= TodoList.Android /application  /manifest   Once that is done:   Right-Click the  TaskList.Droid  project, then select  Set as StartUp Project .  Right-Click the  TaskList.Droid  project again, then select  Build .    Updating Xamarin.Android Support Packages  If you use NuGet to update the referenced packages, ensure all the Xamarin Android support packages are the same version.  Your build will fail if this is not the case.   You may see build failures for a variety of reasons.  Generally, a search on the Xamarin Forums will turn up the root cause of the failure.  Xamarin Android builds seem to have more errors than either Windows or iOS, but they are easily correctable issues with the environment.  In Visual Studio 2017, we use the Google Emulator for Android to test our Android application.  Four devices will be defined for you by default:   Select the  VisualStudio_android-23_x86_phone  emulator.   Disable Hyper-V before using the Google Emulator  The Google Android Emulator is incompatible with Hyper-V.  If you see deployment errors, check the output window.  You may see a message asking you to disable Hyper-V.  Open up a PowerShell prompt with \"Run as Administrator\" and type  bcdedit /set hypervisorlaunchtype off , then restart your computer.  When the computer is restarted, Hyper-V will not be running.   If everything is working, you should see the Google Android Emulator display your mobile client:   Note that the task list view is a \"light\" style and the rest of the app is a \"dark\" style.  This is because the default styling on an Android device is dark.  We are using the default styling on two of the pages and specifying colors on the list page.  Fortunately, Xamarin Forms allows for  platform-specific styling .  The  final sample  has platform-specific styling for the list page.", 
            "title": "Building the Client for Android"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#building-the-client-for-ios", 
            "text": "Finally, we get to the iOS platform.  You will need to ensure your Mac is turned on and accessible via ssh, that it has XCode installed (and you have run XCode once so that you can accept the license agreement), and it has Visual Studio for Mac installed.   Use Visual Studio Mobile Center for Mac Builds  If you don't have a Mac handy, you can use  Visual Studio Mobile Center  to build your iOS version.  You will need a signing certificate for this as you will obviously not have access to the iOS Simulator that is available on a Mac.   When you created the projects, you were asked to link Visual Studio to your mac.  This linkage is used for building the project.  In essence, the entire project is sent to the Mac and the build tools that are supplied with Visual Studio for Mac are used to build the project.   Right-click the  TaskList.iOS  project and select  Set as StartUp Project .  Right-click the  TaskList.iOS  project and select  Build .   You knew it was not going to be that easy, right?  Here are the errors that I received when building for the first time:   The error about  No valid iOS code signing keys found in keychain  is because we selected (by default) a package to deploy onto an iPhone and have not signed  up for an Apple Developer Account and linked it to our Mac development environment.  To fix this, select the \"iPhoneSimulator\" Configuration:   Once you have selected the iPhoneSimulator configuration, re-build the mobile application.  It should work this time.  Run the simulator using the normal Visual Studio method.  Visual Studio 2017 will connect to the Mac to run the simulator but display the simulator on your PC.  Before long, you should see the following:   You need a platform initialization call on most platforms for offline sync, and you always need a platform initializer for iOS. In TaskList.Droid, this should be added in  MainActivity.cs  and in TaskList.iOS, it's  AppDelegate.cs .  In each of these files, there is a line that initializes the Xamarin Forms system.  // Android Version\nglobal::Xamarin.Forms.Forms.Init(this, bundle);\n// iOS Version\nglobal::Xamarin.Forms.Forms.Init();  Immediately before this line, you should add the  initializer for Azure Mobile Apps. It's important that the Azure Mobile Apps subsystem is initialized prior to Xamarin Forms being initialized and any UI code being called.  Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();  This initializer is not required on the UWP project.  Once this is done and you have re-built the project, you can see the fruits of your labor.  The final product screens look like this:", 
            "title": "Building the Client for iOS"
        }, 
        {
            "location": "/chapter1/firstapp_pc/#some-final-thoughts", 
            "text": "If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations.  There are a lot of places where things can go wrong.  You are really integrating the build systems across Visual Studio, Android, Xamarin, and XCode.  Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work.  If you would rather develop code on your mac, the next sectiom is for you - it gets into the nitty gritty of developing the exact same app, but using Visual Studio for Mac instead.  The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order. If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.", 
            "title": "Some Final Thoughts"
        }, 
        {
            "location": "/chapter1/firstapp_mac/", 
            "text": "Your First Mobile App on a Mac\n\n\nThere is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects.  First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great \nfirst-steps tutorial\n that takes you via the quickest possible route from creating a mobile backend to having a functional backend.  I would like to take things a little slower so that we can understand what is going on while we are doing the process.  We will have practically the same application at the end.  The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly.  If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up.  This chapter covers the set up required for a MacOS computer.  If you wish to develop your applications on a Windows PC, then skip to the \nprior section\n.\n\n\nThe application we are going to build together is a simple task list.  The mobile client will have three screens - an entry screen, a task list and a task details page.  I have mocked these pages up using \nMockingBot\n.\n\n\n\n\nTip\n\n\nMocking your screens before you start coding is a great habit to get into.  There are some great tools available including free tools like \nMockingBot\n. Doing mockups before you start coding is a good way to prevent wasted time later on.\n\n\n\n\n\n\n\n\nTip\n\n\nIf you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one.  Other platforms will need it though, so it's best to start with the least common denominator.  It's the same reason I add a refresh button even though it's only valid on Windows Phone!\n\n\n\n\nMy ideas for this app include:\n\n\n\n\nTapping on a task title in the task list will bring up the details page.\n\n\nToggling the completed link in the task list will set the completed flag.\n\n\nTapping the spinner will initiate a network refresh.\n\n\n\n\nNow that we have our client screens planned out, we can move onto the thinking about the mobile backend.\n\n\nThe Mobile Backend\n\n\nThe mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python).  Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.\n\n\nCreating a Simple Azure Mobile Apps Backend\n\n\nTo get started:\n\n\n\n\nFire up \nVisual Studio for Mac\n.\n\n\nCreate a new solution with \nFile\n -\n \nNew Solution...\n.\n\n\nIn the \nNew Project\n window, choose \nOther\n -\n \nASP.NET\n, select \nEmpty ASP.NET Project\n, then click \nNext\n.\n\n\nIn the \nConfigure your Web project\n window, check the \nWeb API\n box and uncheck the \nInclude Unit Test Project\n, then click \nNext\n.\n\n\nIn the second \nConfigure your new project\n window, enter \nBackend\n for the for the Project Name and \nChapter1\n for the Solution name. Click \nCreate\n to generate the files.\n\n\n\n\nAt this point, Visual Studio for Mac will lay down the template files on your disk and download the core ASP.NET libraries from NuGet.  You will need to accept the licenses for the downloaded NuGet packages.\n\n\nVisual Studio for Mac does not have the range of templates that Visual Studio for the PC has.  As a result, we will need to do some additional work putting together a base project.  The core of Azure Mobile Apps runs on .NET Framework 4.6.x.  Right-click your \nBackend\n project in the Solution Explorer and choose \nOptions\n.  The target framework setting is located in the \nBuild\n -\n \nGeneral\n section.  You can choose any .NET Framework in the 4.6.x range.\n\n\n\n\nClick \nOK\n to accept the change and close the Project Options.\n\n\nAlthough a core set of NuGet packages are installed during project creation, we need to add the Azure Mobile Apps NuGet packages.  The easiest way to do this is to install the \nAzure Mobile .NET Server Quickstart\n NuGet package, which contains dependencies on all the other Azure Mobile Apps packages.  Expand the \nBackend\n project in the solution explorer, right-click \nPackages\n, then select \nAdd Packages...\n.  Use the search box to find the appropriate NuGet package.  You also need to add the Owin System host \nMicrosoft.Owin.Host.Systemweb\n NuGet package.   You should take the opportunity to update any NuGet packages that were automatically added to the project.  To do so, right-click \nPackages\n, then choose \nUpdate\n.\n\n\nStart by configuring the Azure Mobile Apps SDK.  The Owin process runs the \nStartup.cs\n object to configure itself.  To create the \nStartup.cs\n class:\n\n\n\n\nRight-click the \nBackend\n folder.\n\n\nSelect \nAdd\n -\n \nNew File...\n.\n\n\nSelect \nGeneral\n -\n \nEmpty Class\n, and set the name of the class to \nStartup.cs\n.\n\n\nClick \nNew\n.\n\n\n\n\n\n\nLeave off the .cs on the end\n\n\nIf your filename does not have a dot in it (and some that we use do), you can leave off the \n.cs\n trailing extension.  Visual Studio for Mac will add it for you.\n\n\n\n\nThe \nStartup.cs\n class looks like this:\n\n\nusing Microsoft.Owin;\nusing Owin;\n\n[assembly: OwinStartup(typeof(Backend.Startup))]\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            ConfigureMobileApp(app);\n        }\n    }\n}\n\n\n\n\nThe \nConfigureMobileApp()\n method is located in the \nApp_Start\\Startup.MobileApp.cs\n file that we will create next.  Create the file the same way you did the \nStartup.cs\n file, then enter the following into the new file:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Data.Entity;\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Owin;\n\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            var config = new HttpConfiguration();\n            var mobileConfig = new MobileAppConfiguration();\n\n            mobileConfig\n                .AddTablesWithEntityFramework()\n                .ApplyTo(config);\n\n            Database.SetInitializer(new MobileServiceInitializer());\n\n            app.UseWebApi(config);\n        }\n    }\n\n    public class MobileServiceInitializer : CreateDatabaseIfNotExists\nMobileServiceContext\n\n    {\n        protected override void Seed(MobileServiceContext context)\n        {\n            List\nTodoItem\n todoItems = new List\nTodoItem\n\n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nFirst item\n, Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nSecond item\n, Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set\nTodoItem\n().Add(todoItem);\n            }\n\n            base.Seed(context);\n        }\n    }\n}\n\n\n\n\nLet's break this down a little bit.  The \nConfigureMobileApp()\n method is called to configure Azure Mobile Apps when the service starts.  The code tells the SDK that we want to use tables and that those tables are backed with Entity Framework.  We also want to initialize the database that we are going to use.  That database is going to use a \nDbContext\n called \nMobileServiceContext\n.  The initialization code will create the database and seed it with two new items if it doesn't already exist.  If it exists, then we assume that we don't need to seed the database with data.\n\n\nThe \nMobileServiceContext\n is used to configure the tables within the database and to project those tables into Entity Framework.  It relies on a model for each table.  In Azure Mobile Apps, this is called the \nData Transfer Object\n or DTO.  The server will serialize a DTO to JSON for transmission.  Our DTO is in a new directory called \nDataObjects\n (right-click on \nBackend\n and choose \nAdd\n -\n \nNew Folder...\n to create it) and is called \nTodoItem.cs\n:\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nWe base each DTO we use on the \nEntityData\n class, since we are using Entity Framework.  This sets up some additional columns in the data model so that we can keep track on mobile device changes.  We will be discussing this in more detail in the \nData Access and Offline Sync\n chapter.\n\n\nWe also need the \nModels\\MobileServiceContext.cs\n which sets up the tables for us:\n\n\nusing System.Data.Entity;\nusing System.Data.Entity.ModelConfiguration.Conventions;\nusing System.Linq;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Tables;\nusing Backend.DataObjects;\n\nnamespace Backend.Models\n{\n    public class MobileServiceContext : DbContext\n    {\n        private const string connectionStringName = \nName=MS_TableConnectionString\n;\n\n        public MobileServiceContext() : base(connectionStringName)\n        {\n        }\n\n        public DbSet\nTodoItem\n TodoItems { get; set; }\n\n        protected override void OnModelCreating(DbModelBuilder modelBuilder)\n        {\n            modelBuilder.Conventions.Add(\n                new AttributeToColumnAnnotationConvention\nTableColumnAttribute, string\n(\n                    \nServiceTableColumn\n, (property, attributes) =\n attributes.Single().ColumnType.ToString()));\n        }\n    }\n}\n\n\n\n\nA \nDbSet\n statement is needed for each table we wish to expose to the mobile clients.  There are a couple of important items here.  Firstly, our connection string is called \nMS_TableConnectionString\n.  This is set up in the \nWeb.config\n file and overridden in the Azure Portal so that you can do both local development and run\nagainst a production database in the cloud.  Do not change this name.\n\n\nThe \nOnModelCreating()\n method sets up the tables to handle the service columns that are contained within the \nEntityData\n class used by the DTO.  Certain fields need to be indexed and triggers need to be added to keep the values updated properly.\n\n\nFinally (in terms of code), we need to create a table controller.  This is the endpoint that is exposed on the Internet that our mobile clients will access to send and receive data.  Create a \nControllers\n folder and add the following \nTodoItemController.cs\n class:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems() =\n Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id) =\n Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch) =\n UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =\n DeleteAsync(id);\n    }\n}\n\n\n\n\nThe \nTableController\n is the central processing for the database access layer.  It handles all the \nOData\n capabilities for us and exposes these as REST endpoints within our WebAPI.  This means that the actual code for this controller is tiny - just 12 lines of code - but very powerful.\n\n\n\n\nInfo\n\n\nOData\n is a specification for accessing table data on the Internet.  It provides a mechanism for querying and manipulating data within a table.  Entity Framework is a common data access layer for ASP.NET applications.\n\n\n\n\nOur last step in our backend before publishing it is to edit the \nWeb.config\n file.  The \nWeb.config\n file tells IIS about the run-time settings for this application.  We need to set up the \nMS_TableConnectionString\n and several app settings.  Inevitably, I copy \na created Web.config\n rather than starting from scratch:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n?\n\n\nconfiguration\n\n  \nconfigSections\n\n    \nsection name=\nentityFramework\n type=\nSystem.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\n requirePermission=\nfalse\n /\n\n  \n/configSections\n\n  \nconnectionStrings\n\n    \nadd name=\nMS_TableConnectionString\n connectionString=\nData Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend-20170308080621.mdf;Initial Catalog=aspnet-Backend-20170308080621;Integrated Security=True;MultipleActiveResultSets=True\n providerName=\nSystem.Data.SqlClient\n /\n\n  \n/connectionStrings\n\n  \nappSettings\n\n    \nadd key=\nPreserveLoginUrl\n value=\ntrue\n /\n\n    \nadd key=\nMS_SigningKey\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nEMA_RuntimeUrl\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nMS_NotificationHubName\n value=\nOverridden by portal settings\n /\n\n  \n/appSettings\n\n  \nsystem.web\n\n    \nhttpRuntime targetFramework=\n4.6\n /\n\n    \ncompilation debug=\ntrue\n targetFramework=\n4.6\n /\n\n  \n/system.web\n\n  \nsystem.webServer\n\n    \nvalidation validateIntegratedModeConfiguration=\nfalse\n /\n\n    \nmodules runAllManagedModulesForAllRequests=\ntrue\n /\n\n    \nhandlers\n\n      \nremove name=\nExtensionlessUrlHandler-Integrated-4.0\n /\n\n      \nremove name=\nOPTIONSVerbHandler\n /\n\n      \nremove name=\nTRACEVerbHandler\n /\n\n      \nadd name=\nExtensionlessUrlHandler-Integrated-4.0\n path=\n*.\n verb=\n*\n type=\nSystem.Web.Handlers.TransferRequestHandler\n preCondition=\nintegratedMode,runtimeVersionv4.0\n /\n\n    \n/handlers\n\n  \n/system.webServer\n\n  \nruntime\n\n    \nassemblyBinding xmlns=\nurn:schemas-microsoft-com:asm.v1\n xmlns:bcl=\nurn:schemas-microsoft-com:bcl\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nNewtonsoft.Json\n publicKeyToken=\n30ad4fe6b2a6aeed\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-9.0.0.0\n newVersion=\n9.0.0.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nSystem.Web.Http\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-5.2.3.0\n newVersion=\n5.2.3.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nSystem.Net.Http.Formatting\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-5.2.3.0\n newVersion=\n5.2.3.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nMicrosoft.Data.Edm\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-5.8.1.0\n newVersion=\n5.8.1.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nMicrosoft.Data.OData\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-5.8.1.0\n newVersion=\n5.8.1.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nSystem.Spatial\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-5.8.1.0\n newVersion=\n5.8.1.0\n /\n\n      \n/dependentAssembly\n\n      \ndependentAssembly\n\n        \nassemblyIdentity name=\nMicrosoft.Owin\n publicKeyToken=\n31bf3856ad364e35\n culture=\nneutral\n /\n\n        \nbindingRedirect oldVersion=\n0.0.0.0-3.0.1.0\n newVersion=\n3.0.1.0\n /\n\n      \n/dependentAssembly\n\n    \n/assemblyBinding\n\n  \n/runtime\n\n  \nentityFramework\n\n    \ndefaultConnectionFactory type=\nSystem.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework\n /\n\n    \nproviders\n\n      \nprovider invariantName=\nSystem.Data.SqlClient\n type=\nSystem.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer\n /\n\n    \n/providers\n\n  \n/entityFramework\n\n\n/configuration\n\n\n\n\n\nChoose \nBuild All\n from the \nBuild\n menu and ensure your project compiles without errors.\n\n\nBuilding an Azure App Service for Mobile Apps\n\n\nThe next step in the process is to build the resources on Azure that will run your mobile backend.  Start by logging into the \nAzure portal\n, then follow these instructions:\n\n\n\n\nClick the big \n+ New\n button in the top-left corner.\n\n\nClick \nWeb + Mobile\n, then \nMobile App\n.\n\n\n\n\nEnter a unique name in the \nApp name\n box.\n\n\n\n\nTip\n\n\nSince the name doesn't matter and it has to be unique, you can use \na GUID generator\n to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID  (example: chapter1-GUID) to aid in discovery\nlater on.  Generally, the first four digits of a GUID are enough to identify individual resources.\n\n\n\n\n\n\n\n\nIf you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the \nSubscription\n drop-down.\n\n\n\n\n\n\nSelect \nCreate new\n under resource group and enter a name for this mobile application.\n\n\n\n\nResource Groups\n\n\nResource groups are great for grouping all the resources associated with a mobile application together.  During development, it means you can delete all the resources in one operation.  For production, it means you can see how much the service is costing you and how the resources are being used.\n\n\n\n\n\n\n\n\nFinally, select or create a new \nApp Service Plan\n.\n\n\n\n\nApp Service Plan\n\n\nThe App Service Plan is the thing that actually bills you - not the web or mobile backend.  You can run a number of web or mobile backends on the same App Service Plan.\n\n\n\n\nI tend to create a new App Service Plan for each mobile application.  This is because the App Service Plan lives inside the Resource Group that you create.  The process for creating an App Service Plan is straight forward.  You have two decisions to make.  The first decision is where is the service going to run.  In a production environment, the correct choice is \"near your customers\".  \"Close to the developers\" is a good choice during development.  Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location.  With 16 regions to choose from, you have a lot of choice.\n\n\nThe second decision you have to make is what to run the service on; also known as the Pricing tier.   If you Click \nView all\n, you will see you have lots of choices.  F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota.  That leaves Basic, Standard and Premium.  Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks.  Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example.  Finally, there is a number after the plan.  This tells you how big the virtual machine is that the plan is running on.  The numbers differ by number of cores and memory.\n\n\nFor our purposes, an F1 Free site is enough to run this small demonstration project.  More complex development projects should use something in the Basic range of pricing plans.  Production apps should be set up in Standard or Premium pricing plans.\n\n\n\n\n\n\nOnce you have created your app service plan and saved it, Click \nCreate\n.\n\n\n\n\n\n\nThe creation of the service can take a couple of minutes.  You can monitor the process of deployment by clicking on the Notifications icon.  This is in the top bar on the right-hand side and looks like a Bell.  Clicking on a specific notification will provide more information about the activity.  Once you have created your app service, the App Service blade will open.\n\n\nWe will also want a place to store our data.  This role is taken on by a SQL Azure instance.  We could link an existing database if we had one defined.  However, we can also create a test database.\n\n\n\n\nTip\n\n\nCreating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database.  This option is not normally available through other SQL database creation flows.\n\n\n\n\nBefore we can create a database, we need to create a logical server for the database.  The SQL Server (the logical server) sets the region and the login credentials for all the contained databases:\n\n\n\n\nClick \nResource groups\n in the left hand side menu.\n\n\nClick the resource group you created.\n\n\nClick \nAdd\n at the top of the blade.\n\n\nEnter \nSQL Server\n into the search box, then press Enter.\n\n\nClick \nSQL Server (logical server)\n.\n\n\nClick \nCreate\n.\n\n\nEnter the information required by the form:\n\n\nA server name (which must be unique in the world - this is a great place to use a GUID).\n\n\nA username and password for accessing the databases on the server.\n\n\nSelect the existing resource group.\n\n\nPick the same Location as you did for the App Service Plan.\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nOnce the deployment has completed, you can move on to creating and linking a database.  You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner.\n\n\nTo create and link the database:\n\n\n\n\nClick \nResource groups\n in the left hand side menu.\n\n\nClick the resource group you created.\n\n\n\n\nClick the App Service your created.\n\n\n\n\nTip\n\n\nIf you pinned your App Service to the dashboard, you can Click the pinned App Service instead.  It will bring you to the same place.\n\n\n\n\n\n\n\n\nClick \nData connections\n in the \nMOBILE\n menu.  You can also search for Data connections in the left hand menu.\n\n\n\n\n\n\nClick \nAdd\n.\n\n\n\n\nIn the \nType\n box, select \nSQL Database\n.\n\n\nClick the unconfigured \nSQL Database\n link:\n\n\n\n\n\n\n\n\nIn the \nDatabase\n blade, select \nCreate a new database\n.\n\n\nEnter a name for the database (like \nchapter1-db\n).\n\n\nClick the Target server box and select the logical server you created earlier.\n\n\nSelect a Pricing Tier, then click \nApply\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n to close the SQL Database blade.\n\n\nClick the \nConnection string\n box.\n\n\nEnter the username and password you set up for the SQL logical server.\n\n\nClick \nOK\n.  The username and password will be validated before proceeding.\n\n\nClick \nOK\n to close the Add data connection blade.\n\n\n\n\n\n\n\n\nThis produces another deployment step that creates a SQL database with your settings and binds it to the App Service.  Once complete, the connection \nMS_TableConnectionString\n will be listed in Data Connections blade.\n\n\n\n\nDeploying the Azure Mobile Apps Backend\n\n\nOne of the areas I love Visual Studio for the PC over the Mac edition is in publishing to Azure.  On the PC, that's a right-click action.  There is no publish action in Visual Studio for Mac.  However, Azure App Service has enough other tools available to publish a service.  These include continuous integration technologies that link to Visual Studio Team Services or GitHub, integrations with cloud storage providers like OneDrive and the venerable but trusty ftp mechanisms.  If none of those suit you, you can use drag-and-drop, which is what I am going to do here.\n\n\n\n\nReturn to the browser and log into the \nAzure portal\n.\n\n\nGo to the \nSettings\n blade for your Mobile App.\n\n\nClick \nAdvanced Tools\n in the \nDEVELOPMENT TOOLS\n menu.\n\n\n\n\nClick \nGo\n in the \nAdvanced Tools\n blade.\n\n\n\n\nThe page that loads should match https://{YourMobileApp}.scm.azurewebsites.net/.\n\n\n\n\n\n\nSelect the \nDebug Console\n menu from the top and choose \nCMD\n.\n\n\n\n\nWithin the file structure listing, click \nsite\n, then \nwwwroot\n.\n\n\nRemove the \nhostingstart.html\n file; click the circle with a minus symbol in it to the left of that file and confirm the dialog to delete this file.\n\n\nOn your Mac, use the Finder to navigate to the folder that contains your Mobile App Backend.\n\n\n\n\nSelect the following folder and files:\n\n\n\n\nbin\n\n\npackages.config\n\n\nWeb.config\n\n\n\n\n\n\n\n\nDrag and drop those files into the browser window where the \nhostingstart.html\n file used to be.\n\n\n\n\n\n\nA progress indicator should appear near the top right.  Upon completion you should see the files appear in the file list:\n\n\n\n\n\n\n\n\nYou can test your deployed app by browsing to \nhttps://{yourmobileapp}.azurewebsites.net/tables/todoitem?ZUMO-API-VERSION=2.0.0\n - this is the same URL that your mobile app will connect to later on.  Replace \n{yourmobileapp}\n with the name of the App Service that you created.  If everything is working, you should see some JSON results in your window:\n\n\n\n\nThe first request will take some time as the App Service is waking up your service, which is initializing the database and seeding the data into that database.\n\n\n\n\nInfo\n\n\nYou will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code.  ZUMO was the original code name within Microsoft for A\nZU\nre \nMO\nbile.\n\n\n\n\nThe Mobile Client\n\n\nNow that the mobile backend is created and deployed, we can move onto the mobile application that your users would install on their phones.  We are going to use \nXamarin.Forms\n to produce a cross-platform application for iOS and Android, and the majority of the code will be placed in a shared project that both platforms will use.  You can get as high as 95% code re-use using Xamarin.Forms which is a major productivity boost in complex applications.\n\n\n\n\nInfo\n\n\nWhen you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows.\n\n\n\n\nRight-click the \nChapter1\n solution, then select \nAdd\n -\n \nAdd New Project\n.  This will bring up the familiar New Project dialog.  Select the \nMultiplatform\n -\n \nApp\n -\n \nForms App\n template, then click \nNext\n.   Give the project a name such as \nTaskList\n, ensure Android and iOS are both selected, select \nUse Shared Library\n and click \nNext\n.\n\n\n\n\nClick \nCreate\n on the next screen to create the projects.  You will see that three new projects are created:  a common library that you named, plus a project for each platform that you choise (in this case two platforms - Android and iOS):\n\n\n\n\nMost of our work will happen in the common library in this walkthrough.  However, we can introduce platform-specific code at any point.  The platform-specific code is stored in the platform-specific project.\n\n\nThere is one final item we must do before we leave the set up of the project.  There are a number of platform upgrades that inevitably have to happen.  The Xamarin Platform is updated much more often than the project templates in Visual Studio for Mac. Updates to the Xamarin platform are released via NuGet. Since we are going to be integrating the Azure Mobile Apps client SDK, you should add the \nMicrosoft.Azure.Mobile.Client\n NuGet package.  This will need to be done within each platform-specific project (TaskList.Droid and TaskList.iOS).\n\n\n\n\nWarn\n\n\nAlthough it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services.  There are many differences between the wire protocols of the two products.\n\n\n\n\nYou can start by updating the existing NuGet packages.  Right-click the \nPackages\n folder in each project, then select \nUpdate\n.  Then add the \nMicrosoft.Azure.Mobile.Client\n SDK.  Right-click the \nPackages\n folder again, select \nAdd Packages...\n, then enter the package name in the search box.\n\n\n\n\n\n\nInfo\n\n\nAndroid generally has more updates than the other platforms.  Ensure that you update the main Xamarin.Forms package and then refresh the update list.  This will ensure the right list of support packages is updated.\n\n\n\n\nBuilding the Common Library\n\n\nThere are two parts that we must concentrate on within the common library.  Firstly, we must deal with the connection to our mobile backend using the Azure Mobile Apps client SDK.  Secondly, we need to create the three pages that our application will show.\n\n\nStart by adding the following folders to your Shared Library project:\n\n\n\n\nAbstractions\n\n\nModels\n\n\nPages\n\n\nServices\n\n\nViewModels\n\n\n\n\nOur application will use MVVM (Model-View-ViewModel) for the interaction.  The UI will be written in XAML, and the ViewModel will be used to provide information to that UI.  The Services folder will hold the classes necessary to communicate with the Azure Mobile Apps backend.  Finally, we have a number of interfaces and common classes that we will need to make everything work.\n\n\nRemove the other contents of the shared project.  They will not be required.\n\n\nBuilding an Azure Mobile Apps Connection\n\n\nOne of the things I often stress is to set yourself up assuming you are going to test your product.  Testing is a good thing.  In cloud development, we often set up a \"mock\" service where we swap out the \"real\" service and use something that has the same interface, but deals with local data only.  This application will have two interfaces.  The first represents a cloud service, which will have a collection of tables we want to access.  This will be defined in  \nAbstractions\\ICloudService.cs\n.\n\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n    }\n}\n\n\n\n\nWe haven't defined \nICloudTable\n nor \nTableData\n yet.  The \nICloudTable\nT\n interface defines what the application can do to a table.  It will be defined in \nAbstractions\\ICloudTable.cs\n:\n\n\nusing System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable\nT\n where T : TableData\n    {\n        Task\nT\n CreateItemAsync(T item);\n        Task\nT\n ReadItemAsync(string id);\n        Task\nT\n UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n\n        Task\nICollection\nT\n ReadAllItemsAsync();\n    }\n}\n\n\n\n\nThe \nICloudTable\nT\n interface defines the normal CRUD operations: Create, Read, Update, and Delete.  However, it does so asynchronously.  We are dealing with network operations and it is easy for those operations to tie up the UI thread for an appreciable amount of time.  Making them async provides the ability to respond to other events and ensure your application is responsive to the user.\n\n\nThere are some fields that every single record within an Azure Mobile Apps table provides.  These fields are required for offline sync capabilities like incremental sync and conflict resolution.  On the server, this is represented by the \nEntityData\n class.  We cannot use that class as it contains the Entity Framework additions for indexing and triggers.  The fields are instead provided by a new abstract base class on the client called \nAbstractions\\TableData\n:\n\n\nusing System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public DateTimeOffset? CreatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}\n\n\n\n\nAs we will learn when we deal with \ntable data\n, these fields need to be defined with the same name and semantics as on the server.  Our model on the server was sub-classed from \nEntityData\n and the \nEntityData\n class on the server defines these fields.  It's tempting to call the client version of the class the same as the server version.  If we did that, the models on both the client and server would look the same.  However, I find that this confuses the issue.  The models on the client and server are not the same.  They are missing the \nDeleted\n flag and they do not contain any relationship information on the client.  I choose to deliberately call the base class something else on the client to avoid this confusion.\n\n\nWe will be adding to these interfaces in future chapters as we add more capabilities to the application.\n\n\nThe concrete implementations of these classes are similarly easily defined.  The Azure Mobile Apps Client SDK does most of the work for us.  Here is the concrete implementation of the \nICloudService\n (in \nServices\\AzureCloudService.cs\n):\n\n\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(\nhttps://my-backend.azurewebsites.net\n);\n        }\n\n        public ICloudTable\nT\n GetTable\nT\n() where T : TableData\n        {\n            return new AzureCloudTable\nT\n(client);\n        }\n    }\n}\n\n\n\n\n\n\nEnsure you use HTTPS\n\n\nIf you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint.  You must provide the https version of the endpoint when using App Service.  The http endpoint redirects to https and the standard HttpClient does not handle redirects.\n\n\n\n\nThe Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published.  Just swap \nmy-backend\n out for the name of your mobile backend and the rest is silently dealt with.\n\n\n\n\nWarn\n\n\nThe name \nMicrosoft.WindowsAzure.MobileServices\n is a hold-over from the old Azure Mobile Services code-base.  Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps.\n\n\n\n\nWe also need a concrete implementation of the \nICloudTable\nT\n interface (in \nServices\\AzureCloudTable.cs\n):\n\n\nusing System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable\nT\n : ICloudTable\nT\n where T : TableData\n    {\n        MobileServiceClient client;\n        IMobileServiceTable\nT\n table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.client = client;\n            this.table = client.GetTable\nT\n();\n        }\n\n        #region ICloudTable implementation\n        public async Task\nT\n CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            await table.DeleteAsync(item);\n        }\n\n        public async Task\nICollection\nT\n ReadAllItemsAsync()\n        {\n            return await table.ToListAsync();\n        }\n\n        public async Task\nT\n ReadItemAsync(string id)\n        {\n            return await table.LookupAsync(id);\n        }\n\n        public async Task\nT\n UpdateItemAsync(T item)\n        {\n          await table.UpdateAsync(item);\n          return item;\n        }\n        #endregion\n    }\n}\n\n\n\n\nThe Azure Mobile Apps Client SDK does a lot of the work for us.  In fact, we are just wrapping the basic interface here.  The majority of the code for dealing with the remote server is done for us.\n\n\n\n\nTip\n\n\nYou can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as:\n\n\npublic async Task DeleteItemAsync(T item) =\n await table.DeleteAsync(item);\n\n\nYou may see this sort of short hand in samples.\n\n\n\n\nWe also need to create the model that we will use for the data.  This should look very similar to the model on the server - including having the same name and fields.  In this case, it's \nModels\\TodoItem.cs\n:\n\n\nusing TaskList.Abstractions;\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nWe have a final piece of code to write before we move on to the views, but it's an important piece.  The \nICloudService\n must be a singleton in the client.  We will add authentication and offline sync capabilities in future versions of this code.  The singleton becomes critical when using those features.  For right now, it's good practice and saves on memory if you only have one copy of the \nICloudService\n in your mobile client.  Since there is only one copy of the \nApp.cs\n in any given app, I can place it there.  Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this.  Here is the \nApp.cs\n:\n\n\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n\n        // There are life cycle methods here...\n    }\n}\n\n\n\n\nWe haven't written \nPages.EntryPage\n yet, but that's coming.  This file replaces the \nApp.xaml\n and \nApp.xaml.cs\n files from the original shared project.  If you have not done so already, ensure you remove those files now.\n\n\nBuilding the UI for the App\n\n\nEarlier, I showed the mockup for my UI.  It included three pages - an entry page, a list page, and a detail page.  These pages have three elements - a XAML definition file, a (simple) code-behind file, and a view model.\n\n\n\n\nInfo\n\n\nThis book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML.  If you wish to have that sort of introduction,\nthen I recommend reading the excellent book by Charles Petzold: \nCreating Mobile Apps with Xamarin.Forms\n.\n\n\n\n\nI tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications.  It's a nice clean pattern and is well understood and documented.  In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements.  In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed.\n\n\nTo do this, we will use a \nBaseViewModel\n class that implements the base functionality for each view.  Aside from the \nINotifyPropertyChanged\n interface, there are some common properties we need for each page.  Each page needs a title, for example, and an indicator of network activity.  These can be placed in the \nAbstractions\\BaseViewModel.cs\n class:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.ComponentModel;\n\nnamespace TaskList.Abstractions\n{\n    public class BaseViewModel : INotifyPropertyChanged\n    {\n        public event PropertyChangedEventHandler PropertyChanged;\n        string _propTitle = string.Empty;\n        bool _propIsBusy;\n\n        public string Title\n        {\n            get { return _propTitle; }\n            set { SetProperty(ref _propTitle, value, \nTitle\n); }\n        }\n\n        public bool IsBusy\n        {\n            get { return _propIsBusy; }\n            set { SetProperty(ref _propIsBusy, value, \nIsBusy\n); }\n        }\n\n        protected void SetProperty\nT\n(ref T store, T value, string propName, Action onChanged = null)\n        {\n            if (EqualityComparer\nT\n.Default.Equals(store, value))\n                return;\n            store = value;\n            if (onChanged != null)\n                onChanged();\n            OnPropertyChanged(propName);\n        }\n\n        public void OnPropertyChanged(string propName)\n        {\n            if (PropertyChanged == null)\n                return;\n            PropertyChanged(this, new PropertyChangedEventArgs(propName));\n        }\n    }\n}\n\n\n\n\nThis is a fairly common \nINotifyPropertyChanged\n interface implementation pattern. Each property that we want to expose is a standard property, but the \nset\n operation is replaced by the \nSetProperty()\n call.  The \nSetProperty()\n call deals with the notification; calling the event emitter if the property has changed value.  We only need two properties on the \nBaseViewModel\n: the title and the network indicator.\n\n\nI tend to write my apps in two stages.  I concentrate on the functionality of the app in the first stage.  There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking.   The page is all about the functionality of the various interactions.  Once I have the functionality working, I work on the styling of the page.  We won't be doing any styling work in the demonstration apps that we write during the course of this book.\n\n\nThe EntryPage has just one thing to do.  It provides a button that enters the app.  When we cover authentication later on, we'll use this to log in to the backend.  If you are looking at the perfect app, this is a great place to put the introductory screen.\n\n\nCreating a XAML file is relatively simple.  We already created a \nPages\n directory to hold the pages of our application.  Right-click the \nPages\n directory in the solution explorer and choose \nAdd\n -\n \nNew File...\n.  In the \nAdd New File\n dialog, pick \nForms\n -\n \nForms ContentPage Xaml\n.  Name the new page \nEntryPage\n.  This will create two files - \nEntryPage.xaml\n and \nEntryPage.xaml.cs\n.  Let's center a button on the page and wire it up with a command.  Here is the \nPages\\EntryPage.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.EntryPage\n\n             Title=\n{Binding Title}\n\n    \nContentPage.Content\n\n        \nStackLayout HorizontalOptions=\nCenter\n\n                     Orientation=\nVertical\n\n                     VerticalOptions=\nCenter\n\n            \nButton BackgroundColor=\nTeal\n\n                    BorderRadius=\n10\n\n                    Command=\n{Binding LoginCommand}\n\n                    Text=\nLogin\n\n                    TextColor=\nWhite\n /\n\n        \n/StackLayout\n\n    \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThe \nStackLayout\n element is our layout element.  It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are.  The only contents are a button.\n\n\nThere are two bindings.  These are bound to properties in the view-model.  We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command.  When the button is tapped, the login command will be run.  We'll get onto that in the view-model later.\n\n\nThe other file created is the code-behind file.  Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial:\n\n\nusing TaskList.ViewModels;\nusing Xamarin.Forms;\n\nnamespace TaskList.Pages\n{\n    public partial class EntryPage : ContentPage\n    {\n        public EntryPage()\n        {\n            InitializeComponent();\n            BindingContext = new EntryPageViewModel();\n        }\n    }\n}\n\n\n\n\nThis is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM.  We initialize the UI, then bind all the bindings to a new instance of the view model.\n\n\nSpeaking of which, the view-model just needs to handle the login click.  Note that the location or namespace is \nTaskList.ViewModels\n.  I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together.  However, just about all the sample code that I see has the view-models in a separate namespace.  Which one is correct?  I'll copy the samples for now.  Here is the code for \nViewModels\\EntryPageViewModel.cs\n:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title = \nTask List\n;\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =\n loginCmd ?? (loginCmd = new Command(async () =\n await ExecuteLoginCommand()));\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[Login] Error = {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}\n\n\n\n\nThis is a fairly simple view-model but there are some patterns here that are worth explaining.  Firstly, note the way we create the \nLoginCommand\n property. This is the property that is bound to the \nCommand\n parameter in the \nButton\n of our view.  This recipe is the method of invoking a UI action asynchronously.  It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity.\n\n\nThe second is the pattern for the \nExecuteLoginCommand\n method.  Firstly, I ensure nothing else is happening by checking the IsBusy flag.   If nothing is happening, I set the IsBusy flag.  Then I do what I need to do in a try/catch block.  If an exception is thrown, I deal with it.  Most of the time this involves displaying an error condition.  There are several cross-platform dialog packages to choose from or you can roll your own.  That is not covered here.  We just write a debug log statement so we can see the result in the debug log.  Once everything is done, we clear the IsBusy flag.\n\n\nThe only thing we are doing now is swapping out our main page for a new main page.  This is where we will attach authentication later on.\n\n\nThe next page is the Task List page, which is in \nPages\\TaskList.xaml\n:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.TaskList\n\n             Title=\n{Binding Title}\n\n  \nContentPage.Content\n\n    \nStackLayout\n\n      \nListView BackgroundColor=\n#7F7F7F\n\n                CachingStrategy=\nRecycleElement\n\n                IsPullToRefreshEnabled=\nTrue\n\n                IsRefreshing=\n{Binding IsBusy, Mode=OneWay}\n\n                ItemsSource=\n{Binding Items}\n\n                RefreshCommand=\n{Binding RefreshCommand}\n\n                RowHeight=\n50\n\n                SelectedItem=\n{Binding SelectedItem, Mode=TwoWay}\n\n        \nListView.ItemTemplate\n\n          \nDataTemplate\n\n            \nViewCell\n\n              \nStackLayout HorizontalOptions=\nFillAndExpand\n\n                           Orientation=\nHorizontal\n\n                           Padding=\n10\n\n                           VerticalOptions=\nCenterAndExpand\n\n                \nLabel HorizontalOptions=\nFillAndExpand\n\n                       Text=\n{Binding Text}\n\n                       TextColor=\n#272832\n /\n\n                \nSwitch IsToggled=\n{Binding Complete, Mode=OneWay}\n /\n\n              \n/StackLayout\n\n            \n/ViewCell\n\n          \n/DataTemplate\n\n        \n/ListView.ItemTemplate\n\n      \n/ListView\n\n      \nStackLayout HorizontalOptions=\nCenter\n\n                   Orientation=\nHorizontal\n\n        \nButton BackgroundColor=\nTeal\n\n                Command=\n{Binding AddNewItemCommand}\n\n                Text=\nAdd New Item\n\n                TextColor=\nWhite\n /\n\n      \n/StackLayout\n\n    \n/StackLayout\n\n  \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nNote that some bindings here are one-way.  This means that the value in the view-model drives the value in the UI.  There is nothing within the UI that you can do to alter the state of the underlying property.  Some bindings are two-way.  Doing something in the UI (for example, toggling the switch) alters the underlying property.\n\n\nThis view is a little more complex.  It can be split into two parts - the list at the top of the page and the button area at the bottom of the page.  The list area uses a template to help with the display of each item.\n\n\nNote that the \nListView\n object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand.  It also has an indicator that I have wired up to the IsBusy indicator.  Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does.\n\n\nThe code behind for the TaskList can be found in \nPages\\TaskList.xaml.cs\n:\n\n\nusing TaskList.ViewModels;using Xamarin.Forms;\nnamespace TaskList.Pages\n{\n    public partial class TaskList : ContentPage\n    {\n        public TaskList()\n        {\n            InitializeComponent();\n            BindingContext = new TaskListViewModel();\n        }\n    }\n}\n\n\n\n\nThere is a view-model that goes along with the view (in \nViewModels\\TaskListViewModel.cs\n):\n\n\nusing System;\nusing System.Collections.ObjectModel;\nusing System.Collections.Specialized;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskListViewModel : BaseViewModel\n    {\n        public TaskListViewModel()\n        {\n            Title = \nTask List\n;\n            RefreshList();\n        }\n\n        ObservableCollection\nTodoItem\n items = new ObservableCollection\nTodoItem\n();\n        public ObservableCollection\nTodoItem\n Items\n        {\n            get { return items; }\n            set { SetProperty(ref items, value, \nItems\n); }\n        }\n\n        TodoItem selectedItem;\n        public TodoItem SelectedItem\n        {\n            get { return selectedItem; }\n            set\n            {\n                SetProperty(ref selectedItem, value, \nSelectedItem\n);\n                if (selectedItem != null)\n                {\n                    Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem));\n                    SelectedItem = null;\n                }\n            }\n        }\n\n        Command refreshCmd;\n        public Command RefreshCommand =\n refreshCmd ?? (refreshCmd = new Command(async () =\n await ExecuteRefreshCommand()));\n\n        async Task ExecuteRefreshCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var table = App.CloudService.GetTable\nTodoItem\n();\n                var list = await table.ReadAllItemsAsync();\n                Items.Clear();\n                foreach (var item in list)\n                    Items.Add(item);\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskList] Error loading items: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command addNewCmd;\n        public Command AddNewItemCommand =\n addNewCmd ?? (addNewCmd = new Command(async () =\n await ExecuteAddNewItemCommand()));\n\n        async Task ExecuteAddNewItemCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskList] Error in AddNewItem: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        async Task RefreshList()\n        {\n            await ExecuteRefreshCommand();\n            MessagingCenter.Subscribe\nTaskDetailViewModel\n(this, \nItemsChanged\n, async (sender) =\n\n            {\n                await ExecuteRefreshCommand();\n            });\n        }\n    }\n}\n\n\n\n\nThis is a combination of the patterns we have seen earlier.  The Add New Item and Refresh commands should be fairly normal patterns now.  We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the \nSelectedItem\n property through a two-way binding) and when the user clicks on the Add New Item button.  When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed.  It is fairly common to use an \nObservableCollection\n or another class that uses the \nICollectionChanged\n event handler for the list storage.  Doing so allows the UI to react to changes in the items.\n\n\nNote the use of the \nICloudTable\n interface here.  We are using the \nReadAllItemsAsync()\n method to get a list of items, then we copy the items we received into the \nObservableCollection\n.\n\n\nFinally, there is the TaskDetail page.  This is defined in the \nPages\\TaskDetail.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             x:Class=\nTaskList.Pages.TaskDetail\n\n             Title=\n{Binding Title}\n\n  \nContentPage.Content\n\n    \nStackLayout Padding=\n10\n Spacing=\n10\n\n      \nLabel Text=\nWhat should I be doing?\n/\n\n      \nEntry Text=\n{Binding Item.Text}\n/\n\n      \nLabel Text=\nCompleted?\n/\n\n      \nSwitch IsToggled=\n{Binding Item.Complete}\n/\n\n      \nStackLayout VerticalOptions=\nCenterAndExpand\n/\n\n      \nStackLayout Orientation=\nVertical\n VerticalOptions=\nEnd\n\n        \nStackLayout HorizontalOptions=\nCenter\n Orientation=\nHorizontal\n\n          \nButton BackgroundColor=\n#A6E55E\n\n                  Command=\n{Binding SaveCommand}\n\n                  Text=\nSave\n TextColor=\nWhite\n/\n\n          \nButton BackgroundColor=\nRed\n\n                  Command=\n{Binding DeleteCommand}\n\n                  Text=\nDelete\n TextColor=\nWhite\n/\n\n        \n/StackLayout\n\n      \n/StackLayout\n\n    \n/StackLayout\n\n  \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThis page is a simple form with just two buttons that need to have commands wired up.  However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture.  As a result of this, we need to handle the passing of the item to be edited.  This is done in the \nPages\\TaskDetail.xaml.cs\n code-behind file:\n\n\nusing TaskList.Models;\nusing TaskList.ViewModels;\nusing Xamarin.Forms;\n\nnamespace TaskList.Pages\n{\n    public partial class TaskDetail : ContentPage\n    {\n        public TaskDetail(TodoItem item = null)\n        {\n            InitializeComponent();\n            BindingContext = new TaskDetailViewModel(item);\n        }\n    }\n}\n\n\n\n\nThe item that is passed in from the \nTaskList\n page is used to create a specific view-model for that item.  The view-model is similarly configured to use that item:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskDetailViewModel : BaseViewModel\n    {\n        ICloudTable\nTodoItem\n table = App.CloudService.GetTable\nTodoItem\n();\n\n        public TaskDetailViewModel(TodoItem item = null)\n        {\n            if (item != null)\n            {\n                Item = item;\n                Title = item.Text;\n            }\n            else\n            {\n                Item = new TodoItem { Text = \nNew Item\n, Complete = false };\n                Title = \nNew Item\n;\n            }\n        }\n\n        public TodoItem Item { get; set; }\n\n        Command cmdSave;\n        public Command SaveCommand =\n cmdSave ?? (cmdSave = new Command(async () =\n await ExecuteSaveCommand()));\n\n        async Task ExecuteSaveCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id == null)\n                {\n                    await table.CreateItemAsync(Item);\n                }\n                else\n                {\n                    await table.UpdateItemAsync(Item);\n                }\n                MessagingCenter.Send\nTaskDetailViewModel\n(this, \nItemsChanged\n);\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskDetail] Save error: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command cmdDelete;\n        public Command DeleteCommand =\n cmdDelete ?? (cmdDelete = new Command(async () =\n await ExecuteDeleteCommand()));\n\n        async Task ExecuteDeleteCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id != null)\n                {\n                    await table.DeleteItemAsync(Item);\n                }\n                MessagingCenter.Send\nTaskDetailViewModel\n(this, \nItemsChanged\n);\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[TaskDetail] Save error: {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}\n\n\n\n\nThe save command uses the \nICloudTable\n interface again - this time doing either \nCreateItemAsync()\n or \nUpdateItemAsync()\n to create or update the item.  The delete command, as you would expect, deletes the item with the \nDeleteItemAsync()\n method.\n\n\nThe final thing to note from our views is that I am using the \nMessagingCenter\n to communicate between the TaskDetail and TaskList views.  If I change the item in the \nTaskDetail\n list, then I want to update the list in the \nTaskList\n view.\n\n\nNote that all the code we have added to the solution thus far is in the common \nTaskList\n project.  Nothing is required for this simple example in a platform specific project.  That isn't normal, as we shall see.\n\n\nBuilding the Client for Android\n\n\nNow we're ready to build our client applications.  We'll start with the Android version.  Prior to running the application, we need to make two additional changes.  Go to your Android project and open the \nMainActivity.cs\n file.  In the \nOnCreate\n method we need to add an initalizer for our Mobile Apps SDK:\n\n\nprotected override void OnCreate(Bundle bundle)\n{\n    TabLayoutResource = Resource.Layout.Tabbar;\n    ToolbarResource = Resource.Layout.Toolbar;\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n    LoadApplication(new App());\n}\n\n\n\n\nFinally, as Android has an explicit permission model (this has somewhat changed in the latest version of Android), we need to say the application requires the Internet permission.  Right-click on the Android project and go to \nOptions\n.  Select \nAndroid Application\n from under the \nBuild\n section.  At the bottom of the options panel, you'll see a list of permissions.  Find Internet and check it and then click the OK button.\n\n\n\n\nNext we need to configure the solution to run the Android project.\n\n\n\n\nRight-click the \nTaskList.Droid\n project, then select \nSet as StartUp Project\n.\n\n\nRight-click the \nTaskList.Droid\n project again, then select \nBuild TaskList.Droid\n.\n\n\n\n\nThe drop-down between the run button in the top left of Visual Studio for Mac and the Build status at the top of Visual Studio for Mac, now allows you to choose an emulator or device to run your app against.  By default, Visual Studio for Mac will create several emulators for you.  You can also use the \nManage Google Emulators...\n option to create additional Android Virtual Devices (AVDs) and download other images online.\n\n\n\n\nTip\n\n\nWhen testing the mobile client manually through the Android Emulator, you are likely to need to rebuild the application.  You do not have to shut down the emulator between runs.  You can leave it running.  The application will be stopped and replaced before starting again.  This can significantly speed up the debug cycle since you are not waiting for the emulator to start each time.\n\n\n\n\nWatch the Output window.  If the debugger won't connect or the application won't start, you may need to restart your computer or the emulator to get the network working.\n\n\n\n\nTip\n\n\nI have had some reports of the TaskList app crashing when using an x86 emulator.  If you run into these problems, check out the \nAndroid Tips\n section.\n\n\n\n\nIf everything is working, you should see the Android Emulator display your mobile client:\n\n\n\n\n\n\nWarn\n\n\nYou can also build the Android version on Windows with Visual Studio.  However, I find that version mismatches between Mono (which is used on the mac) and Visual Studio - particularly in reference to the version of the .NET framework - cause issues when swapping between the two environments.  For best results, stay in one environment.\n\n\n\n\nNote that the task list view is a \"dark\" style and the rest of the app is a \"light\" style.  This is because the default styling on an Android device is light.  We are using the default styling on two of the pages and specifying colors on the list page.  Fortunately, Xamarin Forms allows for \nplatform-specific styling\n.  The \nfinal sample\n has platform-specific styling for the list page.\n\n\nBuilding the Client for iOS\n\n\nWith Android done, we can now turn to the iOS platform.  Like we did for Android, we must first initalize our Mobile Apps SDK for our platform.  Open the \nAppDelegate.cs\n file in your iOS project.  In the FinishedLaunching method, we will initalize our SDK:\n\n\npublic override bool FinishedLaunching(UIApplication app, NSDictionary options)\n{\n    global::Xamarin.Forms.Forms.Init();\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    LoadApplication(new App());\n    return base.FinishedLaunching(app, options);\n}\n\n\n\n\nNow we can build and run our app:\n\n\n\n\nRight-click the \nTaskList.iOS\n project and select \nSet as StartUp Project\n.\n\n\nChange the \nDebug\n configuration (top bar) to \nDebug | iPhoneSimulator\n.\n\n\nRight-click the \nTaskList.iOS\n project and select \nBuild TaskList.iOS\n.\n\n\n\n\nIf you have never used Visual Studio for Mac to build and run an iOS app before, it is possible that you will receive an error having to do with code signing keys, provisioning profiles, or signing identities.  This is because you did not select the iPhoneSimulator prior to building.  Right-click the \nTaskList.iOS\n project and select \nClean TaskList.iOS\n, then restart the process, but selecting the \niPhoneSimulator\n configuration.\n\n\nYou can now select from several simulator options from the drop-down to the left of the build status. You should only use \nDevice\n if you have signed up for the Apple Developer Program and linked a provisioning profile to your XCode configuration.  Pick one of the simulator options like the \niPhone 7 iOS 10.0\n simulator, then click on \nRun\n to run the simulator.\n\n\nThe final product screens look like this:\n\n\n\n\nSome Final Thoughts\n\n\nIf you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations.  There are a lot of places where things can go wrong.  You are really integrating the build systems across Android, iOS, Xamarin, Xcode, and Azure.\n\n\nFortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work.\n\n\nThe following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order.  If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.", 
            "title": "Your First App - Mac Edition"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#your-first-mobile-app-on-a-mac", 
            "text": "There is a lot of detail to absorb about the possible services that the mobile client can consume and I will go into significant depth on those subjects.  First, wouldn't it be nice to write some code and get something working? Microsoft Azure has a great  first-steps tutorial  that takes you via the quickest possible route from creating a mobile backend to having a functional backend.  I would like to take things a little slower so that we can understand what is going on while we are doing the process.  We will have practically the same application at the end.  The primary reason for going through this slowly is to ensure that all our build and run processes are set up properly.  If this is the first mobile app you have ever written, you will see that there are quite a few things that need to be set up.  This chapter covers the set up required for a MacOS computer.  If you wish to develop your applications on a Windows PC, then skip to the  prior section .  The application we are going to build together is a simple task list.  The mobile client will have three screens - an entry screen, a task list and a task details page.  I have mocked these pages up using  MockingBot .   Tip  Mocking your screens before you start coding is a great habit to get into.  There are some great tools available including free tools like  MockingBot . Doing mockups before you start coding is a good way to prevent wasted time later on.     Tip  If you are using iOS, then you may want to remove the back button as the style guides suggest you don't need one.  Other platforms will need it though, so it's best to start with the least common denominator.  It's the same reason I add a refresh button even though it's only valid on Windows Phone!   My ideas for this app include:   Tapping on a task title in the task list will bring up the details page.  Toggling the completed link in the task list will set the completed flag.  Tapping the spinner will initiate a network refresh.   Now that we have our client screens planned out, we can move onto the thinking about the mobile backend.", 
            "title": "Your First Mobile App on a Mac"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#the-mobile-backend", 
            "text": "The mobile backend is an ASP.NET WebApi that is served from within Azure App Service: a highly scalable and redundant web hosting facility that supports all the major web languages (like ASP.NET, Node, PHP and Python).  Azure Mobile Apps is an SDK (which is available in ASP.NET and Node) that runs on top of Azure App Service.", 
            "title": "The Mobile Backend"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#creating-a-simple-azure-mobile-apps-backend", 
            "text": "To get started:   Fire up  Visual Studio for Mac .  Create a new solution with  File  -   New Solution... .  In the  New Project  window, choose  Other  -   ASP.NET , select  Empty ASP.NET Project , then click  Next .  In the  Configure your Web project  window, check the  Web API  box and uncheck the  Include Unit Test Project , then click  Next .  In the second  Configure your new project  window, enter  Backend  for the for the Project Name and  Chapter1  for the Solution name. Click  Create  to generate the files.   At this point, Visual Studio for Mac will lay down the template files on your disk and download the core ASP.NET libraries from NuGet.  You will need to accept the licenses for the downloaded NuGet packages.  Visual Studio for Mac does not have the range of templates that Visual Studio for the PC has.  As a result, we will need to do some additional work putting together a base project.  The core of Azure Mobile Apps runs on .NET Framework 4.6.x.  Right-click your  Backend  project in the Solution Explorer and choose  Options .  The target framework setting is located in the  Build  -   General  section.  You can choose any .NET Framework in the 4.6.x range.   Click  OK  to accept the change and close the Project Options.  Although a core set of NuGet packages are installed during project creation, we need to add the Azure Mobile Apps NuGet packages.  The easiest way to do this is to install the  Azure Mobile .NET Server Quickstart  NuGet package, which contains dependencies on all the other Azure Mobile Apps packages.  Expand the  Backend  project in the solution explorer, right-click  Packages , then select  Add Packages... .  Use the search box to find the appropriate NuGet package.  You also need to add the Owin System host  Microsoft.Owin.Host.Systemweb  NuGet package.   You should take the opportunity to update any NuGet packages that were automatically added to the project.  To do so, right-click  Packages , then choose  Update .  Start by configuring the Azure Mobile Apps SDK.  The Owin process runs the  Startup.cs  object to configure itself.  To create the  Startup.cs  class:   Right-click the  Backend  folder.  Select  Add  -   New File... .  Select  General  -   Empty Class , and set the name of the class to  Startup.cs .  Click  New .    Leave off the .cs on the end  If your filename does not have a dot in it (and some that we use do), you can leave off the  .cs  trailing extension.  Visual Studio for Mac will add it for you.   The  Startup.cs  class looks like this:  using Microsoft.Owin;\nusing Owin;\n\n[assembly: OwinStartup(typeof(Backend.Startup))]\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            ConfigureMobileApp(app);\n        }\n    }\n}  The  ConfigureMobileApp()  method is located in the  App_Start\\Startup.MobileApp.cs  file that we will create next.  Create the file the same way you did the  Startup.cs  file, then enter the following into the new file:  using System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Data.Entity;\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Owin;\n\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            var config = new HttpConfiguration();\n            var mobileConfig = new MobileAppConfiguration();\n\n            mobileConfig\n                .AddTablesWithEntityFramework()\n                .ApplyTo(config);\n\n            Database.SetInitializer(new MobileServiceInitializer());\n\n            app.UseWebApi(config);\n        }\n    }\n\n    public class MobileServiceInitializer : CreateDatabaseIfNotExists MobileServiceContext \n    {\n        protected override void Seed(MobileServiceContext context)\n        {\n            List TodoItem  todoItems = new List TodoItem \n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  First item , Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  Second item , Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set TodoItem ().Add(todoItem);\n            }\n\n            base.Seed(context);\n        }\n    }\n}  Let's break this down a little bit.  The  ConfigureMobileApp()  method is called to configure Azure Mobile Apps when the service starts.  The code tells the SDK that we want to use tables and that those tables are backed with Entity Framework.  We also want to initialize the database that we are going to use.  That database is going to use a  DbContext  called  MobileServiceContext .  The initialization code will create the database and seed it with two new items if it doesn't already exist.  If it exists, then we assume that we don't need to seed the database with data.  The  MobileServiceContext  is used to configure the tables within the database and to project those tables into Entity Framework.  It relies on a model for each table.  In Azure Mobile Apps, this is called the  Data Transfer Object  or DTO.  The server will serialize a DTO to JSON for transmission.  Our DTO is in a new directory called  DataObjects  (right-click on  Backend  and choose  Add  -   New Folder...  to create it) and is called  TodoItem.cs :  using Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}  We base each DTO we use on the  EntityData  class, since we are using Entity Framework.  This sets up some additional columns in the data model so that we can keep track on mobile device changes.  We will be discussing this in more detail in the  Data Access and Offline Sync  chapter.  We also need the  Models\\MobileServiceContext.cs  which sets up the tables for us:  using System.Data.Entity;\nusing System.Data.Entity.ModelConfiguration.Conventions;\nusing System.Linq;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Tables;\nusing Backend.DataObjects;\n\nnamespace Backend.Models\n{\n    public class MobileServiceContext : DbContext\n    {\n        private const string connectionStringName =  Name=MS_TableConnectionString ;\n\n        public MobileServiceContext() : base(connectionStringName)\n        {\n        }\n\n        public DbSet TodoItem  TodoItems { get; set; }\n\n        protected override void OnModelCreating(DbModelBuilder modelBuilder)\n        {\n            modelBuilder.Conventions.Add(\n                new AttributeToColumnAnnotationConvention TableColumnAttribute, string (\n                     ServiceTableColumn , (property, attributes) =  attributes.Single().ColumnType.ToString()));\n        }\n    }\n}  A  DbSet  statement is needed for each table we wish to expose to the mobile clients.  There are a couple of important items here.  Firstly, our connection string is called  MS_TableConnectionString .  This is set up in the  Web.config  file and overridden in the Azure Portal so that you can do both local development and run\nagainst a production database in the cloud.  Do not change this name.  The  OnModelCreating()  method sets up the tables to handle the service columns that are contained within the  EntityData  class used by the DTO.  Certain fields need to be indexed and triggers need to be added to keep the values updated properly.  Finally (in terms of code), we need to create a table controller.  This is the endpoint that is exposed on the Internet that our mobile clients will access to send and receive data.  Create a  Controllers  folder and add the following  TodoItemController.cs  class:  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems() =  Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id) =  Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch) =  UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =  DeleteAsync(id);\n    }\n}  The  TableController  is the central processing for the database access layer.  It handles all the  OData  capabilities for us and exposes these as REST endpoints within our WebAPI.  This means that the actual code for this controller is tiny - just 12 lines of code - but very powerful.   Info  OData  is a specification for accessing table data on the Internet.  It provides a mechanism for querying and manipulating data within a table.  Entity Framework is a common data access layer for ASP.NET applications.   Our last step in our backend before publishing it is to edit the  Web.config  file.  The  Web.config  file tells IIS about the run-time settings for this application.  We need to set up the  MS_TableConnectionString  and several app settings.  Inevitably, I copy  a created Web.config  rather than starting from scratch:  ?xml version= 1.0  encoding= utf-8 ?  configuration \n   configSections \n     section name= entityFramework  type= System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089  requirePermission= false  / \n   /configSections \n   connectionStrings \n     add name= MS_TableConnectionString  connectionString= Data Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend-20170308080621.mdf;Initial Catalog=aspnet-Backend-20170308080621;Integrated Security=True;MultipleActiveResultSets=True  providerName= System.Data.SqlClient  / \n   /connectionStrings \n   appSettings \n     add key= PreserveLoginUrl  value= true  / \n     add key= MS_SigningKey  value= Overridden by portal settings  / \n     add key= EMA_RuntimeUrl  value= Overridden by portal settings  / \n     add key= MS_NotificationHubName  value= Overridden by portal settings  / \n   /appSettings \n   system.web \n     httpRuntime targetFramework= 4.6  / \n     compilation debug= true  targetFramework= 4.6  / \n   /system.web \n   system.webServer \n     validation validateIntegratedModeConfiguration= false  / \n     modules runAllManagedModulesForAllRequests= true  / \n     handlers \n       remove name= ExtensionlessUrlHandler-Integrated-4.0  / \n       remove name= OPTIONSVerbHandler  / \n       remove name= TRACEVerbHandler  / \n       add name= ExtensionlessUrlHandler-Integrated-4.0  path= *.  verb= *  type= System.Web.Handlers.TransferRequestHandler  preCondition= integratedMode,runtimeVersionv4.0  / \n     /handlers \n   /system.webServer \n   runtime \n     assemblyBinding xmlns= urn:schemas-microsoft-com:asm.v1  xmlns:bcl= urn:schemas-microsoft-com:bcl \n       dependentAssembly \n         assemblyIdentity name= Newtonsoft.Json  publicKeyToken= 30ad4fe6b2a6aeed  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-9.0.0.0  newVersion= 9.0.0.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= System.Web.Http  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-5.2.3.0  newVersion= 5.2.3.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= System.Net.Http.Formatting  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-5.2.3.0  newVersion= 5.2.3.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= Microsoft.Data.Edm  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-5.8.1.0  newVersion= 5.8.1.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= Microsoft.Data.OData  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-5.8.1.0  newVersion= 5.8.1.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= System.Spatial  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-5.8.1.0  newVersion= 5.8.1.0  / \n       /dependentAssembly \n       dependentAssembly \n         assemblyIdentity name= Microsoft.Owin  publicKeyToken= 31bf3856ad364e35  culture= neutral  / \n         bindingRedirect oldVersion= 0.0.0.0-3.0.1.0  newVersion= 3.0.1.0  / \n       /dependentAssembly \n     /assemblyBinding \n   /runtime \n   entityFramework \n     defaultConnectionFactory type= System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework  / \n     providers \n       provider invariantName= System.Data.SqlClient  type= System.Data.Entity.SqlServer.SqlProviderServices, EntityFramework.SqlServer  / \n     /providers \n   /entityFramework  /configuration   Choose  Build All  from the  Build  menu and ensure your project compiles without errors.", 
            "title": "Creating a Simple Azure Mobile Apps Backend"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-an-azure-app-service-for-mobile-apps", 
            "text": "The next step in the process is to build the resources on Azure that will run your mobile backend.  Start by logging into the  Azure portal , then follow these instructions:   Click the big  + New  button in the top-left corner.  Click  Web + Mobile , then  Mobile App .   Enter a unique name in the  App name  box.   Tip  Since the name doesn't matter and it has to be unique, you can use  a GUID generator  to generate a unique name. GUIDs are not the best names to use when you need to actually find resources, but using a GUID prevents conflicts when deploying, so I prefer them as a naming scheme. You can prefix the GUID  (example: chapter1-GUID) to aid in discovery\nlater on.  Generally, the first four digits of a GUID are enough to identify individual resources.     If you have more than one subscription (for example, you have a trial and an MSDN subscription), then ensure you select the right subscription in the  Subscription  drop-down.    Select  Create new  under resource group and enter a name for this mobile application.   Resource Groups  Resource groups are great for grouping all the resources associated with a mobile application together.  During development, it means you can delete all the resources in one operation.  For production, it means you can see how much the service is costing you and how the resources are being used.     Finally, select or create a new  App Service Plan .   App Service Plan  The App Service Plan is the thing that actually bills you - not the web or mobile backend.  You can run a number of web or mobile backends on the same App Service Plan.   I tend to create a new App Service Plan for each mobile application.  This is because the App Service Plan lives inside the Resource Group that you create.  The process for creating an App Service Plan is straight forward.  You have two decisions to make.  The first decision is where is the service going to run.  In a production environment, the correct choice is \"near your customers\".  \"Close to the developers\" is a good choice during development.  Unfortunately, neither of those is an option you can actually choose in the portal, so you will have to translate into some sort of geographic location.  With 16 regions to choose from, you have a lot of choice.  The second decision you have to make is what to run the service on; also known as the Pricing tier.   If you Click  View all , you will see you have lots of choices.  F1 Free and D1 Shared, for example, run on shared resources and are CPU limited. You should avoid these as the service will stop responding when you are over the CPU quota.  That leaves Basic, Standard and Premium.  Basic has no automatic scaling and can run up to 3 instances - perfect for development tasks.  Standard and Premium both have automatic scaling, automatic backups, and large amounts of storage; they differ in features: the number of sites or instances you can run on them, for example.  Finally, there is a number after the plan.  This tells you how big the virtual machine is that the plan is running on.  The numbers differ by number of cores and memory.  For our purposes, an F1 Free site is enough to run this small demonstration project.  More complex development projects should use something in the Basic range of pricing plans.  Production apps should be set up in Standard or Premium pricing plans.    Once you have created your app service plan and saved it, Click  Create .    The creation of the service can take a couple of minutes.  You can monitor the process of deployment by clicking on the Notifications icon.  This is in the top bar on the right-hand side and looks like a Bell.  Clicking on a specific notification will provide more information about the activity.  Once you have created your app service, the App Service blade will open.  We will also want a place to store our data.  This role is taken on by a SQL Azure instance.  We could link an existing database if we had one defined.  However, we can also create a test database.   Tip  Creating a Test Database through the App Service Data Connections (as I describe here) allows you to create a free database.  This option is not normally available through other SQL database creation flows.   Before we can create a database, we need to create a logical server for the database.  The SQL Server (the logical server) sets the region and the login credentials for all the contained databases:   Click  Resource groups  in the left hand side menu.  Click the resource group you created.  Click  Add  at the top of the blade.  Enter  SQL Server  into the search box, then press Enter.  Click  SQL Server (logical server) .  Click  Create .  Enter the information required by the form:  A server name (which must be unique in the world - this is a great place to use a GUID).  A username and password for accessing the databases on the server.  Select the existing resource group.  Pick the same Location as you did for the App Service Plan.    Click  Create .   Once the deployment has completed, you can move on to creating and linking a database.  You can check the status of the deployment by clicking on the icon that looks like a bell in the top banner.  To create and link the database:   Click  Resource groups  in the left hand side menu.  Click the resource group you created.   Click the App Service your created.   Tip  If you pinned your App Service to the dashboard, you can Click the pinned App Service instead.  It will bring you to the same place.     Click  Data connections  in the  MOBILE  menu.  You can also search for Data connections in the left hand menu.    Click  Add .   In the  Type  box, select  SQL Database .  Click the unconfigured  SQL Database  link:     In the  Database  blade, select  Create a new database .  Enter a name for the database (like  chapter1-db ).  Click the Target server box and select the logical server you created earlier.  Select a Pricing Tier, then click  Apply .     Click  Select  to close the SQL Database blade.  Click the  Connection string  box.  Enter the username and password you set up for the SQL logical server.  Click  OK .  The username and password will be validated before proceeding.  Click  OK  to close the Add data connection blade.     This produces another deployment step that creates a SQL database with your settings and binds it to the App Service.  Once complete, the connection  MS_TableConnectionString  will be listed in Data Connections blade.", 
            "title": "Building an Azure App Service for Mobile Apps"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#deploying-the-azure-mobile-apps-backend", 
            "text": "One of the areas I love Visual Studio for the PC over the Mac edition is in publishing to Azure.  On the PC, that's a right-click action.  There is no publish action in Visual Studio for Mac.  However, Azure App Service has enough other tools available to publish a service.  These include continuous integration technologies that link to Visual Studio Team Services or GitHub, integrations with cloud storage providers like OneDrive and the venerable but trusty ftp mechanisms.  If none of those suit you, you can use drag-and-drop, which is what I am going to do here.   Return to the browser and log into the  Azure portal .  Go to the  Settings  blade for your Mobile App.  Click  Advanced Tools  in the  DEVELOPMENT TOOLS  menu.   Click  Go  in the  Advanced Tools  blade.   The page that loads should match https://{YourMobileApp}.scm.azurewebsites.net/.    Select the  Debug Console  menu from the top and choose  CMD .   Within the file structure listing, click  site , then  wwwroot .  Remove the  hostingstart.html  file; click the circle with a minus symbol in it to the left of that file and confirm the dialog to delete this file.  On your Mac, use the Finder to navigate to the folder that contains your Mobile App Backend.   Select the following folder and files:   bin  packages.config  Web.config     Drag and drop those files into the browser window where the  hostingstart.html  file used to be.    A progress indicator should appear near the top right.  Upon completion you should see the files appear in the file list:     You can test your deployed app by browsing to  https://{yourmobileapp}.azurewebsites.net/tables/todoitem?ZUMO-API-VERSION=2.0.0  - this is the same URL that your mobile app will connect to later on.  Replace  {yourmobileapp}  with the name of the App Service that you created.  If everything is working, you should see some JSON results in your window:   The first request will take some time as the App Service is waking up your service, which is initializing the database and seeding the data into that database.   Info  You will see the word ZUMO all over the SDK, including in optional HTTP headers and throughout the SDK source code.  ZUMO was the original code name within Microsoft for A ZU re  MO bile.", 
            "title": "Deploying the Azure Mobile Apps Backend"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#the-mobile-client", 
            "text": "Now that the mobile backend is created and deployed, we can move onto the mobile application that your users would install on their phones.  We are going to use  Xamarin.Forms  to produce a cross-platform application for iOS and Android, and the majority of the code will be placed in a shared project that both platforms will use.  You can get as high as 95% code re-use using Xamarin.Forms which is a major productivity boost in complex applications.   Info  When you compile a Xamarin.Forms application for a specific platform, you are producing a true native application for that platform - whether it be iOS, Android or Windows.   Right-click the  Chapter1  solution, then select  Add  -   Add New Project .  This will bring up the familiar New Project dialog.  Select the  Multiplatform  -   App  -   Forms App  template, then click  Next .   Give the project a name such as  TaskList , ensure Android and iOS are both selected, select  Use Shared Library  and click  Next .   Click  Create  on the next screen to create the projects.  You will see that three new projects are created:  a common library that you named, plus a project for each platform that you choise (in this case two platforms - Android and iOS):   Most of our work will happen in the common library in this walkthrough.  However, we can introduce platform-specific code at any point.  The platform-specific code is stored in the platform-specific project.  There is one final item we must do before we leave the set up of the project.  There are a number of platform upgrades that inevitably have to happen.  The Xamarin Platform is updated much more often than the project templates in Visual Studio for Mac. Updates to the Xamarin platform are released via NuGet. Since we are going to be integrating the Azure Mobile Apps client SDK, you should add the  Microsoft.Azure.Mobile.Client  NuGet package.  This will need to be done within each platform-specific project (TaskList.Droid and TaskList.iOS).   Warn  Although it is tempting, do not include a v1.x version of the Mobile Client. This is for the earlier Azure Mobile Services.  There are many differences between the wire protocols of the two products.   You can start by updating the existing NuGet packages.  Right-click the  Packages  folder in each project, then select  Update .  Then add the  Microsoft.Azure.Mobile.Client  SDK.  Right-click the  Packages  folder again, select  Add Packages... , then enter the package name in the search box.    Info  Android generally has more updates than the other platforms.  Ensure that you update the main Xamarin.Forms package and then refresh the update list.  This will ensure the right list of support packages is updated.", 
            "title": "The Mobile Client"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-the-common-library", 
            "text": "There are two parts that we must concentrate on within the common library.  Firstly, we must deal with the connection to our mobile backend using the Azure Mobile Apps client SDK.  Secondly, we need to create the three pages that our application will show.  Start by adding the following folders to your Shared Library project:   Abstractions  Models  Pages  Services  ViewModels   Our application will use MVVM (Model-View-ViewModel) for the interaction.  The UI will be written in XAML, and the ViewModel will be used to provide information to that UI.  The Services folder will hold the classes necessary to communicate with the Azure Mobile Apps backend.  Finally, we have a number of interfaces and common classes that we will need to make everything work.  Remove the other contents of the shared project.  They will not be required.", 
            "title": "Building the Common Library"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-an-azure-mobile-apps-connection", 
            "text": "One of the things I often stress is to set yourself up assuming you are going to test your product.  Testing is a good thing.  In cloud development, we often set up a \"mock\" service where we swap out the \"real\" service and use something that has the same interface, but deals with local data only.  This application will have two interfaces.  The first represents a cloud service, which will have a collection of tables we want to access.  This will be defined in   Abstractions\\ICloudService.cs .  namespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n    }\n}  We haven't defined  ICloudTable  nor  TableData  yet.  The  ICloudTable T  interface defines what the application can do to a table.  It will be defined in  Abstractions\\ICloudTable.cs :  using System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable T  where T : TableData\n    {\n        Task T  CreateItemAsync(T item);\n        Task T  ReadItemAsync(string id);\n        Task T  UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n\n        Task ICollection T  ReadAllItemsAsync();\n    }\n}  The  ICloudTable T  interface defines the normal CRUD operations: Create, Read, Update, and Delete.  However, it does so asynchronously.  We are dealing with network operations and it is easy for those operations to tie up the UI thread for an appreciable amount of time.  Making them async provides the ability to respond to other events and ensure your application is responsive to the user.  There are some fields that every single record within an Azure Mobile Apps table provides.  These fields are required for offline sync capabilities like incremental sync and conflict resolution.  On the server, this is represented by the  EntityData  class.  We cannot use that class as it contains the Entity Framework additions for indexing and triggers.  The fields are instead provided by a new abstract base class on the client called  Abstractions\\TableData :  using System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public DateTimeOffset? CreatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}  As we will learn when we deal with  table data , these fields need to be defined with the same name and semantics as on the server.  Our model on the server was sub-classed from  EntityData  and the  EntityData  class on the server defines these fields.  It's tempting to call the client version of the class the same as the server version.  If we did that, the models on both the client and server would look the same.  However, I find that this confuses the issue.  The models on the client and server are not the same.  They are missing the  Deleted  flag and they do not contain any relationship information on the client.  I choose to deliberately call the base class something else on the client to avoid this confusion.  We will be adding to these interfaces in future chapters as we add more capabilities to the application.  The concrete implementations of these classes are similarly easily defined.  The Azure Mobile Apps Client SDK does most of the work for us.  Here is the concrete implementation of the  ICloudService  (in  Services\\AzureCloudService.cs ):  using Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient( https://my-backend.azurewebsites.net );\n        }\n\n        public ICloudTable T  GetTable T () where T : TableData\n        {\n            return new AzureCloudTable T (client);\n        }\n    }\n}   Ensure you use HTTPS  If you copy the URL on the Overview page of your App Service, you will get the http version of the endpoint.  You must provide the https version of the endpoint when using App Service.  The http endpoint redirects to https and the standard HttpClient does not handle redirects.   The Azure Mobile Apps Client SDK takes a lot of the pain out of communicating with the mobile backend that we have already published.  Just swap  my-backend  out for the name of your mobile backend and the rest is silently dealt with.   Warn  The name  Microsoft.WindowsAzure.MobileServices  is a hold-over from the old Azure Mobile Services code-base.  Don't be fooled - clients for Azure Mobile Services are not interchangeable with clients for Azure Mobile Apps.   We also need a concrete implementation of the  ICloudTable T  interface (in  Services\\AzureCloudTable.cs ):  using System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable T  : ICloudTable T  where T : TableData\n    {\n        MobileServiceClient client;\n        IMobileServiceTable T  table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.client = client;\n            this.table = client.GetTable T ();\n        }\n\n        #region ICloudTable implementation\n        public async Task T  CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            await table.DeleteAsync(item);\n        }\n\n        public async Task ICollection T  ReadAllItemsAsync()\n        {\n            return await table.ToListAsync();\n        }\n\n        public async Task T  ReadItemAsync(string id)\n        {\n            return await table.LookupAsync(id);\n        }\n\n        public async Task T  UpdateItemAsync(T item)\n        {\n          await table.UpdateAsync(item);\n          return item;\n        }\n        #endregion\n    }\n}  The Azure Mobile Apps Client SDK does a lot of the work for us.  In fact, we are just wrapping the basic interface here.  The majority of the code for dealing with the remote server is done for us.   Tip  You can use a shorthand (called a lambda expression) for methods with only one line. For instance, the delete method could just as easily have been written as:  public async Task DeleteItemAsync(T item) =  await table.DeleteAsync(item);  You may see this sort of short hand in samples.   We also need to create the model that we will use for the data.  This should look very similar to the model on the server - including having the same name and fields.  In this case, it's  Models\\TodoItem.cs :  using TaskList.Abstractions;\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}  We have a final piece of code to write before we move on to the views, but it's an important piece.  The  ICloudService  must be a singleton in the client.  We will add authentication and offline sync capabilities in future versions of this code.  The singleton becomes critical when using those features.  For right now, it's good practice and saves on memory if you only have one copy of the  ICloudService  in your mobile client.  Since there is only one copy of the  App.cs  in any given app, I can place it there.  Ideally, I'd use some sort of dependency injection system or a singleton manager to deal with this.  Here is the  App.cs :  using TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n\n        // There are life cycle methods here...\n    }\n}  We haven't written  Pages.EntryPage  yet, but that's coming.  This file replaces the  App.xaml  and  App.xaml.cs  files from the original shared project.  If you have not done so already, ensure you remove those files now.", 
            "title": "Building an Azure Mobile Apps Connection"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-the-ui-for-the-app", 
            "text": "Earlier, I showed the mockup for my UI.  It included three pages - an entry page, a list page, and a detail page.  These pages have three elements - a XAML definition file, a (simple) code-behind file, and a view model.   Info  This book is not intending to introduce you to everything that there is to know about Xamarin and UI programming with XAML.  If you wish to have that sort of introduction,\nthen I recommend reading the excellent book by Charles Petzold:  Creating Mobile Apps with Xamarin.Forms .   I tend to use MVVM (or Model-View-ViewModel) for UI development in Xamarin based applications.  It's a nice clean pattern and is well understood and documented.  In MVVM, there is a 1:1 correlation between the view and the view-model, 2-way communication between the view and the view-model and properties within the view-model are bound directly to UI elements.  In general (and in all my code), view-models expose an INotifyPropertyChanged event to tell the UI that something within the view-model has been changed.  To do this, we will use a  BaseViewModel  class that implements the base functionality for each view.  Aside from the  INotifyPropertyChanged  interface, there are some common properties we need for each page.  Each page needs a title, for example, and an indicator of network activity.  These can be placed in the  Abstractions\\BaseViewModel.cs  class:  using System;\nusing System.Collections.Generic;\nusing System.ComponentModel;\n\nnamespace TaskList.Abstractions\n{\n    public class BaseViewModel : INotifyPropertyChanged\n    {\n        public event PropertyChangedEventHandler PropertyChanged;\n        string _propTitle = string.Empty;\n        bool _propIsBusy;\n\n        public string Title\n        {\n            get { return _propTitle; }\n            set { SetProperty(ref _propTitle, value,  Title ); }\n        }\n\n        public bool IsBusy\n        {\n            get { return _propIsBusy; }\n            set { SetProperty(ref _propIsBusy, value,  IsBusy ); }\n        }\n\n        protected void SetProperty T (ref T store, T value, string propName, Action onChanged = null)\n        {\n            if (EqualityComparer T .Default.Equals(store, value))\n                return;\n            store = value;\n            if (onChanged != null)\n                onChanged();\n            OnPropertyChanged(propName);\n        }\n\n        public void OnPropertyChanged(string propName)\n        {\n            if (PropertyChanged == null)\n                return;\n            PropertyChanged(this, new PropertyChangedEventArgs(propName));\n        }\n    }\n}  This is a fairly common  INotifyPropertyChanged  interface implementation pattern. Each property that we want to expose is a standard property, but the  set  operation is replaced by the  SetProperty()  call.  The  SetProperty()  call deals with the notification; calling the event emitter if the property has changed value.  We only need two properties on the  BaseViewModel : the title and the network indicator.  I tend to write my apps in two stages.  I concentrate on the functionality of the app in the first stage.  There is no fancy graphics, custom UI widgets, or anything else to clutter the thinking.   The page is all about the functionality of the various interactions.  Once I have the functionality working, I work on the styling of the page.  We won't be doing any styling work in the demonstration apps that we write during the course of this book.  The EntryPage has just one thing to do.  It provides a button that enters the app.  When we cover authentication later on, we'll use this to log in to the backend.  If you are looking at the perfect app, this is a great place to put the introductory screen.  Creating a XAML file is relatively simple.  We already created a  Pages  directory to hold the pages of our application.  Right-click the  Pages  directory in the solution explorer and choose  Add  -   New File... .  In the  Add New File  dialog, pick  Forms  -   Forms ContentPage Xaml .  Name the new page  EntryPage .  This will create two files -  EntryPage.xaml  and  EntryPage.xaml.cs .  Let's center a button on the page and wire it up with a command.  Here is the  Pages\\EntryPage.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.EntryPage \n             Title= {Binding Title} \n     ContentPage.Content \n         StackLayout HorizontalOptions= Center \n                     Orientation= Vertical \n                     VerticalOptions= Center \n             Button BackgroundColor= Teal \n                    BorderRadius= 10 \n                    Command= {Binding LoginCommand} \n                    Text= Login \n                    TextColor= White  / \n         /StackLayout \n     /ContentPage.Content  /ContentPage   The  StackLayout  element is our layout element.  It occupies the entire screen (since it is a direct child of the content page) and the options just center whatever the contents are.  The only contents are a button.  There are two bindings.  These are bound to properties in the view-model.  We've already seen the Title property - this is a text field that specifies the title of the page. The other binding is a login command.  When the button is tapped, the login command will be run.  We'll get onto that in the view-model later.  The other file created is the code-behind file.  Because we are moving all of the non-UI code into a view-model, the code-behind file is trivial:  using TaskList.ViewModels;\nusing Xamarin.Forms;\n\nnamespace TaskList.Pages\n{\n    public partial class EntryPage : ContentPage\n    {\n        public EntryPage()\n        {\n            InitializeComponent();\n            BindingContext = new EntryPageViewModel();\n        }\n    }\n}  This is a recipe that will be repeated over and over again for the code-behind when you are using a XAML-based project with MVVM.  We initialize the UI, then bind all the bindings to a new instance of the view model.  Speaking of which, the view-model just needs to handle the login click.  Note that the location or namespace is  TaskList.ViewModels .  I'm of two minds about location. There tends to be a 1:1 relationship between the XAML file and the View Model, so it makes sense that they are stored together.  However, just about all the sample code that I see has the view-models in a separate namespace.  Which one is correct?  I'll copy the samples for now.  Here is the code for  ViewModels\\EntryPageViewModel.cs :  using System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title =  Task List ;\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =  loginCmd ?? (loginCmd = new Command(async () =  await ExecuteLoginCommand()));\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [Login] Error = {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}  This is a fairly simple view-model but there are some patterns here that are worth explaining.  Firstly, note the way we create the  LoginCommand  property. This is the property that is bound to the  Command  parameter in the  Button  of our view.  This recipe is the method of invoking a UI action asynchronously.  It isn't important now, but we will want this technique repeatedly as our UI actions kick off network activity.  The second is the pattern for the  ExecuteLoginCommand  method.  Firstly, I ensure nothing else is happening by checking the IsBusy flag.   If nothing is happening, I set the IsBusy flag.  Then I do what I need to do in a try/catch block.  If an exception is thrown, I deal with it.  Most of the time this involves displaying an error condition.  There are several cross-platform dialog packages to choose from or you can roll your own.  That is not covered here.  We just write a debug log statement so we can see the result in the debug log.  Once everything is done, we clear the IsBusy flag.  The only thing we are doing now is swapping out our main page for a new main page.  This is where we will attach authentication later on.  The next page is the Task List page, which is in  Pages\\TaskList.xaml :  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.TaskList \n             Title= {Binding Title} \n   ContentPage.Content \n     StackLayout \n       ListView BackgroundColor= #7F7F7F \n                CachingStrategy= RecycleElement \n                IsPullToRefreshEnabled= True \n                IsRefreshing= {Binding IsBusy, Mode=OneWay} \n                ItemsSource= {Binding Items} \n                RefreshCommand= {Binding RefreshCommand} \n                RowHeight= 50 \n                SelectedItem= {Binding SelectedItem, Mode=TwoWay} \n         ListView.ItemTemplate \n           DataTemplate \n             ViewCell \n               StackLayout HorizontalOptions= FillAndExpand \n                           Orientation= Horizontal \n                           Padding= 10 \n                           VerticalOptions= CenterAndExpand \n                 Label HorizontalOptions= FillAndExpand \n                       Text= {Binding Text} \n                       TextColor= #272832  / \n                 Switch IsToggled= {Binding Complete, Mode=OneWay}  / \n               /StackLayout \n             /ViewCell \n           /DataTemplate \n         /ListView.ItemTemplate \n       /ListView \n       StackLayout HorizontalOptions= Center \n                   Orientation= Horizontal \n         Button BackgroundColor= Teal \n                Command= {Binding AddNewItemCommand} \n                Text= Add New Item \n                TextColor= White  / \n       /StackLayout \n     /StackLayout \n   /ContentPage.Content  /ContentPage   Note that some bindings here are one-way.  This means that the value in the view-model drives the value in the UI.  There is nothing within the UI that you can do to alter the state of the underlying property.  Some bindings are two-way.  Doing something in the UI (for example, toggling the switch) alters the underlying property.  This view is a little more complex.  It can be split into two parts - the list at the top of the page and the button area at the bottom of the page.  The list area uses a template to help with the display of each item.  Note that the  ListView  object has a \"pull-to-refresh\" option that I have wired up so that when pulled, it calls the RefreshCommand.  It also has an indicator that I have wired up to the IsBusy indicator.  Anyone who is familiar with the iOS \"pull-to-refresh\" gesture can probably guess what this does.  The code behind for the TaskList can be found in  Pages\\TaskList.xaml.cs :  using TaskList.ViewModels;using Xamarin.Forms;\nnamespace TaskList.Pages\n{\n    public partial class TaskList : ContentPage\n    {\n        public TaskList()\n        {\n            InitializeComponent();\n            BindingContext = new TaskListViewModel();\n        }\n    }\n}  There is a view-model that goes along with the view (in  ViewModels\\TaskListViewModel.cs ):  using System;\nusing System.Collections.ObjectModel;\nusing System.Collections.Specialized;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskListViewModel : BaseViewModel\n    {\n        public TaskListViewModel()\n        {\n            Title =  Task List ;\n            RefreshList();\n        }\n\n        ObservableCollection TodoItem  items = new ObservableCollection TodoItem ();\n        public ObservableCollection TodoItem  Items\n        {\n            get { return items; }\n            set { SetProperty(ref items, value,  Items ); }\n        }\n\n        TodoItem selectedItem;\n        public TodoItem SelectedItem\n        {\n            get { return selectedItem; }\n            set\n            {\n                SetProperty(ref selectedItem, value,  SelectedItem );\n                if (selectedItem != null)\n                {\n                    Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail(selectedItem));\n                    SelectedItem = null;\n                }\n            }\n        }\n\n        Command refreshCmd;\n        public Command RefreshCommand =  refreshCmd ?? (refreshCmd = new Command(async () =  await ExecuteRefreshCommand()));\n\n        async Task ExecuteRefreshCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var table = App.CloudService.GetTable TodoItem ();\n                var list = await table.ReadAllItemsAsync();\n                Items.Clear();\n                foreach (var item in list)\n                    Items.Add(item);\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskList] Error loading items: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command addNewCmd;\n        public Command AddNewItemCommand =  addNewCmd ?? (addNewCmd = new Command(async () =  await ExecuteAddNewItemCommand()));\n\n        async Task ExecuteAddNewItemCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                await Application.Current.MainPage.Navigation.PushAsync(new Pages.TaskDetail());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskList] Error in AddNewItem: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        async Task RefreshList()\n        {\n            await ExecuteRefreshCommand();\n            MessagingCenter.Subscribe TaskDetailViewModel (this,  ItemsChanged , async (sender) = \n            {\n                await ExecuteRefreshCommand();\n            });\n        }\n    }\n}  This is a combination of the patterns we have seen earlier.  The Add New Item and Refresh commands should be fairly normal patterns now.  We navigate to the detail page (more on that later) in the case of selecting an item (which occurs when the UI sets the  SelectedItem  property through a two-way binding) and when the user clicks on the Add New Item button.  When the Refresh button is clicked (or when the user opens the view for the first time), the list is refreshed.  It is fairly common to use an  ObservableCollection  or another class that uses the  ICollectionChanged  event handler for the list storage.  Doing so allows the UI to react to changes in the items.  Note the use of the  ICloudTable  interface here.  We are using the  ReadAllItemsAsync()  method to get a list of items, then we copy the items we received into the  ObservableCollection .  Finally, there is the TaskDetail page.  This is defined in the  Pages\\TaskDetail.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             x:Class= TaskList.Pages.TaskDetail \n             Title= {Binding Title} \n   ContentPage.Content \n     StackLayout Padding= 10  Spacing= 10 \n       Label Text= What should I be doing? / \n       Entry Text= {Binding Item.Text} / \n       Label Text= Completed? / \n       Switch IsToggled= {Binding Item.Complete} / \n       StackLayout VerticalOptions= CenterAndExpand / \n       StackLayout Orientation= Vertical  VerticalOptions= End \n         StackLayout HorizontalOptions= Center  Orientation= Horizontal \n           Button BackgroundColor= #A6E55E \n                  Command= {Binding SaveCommand} \n                  Text= Save  TextColor= White / \n           Button BackgroundColor= Red \n                  Command= {Binding DeleteCommand} \n                  Text= Delete  TextColor= White / \n         /StackLayout \n       /StackLayout \n     /StackLayout \n   /ContentPage.Content  /ContentPage   This page is a simple form with just two buttons that need to have commands wired up.  However, this page is used for both the \"Add New Item\" gesture and the \"Edit Item\" gesture.  As a result of this, we need to handle the passing of the item to be edited.  This is done in the  Pages\\TaskDetail.xaml.cs  code-behind file:  using TaskList.Models;\nusing TaskList.ViewModels;\nusing Xamarin.Forms;\n\nnamespace TaskList.Pages\n{\n    public partial class TaskDetail : ContentPage\n    {\n        public TaskDetail(TodoItem item = null)\n        {\n            InitializeComponent();\n            BindingContext = new TaskDetailViewModel(item);\n        }\n    }\n}  The item that is passed in from the  TaskList  page is used to create a specific view-model for that item.  The view-model is similarly configured to use that item:  using System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class TaskDetailViewModel : BaseViewModel\n    {\n        ICloudTable TodoItem  table = App.CloudService.GetTable TodoItem ();\n\n        public TaskDetailViewModel(TodoItem item = null)\n        {\n            if (item != null)\n            {\n                Item = item;\n                Title = item.Text;\n            }\n            else\n            {\n                Item = new TodoItem { Text =  New Item , Complete = false };\n                Title =  New Item ;\n            }\n        }\n\n        public TodoItem Item { get; set; }\n\n        Command cmdSave;\n        public Command SaveCommand =  cmdSave ?? (cmdSave = new Command(async () =  await ExecuteSaveCommand()));\n\n        async Task ExecuteSaveCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id == null)\n                {\n                    await table.CreateItemAsync(Item);\n                }\n                else\n                {\n                    await table.UpdateItemAsync(Item);\n                }\n                MessagingCenter.Send TaskDetailViewModel (this,  ItemsChanged );\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskDetail] Save error: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n\n        Command cmdDelete;\n        public Command DeleteCommand =  cmdDelete ?? (cmdDelete = new Command(async () =  await ExecuteDeleteCommand()));\n\n        async Task ExecuteDeleteCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                if (Item.Id != null)\n                {\n                    await table.DeleteItemAsync(Item);\n                }\n                MessagingCenter.Send TaskDetailViewModel (this,  ItemsChanged );\n                await Application.Current.MainPage.Navigation.PopAsync();\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [TaskDetail] Save error: {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}  The save command uses the  ICloudTable  interface again - this time doing either  CreateItemAsync()  or  UpdateItemAsync()  to create or update the item.  The delete command, as you would expect, deletes the item with the  DeleteItemAsync()  method.  The final thing to note from our views is that I am using the  MessagingCenter  to communicate between the TaskDetail and TaskList views.  If I change the item in the  TaskDetail  list, then I want to update the list in the  TaskList  view.  Note that all the code we have added to the solution thus far is in the common  TaskList  project.  Nothing is required for this simple example in a platform specific project.  That isn't normal, as we shall see.", 
            "title": "Building the UI for the App"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-the-client-for-android", 
            "text": "Now we're ready to build our client applications.  We'll start with the Android version.  Prior to running the application, we need to make two additional changes.  Go to your Android project and open the  MainActivity.cs  file.  In the  OnCreate  method we need to add an initalizer for our Mobile Apps SDK:  protected override void OnCreate(Bundle bundle)\n{\n    TabLayoutResource = Resource.Layout.Tabbar;\n    ToolbarResource = Resource.Layout.Toolbar;\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n    LoadApplication(new App());\n}  Finally, as Android has an explicit permission model (this has somewhat changed in the latest version of Android), we need to say the application requires the Internet permission.  Right-click on the Android project and go to  Options .  Select  Android Application  from under the  Build  section.  At the bottom of the options panel, you'll see a list of permissions.  Find Internet and check it and then click the OK button.   Next we need to configure the solution to run the Android project.   Right-click the  TaskList.Droid  project, then select  Set as StartUp Project .  Right-click the  TaskList.Droid  project again, then select  Build TaskList.Droid .   The drop-down between the run button in the top left of Visual Studio for Mac and the Build status at the top of Visual Studio for Mac, now allows you to choose an emulator or device to run your app against.  By default, Visual Studio for Mac will create several emulators for you.  You can also use the  Manage Google Emulators...  option to create additional Android Virtual Devices (AVDs) and download other images online.   Tip  When testing the mobile client manually through the Android Emulator, you are likely to need to rebuild the application.  You do not have to shut down the emulator between runs.  You can leave it running.  The application will be stopped and replaced before starting again.  This can significantly speed up the debug cycle since you are not waiting for the emulator to start each time.   Watch the Output window.  If the debugger won't connect or the application won't start, you may need to restart your computer or the emulator to get the network working.   Tip  I have had some reports of the TaskList app crashing when using an x86 emulator.  If you run into these problems, check out the  Android Tips  section.   If everything is working, you should see the Android Emulator display your mobile client:    Warn  You can also build the Android version on Windows with Visual Studio.  However, I find that version mismatches between Mono (which is used on the mac) and Visual Studio - particularly in reference to the version of the .NET framework - cause issues when swapping between the two environments.  For best results, stay in one environment.   Note that the task list view is a \"dark\" style and the rest of the app is a \"light\" style.  This is because the default styling on an Android device is light.  We are using the default styling on two of the pages and specifying colors on the list page.  Fortunately, Xamarin Forms allows for  platform-specific styling .  The  final sample  has platform-specific styling for the list page.", 
            "title": "Building the Client for Android"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#building-the-client-for-ios", 
            "text": "With Android done, we can now turn to the iOS platform.  Like we did for Android, we must first initalize our Mobile Apps SDK for our platform.  Open the  AppDelegate.cs  file in your iOS project.  In the FinishedLaunching method, we will initalize our SDK:  public override bool FinishedLaunching(UIApplication app, NSDictionary options)\n{\n    global::Xamarin.Forms.Forms.Init();\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    LoadApplication(new App());\n    return base.FinishedLaunching(app, options);\n}  Now we can build and run our app:   Right-click the  TaskList.iOS  project and select  Set as StartUp Project .  Change the  Debug  configuration (top bar) to  Debug | iPhoneSimulator .  Right-click the  TaskList.iOS  project and select  Build TaskList.iOS .   If you have never used Visual Studio for Mac to build and run an iOS app before, it is possible that you will receive an error having to do with code signing keys, provisioning profiles, or signing identities.  This is because you did not select the iPhoneSimulator prior to building.  Right-click the  TaskList.iOS  project and select  Clean TaskList.iOS , then restart the process, but selecting the  iPhoneSimulator  configuration.  You can now select from several simulator options from the drop-down to the left of the build status. You should only use  Device  if you have signed up for the Apple Developer Program and linked a provisioning profile to your XCode configuration.  Pick one of the simulator options like the  iPhone 7 iOS 10.0  simulator, then click on  Run  to run the simulator.  The final product screens look like this:", 
            "title": "Building the Client for iOS"
        }, 
        {
            "location": "/chapter1/firstapp_mac/#some-final-thoughts", 
            "text": "If you have got through the entire process outlined in this Chapter and built the application for each platform, then congratulations.  There are a lot of places where things can go wrong.  You are really integrating the build systems across Android, iOS, Xamarin, Xcode, and Azure.  Fortunately, once these are set up, it's likely that they will continue working and you won't have to think too much about them again. The Android and iOS build tools and simulators will just work.  The following 7 chapters each take one aspect of the cloud services that can be provided to mobile apps and explores it in detail, using an Azure Mobile App as a beginning. You can jump around at this point, but be aware that we expect you to cover these topics in order.  If you do the data chapter before covering authentication, it's likely you will have missed important functionality in your app to complete the work.", 
            "title": "Some Final Thoughts"
        }, 
        {
            "location": "/chapter2/authconcepts/", 
            "text": "Authentication Concepts\n\n\nOne of the very first things you want to do when developing a mobile app is to provide users with a unique experience.  For our example task list application, this could be as simple as providing a task list for the user who is logged in.  In more complex applications, this is the gateway to role-based access controls, group rules, and sharing with your friends.  In all these cases, properly identifying the user using the phone is the starting point.\n\n\nAuthentication provides a process by which the user that is using the mobile device can be identified securely. This is generally done by entering a username and password.  However, modern systems can also provide \nmulti-factor authentication\n, send you a text message to a registered device, or \nuse your fingerprint\n as the password.\n\n\nThe OAuth Process\n\n\nIn just about every single mobile application, a process called \nOAuth\n is used to properly identify a user to the mobile backend.  OAuth is not an authentication mechanism in its own right.  It is used to route the authentication request to the right place and to verify that the authentication took place. There are three actors in the OAuth protocol:\n\n\n\n\nThe \nClient\n is the application attempting to get access to the resource.\n\n\nThe \nResource\n is the mobile backend that the client is attempting to access.\n\n\nThe \nIdentity Provider\n (or IdP) is the service that is responsible for authenticating the client.\n\n\n\n\nAt the end of the process, a cryptographically signed token is minted.  This token is added to every request made by the client to the resource to securely identify the user.\n\n\nServer Side vs. Client Side\n\n\nThere are two types of authentication flow: Server-flow and Client-flow.  They are so named because of who controls the flow of the actual authentication.\n\n\n\n\nServer-flow is named because the authentication flow is managed by the Azure App Service (the server) through a webview-based work flow.  It is generally used in two cases:\n\n\n\n\nYou want a simple placeholder for authentication in your mobile app while you are developing other code.\n\n\nYou are developing a traditional web app.\n\n\n\n\n\n\nTip\n\n\nIf you are developing a single-page application (SPA), then client-flow is the more appropriate model for authentication.  The SPA looks more like a mobile client than a traditional web app.  In particular, you will be redirected away from your single page and returned to the app at a specific entry point, removing any context from the app.\n\n\n\n\nIn the case of server-flow:\n\n\n\n\nThe client brings up a web view and asks for the login page from the resource.\n\n\nThe resource redirects the client to the identity provider.\n\n\nThe identity provider does the authentication before redirecting the client back to the resource (with an identity provider token).\n\n\nThe resource validates the identity provider token with the identity provider.\n\n\nFinally, the resource mints a new resource token that it returns to the client.\n\n\n\n\nClient-flow authentication uses an IdP provided SDK to integrate a more native feel to the authentication flow.  The actual flow happens on the client, communicating only with the identity provider.  It is used is most cases:\n\n\n\n\nYou want a more integrated experience for your users.\n\n\nYou want the most native feel to your authentication flow.\n\n\nYou are developing a single-page web application.\n\n\n\n\nA client-flow feels similar to the server-flow, but using a native SDK instead of a web view.\n\n\n\n\nThe client uses the identity provider SDK to communicate with the identity provider.\n\n\nThe identity provider authenticates the user, returning an identity provider token.\n\n\nThe client presents the identity provider token to the resource.\n\n\nThe resource validates the identity provider token with the identity provider.\n\n\nFinally, the resource mints a new resource token that it returns to the client.\n\n\n\n\nFor example, if you use the Facebook SDK for authentication, your app will seamlessly switch over into the Facebook app and ask you to authorize your client application before switching you back to your client application.\n\n\nYou should use the identity provider SDK when developing an app that will be released on the app store.  The identity providers will advise you to use their SDK and it provides the best experience for your end users.\n\n\n\n\nInfo\n\n\nThe Azure App Service Authentication / Authorization service works with any App Service, including web apps and API apps.  It's not just for Mobile Apps.\n\n\n\n\nAuthentication Providers\n\n\nAzure Mobile Apps supports five identity providers natively:\n\n\n\n\nAzure Active Directory\n\n\nFacebook\n\n\nGoogle\n\n\nMicrosoft (MSA)\n\n\nTwitter\n\n\n\n\n\n\nInfo\n\n\nAzure App Service Authentication / Authorization maintains a token store in the XDrive (which is the drive that is shared among all instances of the backend within the same App Service Plan).  The token store is located at \nD:\\\\home\\\\data\\\\.auth\\\\tokens\n on the backend.  The tokens are encrypted and stored in a per-user encrypted file.\n\n\n\n\nIn addition, you can set up client-flow custom authentication that allows you to mint a ZUMO token to your specifications for any provider using a client-flow.  For example, you could use authentication providers like \nAzure AD B2C\n, \nLinkedIn\n or \nGitHub\n, a third-party authentication provider like \nAuth0\n, or you could set up an identity table in your database so that you can check  username and password without an identity provider.", 
            "title": "Concepts"
        }, 
        {
            "location": "/chapter2/authconcepts/#authentication-concepts", 
            "text": "One of the very first things you want to do when developing a mobile app is to provide users with a unique experience.  For our example task list application, this could be as simple as providing a task list for the user who is logged in.  In more complex applications, this is the gateway to role-based access controls, group rules, and sharing with your friends.  In all these cases, properly identifying the user using the phone is the starting point.  Authentication provides a process by which the user that is using the mobile device can be identified securely. This is generally done by entering a username and password.  However, modern systems can also provide  multi-factor authentication , send you a text message to a registered device, or  use your fingerprint  as the password.", 
            "title": "Authentication Concepts"
        }, 
        {
            "location": "/chapter2/authconcepts/#the-oauth-process", 
            "text": "In just about every single mobile application, a process called  OAuth  is used to properly identify a user to the mobile backend.  OAuth is not an authentication mechanism in its own right.  It is used to route the authentication request to the right place and to verify that the authentication took place. There are three actors in the OAuth protocol:   The  Client  is the application attempting to get access to the resource.  The  Resource  is the mobile backend that the client is attempting to access.  The  Identity Provider  (or IdP) is the service that is responsible for authenticating the client.   At the end of the process, a cryptographically signed token is minted.  This token is added to every request made by the client to the resource to securely identify the user.", 
            "title": "The OAuth Process"
        }, 
        {
            "location": "/chapter2/authconcepts/#server-side-vs-client-side", 
            "text": "There are two types of authentication flow: Server-flow and Client-flow.  They are so named because of who controls the flow of the actual authentication.   Server-flow is named because the authentication flow is managed by the Azure App Service (the server) through a webview-based work flow.  It is generally used in two cases:   You want a simple placeholder for authentication in your mobile app while you are developing other code.  You are developing a traditional web app.    Tip  If you are developing a single-page application (SPA), then client-flow is the more appropriate model for authentication.  The SPA looks more like a mobile client than a traditional web app.  In particular, you will be redirected away from your single page and returned to the app at a specific entry point, removing any context from the app.   In the case of server-flow:   The client brings up a web view and asks for the login page from the resource.  The resource redirects the client to the identity provider.  The identity provider does the authentication before redirecting the client back to the resource (with an identity provider token).  The resource validates the identity provider token with the identity provider.  Finally, the resource mints a new resource token that it returns to the client.   Client-flow authentication uses an IdP provided SDK to integrate a more native feel to the authentication flow.  The actual flow happens on the client, communicating only with the identity provider.  It is used is most cases:   You want a more integrated experience for your users.  You want the most native feel to your authentication flow.  You are developing a single-page web application.   A client-flow feels similar to the server-flow, but using a native SDK instead of a web view.   The client uses the identity provider SDK to communicate with the identity provider.  The identity provider authenticates the user, returning an identity provider token.  The client presents the identity provider token to the resource.  The resource validates the identity provider token with the identity provider.  Finally, the resource mints a new resource token that it returns to the client.   For example, if you use the Facebook SDK for authentication, your app will seamlessly switch over into the Facebook app and ask you to authorize your client application before switching you back to your client application.  You should use the identity provider SDK when developing an app that will be released on the app store.  The identity providers will advise you to use their SDK and it provides the best experience for your end users.   Info  The Azure App Service Authentication / Authorization service works with any App Service, including web apps and API apps.  It's not just for Mobile Apps.", 
            "title": "Server Side vs. Client Side"
        }, 
        {
            "location": "/chapter2/authconcepts/#authentication-providers", 
            "text": "Azure Mobile Apps supports five identity providers natively:   Azure Active Directory  Facebook  Google  Microsoft (MSA)  Twitter    Info  Azure App Service Authentication / Authorization maintains a token store in the XDrive (which is the drive that is shared among all instances of the backend within the same App Service Plan).  The token store is located at  D:\\\\home\\\\data\\\\.auth\\\\tokens  on the backend.  The tokens are encrypted and stored in a per-user encrypted file.   In addition, you can set up client-flow custom authentication that allows you to mint a ZUMO token to your specifications for any provider using a client-flow.  For example, you could use authentication providers like  Azure AD B2C ,  LinkedIn  or  GitHub , a third-party authentication provider like  Auth0 , or you could set up an identity table in your database so that you can check  username and password without an identity provider.", 
            "title": "Authentication Providers"
        }, 
        {
            "location": "/chapter2/backend/", 
            "text": "Adding Authentication to a Mobile Backend\n\n\nThe Azure App Service Authentication / Authorization service integrates seamlessly into an Azure Mobile Apps\nbackend as a piece of middleware that fills in the Identity information for ASP.NET.  That means the only\nthing we have to worry about is authorization.  The authentication piece (determining that a user is who they\nsay they are) is already taken care of.\n\n\nAuthorization (which is the determination of whether an authenticated user can use a specific API) can happen\nat either the controller level or an individual operation level.  We can add authorization to an entire table\ncontroller by adding the \n[Authorize]\n attribute to the table controller.  For example, here is our table\ncontroller from the first chapter with authorization required for all operations:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    [Authorize]\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems() =\n Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id) =\n Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch) =\n UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =\n DeleteAsync(id);\n    }\n}\n\n\n\n\nAuthorization can also happen on a per-operation basis by adding the \n[Authorize]\n attribute to a single method\nwithin the table controller.  For example, instead of requiring authorization on the entire table, we want a\nversion where reading was possible anonymously but updating the database required authentication:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems() =\n Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id) =\n Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        [Authorize]\n        public Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch) =\n UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        [Authorize]\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        [Authorize]\n        public Task DeleteTodoItem(string id) =\n DeleteAsync(id);\n    }\n}\n\n\n\n\nNote that the \n[Authorize]\n attribute can do much more than what is provided here.  Underneath there are various\nparameters that you can adjust to see if the user belongs to a specific group or role.  However, the token that\nis checked to see if the user is authenticated does not pull in any of the other information that is normally\nneeded for such authorization tasks.  As a result, the \n[Authorize]\n tags is really only checking whether a\nrequest requires authentication or not.\n\n\nConfiguring an Authentication Provider\n\n\nConfiguration of the identity provider is very dependent on the identity provider and whether the client is using\na client-flow or server-flow.  Choose one of the several options for authentication:\n\n\n\n\nEnterprise Authentication\n covers Azure Active Directory.\n\n\nSocial Authentication\n covers Facebook, Google, Microsoft and Twitter.\n\n\n\n\nWe can also configure authentication using custom routes.  This allows us to use other (non-supported) services\nor to completely customize our flow (for example, to use an existing identity database).  We will cover custom\nauthentication later on.", 
            "title": "Authentication in the Backend"
        }, 
        {
            "location": "/chapter2/backend/#adding-authentication-to-a-mobile-backend", 
            "text": "The Azure App Service Authentication / Authorization service integrates seamlessly into an Azure Mobile Apps\nbackend as a piece of middleware that fills in the Identity information for ASP.NET.  That means the only\nthing we have to worry about is authorization.  The authentication piece (determining that a user is who they\nsay they are) is already taken care of.  Authorization (which is the determination of whether an authenticated user can use a specific API) can happen\nat either the controller level or an individual operation level.  We can add authorization to an entire table\ncontroller by adding the  [Authorize]  attribute to the table controller.  For example, here is our table\ncontroller from the first chapter with authorization required for all operations:  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    [Authorize]\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems() =  Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id) =  Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch) =  UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id) =  DeleteAsync(id);\n    }\n}  Authorization can also happen on a per-operation basis by adding the  [Authorize]  attribute to a single method\nwithin the table controller.  For example, instead of requiring authorization on the entire table, we want a\nversion where reading was possible anonymously but updating the database required authentication:  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems() =  Query();\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id) =  Lookup(id);\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        [Authorize]\n        public Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch) =  UpdateAsync(id, patch);\n\n        // POST tables/TodoItem\n        [Authorize]\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        [Authorize]\n        public Task DeleteTodoItem(string id) =  DeleteAsync(id);\n    }\n}  Note that the  [Authorize]  attribute can do much more than what is provided here.  Underneath there are various\nparameters that you can adjust to see if the user belongs to a specific group or role.  However, the token that\nis checked to see if the user is authenticated does not pull in any of the other information that is normally\nneeded for such authorization tasks.  As a result, the  [Authorize]  tags is really only checking whether a\nrequest requires authentication or not.", 
            "title": "Adding Authentication to a Mobile Backend"
        }, 
        {
            "location": "/chapter2/backend/#configuring-an-authentication-provider", 
            "text": "Configuration of the identity provider is very dependent on the identity provider and whether the client is using\na client-flow or server-flow.  Choose one of the several options for authentication:   Enterprise Authentication  covers Azure Active Directory.  Social Authentication  covers Facebook, Google, Microsoft and Twitter.   We can also configure authentication using custom routes.  This allows us to use other (non-supported) services\nor to completely customize our flow (for example, to use an existing identity database).  We will cover custom\nauthentication later on.", 
            "title": "Configuring an Authentication Provider"
        }, 
        {
            "location": "/chapter2/enterprise/", 
            "text": "Enterprise Authentication\n\n\nEnterprise Authentication is handled by Azure Active Directory, which is fairly commonly configured within Azure.  Every Azure subscription has a default directory associated with it that you can leverage for this section.  In addition, if your organization has an Office 365 subscription, this will likely be tied to an Azure Active Directory domain to allow enterprise sign-in.  In either case, you have a directory you can use for providing authentication to your app.\n\n\nIn general, you will need to get special permissions to update the directory. If you want to use your organizations corporate directory, then you are likely to have to get your IT department involved to set it up.\n\n\nAzure Active Directory: Server-Flow setup\n\n\nThe Azure Active Directory server-flow is perhaps the easiest of all the authentication methods to configure.  No\nmatter if you are doing a client flow or server flow, you need to set up the server flow first.\n\n\n\n\nTip\n\n\nWe recommend that you implement Client Flow in any non-trivial application.\n\n\n\n\nIf you are using your default directory and you want to add a couple of test users, you will need to set those up\nfirst.   Start by logging in to the \nAzure portal\n.\n\n\n\n\nIn the left-hand menu, click \nMore services\n.\n\n\nEnter \nActive Directory\n in the search box, then click \nAzure Active Directory\n.\n\n\n(Optional) If you need to manage a different directory to the default directory, click \nSwitch directory\n and choose a different directory.\n\n\n\n\nClick \nUsers and groups\n, then \nAll users\n.\n\n\n\n\n\n\n\n\nClick \nAdd\n.\n\n\n\n\n\n\n\n\nFill in the information.  Ensure you add the user to a group, if necessary.  You can also add additional information (like a real name) in the \nProfile\n section.\n\n\n\n\nCheck \nShow Password\n and make a note of the new password.\n\n\nClick \nCreate\n.\n\n\n\n\nRepeat for each test user you wish to use.  Once done, move onto configuring your App Service for authentication:\n\n\n\n\nClick \nAll resources\n in the left hand menu.\n\n\nClick your App Service or Mobile App.\n\n\nSearch for and click \nAuthentication / Authorization\n (it's under SETTINGS).\n\n\nChange App Service Authentication to \nOn\n. \n\n\nEnsure the \nAction to take when request is not authenticated\n is set to \nAllow Anonymous requests\n.\n\n\nClick \nAzure Active Directory\n.\n\n\n\n\nClick \nExpress\n.\n\n\n\n\n\n\n\n\nAll the information is filled in for you.  Click \nOK\n, then \nSave\n.\n\n\n\n\n\n\n\n\nInfo\n\n\nMake sure you create the app service in the right directory / subscription.  If you have access to more than one directory, you can choose the right one by selecting it under your account drop-down in the top-right corner.\n\n\n\n\nThere is also an \nAdvanced\n track.  This is used in client-flow situations and in situations where you have more\nthan one directory.  The Express flow is great for getting started quickly.\n\n\n\n\nPreview Portal Access\n\n\nAzure Active Directory portal access is in preview right now.  Certain things can only be done through\nthe \nAzure Classic Portal\n.  The list of things that cannot be done in the Azure Portal\nis thankfully dwindling.\n\n\n\n\nYou can walk through a server-flow authentication to test that you have all the settings correct.  Point your browser at https://\nyoursite\n.azurewebsites.net/.auth/login/aad.  The browser will take you through an authentication flow before giving you a successful authentication image:\n\n\n\n\nIf you get an error akin to \"We are having problems signing you in\", use an Incognito or InPrivate browsing window.\n\n\nAdding Authentication to a Mobile Client\n\n\nNow that the backend is completely configured, we can move our attention to the mobile client.  We are going to be\nusing the same mobile client that we developed in the first chapter, but we are now going to add authentication to\nit.  Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought\nof this and provided a facility for running platform specific code called the \nDependencyService\n.\n\n\n\n\nInfo\n\n\nIf we run our application right now, clicking on the \"Enter the App\" button will result in an error.  We will be\nable to see the Unauthorized error in the debug window of Visual Studio.\n\n\n\n\nOur first step is to define an \nAbstractions\\ILoginProvider.cs\n interface within the  shared project:\n\n\nusing Microsoft.WindowsAzure.MobileServices;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        Task LoginAsync(MobileServiceClient client);\n    }\n}\n\n\n\n\nNext, we are going to extend our \nAbstractions\\ICloudService.cs\n interface so that the main application can call\nthe login routine:\n\n\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n\n        Task LoginAsync();\n    }\n}\n\n\n\n\nOur code will call \nLoginAsync()\n in the \nICloudService\n, which will get the platform-specific version of the\nlogin provider and call \nLoginAsync()\n there, but with our defined mobile service client.  That is defined in the\n\nServices\\AzureCloudService.cs\n class:\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing Xamarin.Forms;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n        }\n\n        public ICloudTable\nT\n GetTable\nT\n() where T : TableData =\n new AzureCloudTable\nT\n(client);\n\n        public Task LoginAsync()\n        {\n            var loginProvider = DependencyService.Get\nILoginProvider\n();\n            return loginProvider.LoginAsync(client);\n        }\n    }\n}\n\n\n\n\nThe method looks up the platform dependent version of the login provider and executes the login method, passing\nalong the client (which we will need later).\n\n\nFinally, we will want to easily instantiate the cloud provider.  In the shared project, add the following to the\nconstructor for \nApp.cs\n:\n\n\n    public App()\n    {\n        ServiceLocator.Instance.Add\nICloudService, AzureCloudService\n();\n        MainPage = new NavigationPage(new Pages.EntryPage());\n    }\n\n\n\n\nThe \nServiceLocator\n class is used to manage the singletons in an application.  The cloud service object can now be retrieved in any view using the following snippet:\n\n\n    ICloudService cloudService = ServiceLocator.Instance.Resolve\nICloudService\n();\n\n\n\n\nWe will need to do this in the \nViewModels/TaskDetailViewModel.cs\n and \nViewModels/TaskKListViewModel.cs\n classes.  Refer to the \nproject in GitHub\n if you run into issues here.\n\n\nIn each platform-specific project, we need to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow.  Here is the droid \nServices\\DroidLoginProvider.cs\n (in the\nTaskList.Droid project):\n\n\nusing System.Threading.Tasks;\nusing Android.Content;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(context, \naad\n);\n        }\n    }\n}\n\n\n\n\nLet us take a closer look at this implementation.  The \nLoginAsync()\n method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service.  The clever piece is the \nXamarin.Forms.Dependency\n call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service.\n\n\nNote that we need an extra initialization routine for Android that must be called prior the login provider being\ncalled to pass along the main window of the app (also known as the context).  This is done in the \nMainActivity.cs\n\nfile \nafter\n the Xamarin Forms initialization call.  The dependency service is not set up until after the Xamarin\nForms library is initialized, so we will not be able to get the login provider reference before that point:\n\n\nprotected override void OnCreate(Bundle bundle)\n{\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n\n    ((DroidLoginProvider)DependencyService.Get\nILoginProvider\n()).Init(this);\n\n    LoadApplication(new App());\n}\n\n\n\n\niOS is similar, but does not require the initialization step in the main startup class.  The login provider class\nis in \nServices\\iOSLoginProvider.cs\n (in the \nTaskList.iOS\n project):\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(RootView, \naad\n);\n        }\n\n        public UIViewController RootView =\n UIApplication.SharedApplication.KeyWindow.RootViewController;\n    }\n}\n\n\n\n\nNote that we are using the same pattern here for registering the concrete implementation with the dependency service,\nso we can get it the same way. Finally, here is the UWP \nServices\\UWPLoginProvider.cs\n (in the TaskList.UWP project):\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(\naad\n);\n        }\n    }\n}\n\n\n\n\nNow that we have all the platform-specific login routines registered, we can move on to adding the login routine to\nthe UI.  We have already got a button on the entry page to enter the app.  It makes sense to wire up that button so\nthat it logs us in as well. The Command for the login button is in the \nViewModels\\EntryPageViewModel.cs\n:\n\n\nasync Task ExecuteLoginCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var cloudService = ServiceLocator.Instance.Resolve\nICloudService\n();\n        await cloudService.LoginAsync();\n        Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($\n[ExecuteLoginCommand] Error = {ex.Message}\n);\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}\n\n\n\n\n\n\nInfo\n\n\nThe \nServiceLocator\n class is a basic singleton handler.  It is available in the \nChapter2\n project.  It\nreturns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1.\n\n\n\n\nWhen you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window:\n\n\n\n\nGoing through the authentication process will get you to the task list again.  If the authentication process fails,\nthen \nLoginAsync()\n will throw an error, which is caught at the ViewModel.  Right now, the \nEntryPageViewModel\n\ndoes nothing more than print a diagnostic message to the debug window of Visual Studio.\n\n\nAzure Active Directory: Client-Flow Setup\n\n\nConfiguring Azure Active Directory for client-flow is a three-step process.  First, we need to create a WEB\napplication.  This represents the resource: in our case, the resource is the Azure Mobile Apps backend.  Then\nwe need to create a NATIVE application.  This represents the client: in our case, the ADAL (Active Directory\nAccess Library) library will need this information.  Finally, we need to give the NATIVE application permission\nto access the WEB application.\n\n\nIt starts with configuring a server-flow to protect the resource.  We've already done that above. Then configure\na \"Native Application\" and give it permissions to the web application:\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\nSelect \nAzure Active Directory\n from the left hand menu.\n\n\nClick \nApp registrations\n.\n\n\n\n\n\n\n\n\nNote that our existing web application is already there.  You will see more applications, depending on what you\n  have set up.\n\n\nClick \n+ Add\n at the top of the page.\n\n\n\n\n\n\n\n\nEnter a name for the app registration.  Select \nNative\n as the application type.  Enter a valid URI in the Redirect URI.  It can be anything, but it has to be valid.\n\n\nClick \nCreate\n.\n\n\nIn the \nSettings\n blade, click \nRedirect URIs\n.\n\n\nAdd a Redirect URI of the form: \nhttps://yoursite.azurewebsites.net/.auth/login/done\n.\n\n\n\n\n\n\n\n\nClick \nSave\n.\n\n\nClick \nRequired permissions\n.\n\n\nClick \n+ Add\n.\n\n\nClick \nSelect an API\n.\n\n\n\n\nEnter the name of your web application in the search box, and press Enter.\n\n\n\n\n\n\n\n\nClick the name of your web application, then click \nSelect\n.\n\n\n\n\n\n\nYou will be taken to \nSelect permissions\n.  Click \nAccess \nyour web application\n\n\n\n\n\n\n\n\nClick \nSelect\n, then \nDone\n.\n\n\n\n\n\n\nSo, what did we just do there?  We created a new Azure AD app for the native application.  We then gave permission\nfor the native application to access resources that are protected by the web application.  In our Azure App Service, we configured the service so that the Azure AD web application is used to protect our resources.  The net effect is that our native application OR our web application can access the App Service resources that are protected via the \n[Authorize]\n attribute.\n\n\nBefore continuing, you will need the \nApplication ID\n and the \nRedirect URI\n for the NATIVE application.  The Application ID for the native app is available in the \nProperties\n section of the \nSettings\n blade in the App Registrations blade:\n\n\n\n\nYou can enter\nthese into the \nHelpers\\Locations.cs\n file in the shared project:\n\n\nnamespace TaskList.Helpers\n{\n    public static class Locations\n    {\n        public static readonly string AppServiceUrl = \nhttps://zumobook-chapter2.azurewebsites.net\n;\n\n        public static readonly string AadClientId = \n0c3309fe-e392-4ca5-8d54-55f69ae1e0f8\n;\n\n        public static readonly string AadRedirectUri = \nhttps://zumobook-chapter2.azurewebsites.net/.auth/login/done\n;\n\n        public static readonly string AadAuthority = \nhttps://login.windows.net/photoadrianoutlook.onmicrosoft.com\n;\n    }\n}\n\n\n\n\nThe \nAadClientId\n and \nAadRedirectUri\n must match what was configured in Azure AD for the native app.  The\nother piece of information we need to add is the Azure AD Authority for the directory.  This is available in the \nDomain names\n blade within the Azure Active Directory blade.\n\n\nAdd the \nMicrosoft.IdentityModel.Clients.ActiveDirectory\n NuGet package using \nManage NuGet Packages...\n to\neach platform project.  This package contains the ADAL library as a portable class library.\n\n\n\n\nNow you can add the client flow to each project.  Start with the login provider in the \nTaskList.UWP\n project,\nlocated in the \nServices\\UWPLoginProvider.cs\n file:\n\n\nusing System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n\n        /// \nsummary\n\n        /// Login via ADAL\n        /// \n/summary\n\n        /// \nreturns\n(async) token from the ADAL process\n/returns\n\n        public async Task\nstring\n LoginADALAsync()\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count() \n 0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters(PromptBehavior.Auto, false));\n            return authResult.AccessToken;\n        }\n\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginADALAsync();\n            var zumoPayload = new JObject()\n            {\n                [\naccess_token\n] = accessToken\n            };\n            await client.LoginAsync(\naad\n, zumoPayload);\n\n            // Server-Flow Version\n            // await client.LoginAsync(\naad\n);\n        }\n    }\n}\n\n\n\n\nThe \nLoginADALAsync()\n method does the actual client-flow - using the ADAL library to authenticate the user and\nreturn the access token.  The \nLoginAsync()\n method initiates the client-flow.  It uses the token it receives\nfrom the client-flow to log in to the App Service, by packaging the token into a JSON object.  I have placed\nthe client and server flow next to each other so you can compare the two.\n\n\nIn the \nTaskList.Droid\n project, we need to deal with the \nContext\n, as is common with Android libraries.  The\nclient flow in \nServices\\DroidLoginProvider.cs\n is remarkably similar though:\n\n\nusing System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Android.App;\nusing Android.Content;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\nusing TaskList.Helpers;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        /// \nsummary\n\n        /// Login via ADAL\n        /// \n/summary\n\n        /// \nreturns\n(async) token from the ADAL process\n/returns\n\n        public async Task\nstring\n LoginADALAsync()\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count() \n 0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters((Activity)context));\n            return authResult.AccessToken;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginADALAsync();\n            var zumoPayload = new JObject();\n            zumoPayload[\naccess_token\n] = accessToken;\n            await client.LoginAsync(\naad\n, zumoPayload);\n\n            // Server-Flow Version\n            // await client.LoginAsync(context, \naad\n);\n        }\n    }\n}\n\n\n\n\nThe only real difference between this one and the Universal Windows edition is the PlatformParameters. We need to\npass in the context of the MainActivity (which is passed in through the \nInit()\n call).  However, we must also handle\nthe response from the ADAL library.  This is done in \nMainActivity.cs\n. Add the following method to the \nMainActivity\n\nclass:\n\n\nprotected override void OnActivityResult(int requestCode, Result resultCode, Intent data)\n{\n    base.OnActivityResult(requestCode, resultCode, data);\n    AuthenticationAgentContinuationHelper.SetAuthenticationAgentContinuationEventArgs(requestCode, resultCode, data);\n}\n\n\n\n\nFinally, the iOS version also requires access to the root view, so its \nPlatformParameters\n are also slightly\ndifferent.  Here is \nServices\\iOSLoginProvider.cs\n:\n\n\nusing System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        /// \nsummary\n\n        /// Login via ADAL\n        /// \n/summary\n\n        /// \nreturns\n(async) token from the ADAL process\n/returns\n\n        public async Task\nstring\n LoginADALAsync(UIViewController view)\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count() \n 0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters(view));\n            return authResult.AccessToken;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            var rootView = UIApplication.SharedApplication.KeyWindow.RootViewController;\n\n            // Client Flow\n            var accessToken = await LoginADALAsync(rootView);\n            var zumoPayload = new JObject();\n            zumoPayload[\naccess_token\n] = accessToken;\n            await client.LoginAsync(\naad\n, zumoPayload);\n\n            // Server Flow\n            //await client.LoginAsync(rootView, \naad\n);\n        }\n    }\n}\n\n\n\n\nNote that we can balance the needs of each platform by using the dependency service.  The code that is unique to\nthe platform is minimized and stored with the platform.\n\n\nIf you aren't interested in social authentication (Facebook, Google, Microsoft or Twitter authentication providers),\nyou can \nskip the Social Authentication section\n.", 
            "title": "Enterprise Authentication"
        }, 
        {
            "location": "/chapter2/enterprise/#enterprise-authentication", 
            "text": "Enterprise Authentication is handled by Azure Active Directory, which is fairly commonly configured within Azure.  Every Azure subscription has a default directory associated with it that you can leverage for this section.  In addition, if your organization has an Office 365 subscription, this will likely be tied to an Azure Active Directory domain to allow enterprise sign-in.  In either case, you have a directory you can use for providing authentication to your app.  In general, you will need to get special permissions to update the directory. If you want to use your organizations corporate directory, then you are likely to have to get your IT department involved to set it up.", 
            "title": "Enterprise Authentication"
        }, 
        {
            "location": "/chapter2/enterprise/#azure-active-directory-server-flow-setup", 
            "text": "The Azure Active Directory server-flow is perhaps the easiest of all the authentication methods to configure.  No\nmatter if you are doing a client flow or server flow, you need to set up the server flow first.   Tip  We recommend that you implement Client Flow in any non-trivial application.   If you are using your default directory and you want to add a couple of test users, you will need to set those up\nfirst.   Start by logging in to the  Azure portal .   In the left-hand menu, click  More services .  Enter  Active Directory  in the search box, then click  Azure Active Directory .  (Optional) If you need to manage a different directory to the default directory, click  Switch directory  and choose a different directory.   Click  Users and groups , then  All users .     Click  Add .     Fill in the information.  Ensure you add the user to a group, if necessary.  You can also add additional information (like a real name) in the  Profile  section.   Check  Show Password  and make a note of the new password.  Click  Create .   Repeat for each test user you wish to use.  Once done, move onto configuring your App Service for authentication:   Click  All resources  in the left hand menu.  Click your App Service or Mobile App.  Search for and click  Authentication / Authorization  (it's under SETTINGS).  Change App Service Authentication to  On .   Ensure the  Action to take when request is not authenticated  is set to  Allow Anonymous requests .  Click  Azure Active Directory .   Click  Express .     All the information is filled in for you.  Click  OK , then  Save .     Info  Make sure you create the app service in the right directory / subscription.  If you have access to more than one directory, you can choose the right one by selecting it under your account drop-down in the top-right corner.   There is also an  Advanced  track.  This is used in client-flow situations and in situations where you have more\nthan one directory.  The Express flow is great for getting started quickly.   Preview Portal Access  Azure Active Directory portal access is in preview right now.  Certain things can only be done through\nthe  Azure Classic Portal .  The list of things that cannot be done in the Azure Portal\nis thankfully dwindling.   You can walk through a server-flow authentication to test that you have all the settings correct.  Point your browser at https:// yoursite .azurewebsites.net/.auth/login/aad.  The browser will take you through an authentication flow before giving you a successful authentication image:   If you get an error akin to \"We are having problems signing you in\", use an Incognito or InPrivate browsing window.", 
            "title": "Azure Active Directory: Server-Flow setup"
        }, 
        {
            "location": "/chapter2/enterprise/#adding-authentication-to-a-mobile-client", 
            "text": "Now that the backend is completely configured, we can move our attention to the mobile client.  We are going to be\nusing the same mobile client that we developed in the first chapter, but we are now going to add authentication to\nit.  Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought\nof this and provided a facility for running platform specific code called the  DependencyService .   Info  If we run our application right now, clicking on the \"Enter the App\" button will result in an error.  We will be\nable to see the Unauthorized error in the debug window of Visual Studio.   Our first step is to define an  Abstractions\\ILoginProvider.cs  interface within the  shared project:  using Microsoft.WindowsAzure.MobileServices;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        Task LoginAsync(MobileServiceClient client);\n    }\n}  Next, we are going to extend our  Abstractions\\ICloudService.cs  interface so that the main application can call\nthe login routine:  using System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n\n        Task LoginAsync();\n    }\n}  Our code will call  LoginAsync()  in the  ICloudService , which will get the platform-specific version of the\nlogin provider and call  LoginAsync()  there, but with our defined mobile service client.  That is defined in the Services\\AzureCloudService.cs  class:  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing Xamarin.Forms;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n        }\n\n        public ICloudTable T  GetTable T () where T : TableData =  new AzureCloudTable T (client);\n\n        public Task LoginAsync()\n        {\n            var loginProvider = DependencyService.Get ILoginProvider ();\n            return loginProvider.LoginAsync(client);\n        }\n    }\n}  The method looks up the platform dependent version of the login provider and executes the login method, passing\nalong the client (which we will need later).  Finally, we will want to easily instantiate the cloud provider.  In the shared project, add the following to the\nconstructor for  App.cs :      public App()\n    {\n        ServiceLocator.Instance.Add ICloudService, AzureCloudService ();\n        MainPage = new NavigationPage(new Pages.EntryPage());\n    }  The  ServiceLocator  class is used to manage the singletons in an application.  The cloud service object can now be retrieved in any view using the following snippet:      ICloudService cloudService = ServiceLocator.Instance.Resolve ICloudService ();  We will need to do this in the  ViewModels/TaskDetailViewModel.cs  and  ViewModels/TaskKListViewModel.cs  classes.  Refer to the  project in GitHub  if you run into issues here.  In each platform-specific project, we need to define a concrete implementation of the login provider that uses a web view to hold the actual authentication flow.  Here is the droid  Services\\DroidLoginProvider.cs  (in the\nTaskList.Droid project):  using System.Threading.Tasks;\nusing Android.Content;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(context,  aad );\n        }\n    }\n}  Let us take a closer look at this implementation.  The  LoginAsync()  method on the Azure Mobile Apps client object takes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\", \"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service.  The clever piece is the  Xamarin.Forms.Dependency  call at the top - that registers the class as a platform service so we can access it through the Xamarin dependency service.  Note that we need an extra initialization routine for Android that must be called prior the login provider being\ncalled to pass along the main window of the app (also known as the context).  This is done in the  MainActivity.cs \nfile  after  the Xamarin Forms initialization call.  The dependency service is not set up until after the Xamarin\nForms library is initialized, so we will not be able to get the login provider reference before that point:  protected override void OnCreate(Bundle bundle)\n{\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n\n    ((DroidLoginProvider)DependencyService.Get ILoginProvider ()).Init(this);\n\n    LoadApplication(new App());\n}  iOS is similar, but does not require the initialization step in the main startup class.  The login provider class\nis in  Services\\iOSLoginProvider.cs  (in the  TaskList.iOS  project):  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(RootView,  aad );\n        }\n\n        public UIViewController RootView =  UIApplication.SharedApplication.KeyWindow.RootViewController;\n    }\n}  Note that we are using the same pattern here for registering the concrete implementation with the dependency service,\nso we can get it the same way. Finally, here is the UWP  Services\\UWPLoginProvider.cs  (in the TaskList.UWP project):  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync( aad );\n        }\n    }\n}  Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to\nthe UI.  We have already got a button on the entry page to enter the app.  It makes sense to wire up that button so\nthat it logs us in as well. The Command for the login button is in the  ViewModels\\EntryPageViewModel.cs :  async Task ExecuteLoginCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var cloudService = ServiceLocator.Instance.Resolve ICloudService ();\n        await cloudService.LoginAsync();\n        Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($ [ExecuteLoginCommand] Error = {ex.Message} );\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}   Info  The  ServiceLocator  class is a basic singleton handler.  It is available in the  Chapter2  project.  It\nreturns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1.   When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window:   Going through the authentication process will get you to the task list again.  If the authentication process fails,\nthen  LoginAsync()  will throw an error, which is caught at the ViewModel.  Right now, the  EntryPageViewModel \ndoes nothing more than print a diagnostic message to the debug window of Visual Studio.", 
            "title": "Adding Authentication to a Mobile Client"
        }, 
        {
            "location": "/chapter2/enterprise/#azure-active-directory-client-flow-setup", 
            "text": "Configuring Azure Active Directory for client-flow is a three-step process.  First, we need to create a WEB\napplication.  This represents the resource: in our case, the resource is the Azure Mobile Apps backend.  Then\nwe need to create a NATIVE application.  This represents the client: in our case, the ADAL (Active Directory\nAccess Library) library will need this information.  Finally, we need to give the NATIVE application permission\nto access the WEB application.  It starts with configuring a server-flow to protect the resource.  We've already done that above. Then configure\na \"Native Application\" and give it permissions to the web application:   Log on to the  Azure portal .  Select  Azure Active Directory  from the left hand menu.  Click  App registrations .     Note that our existing web application is already there.  You will see more applications, depending on what you\n  have set up.  Click  + Add  at the top of the page.     Enter a name for the app registration.  Select  Native  as the application type.  Enter a valid URI in the Redirect URI.  It can be anything, but it has to be valid.  Click  Create .  In the  Settings  blade, click  Redirect URIs .  Add a Redirect URI of the form:  https://yoursite.azurewebsites.net/.auth/login/done .     Click  Save .  Click  Required permissions .  Click  + Add .  Click  Select an API .   Enter the name of your web application in the search box, and press Enter.     Click the name of your web application, then click  Select .    You will be taken to  Select permissions .  Click  Access  your web application     Click  Select , then  Done .    So, what did we just do there?  We created a new Azure AD app for the native application.  We then gave permission\nfor the native application to access resources that are protected by the web application.  In our Azure App Service, we configured the service so that the Azure AD web application is used to protect our resources.  The net effect is that our native application OR our web application can access the App Service resources that are protected via the  [Authorize]  attribute.  Before continuing, you will need the  Application ID  and the  Redirect URI  for the NATIVE application.  The Application ID for the native app is available in the  Properties  section of the  Settings  blade in the App Registrations blade:   You can enter\nthese into the  Helpers\\Locations.cs  file in the shared project:  namespace TaskList.Helpers\n{\n    public static class Locations\n    {\n        public static readonly string AppServiceUrl =  https://zumobook-chapter2.azurewebsites.net ;\n\n        public static readonly string AadClientId =  0c3309fe-e392-4ca5-8d54-55f69ae1e0f8 ;\n\n        public static readonly string AadRedirectUri =  https://zumobook-chapter2.azurewebsites.net/.auth/login/done ;\n\n        public static readonly string AadAuthority =  https://login.windows.net/photoadrianoutlook.onmicrosoft.com ;\n    }\n}  The  AadClientId  and  AadRedirectUri  must match what was configured in Azure AD for the native app.  The\nother piece of information we need to add is the Azure AD Authority for the directory.  This is available in the  Domain names  blade within the Azure Active Directory blade.  Add the  Microsoft.IdentityModel.Clients.ActiveDirectory  NuGet package using  Manage NuGet Packages...  to\neach platform project.  This package contains the ADAL library as a portable class library.   Now you can add the client flow to each project.  Start with the login provider in the  TaskList.UWP  project,\nlocated in the  Services\\UWPLoginProvider.cs  file:  using System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n\n        ///  summary \n        /// Login via ADAL\n        ///  /summary \n        ///  returns (async) token from the ADAL process /returns \n        public async Task string  LoginADALAsync()\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count()   0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters(PromptBehavior.Auto, false));\n            return authResult.AccessToken;\n        }\n\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginADALAsync();\n            var zumoPayload = new JObject()\n            {\n                [ access_token ] = accessToken\n            };\n            await client.LoginAsync( aad , zumoPayload);\n\n            // Server-Flow Version\n            // await client.LoginAsync( aad );\n        }\n    }\n}  The  LoginADALAsync()  method does the actual client-flow - using the ADAL library to authenticate the user and\nreturn the access token.  The  LoginAsync()  method initiates the client-flow.  It uses the token it receives\nfrom the client-flow to log in to the App Service, by packaging the token into a JSON object.  I have placed\nthe client and server flow next to each other so you can compare the two.  In the  TaskList.Droid  project, we need to deal with the  Context , as is common with Android libraries.  The\nclient flow in  Services\\DroidLoginProvider.cs  is remarkably similar though:  using System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Android.App;\nusing Android.Content;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\nusing TaskList.Helpers;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        ///  summary \n        /// Login via ADAL\n        ///  /summary \n        ///  returns (async) token from the ADAL process /returns \n        public async Task string  LoginADALAsync()\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count()   0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters((Activity)context));\n            return authResult.AccessToken;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginADALAsync();\n            var zumoPayload = new JObject();\n            zumoPayload[ access_token ] = accessToken;\n            await client.LoginAsync( aad , zumoPayload);\n\n            // Server-Flow Version\n            // await client.LoginAsync(context,  aad );\n        }\n    }\n}  The only real difference between this one and the Universal Windows edition is the PlatformParameters. We need to\npass in the context of the MainActivity (which is passed in through the  Init()  call).  However, we must also handle\nthe response from the ADAL library.  This is done in  MainActivity.cs . Add the following method to the  MainActivity \nclass:  protected override void OnActivityResult(int requestCode, Result resultCode, Intent data)\n{\n    base.OnActivityResult(requestCode, resultCode, data);\n    AuthenticationAgentContinuationHelper.SetAuthenticationAgentContinuationEventArgs(requestCode, resultCode, data);\n}  Finally, the iOS version also requires access to the root view, so its  PlatformParameters  are also slightly\ndifferent.  Here is  Services\\iOSLoginProvider.cs :  using System;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        ///  summary \n        /// Login via ADAL\n        ///  /summary \n        ///  returns (async) token from the ADAL process /returns \n        public async Task string  LoginADALAsync(UIViewController view)\n        {\n            Uri returnUri = new Uri(Locations.AadRedirectUri);\n\n            var authContext = new AuthenticationContext(Locations.AadAuthority);\n            if (authContext.TokenCache.ReadItems().Count()   0)\n            {\n                authContext = new AuthenticationContext(authContext.TokenCache.ReadItems().First().Authority);\n            }\n            var authResult = await authContext.AcquireTokenAsync(\n                Locations.AppServiceUrl, /* The resource we want to access  */\n                Locations.AadClientId,   /* The Client ID of the Native App */\n                returnUri,               /* The Return URI we configured    */\n                new PlatformParameters(view));\n            return authResult.AccessToken;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            var rootView = UIApplication.SharedApplication.KeyWindow.RootViewController;\n\n            // Client Flow\n            var accessToken = await LoginADALAsync(rootView);\n            var zumoPayload = new JObject();\n            zumoPayload[ access_token ] = accessToken;\n            await client.LoginAsync( aad , zumoPayload);\n\n            // Server Flow\n            //await client.LoginAsync(rootView,  aad );\n        }\n    }\n}  Note that we can balance the needs of each platform by using the dependency service.  The code that is unique to\nthe platform is minimized and stored with the platform.  If you aren't interested in social authentication (Facebook, Google, Microsoft or Twitter authentication providers),\nyou can  skip the Social Authentication section .", 
            "title": "Azure Active Directory: Client-Flow Setup"
        }, 
        {
            "location": "/chapter2/social/", 
            "text": "Social Authentication\n\n\nAzure App Service provides built-in support for Facebook, Google, Microsoft and Twitter.  Irrespective of whether\nyou intend to use server-flow or client-flow, you will need to configure the Azure App Service Authentication /\nAuthorization service.  The method is pretty similar in each case:\n\n\n\n\nObtain a Developer Account for the provider.\n\n\nCreate a new application, obtaining a Client ID and Secret.\n\n\nTurn on Azure App Service Authentication.\n\n\nEnter the Client ID and Secret into the specific provider setup.\n\n\nSave the configuration.\n\n\n\n\nBefore you start any of this, create a new Azure Mobile Apps as we described in \nChapter 1\n.  If you want\na site to deploy for the configuration, the \nBackend\n project in the \nChapter2\n solution is pre-configured for\nauthorization. You just need to deploy it to Azure App Service.\n\n\nFacebook Configuration\n\n\nI am going to assume you have a Facebook account already.  If you do not have a Facebook account, go to \nFacebook\n\nand sign up.  All your friends are likely there already!  Now log in to the \nFacebook Developers\n web site.  Create\na new Facebook application:\n\n\n\n\nNote\n: Facebook updates the look and feel of their developer site on a regular basis.  As a result, the screen shots\nI have provided here may be different.  If in doubt, follow the bullet descriptions to find your way.\n\n\n\n\nInfo\n\n\nIf you are not already registered, Click the drop-down in the top-right corner and \nRegister as a Developer\n\nbefore continuing.\n\n\n\n\n\n\nClick the \nMy Apps\n link in the top right corner of the screen.\n\n\nClick \nCreate a New App\n.\n\n\nFill in the form:\n\n\n\n\n\n\n\n\n\n\nIf required, verify your account according to the instructions.  This usually involves typing a CAPTCHA text, adding a credit card number and/or verifying your mobile phone number.\n\n\n\n\n\n\nClick the \nGet Started\n button next to \nFacebook Login\n.\n\n\n\n\nClick \nSettings\n under \nFacebook Login\n in the left-hand menu.\n\n\n\n\n\n\n\n\nEnter your application URL + \n/.auth/login/facebook/callback\n in the \nValid OAuth redirect URIs\n.\n\n\n\n\n\n\n\n\nClick \nSave Changes\n.\n\n\nClick the \nSettings\n (under \nDashboard\n) in the left-hand menu.\n\n\nClick the \nShow\n button next to the App Secret\n\n\n\n\nNow that you have the \nApp ID\n and \nApp Secret\n, you can continue configuration of your app within the\n\nAzure Portal\n.\n\n\n\n\nOpen up your App Service by clicking on \nAll Resources\n or \nApp Services\n followed by the name of your app service.\n\n\nIn the \nSettings\n blade, Click \nAuthentication / Authorization\n which is under \nFeatures\n.\n\n\nTurn \nApp Service Authentication\n to \nOn\n.\n\n\nIn the \nAction to take when request is not authenticated\n, select \nAllow Anonymous requests (no action)\n.\n\n\n\n\n\n\nDanger\n\n\nIt is very tempting to choose \nLog in with Facebook\n.  However, you need to avoid this.  Selecting this option will mean that all requests need to be authenticated and you will not get the information about the identity on the back end.  Selecting \nAllow Anonymous requests\n means your app is in charge of what gets authenticated and what does not require authentication.\n\n\n\n\n\n\nClick \nFacebook\n (which should show \nNot Configured\n).\n\n\nCut and Paste the \nApp ID\n and \nApp Secret\n into the boxes provided.\n\n\nSelect \npublic_profile\n and \nemail\n for Scopes.\n\n\n\n\n\n\nWarn\n\n\nIf you request anything but public_profile, user_friends, and email, your app will need further review by\nFacebook, which will take time.  This process is not worth it for test apps like this one.\n\n\n\n\n\n\nClick \nOK\n (at the bottom of the blade) to close the Facebook configuration blade.\n\n\nClick \nSave\n (at the top of the blade) to save your Authentication changes.\n\n\n\n\nYou can test your authentication process by browsing to https://\nyoursite\n.azurewebsites.net/.auth/login/facebook;\nthis is the same endpoint that the Azure Mobile Apps Client SDK calls when it is time to integrate authentication\ninto the mobile client.\n\n\n\n\nIf you are not logged in to facebook already, you will be prompted for your facebook credentials first.  Finally,\nhere is your happy page - the page that signifies you have done everything right:\n\n\n\n\n\n\nWarn\n\n\nEvery single OAuth provider will ask you what sort of information you want to have access to.  These \"claims\"\ntranslate into permissions.  The more permissions you request, the less likely the user is going to accept\nthem.  Be a good net citizen and only request the information you are actually going to use.\n\n\n\n\nGoogle Configuration\n\n\nIt should be no shock that you need a \nGoogle Account\n to get started.  If you do not have one already (or you\nwant a different account for your development activities), create a new account now.  Then log in to the\n\nGoogle Developer Portal\n.  Click the \nCreate Project\n link at the top:\n\n\n\n\nEnter a nice name, then Click \nCreate\n.  The screen will show the progress and eventually the project\nwill be listed in the \nAll Projects\n list.  It takes about 30 seconds to create a project.  Once you have your\nGoogle project, the API Manager Library will show up.  If Google decides to change the end place, Click the hamburger menu (in the top-left corner), select \nAPI Manager\n, followed by \nLibrary\n to see all the Google APIs you can enable:\n\n\n\n\nThere is no \"Google Login\" that can guide you here.  The API you need to add is called \nGoogle+\n and is listed under the \nSocial APIs\n.  Click \nGoogle+ API\n, then click \nEnable\n at the top of the screen.\n\n\nAdding an API to a Google app doesn't mean it is ready to use.  In our case, we need to configure the login process.  Google has, of course, set this up so that it's easy to access a Google service, but difficult to use the authentication for other purposes.  Configure Google login as follows:\n\n\n\n\nClick \nCredentials\n in the left-hand menu.\n\n\n\n\nSelect the \nOAuth consent screen\n tab:\n\n\n\n\nFill in the form and Click \nSave\n.\n\n3.  \nCreate Credentials\n button.  This pops up a drop-down menu.  You want the \nOAuth Client ID\n.\n\n\n\n\n\n\n\n\nSelect \nWeb application\n.\n\n\n\n\n\n\n\n\nEnter the URL of your App Service in the \nAuthorized JavaScript origins\n box.  Ensure you use the https version of the URL.\n\n\n\n\nEnter \nhttps://_yoursite_.azurewebsites.net/.auth/login/google/callback\n in the \nAuthorized redirect URIs\n box.\n\n\nClick \nCreate\n.\n\n\nGoogle will display your client ID and client secret.\n\n\n\n\nYou can also view the Client ID and Client Secret from the interface by clicking on the \nCredentials\n link on the left-hand menu.\n\n\nThe process from here is practically the same as Facebook.  Open your App Service within the Azure Portal, Click\n\nAll Settings\n, then \nAuthentication / Authorization\n and finally \nGoogle\n (assuming you have already turned\non the authentication service).  Copy and paste the Client ID and Client Secret into the boxes provided.  Click\n\nOK\n (at the bottom) followed by \nSave\n (at the top of the page).\n\n\n\n\nInfo\n\n\nYou can define multiple providers at the same time.  The code in the client determines what authentication mechanism is used.\n\n\n\n\nYou can test this just like Facebook.  Go to https://\nyoursite\n/.auth/login/google with your browser.  You should see something like the following:\n\n\n\n\nConfirming here should get us to the same happy screen we achieved with Facebook.\n\n\nIf you happen to mis-type the Authorized redirect URI, Google will inform you that the URI is wrong.  I inevitably swap http for https.  When this happens, it is an easy fix, but you have to wait a few minutes before the authentication system updates itself.\n\n\n\n\nWarn\n\n\nGoogle has changed the security semantics for its authentication service.  You must use the v3.1.0 of the Azure Mobile Apps Client SDK for Server Flow authentication with Google to work.\n\n\n\n\nMicrosoft Account Configuration\n\n\nThe advantage of the Microsoft Account (or MSA, as it is known) is that you already have an account - you need \none for accessing Azure in general.  Go to the \nMicrosoft Account Developer Center\n and log on with \nyour Microsoft account.  You should use the same one as you use for Azure, but it is not required.\n\n\n\n\nJust to confuse us, there are two \nAdd an App\n buttons. Strangely, they are different. Click the one next to\n\nConverged applications\n.\n\n\n\n\nEnter an awesome name, then lick \nCreate application\n.\n\n\n\n\nClick \nAdd Platform\n, followed by \nWeb\n.  In the \nRedirect URIs\n, enter your app URL +\n\n/.auth/login/microsoftaccount/callback\n. Then Click \nSave\n.\n\n\n\n\nNow Click \nGenerate New Password\n under \nApplication Secrets\n.\n\n\n\n\nUnlike the other social providers, this is the only time you will get to see your client secret, so make a note of it or copy and paste it into a notepad.  Once you have it copied somewhere, Click \nOK\n, followed by \nSave\n.\n\n\nYou now have all the information you need to configure the Microsoft Account section within your App Server\nAuthentication / Authorization.  The Client ID you need to enter is the Application ID and the Client Secret is the\npassword you just copied somewhere.\n\n\n\n\nNote that you have to choose claims that you want to read.  The \nwl.basic\n and \nwl.emails\n will give you enough\ninformation to get started with this tutorial.\n\n\nClick \nOK\n (at the bottom), followed by \nSave\n (at the top).  You can test the settings by pointing your browser\nto https://\nyoursite\n.azurewebsites.net/.auth/login/microsoftaccount.  You will see what should be a normal claims\nrequest page:\n\n\n\n\nClicking on \nYes\n should take you to the normal success page.\n\n\nTwitter Configuration\n\n\nI hope you are seeing that all the OAuth providers take a very similar route to configuring their service.  The semantics of the service are slightly different in each case.  Twitter is no different.  As you might expect, before continuing, sign up for \nTwitter\n.  Once you have signed up, the \nTwitter Developers Portal\n is your next stop. Once there, click \nCreate New App\n:\n\n\n\n\nMost of the fields are self-explanatory.  The \nCallback URL\n is the same thing that the other social providers have\ncalled the Redirect URL.  The appropriate value is your app URL + \n/.auth/login/twitter/callback\n.  There is a legal\nagreement at the bottom of the page, then you can Click \nCreate your Twitter application\n button.\n\n\n\n\nDanger\n\n\nAll social authentication providers have some sort of legal agreement that governs their use.  In general, demo or\nPoC apps are fair use.  However, you should get a legal opinion before using a social authentication provider in a\nproduction app.\n\n\n\n\nOnce you have created the app, you will get a tabbed display with all the settings.  Click the \nKeys and Access\nTokens\n tab:\n\n\n\n\nNote the values for the \nConsumer Key (API Key)\n and \nConsumer Secret (API Secret)\n.  They get entered into the\nAzure Portal.\n\n\n\n\nWarn\n\n\nThere is a check box in the \nSettings\n tab that says \nAllow this application to be used to Sign in with Twitter\n.\nAt the time of writing, this is checked by default.  However, if you find you can not log in for some reason, then\nensure this checkbox is checked.\n\n\n\n\nBack in the Azure Portal, select your app service, then \nAll Settings\n, \nAuthentication / Authorization\n, and\nfinally \nTwitter\n (assuming you have already turned Authentication on).  You can now cut and paste the Consumer\nKey and Consumer Secret into the appropriate boxes, before clicking on \nOK\n (at the bottom) followed by \nSave\n\n(at the top).\n\n\nAs with the other providers, you should test the authentication flow by pointing your browser to\nhttps://\nyoursite\n.azurewebsites.net/.auth/login/twitter.\n\n\n\n\nClicking on \nAuthorize app\n should show you our normal successful authentication screen.\n\n\nThe social authentication providers should now all be configured to handle a web-based or server-flow authentication\nrequest.  There are times when configuring a client-flow authentication is different.  We will point those out when we\nget to them.\n\n\nAdding Authentication to a Mobile Client\n\n\nNow that the backend is completely configured, we can move our attention to the mobile client.  We are going to be\nusing the same mobile client that we developed in the first chapter, but we are now going to add authentication to\nit.  Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought\nof this and provided a facility for running platform specific code called the \nDependencyService\n.\n\n\n\n\nInfo\n\n\nIf you have already implemented authentication during the Enterprise Authentication section, this code is the\nsame.  You just have to alter the provider name.\n\n\n\n\nIf we run our application right now, clicking on the \"Enter the App\" button will result in an error.  You will be\nable to see the Unauthorized error in the debug window of Visual Studio.  Our first step is to define an\n\nAbstractions\\ILoginProvider.cs\n interface within the shared project:\n\n\nusing Microsoft.WindowsAzure.MobileServices;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        Task LoginAsync(MobileServiceClient client);\n    }\n}\n\n\n\n\nNext, we extend our \nAbstractions\\ICloudService.cs\n interface so that the main application can call the login routine:\n\n\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n\n        Task LoginAsync();\n    }\n}\n\n\n\n\nOur code will call \nLoginAsync()\n in the \nICloudService\n, which will get the platform-specific version of the\nlogin provider and call \nLoginAsync()\n there, but with our defined mobile service client.  That is defined in the\n\nServices\\AzureCloudService.cs\n class:\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing Xamarin.Forms;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n        }\n\n        public ICloudTable\nT\n GetTable\nT\n() where T : TableData =\n new AzureCloudTable\nT\n(client);\n\n        public Task LoginAsync()\n        {\n            var loginProvider = DependencyService.Get\nILoginProvider\n();\n            return loginProvider.LoginAsync(client);\n        }\n    }\n}\n\n\n\n\nThe method looks up the platform dependent version of the login provider and executes the login method, passing\nalong the client (which we will need later).\n\n\nIn each platform-specific project, we are going to define a concrete implementation of the login provider that uses\na web view to hold the actual authentication flow.  Here is the droid \nServices\\DroidLoginProvider.cs\n (in the\nTaskList.Droid project):\n\n\nusing System.Threading.Tasks;\nusing Android.Content;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(context, \nfacebook\n);\n        }\n    }\n}\n\n\n\n\n\n\nTip\n\n\nReplace \"facebook\" with \"google\", \"microsoftaccount\" or \"twitter\", depending on your identity provider.\n\n\n\n\nLet us take a closer look at this implementation.  The \nLoginAsync()\n method on the Azure Mobile Apps client object\ntakes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\",\n\"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service.  The\nclever piece is the \nXamarin.Forms.Dependency\n call at the top - that registers the class as a platform service\nso we can access it through the Xamarin dependency service.\n\n\nNote that we need an extra initialization routine for Android that must be called prior the login provider being\ncalled to pass along the main window of the app (also known as the context).  This is done in the \nMainActivity.cs\n\nfile \nafter\n the Xamarin Forms initialization call.  The dependency service is not set up until after the Xamarin\nForms library is initialized, so we will not be able to get the login provider reference before that point:\n\n\nprotected override void OnCreate(Bundle bundle)\n{\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n\n    ((DroidLoginProvider)DependencyService.Get\nILoginProvider\n()).Init(this);\n\n    LoadApplication(new App());\n}\n\n\n\n\niOS is similar, but does not require the initialization step in the main startup class.  The login provider class\nis in \nServices\\iOSLoginProvider.cs\n (in the \nTaskList.iOS\n project):\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(RootView, \nfacebook\n);\n        }\n\n        public UIViewController RootView =\n UIApplication.SharedApplication.KeyWindow.RootViewController;\n    }\n}\n\n\n\n\nNote that we are using the same pattern here for registering the concrete implementation with the dependency service,\nso we can get it the same way. Finally, here is the UWP \nServices\\UWPLoginProvider.cs\n (in the TaskList.UWP project):\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(\nfacebook\n);\n        }\n    }\n}\n\n\n\n\nNow that we have all the platform-specific login routines registered, we can move on to adding the login routine to\nthe UI.  We have already got a button on the entry page to enter the app.  It makes sense to wire up that button so\nthat it logs us in as well. The Command for the login button is in the \nViewModels\\EntryPageViewModel.cs\n:\n\n\nasync Task ExecuteLoginCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var cloudService = ServiceLocator.Instance.Resolve\nICloudService\n();\n        await cloudService.LoginAsync();\n        Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($\n[ExecuteLoginCommand] Error = {ex.Message}\n);\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}\n\n\n\n\n\n\nInfo\n\n\nThe \nServiceLocator\n class is my basic singleton handler.  It is available in the \nChapter2\n project.  It\nreturns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1.\n\n\n\n\nWhen you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window:\n\n\n\n\nGoing through the authentication process will get you to the task list again.  If the authentication process fails,\nthen \nLoginAsync()\n will throw an error, which is caught at the ViewModel.  Right now, the \nEntryPageViewModel\n\ndoes nothing more than print a diagnostic message to the debug window of Visual Studio.\n\n\n\n\nIntegrating with multiple providers\n\n\nEach provider has a slightly different server flow.  In the [Chapter2] project, I use this to call the \nright provider endpoint.  When the user clicks on the Facebook logo, for instance, it logs in with\nFacebook.  It's up to you as to how this UI is presented.\n\n\n\n\nClient-Flow for Social Providers\n\n\nIn each of the social providers, the identity provider SDK (provided by Facebook, Google, Microsoft or Twitter) \nwill need to be integrated.  In general, these SDKs are provided for a native platform (Objective-C or Swift \nfor iOS, Java for Android), use callbacks or delegates (as is common practice in native libraries) and are thus \nmore complicated to integrate with your mobile client than those that have a C#/.NET SDK delivered on NuGet.\n\n\n\n\nWarn\n\n\nTesting Client Flow for social providers requires that the social app be installed on the device.  You cannot\ninstall other apps on the iOS simulator and there may be restrictions on the Android Emulator.  This means that\nyou generally need to test client flow for social providers on an actual device.\n\n\n\n\nThe reward for doing so are a more integrated experience on mobile devices.  For example, if you integrate \nthe Google Play Services SDK in an Android app, the app will seamlessly authenticate itself with the \nconnected Google account in the background, avoiding the need for repeatedly authenticating the client.  It may\nask for a fingerprint instead if the app is not trusted.  If you integrate the Facebook SDK, then the app will \nautomatically switch to the Facebook app and ask you to approve the authentication request there instead of \nauthenticating the user through a web view.  Both of these provide a more integrated experience for the end user, \nso this work is well worth pursuing.\n\n\nAs an example, here is the client flow for Facebook.  I've implemented this using the \nXamarin.Facebook.iOS\n \nlibrary, which can be downloaded and installed into the iOS project from NuGet.  The \nServices\\iOSLoginProvider.cs\n contains the following:\n\n\n        #region Facebook Client Flow\n        private TaskCompletionSource\nstring\n fbtcs;\n\n        public async Task\nstring\n LoginFacebookAsync()\n        {\n            fbtcs = new TaskCompletionSource\nstring\n();\n            var loginManager = new LoginManager();\n\n            loginManager.LogInWithReadPermissions(new[] { \npublic_profile\n }, RootView, LoginTokenHandler);\n            return await fbtcs.Task;\n        }\n\n        private void LoginTokenHandler(LoginManagerLoginResult loginResult, NSError error)\n        {\n            if (loginResult.Token != null)\n            {\n                fbtcs.TrySetResult(loginResult.Token.TokenString);\n            }\n            else\n            {\n                fbtcs.TrySetException(new Exception(\nFacebook Client Flow Login Failed\n));\n            }\n        }\n        #endregion\n\n\n\n\nNote the use of a \nTaskCompletionSource\n()\n here.  This is used often to convert callback APIs into awaitable APIs.\nWe set off the async call with the callback, then await on the completion (which is signified by the\n\nTaskCompletionSource\n).  When the callback is called, it sets the value of the \nTaskCompletionSource\n (or causes\nan exception) and that causes the task to complete.\n\n\nThe \nLoginAsync()\n method can now be updated like this:\n\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            var accessToken = await LoginFacebookAsync();\n            var zumoPayload = new JObject()\n            {\n                [\naccess_token\n] = accessToken\n            };\n            await client.LoginAsync(\nfacebook\n, zumoPayload);\n        }\n\n        public UIViewController RootView =\n UIApplication.SharedApplication.KeyWindow.RootViewController;\n\n\n\n\nFinally, you need to configure your Facebook settings within the \nInfo.plist\n file.  Right-click the \nInfo.plist\n file and select \nOpen with...\n.  Select the XML editor.  Add the following to the file within the \ndict\n element (right before the closing \n/dict\n):\n\n\n    \nkey\nFacebookAppID\n/key\n\n    \nstring\nYOUR-APP-ID\n/string\n\n    \nkey\nLSApplicationQueriesSchemes\n/key\n\n    \narray\n\n        \nstring\nfbauth2\n/string\n\n    \n/array\n\n    \nkey\nCFBundleURLTypes\n/key\n\n    \narray\n\n        \ndict\n\n            \nkey\nCFBundleURLSchemes\n/key\n\n            \narray\n\n                \nstring\nfbYOUR-APP-ID\n/string\n\n            \n/array\n\n        \n/dict\n\n    \n/array\n\n\n\n\n\nReplace \nYOUR-APP-ID\n with the Facebook App ID from the Facebook developers console.\n\n\nWith this version, clicking on the login button will seamlessly switch into the Facebook application and ask the\nuser to confirm the request, before switching back authenticated.  Note that it's likely that the Facebook SDK \nwill not work within a simulator as it requires the Facebook app to be installed.\n\n\nThere are a number of pre-built Xamarin libraries for handling provider authentication.  For iOS, we have already shown Azure Active Directory and Facebook. Android has its own version of the \nFacebook SDK\n.  Use the \nGoogle .NET API\n to access Google accounts.  It's already cross-platform.  In general, using the library from the provider itself is preferable to using one from a third party.", 
            "title": "Social Authentication"
        }, 
        {
            "location": "/chapter2/social/#social-authentication", 
            "text": "Azure App Service provides built-in support for Facebook, Google, Microsoft and Twitter.  Irrespective of whether\nyou intend to use server-flow or client-flow, you will need to configure the Azure App Service Authentication /\nAuthorization service.  The method is pretty similar in each case:   Obtain a Developer Account for the provider.  Create a new application, obtaining a Client ID and Secret.  Turn on Azure App Service Authentication.  Enter the Client ID and Secret into the specific provider setup.  Save the configuration.   Before you start any of this, create a new Azure Mobile Apps as we described in  Chapter 1 .  If you want\na site to deploy for the configuration, the  Backend  project in the  Chapter2  solution is pre-configured for\nauthorization. You just need to deploy it to Azure App Service.", 
            "title": "Social Authentication"
        }, 
        {
            "location": "/chapter2/social/#facebook-configuration", 
            "text": "I am going to assume you have a Facebook account already.  If you do not have a Facebook account, go to  Facebook \nand sign up.  All your friends are likely there already!  Now log in to the  Facebook Developers  web site.  Create\na new Facebook application:   Note : Facebook updates the look and feel of their developer site on a regular basis.  As a result, the screen shots\nI have provided here may be different.  If in doubt, follow the bullet descriptions to find your way.   Info  If you are not already registered, Click the drop-down in the top-right corner and  Register as a Developer \nbefore continuing.    Click the  My Apps  link in the top right corner of the screen.  Click  Create a New App .  Fill in the form:      If required, verify your account according to the instructions.  This usually involves typing a CAPTCHA text, adding a credit card number and/or verifying your mobile phone number.    Click the  Get Started  button next to  Facebook Login .   Click  Settings  under  Facebook Login  in the left-hand menu.     Enter your application URL +  /.auth/login/facebook/callback  in the  Valid OAuth redirect URIs .     Click  Save Changes .  Click the  Settings  (under  Dashboard ) in the left-hand menu.  Click the  Show  button next to the App Secret   Now that you have the  App ID  and  App Secret , you can continue configuration of your app within the Azure Portal .   Open up your App Service by clicking on  All Resources  or  App Services  followed by the name of your app service.  In the  Settings  blade, Click  Authentication / Authorization  which is under  Features .  Turn  App Service Authentication  to  On .  In the  Action to take when request is not authenticated , select  Allow Anonymous requests (no action) .    Danger  It is very tempting to choose  Log in with Facebook .  However, you need to avoid this.  Selecting this option will mean that all requests need to be authenticated and you will not get the information about the identity on the back end.  Selecting  Allow Anonymous requests  means your app is in charge of what gets authenticated and what does not require authentication.    Click  Facebook  (which should show  Not Configured ).  Cut and Paste the  App ID  and  App Secret  into the boxes provided.  Select  public_profile  and  email  for Scopes.    Warn  If you request anything but public_profile, user_friends, and email, your app will need further review by\nFacebook, which will take time.  This process is not worth it for test apps like this one.    Click  OK  (at the bottom of the blade) to close the Facebook configuration blade.  Click  Save  (at the top of the blade) to save your Authentication changes.   You can test your authentication process by browsing to https:// yoursite .azurewebsites.net/.auth/login/facebook;\nthis is the same endpoint that the Azure Mobile Apps Client SDK calls when it is time to integrate authentication\ninto the mobile client.   If you are not logged in to facebook already, you will be prompted for your facebook credentials first.  Finally,\nhere is your happy page - the page that signifies you have done everything right:    Warn  Every single OAuth provider will ask you what sort of information you want to have access to.  These \"claims\"\ntranslate into permissions.  The more permissions you request, the less likely the user is going to accept\nthem.  Be a good net citizen and only request the information you are actually going to use.", 
            "title": "Facebook Configuration"
        }, 
        {
            "location": "/chapter2/social/#google-configuration", 
            "text": "It should be no shock that you need a  Google Account  to get started.  If you do not have one already (or you\nwant a different account for your development activities), create a new account now.  Then log in to the Google Developer Portal .  Click the  Create Project  link at the top:   Enter a nice name, then Click  Create .  The screen will show the progress and eventually the project\nwill be listed in the  All Projects  list.  It takes about 30 seconds to create a project.  Once you have your\nGoogle project, the API Manager Library will show up.  If Google decides to change the end place, Click the hamburger menu (in the top-left corner), select  API Manager , followed by  Library  to see all the Google APIs you can enable:   There is no \"Google Login\" that can guide you here.  The API you need to add is called  Google+  and is listed under the  Social APIs .  Click  Google+ API , then click  Enable  at the top of the screen.  Adding an API to a Google app doesn't mean it is ready to use.  In our case, we need to configure the login process.  Google has, of course, set this up so that it's easy to access a Google service, but difficult to use the authentication for other purposes.  Configure Google login as follows:   Click  Credentials  in the left-hand menu.   Select the  OAuth consent screen  tab:   Fill in the form and Click  Save . \n3.   Create Credentials  button.  This pops up a drop-down menu.  You want the  OAuth Client ID .     Select  Web application .     Enter the URL of your App Service in the  Authorized JavaScript origins  box.  Ensure you use the https version of the URL.   Enter  https://_yoursite_.azurewebsites.net/.auth/login/google/callback  in the  Authorized redirect URIs  box.  Click  Create .  Google will display your client ID and client secret.   You can also view the Client ID and Client Secret from the interface by clicking on the  Credentials  link on the left-hand menu.  The process from here is practically the same as Facebook.  Open your App Service within the Azure Portal, Click All Settings , then  Authentication / Authorization  and finally  Google  (assuming you have already turned\non the authentication service).  Copy and paste the Client ID and Client Secret into the boxes provided.  Click OK  (at the bottom) followed by  Save  (at the top of the page).   Info  You can define multiple providers at the same time.  The code in the client determines what authentication mechanism is used.   You can test this just like Facebook.  Go to https:// yoursite /.auth/login/google with your browser.  You should see something like the following:   Confirming here should get us to the same happy screen we achieved with Facebook.  If you happen to mis-type the Authorized redirect URI, Google will inform you that the URI is wrong.  I inevitably swap http for https.  When this happens, it is an easy fix, but you have to wait a few minutes before the authentication system updates itself.   Warn  Google has changed the security semantics for its authentication service.  You must use the v3.1.0 of the Azure Mobile Apps Client SDK for Server Flow authentication with Google to work.", 
            "title": "Google Configuration"
        }, 
        {
            "location": "/chapter2/social/#microsoft-account-configuration", 
            "text": "The advantage of the Microsoft Account (or MSA, as it is known) is that you already have an account - you need \none for accessing Azure in general.  Go to the  Microsoft Account Developer Center  and log on with \nyour Microsoft account.  You should use the same one as you use for Azure, but it is not required.   Just to confuse us, there are two  Add an App  buttons. Strangely, they are different. Click the one next to Converged applications .   Enter an awesome name, then lick  Create application .   Click  Add Platform , followed by  Web .  In the  Redirect URIs , enter your app URL + /.auth/login/microsoftaccount/callback . Then Click  Save .   Now Click  Generate New Password  under  Application Secrets .   Unlike the other social providers, this is the only time you will get to see your client secret, so make a note of it or copy and paste it into a notepad.  Once you have it copied somewhere, Click  OK , followed by  Save .  You now have all the information you need to configure the Microsoft Account section within your App Server\nAuthentication / Authorization.  The Client ID you need to enter is the Application ID and the Client Secret is the\npassword you just copied somewhere.   Note that you have to choose claims that you want to read.  The  wl.basic  and  wl.emails  will give you enough\ninformation to get started with this tutorial.  Click  OK  (at the bottom), followed by  Save  (at the top).  You can test the settings by pointing your browser\nto https:// yoursite .azurewebsites.net/.auth/login/microsoftaccount.  You will see what should be a normal claims\nrequest page:   Clicking on  Yes  should take you to the normal success page.", 
            "title": "Microsoft Account Configuration"
        }, 
        {
            "location": "/chapter2/social/#twitter-configuration", 
            "text": "I hope you are seeing that all the OAuth providers take a very similar route to configuring their service.  The semantics of the service are slightly different in each case.  Twitter is no different.  As you might expect, before continuing, sign up for  Twitter .  Once you have signed up, the  Twitter Developers Portal  is your next stop. Once there, click  Create New App :   Most of the fields are self-explanatory.  The  Callback URL  is the same thing that the other social providers have\ncalled the Redirect URL.  The appropriate value is your app URL +  /.auth/login/twitter/callback .  There is a legal\nagreement at the bottom of the page, then you can Click  Create your Twitter application  button.   Danger  All social authentication providers have some sort of legal agreement that governs their use.  In general, demo or\nPoC apps are fair use.  However, you should get a legal opinion before using a social authentication provider in a\nproduction app.   Once you have created the app, you will get a tabbed display with all the settings.  Click the  Keys and Access\nTokens  tab:   Note the values for the  Consumer Key (API Key)  and  Consumer Secret (API Secret) .  They get entered into the\nAzure Portal.   Warn  There is a check box in the  Settings  tab that says  Allow this application to be used to Sign in with Twitter .\nAt the time of writing, this is checked by default.  However, if you find you can not log in for some reason, then\nensure this checkbox is checked.   Back in the Azure Portal, select your app service, then  All Settings ,  Authentication / Authorization , and\nfinally  Twitter  (assuming you have already turned Authentication on).  You can now cut and paste the Consumer\nKey and Consumer Secret into the appropriate boxes, before clicking on  OK  (at the bottom) followed by  Save \n(at the top).  As with the other providers, you should test the authentication flow by pointing your browser to\nhttps:// yoursite .azurewebsites.net/.auth/login/twitter.   Clicking on  Authorize app  should show you our normal successful authentication screen.  The social authentication providers should now all be configured to handle a web-based or server-flow authentication\nrequest.  There are times when configuring a client-flow authentication is different.  We will point those out when we\nget to them.", 
            "title": "Twitter Configuration"
        }, 
        {
            "location": "/chapter2/social/#adding-authentication-to-a-mobile-client", 
            "text": "Now that the backend is completely configured, we can move our attention to the mobile client.  We are going to be\nusing the same mobile client that we developed in the first chapter, but we are now going to add authentication to\nit.  Web views are one of those items that are platform dependent. Fortunately for us, Xamarin has already thought\nof this and provided a facility for running platform specific code called the  DependencyService .   Info  If you have already implemented authentication during the Enterprise Authentication section, this code is the\nsame.  You just have to alter the provider name.   If we run our application right now, clicking on the \"Enter the App\" button will result in an error.  You will be\nable to see the Unauthorized error in the debug window of Visual Studio.  Our first step is to define an Abstractions\\ILoginProvider.cs  interface within the shared project:  using Microsoft.WindowsAzure.MobileServices;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        Task LoginAsync(MobileServiceClient client);\n    }\n}  Next, we extend our  Abstractions\\ICloudService.cs  interface so that the main application can call the login routine:  using System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n\n        Task LoginAsync();\n    }\n}  Our code will call  LoginAsync()  in the  ICloudService , which will get the platform-specific version of the\nlogin provider and call  LoginAsync()  there, but with our defined mobile service client.  That is defined in the Services\\AzureCloudService.cs  class:  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing Xamarin.Forms;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudService : ICloudService\n    {\n        MobileServiceClient client;\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n        }\n\n        public ICloudTable T  GetTable T () where T : TableData =  new AzureCloudTable T (client);\n\n        public Task LoginAsync()\n        {\n            var loginProvider = DependencyService.Get ILoginProvider ();\n            return loginProvider.LoginAsync(client);\n        }\n    }\n}  The method looks up the platform dependent version of the login provider and executes the login method, passing\nalong the client (which we will need later).  In each platform-specific project, we are going to define a concrete implementation of the login provider that uses\na web view to hold the actual authentication flow.  Here is the droid  Services\\DroidLoginProvider.cs  (in the\nTaskList.Droid project):  using System.Threading.Tasks;\nusing Android.Content;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        Context context;\n\n        public void Init(Context context)\n        {\n            this.context = context;\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(context,  facebook );\n        }\n    }\n}   Tip  Replace \"facebook\" with \"google\", \"microsoftaccount\" or \"twitter\", depending on your identity provider.   Let us take a closer look at this implementation.  The  LoginAsync()  method on the Azure Mobile Apps client object\ntakes the Android context (which is normally the main window) and a provider - we can pick any of \"facebook\",\n\"google\", \"microsoftaccount\", \"twitter\" or \"aad\" depending on what we have defined in the Azure App Service.  The\nclever piece is the  Xamarin.Forms.Dependency  call at the top - that registers the class as a platform service\nso we can access it through the Xamarin dependency service.  Note that we need an extra initialization routine for Android that must be called prior the login provider being\ncalled to pass along the main window of the app (also known as the context).  This is done in the  MainActivity.cs \nfile  after  the Xamarin Forms initialization call.  The dependency service is not set up until after the Xamarin\nForms library is initialized, so we will not be able to get the login provider reference before that point:  protected override void OnCreate(Bundle bundle)\n{\n    base.OnCreate(bundle);\n\n    Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n    global::Xamarin.Forms.Forms.Init(this, bundle);\n\n    ((DroidLoginProvider)DependencyService.Get ILoginProvider ()).Init(this);\n\n    LoadApplication(new App());\n}  iOS is similar, but does not require the initialization step in the main startup class.  The login provider class\nis in  Services\\iOSLoginProvider.cs  (in the  TaskList.iOS  project):  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.iOS.Services;\nusing UIKit;\n\n[assembly: Xamarin.Forms.Dependency(typeof(iOSLoginProvider))]\nnamespace TaskList.iOS.Services\n{\n    public class iOSLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync(RootView,  facebook );\n        }\n\n        public UIViewController RootView =  UIApplication.SharedApplication.KeyWindow.RootViewController;\n    }\n}  Note that we are using the same pattern here for registering the concrete implementation with the dependency service,\nso we can get it the same way. Finally, here is the UWP  Services\\UWPLoginProvider.cs  (in the TaskList.UWP project):  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\nusing TaskList.UWP.Services;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            await client.LoginAsync( facebook );\n        }\n    }\n}  Now that we have all the platform-specific login routines registered, we can move on to adding the login routine to\nthe UI.  We have already got a button on the entry page to enter the app.  It makes sense to wire up that button so\nthat it logs us in as well. The Command for the login button is in the  ViewModels\\EntryPageViewModel.cs :  async Task ExecuteLoginCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var cloudService = ServiceLocator.Instance.Resolve ICloudService ();\n        await cloudService.LoginAsync();\n        Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($ [ExecuteLoginCommand] Error = {ex.Message} );\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}   Info  The  ServiceLocator  class is my basic singleton handler.  It is available in the  Chapter2  project.  It\nreturns the concrete version of the cloud service, just like the Singleton version we defined in Chapter1.   When you run the application, clicking on the \"Enter the App\" button will now present you with an Authenticate window:   Going through the authentication process will get you to the task list again.  If the authentication process fails,\nthen  LoginAsync()  will throw an error, which is caught at the ViewModel.  Right now, the  EntryPageViewModel \ndoes nothing more than print a diagnostic message to the debug window of Visual Studio.   Integrating with multiple providers  Each provider has a slightly different server flow.  In the [Chapter2] project, I use this to call the \nright provider endpoint.  When the user clicks on the Facebook logo, for instance, it logs in with\nFacebook.  It's up to you as to how this UI is presented.", 
            "title": "Adding Authentication to a Mobile Client"
        }, 
        {
            "location": "/chapter2/social/#client-flow-for-social-providers", 
            "text": "In each of the social providers, the identity provider SDK (provided by Facebook, Google, Microsoft or Twitter) \nwill need to be integrated.  In general, these SDKs are provided for a native platform (Objective-C or Swift \nfor iOS, Java for Android), use callbacks or delegates (as is common practice in native libraries) and are thus \nmore complicated to integrate with your mobile client than those that have a C#/.NET SDK delivered on NuGet.   Warn  Testing Client Flow for social providers requires that the social app be installed on the device.  You cannot\ninstall other apps on the iOS simulator and there may be restrictions on the Android Emulator.  This means that\nyou generally need to test client flow for social providers on an actual device.   The reward for doing so are a more integrated experience on mobile devices.  For example, if you integrate \nthe Google Play Services SDK in an Android app, the app will seamlessly authenticate itself with the \nconnected Google account in the background, avoiding the need for repeatedly authenticating the client.  It may\nask for a fingerprint instead if the app is not trusted.  If you integrate the Facebook SDK, then the app will \nautomatically switch to the Facebook app and ask you to approve the authentication request there instead of \nauthenticating the user through a web view.  Both of these provide a more integrated experience for the end user, \nso this work is well worth pursuing.  As an example, here is the client flow for Facebook.  I've implemented this using the  Xamarin.Facebook.iOS  \nlibrary, which can be downloaded and installed into the iOS project from NuGet.  The  Services\\iOSLoginProvider.cs  contains the following:          #region Facebook Client Flow\n        private TaskCompletionSource string  fbtcs;\n\n        public async Task string  LoginFacebookAsync()\n        {\n            fbtcs = new TaskCompletionSource string ();\n            var loginManager = new LoginManager();\n\n            loginManager.LogInWithReadPermissions(new[] {  public_profile  }, RootView, LoginTokenHandler);\n            return await fbtcs.Task;\n        }\n\n        private void LoginTokenHandler(LoginManagerLoginResult loginResult, NSError error)\n        {\n            if (loginResult.Token != null)\n            {\n                fbtcs.TrySetResult(loginResult.Token.TokenString);\n            }\n            else\n            {\n                fbtcs.TrySetException(new Exception( Facebook Client Flow Login Failed ));\n            }\n        }\n        #endregion  Note the use of a  TaskCompletionSource ()  here.  This is used often to convert callback APIs into awaitable APIs.\nWe set off the async call with the callback, then await on the completion (which is signified by the TaskCompletionSource ).  When the callback is called, it sets the value of the  TaskCompletionSource  (or causes\nan exception) and that causes the task to complete.  The  LoginAsync()  method can now be updated like this:          public async Task LoginAsync(MobileServiceClient client)\n        {\n            var accessToken = await LoginFacebookAsync();\n            var zumoPayload = new JObject()\n            {\n                [ access_token ] = accessToken\n            };\n            await client.LoginAsync( facebook , zumoPayload);\n        }\n\n        public UIViewController RootView =  UIApplication.SharedApplication.KeyWindow.RootViewController;  Finally, you need to configure your Facebook settings within the  Info.plist  file.  Right-click the  Info.plist  file and select  Open with... .  Select the XML editor.  Add the following to the file within the  dict  element (right before the closing  /dict ):       key FacebookAppID /key \n     string YOUR-APP-ID /string \n     key LSApplicationQueriesSchemes /key \n     array \n         string fbauth2 /string \n     /array \n     key CFBundleURLTypes /key \n     array \n         dict \n             key CFBundleURLSchemes /key \n             array \n                 string fbYOUR-APP-ID /string \n             /array \n         /dict \n     /array   Replace  YOUR-APP-ID  with the Facebook App ID from the Facebook developers console.  With this version, clicking on the login button will seamlessly switch into the Facebook application and ask the\nuser to confirm the request, before switching back authenticated.  Note that it's likely that the Facebook SDK \nwill not work within a simulator as it requires the Facebook app to be installed.  There are a number of pre-built Xamarin libraries for handling provider authentication.  For iOS, we have already shown Azure Active Directory and Facebook. Android has its own version of the  Facebook SDK .  Use the  Google .NET API  to access Google accounts.  It's already cross-platform.  In general, using the library from the provider itself is preferable to using one from a third party.", 
            "title": "Client-Flow for Social Providers"
        }, 
        {
            "location": "/chapter2/debugging/", 
            "text": "What is in a JWT\n\n\nAt this point you will have the \"Authentication Success\" screen - perhaps several times.  If you bring up the\nDeveloper Tools for your browser, you can take a look at the token that is being minted for the authentication\nsession.  Take a look at the URL on the \"successful authentication\" page.\n\n\n\n\nThe authentication token is clearly marked (after you strip away the URL encoding).  You can use a\n\nURL Decoder / Encoder\n - just cut and paste the entire URL into the box and click on \nDecode\n.  Note that\nthe token is actually a JSON object.  You can now easily extract the \nauthenticationToken\n field from the JSON\nobject.\n\n\n\n\nTechnically, the authentication token is a \nJSON Web Token\n.  This is a mechanism for transferring claims\nbetween two systems securely.  The JWT is a cryptographically signed JSON object.  You can decode the JWT using\nthe \njwt.io tool\n.  Cut and paste the authentication token into the \nEncoded\n box and it will be decoded.\n\n\n\n\nNote that the contents of the JWT are revealed even without knowing the secret.  The client secret is generated\n(or entered as part of a configuration) at the identity provider and is used to cryptographically sign the token.\nYou will need the client secret to verify the signature of the token.  The client secret is copied from the identity\nprovider to the resource (in this case, the App Service).\n\n\nWe can see other items within the token.  The \nIssuer\n is the place that issued the token.  This is generally \na URI.  The \nAudience\n is an identifier for who the token is for.  In this case, we have a token minted by \nthe Azure App Service for use accessing that same App Service, so the issuer and audience are both the \nURI.  If you look at an Auth0 token, you will see that the issuer is the Auth0 domain and the audience is \nthe Client ID of the Auth0 tenant.\n\n\nEach token will also have a number of claims.  The most common claim is the \nSubject\n of the token.  This \nis generally a security ID, but could be any unique user ID.\n\n\n\n\nInfo\n\n\nAzure App Service sets the subject to a stable SID.  The stable SID is unique to the identity provider that is\nused for the authentication and guaranteed not to change, even if the user changes their email address \nor username on the underlying identity provider.\n\n\n\n\nThe JWT can include any data from the identity provider and there are some identity providers that place just \nabout everything about the user in the JWT.  App Service keeps the amount of data small because the client will \nbe sending the JWT with every request. Imagine adding a few kilobytes to every single request that the client \nmakes.  The bandwidth usage will add up quickly, and your app will be known as a bandwidth hog.\n\n\nHowever, there are some fields that are pretty universal.  Your JWT should always have the following fields:\n\n\n\n\nsub = Subject (the identifier for the token)\n\n\nexp = Expiry (when the token expires)\n\n\nnbf = Not Before (the earliest point in time the token is valid)\n\n\niss = Issuer (the site that issued the token)\n\n\naud = Audience (who is the token for)\n\n\n\n\nThe timestamps (exp and nbf) are all UNIX timestamps (i.e. the number of seconds since January 1, 1970).\n\n\nApp Service adds to this:\n\n\n\n\nstable_sid = Security Id of the user\n\n\nidp = the IdP that was used in the authentication request\n\n\nver = the Version of the token\n\n\n\n\nApp Service will be able to validate any token provided to it when presented in an X-ZUMO-AUTH header.  If you are\nusing Azure Active Directory, you can also use the more standard Bearer Authorization header.  If the token does\nnot match, then the X-ZUMO-AUTH header will be stripped from the request before the request is passed to your site.\n\n\nTesting Authentication without a Client\n\n\nTesting authentication against your App Service without a client requires a REST client.  I use \nPostman\n,\nwhich is based on Google Chrome.  If you use Firefox, you might want to take a look at \nRESTClient\n.  Telerik\nalso distributes a web debugging proxy called \nFiddler\n that can do API testing.  To test the server, we will\nneed a token.  We can get one by testing authentication configuration by pointing the browser to \n/.auth/login/aad\n.\nThe return URL will contain a token in the query string and as \na secure cookie\n.\n\n\n\n\nTip\n\n\nYou can test any of the supported identity providers by replacing \naad\n with the authentication provider name:\n\nfacebook\n, \ngoogle\n, \nmicrosoftaccount\n and \ntwitter\n are possibilities here.\n\n\n\n\nWe can then do a request to \n/tables/todoitem\n to try and obtain the list of current tasks.  We will need to add\ntwo headers:\n\n\n\n\nZUMO-API-VERSION\n should contain a value of \n2.0.0\n.\n\n\nX-ZUMO-AUTH\n should contain the token you received.\n\n\n\n\nMy first request shows authentication failing:\n\n\n\n\nGo through one of the authentication flows and copy the authentication token.  In Postman, add a new header called\n\nX-ZUMO-AUTH\n and paste the authentication token in.\n\n\n\n\nNote that we have tested all this without touching the client.  Separating the backend operations from the client\noperations means we can be sure of where the inevitable bug that creeps in is located.  We have verified that we\ncan do each authentication flow on the server side and that the server is properly rejecting unauthenticated requests,\nplus it is properly returning data when authenticated requests are issued.\n\n\nDeveloping Locally\n\n\nOne would normally be able to run the ASP.NET backend locally and get full functionality without authentication.\nHowever, authentication puts a stop to that because the redirect URLs, secrets and other authentication configuration\nsettings only work with a known endpoint.  To alleviate that, Azure Mobile Apps allows you to run a local server while\nusing an authentication endpoint in Azure App Service.  When the authentication transaction takes place, it is taking\nplace against the Azure App Service.   When it is not doing the OAuth transaction, however, it is operating against a\nlocal server.\n\n\nSetting this up requires a little bit of local machine configuration and a change to the configuration of your client.\n\n\nUpdate your Local Development Environment\n\n\nThe first step in this process is to make your local IIS development environment look more like the Azure App Service,\nparticularly in reference to the authentication settings.  This means setting up a few app settings that should be\npulled from your App Service.\n\n\n\n\nLog on to the \nAzure Portal\n.\n\n\nSelect your App Service from the \nApp Services\n list.\n\n\nClick on \nTools\n, then \nAdvanced Tools\n, then \nGo\n.\n\n\n\n\nKudu (now known as \nAdvanced Tools\n in the Azure portal menu) is the backend debug console for Azure App Service and\nthere is a lot you can do here.  Of note in this instance is that you can gain access to the keys and audience for your\nApp Service.\n\n\n\n\nClick on \nEnvironment\n in the top banner.\n\n\nClick on \nEnvironment variables\n.\n\n\nScroll down to the environment variables starting with \nWEBSITE_AUTH\n.\n\n\nMake a note of the \nWEBSITE_AUTH_SIGNING_KEY\n and \nWEBSITE_HOSTNAME\n values.\n\n\n\n\n\n\nWEBSITE_AUTH_ALLOWED_AUDIENCES Setting\n\n\nThe \nWEBSITE_AUTH_ALLOWED_AUDIENCES\n setting may be seen.  This is set only in the case of Azure Active\nDirectory and may not be present (or not valid) if you configure other providers.\n\n\n\n\nAdd the following to your project Web.config \nappSettings\n section:\n\n\n  \nappSettings\n\n    \nadd key=\nPreserveLoginUrl\n value=\ntrue\n /\n\n    \nadd key=\nMS_SigningKey\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nEMA_RuntimeUrl\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nMS_NotificationHubName\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nSigningKey\n value=\n{Your WEBSITE_AUTH_SIGNING_KEY}\n/\n\n    \nadd key=\nValidAudience\n value=\nhttps://{Your WEBSITE_HOSTNAME}/\n/\n\n    \nadd key=\nValidIssuer\n value=\nhttps://{Your WEBSITE_HOSTNAME}/\n/\n\n  \n/appSettings\n\n\n\n\n\n\n\nTip\n\n\nBoth the ValidAudience and ValidIssuer will have a slash on the end and be a https URL.\n\n\n\n\nThe last three keys are the keys you will need to add.  Make sure you do not have a \nHostName\n key as this is how\nthe startup file determines if you are running locally or remote. Talking of which, edit your\n\nApp_Start\\Startup.MobileApp.cs\n file to include the following:\n\n\n        public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            HttpConfiguration config = new HttpConfiguration();\n\n            new MobileAppConfiguration()\n                .AddTablesWithEntityFramework()\n                .ApplyTo(config);\n\n            // Use Entity Framework Code First to create database tables based on your DbContext\n            Database.SetInitializer(new MobileServiceInitializer());\n\n            MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n            if (string.IsNullOrEmpty(settings.HostName))\n            {\n                app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n                {\n                    SigningKey = ConfigurationManager.AppSettings[\nSigningKey\n],\n                    ValidAudiences = new[] { ConfigurationManager.AppSettings[\nValidAudience\n] },\n                    ValidIssuers = new[] { ConfigurationManager.AppSettings[\nValidIssuer\n] },\n                    TokenHandler = config.GetAppServiceTokenHandler()\n                });\n            }\n\n            app.UseWebApi(config);\n        }\n\n\n\n\nThe \nUserAppServiceAuthentication()\n method sets up authentication checking.  This section is not required when running\nwithin App Service.\n\n\nIf you are running the server locally, you should either set up a local SQL Server instance and put the connection\nstring into the \nWeb.config\n file, or \nopen the firewall on your SQL Azure\n database so that your local development\nenvironment can connect to it, then place the connection string in the \nWeb.config\n.  You can get the connection string\nof the SQL Azure instance by looking at the Connection Strings in the \nApplication properties\n of your App Service.\n\n\nUpdate your Mobile Client\n\n\nFor this demonstration, I have updated the \nTaskList.UWP\n application so that it is using the server-flow\nauthentication for Azure Active Directory.  This means updating the \nLoginAsync()\n method in the\n\nServices\\UWPLoginProvider.cs\n file to be the following:\n\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Server-Flow Version\n            await client.LoginAsync(\naad\n);\n        }\n\n\n\n\nThis is because the default local IIS instance is IIS Express.  IIS Express only listens for local connections.  If\nyou run a client from another device (for example, the Android emulator on a Hyper-V service or the iOS simulator\non a Mac), then that client would be connecting via a network connection.  You can still debug locally, but you need\nto \nconvert your environment to IIS\n first.\n\n\nIn the \nTaskList (Portable)\n project, update the \nHelpers\\Locations.cs\n file:\n\n\nnamespace TaskList.Helpers\n{\n    public static class Locations\n    {\n#if DEBUG\n        public static readonly string AppServiceUrl = \nhttp://localhost:17568/\n;\n        public static readonly string AlternateLoginHost = \nhttps://the-book.azurewebsites.net\n;\n#else\n        public static readonly string AppServiceUrl = \nhttps://the-book.azurewebsites.net\n;\n        public static readonly string AlternateLoginHost = null;\n#endif\n    }\n}\n\n\n\n\nThe \nAppServiceUrl\n is always set to the location of your backend.  In this case, I right-clicked on the \nBackend\n\nproject and selected \nProperties\n then \nWeb\n.  The correct URL for local debugging is listed in the\n\nProject URL\n.  The \nAlternateLoginHost\n is set to the App Service when locally debugging or null if not. You can\nspecify the \nDEBUG\n constant in the \nBuild\n tab.\n\n\nIn the same project, update the \nServices\\AzureCloudService.cs\n constructor to the following:\n\n\n        public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n            if (Locations.AlternateLoginHost != null)\n                client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n        }\n\n\n\n\n\n\nTip\n\n\nIt's a good idea to separate the client and server into different solutions.  Although it doesn't hurt anything to\nhave them in the same solution (like we have), having the client and server separated allows you to attach a debugger\nseparately - which allows you to debug both sides of the connection at the same time.\n\n\n\n\nWith these settings, the client will contact the AlternateLoginHost listed for the authentication process and then\ncontact the local server for the rest of the transaction.\n\n\nRun the Local Server\n\n\nRunning the local server and the client takes a larger machine.  You need to run two instances of Visual Studio: one\nfor the client and one for the server. This is really where you will appreciate multiple monitors (my personal favorite)\nor the snap action to the sides of the screens.\n\n\nEnsure you have your backend and clients in different solutions if you intend to run both client and server.  The\ndebugger in Visual Studio will stop one to run the other when they are in the same solution.", 
            "title": "Debugging Authentication"
        }, 
        {
            "location": "/chapter2/debugging/#what-is-in-a-jwt", 
            "text": "At this point you will have the \"Authentication Success\" screen - perhaps several times.  If you bring up the\nDeveloper Tools for your browser, you can take a look at the token that is being minted for the authentication\nsession.  Take a look at the URL on the \"successful authentication\" page.   The authentication token is clearly marked (after you strip away the URL encoding).  You can use a URL Decoder / Encoder  - just cut and paste the entire URL into the box and click on  Decode .  Note that\nthe token is actually a JSON object.  You can now easily extract the  authenticationToken  field from the JSON\nobject.   Technically, the authentication token is a  JSON Web Token .  This is a mechanism for transferring claims\nbetween two systems securely.  The JWT is a cryptographically signed JSON object.  You can decode the JWT using\nthe  jwt.io tool .  Cut and paste the authentication token into the  Encoded  box and it will be decoded.   Note that the contents of the JWT are revealed even without knowing the secret.  The client secret is generated\n(or entered as part of a configuration) at the identity provider and is used to cryptographically sign the token.\nYou will need the client secret to verify the signature of the token.  The client secret is copied from the identity\nprovider to the resource (in this case, the App Service).  We can see other items within the token.  The  Issuer  is the place that issued the token.  This is generally \na URI.  The  Audience  is an identifier for who the token is for.  In this case, we have a token minted by \nthe Azure App Service for use accessing that same App Service, so the issuer and audience are both the \nURI.  If you look at an Auth0 token, you will see that the issuer is the Auth0 domain and the audience is \nthe Client ID of the Auth0 tenant.  Each token will also have a number of claims.  The most common claim is the  Subject  of the token.  This \nis generally a security ID, but could be any unique user ID.   Info  Azure App Service sets the subject to a stable SID.  The stable SID is unique to the identity provider that is\nused for the authentication and guaranteed not to change, even if the user changes their email address \nor username on the underlying identity provider.   The JWT can include any data from the identity provider and there are some identity providers that place just \nabout everything about the user in the JWT.  App Service keeps the amount of data small because the client will \nbe sending the JWT with every request. Imagine adding a few kilobytes to every single request that the client \nmakes.  The bandwidth usage will add up quickly, and your app will be known as a bandwidth hog.  However, there are some fields that are pretty universal.  Your JWT should always have the following fields:   sub = Subject (the identifier for the token)  exp = Expiry (when the token expires)  nbf = Not Before (the earliest point in time the token is valid)  iss = Issuer (the site that issued the token)  aud = Audience (who is the token for)   The timestamps (exp and nbf) are all UNIX timestamps (i.e. the number of seconds since January 1, 1970).  App Service adds to this:   stable_sid = Security Id of the user  idp = the IdP that was used in the authentication request  ver = the Version of the token   App Service will be able to validate any token provided to it when presented in an X-ZUMO-AUTH header.  If you are\nusing Azure Active Directory, you can also use the more standard Bearer Authorization header.  If the token does\nnot match, then the X-ZUMO-AUTH header will be stripped from the request before the request is passed to your site.", 
            "title": "What is in a JWT"
        }, 
        {
            "location": "/chapter2/debugging/#testing-authentication-without-a-client", 
            "text": "Testing authentication against your App Service without a client requires a REST client.  I use  Postman ,\nwhich is based on Google Chrome.  If you use Firefox, you might want to take a look at  RESTClient .  Telerik\nalso distributes a web debugging proxy called  Fiddler  that can do API testing.  To test the server, we will\nneed a token.  We can get one by testing authentication configuration by pointing the browser to  /.auth/login/aad .\nThe return URL will contain a token in the query string and as  a secure cookie .   Tip  You can test any of the supported identity providers by replacing  aad  with the authentication provider name: facebook ,  google ,  microsoftaccount  and  twitter  are possibilities here.   We can then do a request to  /tables/todoitem  to try and obtain the list of current tasks.  We will need to add\ntwo headers:   ZUMO-API-VERSION  should contain a value of  2.0.0 .  X-ZUMO-AUTH  should contain the token you received.   My first request shows authentication failing:   Go through one of the authentication flows and copy the authentication token.  In Postman, add a new header called X-ZUMO-AUTH  and paste the authentication token in.   Note that we have tested all this without touching the client.  Separating the backend operations from the client\noperations means we can be sure of where the inevitable bug that creeps in is located.  We have verified that we\ncan do each authentication flow on the server side and that the server is properly rejecting unauthenticated requests,\nplus it is properly returning data when authenticated requests are issued.", 
            "title": "Testing Authentication without a Client"
        }, 
        {
            "location": "/chapter2/debugging/#developing-locally", 
            "text": "One would normally be able to run the ASP.NET backend locally and get full functionality without authentication.\nHowever, authentication puts a stop to that because the redirect URLs, secrets and other authentication configuration\nsettings only work with a known endpoint.  To alleviate that, Azure Mobile Apps allows you to run a local server while\nusing an authentication endpoint in Azure App Service.  When the authentication transaction takes place, it is taking\nplace against the Azure App Service.   When it is not doing the OAuth transaction, however, it is operating against a\nlocal server.  Setting this up requires a little bit of local machine configuration and a change to the configuration of your client.", 
            "title": "Developing Locally"
        }, 
        {
            "location": "/chapter2/debugging/#update-your-local-development-environment", 
            "text": "The first step in this process is to make your local IIS development environment look more like the Azure App Service,\nparticularly in reference to the authentication settings.  This means setting up a few app settings that should be\npulled from your App Service.   Log on to the  Azure Portal .  Select your App Service from the  App Services  list.  Click on  Tools , then  Advanced Tools , then  Go .   Kudu (now known as  Advanced Tools  in the Azure portal menu) is the backend debug console for Azure App Service and\nthere is a lot you can do here.  Of note in this instance is that you can gain access to the keys and audience for your\nApp Service.   Click on  Environment  in the top banner.  Click on  Environment variables .  Scroll down to the environment variables starting with  WEBSITE_AUTH .  Make a note of the  WEBSITE_AUTH_SIGNING_KEY  and  WEBSITE_HOSTNAME  values.    WEBSITE_AUTH_ALLOWED_AUDIENCES Setting  The  WEBSITE_AUTH_ALLOWED_AUDIENCES  setting may be seen.  This is set only in the case of Azure Active\nDirectory and may not be present (or not valid) if you configure other providers.   Add the following to your project Web.config  appSettings  section:     appSettings \n     add key= PreserveLoginUrl  value= true  / \n     add key= MS_SigningKey  value= Overridden by portal settings  / \n     add key= EMA_RuntimeUrl  value= Overridden by portal settings  / \n     add key= MS_NotificationHubName  value= Overridden by portal settings  / \n     add key= SigningKey  value= {Your WEBSITE_AUTH_SIGNING_KEY} / \n     add key= ValidAudience  value= https://{Your WEBSITE_HOSTNAME}/ / \n     add key= ValidIssuer  value= https://{Your WEBSITE_HOSTNAME}/ / \n   /appSettings    Tip  Both the ValidAudience and ValidIssuer will have a slash on the end and be a https URL.   The last three keys are the keys you will need to add.  Make sure you do not have a  HostName  key as this is how\nthe startup file determines if you are running locally or remote. Talking of which, edit your App_Start\\Startup.MobileApp.cs  file to include the following:          public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            HttpConfiguration config = new HttpConfiguration();\n\n            new MobileAppConfiguration()\n                .AddTablesWithEntityFramework()\n                .ApplyTo(config);\n\n            // Use Entity Framework Code First to create database tables based on your DbContext\n            Database.SetInitializer(new MobileServiceInitializer());\n\n            MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n            if (string.IsNullOrEmpty(settings.HostName))\n            {\n                app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n                {\n                    SigningKey = ConfigurationManager.AppSettings[ SigningKey ],\n                    ValidAudiences = new[] { ConfigurationManager.AppSettings[ ValidAudience ] },\n                    ValidIssuers = new[] { ConfigurationManager.AppSettings[ ValidIssuer ] },\n                    TokenHandler = config.GetAppServiceTokenHandler()\n                });\n            }\n\n            app.UseWebApi(config);\n        }  The  UserAppServiceAuthentication()  method sets up authentication checking.  This section is not required when running\nwithin App Service.  If you are running the server locally, you should either set up a local SQL Server instance and put the connection\nstring into the  Web.config  file, or  open the firewall on your SQL Azure  database so that your local development\nenvironment can connect to it, then place the connection string in the  Web.config .  You can get the connection string\nof the SQL Azure instance by looking at the Connection Strings in the  Application properties  of your App Service.", 
            "title": "Update your Local Development Environment"
        }, 
        {
            "location": "/chapter2/debugging/#update-your-mobile-client", 
            "text": "For this demonstration, I have updated the  TaskList.UWP  application so that it is using the server-flow\nauthentication for Azure Active Directory.  This means updating the  LoginAsync()  method in the Services\\UWPLoginProvider.cs  file to be the following:          public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Server-Flow Version\n            await client.LoginAsync( aad );\n        }  This is because the default local IIS instance is IIS Express.  IIS Express only listens for local connections.  If\nyou run a client from another device (for example, the Android emulator on a Hyper-V service or the iOS simulator\non a Mac), then that client would be connecting via a network connection.  You can still debug locally, but you need\nto  convert your environment to IIS  first.  In the  TaskList (Portable)  project, update the  Helpers\\Locations.cs  file:  namespace TaskList.Helpers\n{\n    public static class Locations\n    {\n#if DEBUG\n        public static readonly string AppServiceUrl =  http://localhost:17568/ ;\n        public static readonly string AlternateLoginHost =  https://the-book.azurewebsites.net ;\n#else\n        public static readonly string AppServiceUrl =  https://the-book.azurewebsites.net ;\n        public static readonly string AlternateLoginHost = null;\n#endif\n    }\n}  The  AppServiceUrl  is always set to the location of your backend.  In this case, I right-clicked on the  Backend \nproject and selected  Properties  then  Web .  The correct URL for local debugging is listed in the Project URL .  The  AlternateLoginHost  is set to the App Service when locally debugging or null if not. You can\nspecify the  DEBUG  constant in the  Build  tab.  In the same project, update the  Services\\AzureCloudService.cs  constructor to the following:          public AzureCloudService()\n        {\n            client = new MobileServiceClient(Locations.AppServiceUrl);\n            if (Locations.AlternateLoginHost != null)\n                client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n        }   Tip  It's a good idea to separate the client and server into different solutions.  Although it doesn't hurt anything to\nhave them in the same solution (like we have), having the client and server separated allows you to attach a debugger\nseparately - which allows you to debug both sides of the connection at the same time.   With these settings, the client will contact the AlternateLoginHost listed for the authentication process and then\ncontact the local server for the rest of the transaction.", 
            "title": "Update your Mobile Client"
        }, 
        {
            "location": "/chapter2/debugging/#run-the-local-server", 
            "text": "Running the local server and the client takes a larger machine.  You need to run two instances of Visual Studio: one\nfor the client and one for the server. This is really where you will appreciate multiple monitors (my personal favorite)\nor the snap action to the sides of the screens.  Ensure you have your backend and clients in different solutions if you intend to run both client and server.  The\ndebugger in Visual Studio will stop one to run the other when they are in the same solution.", 
            "title": "Run the Local Server"
        }, 
        {
            "location": "/chapter2/custom/", 
            "text": "Custom authentication\n\n\nFor some situations, the social or enterprise flows are not valid for the mobile client.  Perhaps you want the ability\nto provide a sign-up process with a username and password rather than using a social provider.  Perhaps you want to use\nan alternate provider that is not one of the supported five providers.  Whatever the reason, Azure App Service provides\nthe ability to handle all situations.  In this section, I will look at three methods for providing a unique set of\nusernames with no connection to the social or enterprise authentication.\n\n\nUsing an Identity Database.\n\n\nProbably the most common request is to use a custom identity database.  In general, this is desirable because you\nalready have a database of usernames and password.  However, it's probably the least desirable option because of the\nsecurity concerns that come along with this technique.  The news is rife with password leakage for very large\norganizations.  The best way to ensure you do not disclose a users password is to not have it in the first place.\n\n\n\n\nWarn\n\n\nI'm not going to cover the sign-up case here.  This would be an additional process and would use a regular Web API\nto insert data into the database after validation (and probably verification via email or text message).\n\n\n\n\nThe first thing we need to add to our project is a model for the user object.  I created the following in the \nModels\n\nfolder of the \nBackend\n project:\n\n\nusing System.ComponentModel.DataAnnotations;\n\nnamespace Backend.Models\n{\n    public class User\n    {\n        [Key]\n        public int Id { get; set; }\n\n        public string Username { get; set; }\n\n        public string Password { get; set; }\n    }\n}\n\n\n\n\nWe also need to modify the \nMobileServiceContext.cs\n file so that the database table is included in the Entity Framework\ncontext:\n\n\n    public class MobileServiceContext : DbContext\n    {\n        private const string connectionStringName = \nName=MS_TableConnectionString\n;\n\n        public MobileServiceContext() : base(connectionStringName)\n        {\n        }\n\n        public DbSet\nTodoItem\n TodoItems { get; set; }\n        public DbSet\nUser\n Users { get; set; }\n\n        protected override void OnModelCreating(DbModelBuilder modelBuilder)\n        {\n            modelBuilder.Conventions.Add(\n                new AttributeToColumnAnnotationConvention\nTableColumnAttribute, string\n(\n                    \nServiceTableColumn\n, (property, attributes) =\n attributes.Single().ColumnType.ToString()));\n        }\n    }\n\n\n\n\nFinally, we probably want to put some seed data into the database when it is first created so that we can test it.\nAdjust the \nMobileServiceInitializer\n in the \nStartup.MobileApp.cs\n file:\n\n\n        protected override void Seed(MobileServiceContext context)\n        {\n            List\nTodoItem\n todoItems = new List\nTodoItem\n\n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nFirst item\n, Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nSecond item\n, Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set\nTodoItem\n().Add(todoItem);\n            }\n\n            List\nUser\n users = new List\nUser\n\n            {\n                new User { Id = 1, Username = \nadrian\n, Password = \nsupersecret\n }\n            };\n\n            foreach (User user in users)\n            {\n                context.Set\nUser\n().Add(user);\n            }\n\n            base.Seed(context);\n        }\n\n\n\n\nNote that we are storing the passwords in plain text.  This is most definitely frowned upon.  We should be using some\nsort of encryption.  This code is most definitely just for demonstration purposes.  Continuing the code on the backend,\nwe need to handle the request to authenticate from the client.  We will use a custom API controller for this; it is\nlocated in \nControllers\\CustomAuthController.cs\n:\n\n\nusing System;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Web.Http;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Login;\nusing Newtonsoft.Json;\n\nnamespace Backend.Controllers\n{\n    [Route(\n.auth/login/custom\n)]\n    public class CustomAuthController : ApiController\n    {\n        private MobileServiceContext db;\n        private string signingKey, audience, issuer;\n\n        public CustomAuthController()\n        {\n            db = new MobileServiceContext();\n            signingKey = Environment.GetEnvironmentVariable(\nWEBSITE_AUTH_SIGNING_KEY\n);\n            var website = Environment.GetEnvironmentVariable(\nWEBSITE_HOSTNAME\n);\n            audience = $\nhttps://{website}/\n;\n            issuer = $\nhttps://{website}/\n;\n        }\n\n        [HttpPost]\n        public IHttpActionResult Post([FromBody] User body)\n        {\n            if (body == null || body.Username == null || body.Password == null ||\n                body.Username.Length == 0 || body.Password.Length == 0)\n            {\n                return BadRequest(); ;\n            }\n\n            if (!IsValidUser(body))\n            {\n                return Unauthorized();\n            }\n\n            var claims = new Claim[]\n            {\n                new Claim(JwtRegisteredClaimNames.Sub, body.Username)\n            };\n\n            JwtSecurityToken token = AppServiceLoginHandler.CreateToken(\n                claims, signingKey, audience, issuer, TimeSpan.FromDays(30));\n            return Ok(new LoginResult()\n            {\n                AuthenticationToken = token.RawData,\n                User = new LoginResultUser { UserId = body.Username }\n            });\n        }\n\n        protected override void Dispose(bool disposing)\n        {\n            if (disposing)\n            {\n                db.Dispose();\n            }\n            base.Dispose(disposing);\n        }\n\n        private bool IsValidUser(User user)\n        {\n            return db.Users.Count(u =\n u.Username.Equals(user.Username) \n u.Password.Equals(user.Password)) \n 0;\n        }\n    }\n\n    public class LoginResult\n    {\n        [JsonProperty(PropertyName = \nauthenticationToken\n)]\n        public string AuthenticationToken { get; set; }\n\n        [JsonProperty(PropertyName = \nuser\n)]\n        public LoginResultUser User { get; set; }\n    }\n\n    public class LoginResultUser\n    {\n        [JsonProperty(PropertyName = \nuserId\n)]\n        public string UserId { get; set; }\n    }\n}\n\n\n\n\nThere is a lot going on here:\n\n\n\n\nThe constructor reads the signing key and other information that we need for constructing the JWT.  Note that the\n  signing key is only available if you have the Authentication / Authorization is turned on.\n\n\nThe \nLoginResult\n and \nLoginResultUser\n provide the response to the client, when serialized by the JSON serializer.\n\n\nThe \nPost()\n method is where the work happens.  It verifies that you have a valid object, then checks that the\n  username and password match something in the user database.  It then constructs the JWT and returns the required\n  JSON object.\n\n\nThe \nIsValidUser()\n method actually validates the username and password provided in the request with the users in\n  the database.  This version is very simplistic.  I expect your version to at least include encryption of the password.\n\n\n\n\n\n\nWarn\n\n\nYou must turn on Authentication / Authorization in your App Service.  Set the \nAction to take when request\nis not authenticated\n to \nAllow Request (no action)\n and do not configure any of the supported authentication\nproviders.\n\n\n\n\nYou can add additional claims in the token that is passed back to the client by adding additional rows to the \nclaims\n\nobject.  For example:\n\n\n    var claims = new Claim[]\n    {\n        new Claim(JwtRegisteredClaimNames.Sub, body.Username),\n        new Claim(\nfoo\n, \nValue for Foo\n)\n    };\n\n\n\n\nFor example, you could do a custom authentication that includes group information, permissions structures, or\nadditional information about the user from the directory.  Claim names are normally three letters and the value\nis always a string.  It is normal to create a class (just like the \nJwtRegisteredClaimNames\n) with the strings\nin it that can be shared between the client and server projects:\n\n\npublic static class LocalClaimNames\n{\n    public string MainUser =\n \nmus\n\n};\n\n\n\n\nThe only claim that \nmust\n be present is the \"sub\" claim (referenced here by \nJwtRegisteredClaimNames.Sub\n claim\ntype).  The token, when encoded, must fit in a HTTP header.  For Windows systems based on IIS, the maximum size\nof a header is 16Kb.  For Linux systems based on Apache, the maximum size of a header is 8Kb.  The server will\nreturn \n413 Entity Too Large\n if the header is too long.  The token is also transmitted with every single\nrequest so you should make efforts to reduce the size of the token.  It is better to make two requests initially\n(one request for the token followed by an authenticated request for the extra information) than to include the\nextra information in the token.\n\n\nNext, we need to wire the custom authentication controller so that it appears in the same place as all the other\nauthenticators.  We are going to access it via the \n/.auth/login/custom\n endpoint.  The normal ASP.NET methods can be\napplied for this.  In this project, we can enable \nattribute routing\n:\n\n\npublic static void ConfigureMobileApp(IAppBuilder app)\n{\n    HttpConfiguration config = new HttpConfiguration();\n\n    new MobileAppConfiguration()\n        .AddTablesWithEntityFramework()\n        .ApplyTo(config);\n\n    // Map routes by attribute\n    config.MapHttpAttributeRoutes();\n\n    // Use Entity Framework Code First to create database tables based on your DbContext\n    Database.SetInitializer(new MobileServiceInitializer());\n\n    MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n    if (string.IsNullOrEmpty(settings.HostName))\n    {\n        app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n        {\n            SigningKey = ConfigurationManager.AppSettings[\nSigningKey\n],\n            ValidAudiences = new[] { ConfigurationManager.AppSettings[\nValidAudience\n] },\n            ValidIssuers = new[] { ConfigurationManager.AppSettings[\nValidIssuer\n] },\n            TokenHandler = config.GetAppServiceTokenHandler()\n        });\n    }\n\n    app.UseWebApi(config);\n}\n\n\n\n\nAt this point, we can deploy the backend to the App Service and send a suitably formed POST request to\nthe backend.  I use [Postman][19] for this purpose. The request:\n\n\n\n\nA successful POST will return the token and user ID in the response:\n\n\n\n\nAny other request (such as no body or a wrong username or password) should produce the right response.  If the body is\ncorrect, but the information is wrong, then a 401 Unauthorized response should be produced.  If the body is invalid, then\n400 Bad Request should be produced.\n\n\n\n\nInfo\n\n\nThe format of the response is exactly the same as the token response we saw earlier when we were discussing\nthe contents of a JWT.\n\n\n\n\nWe can now turn our attention to the mobile client.  Custom Authentication is always implemented using a client-flow\nmechanism. To implement this, we are going to adjust the entry page so that the username and password fields are\ndisplayed.  The gathered username and password will then be passed to a new ICloudService \nLoginAsync()\n method.\nAll of the UI work is done in the shared project.\n\n\nTo start, we need a copy of the \nUser.cs\n model from the backend project.  Unlike Data Transfer Objects, this model is\nthe same:\n\n\nnamespace TaskList.Models\n{\n    public class User\n    {\n        public string Username { get; set; }\n        public string Password { get; set; }\n    }\n}\n\n\n\n\nThe abstraction we use for the cloud service needs to be adjusted so that we can pass the user object into the login\nmethod.  This is the \nAbstractions\\ICloudService.cs\n interface:\n\n\nusing System.Threading.Tasks;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n\n        Task LoginAsync();\n\n        Task LoginAsync(User user);\n    }\n}\n\n\n\n\nI am adding a new version of the \nLoginAsync()\n method.  The concrete version of this method no longer has to go\nthrough the dependency service since I can use shared code.  Here is the definition of our new \nLoginAsync()\n\nmethod in \nServices\\AzureCloudService.cs\n:\n\n\n        public Task LoginAsync(User user)\n        {\n            return client.LoginAsync(\ncustom\n, JObject.FromObject(user));\n        }\n\n\n\n\nFinally, we need to update the view-model \nViewModels\\EntryPageViewModel.cs\n so that we can store the username and\npassword in the model.  We will also update the call to the \nLoginAsync()\n method of the cloud service so it calls\nour new method:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title = \nTask List\n;\n            User = new Models.User { Username = \n, Password = \n };\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =\n loginCmd ?? (loginCmd = new Command(async () =\n await ExecuteLoginCommand()));\n\n        public Models.User User { get; set; }\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var cloudService = ServiceLocator.Instance.Resolve\nICloudService\n();\n                await cloudService.LoginAsync(User);\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\n[ExecuteLoginCommand] Error = {ex.Message}\n);\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}\n\n\n\n\nThere are three new pieces here.  Firstly, we have the User property (for holding the username and password in our\nform).  Next, the constructor initializes the user object to an empty object.  Finally, the call to \nLoginAsync()\n\npasses the user object to the cloud service.\n\n\nWe also need some UI changes.  Specifically, we need a couple of fields for the username and password added to the\n\nPages\\EntryPage.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage x:Class=\nTaskList.Pages.EntryPage\n\n             xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             Title=\n{Binding Title}\n\n    \nContentPage.Content\n\n        \nStackLayout HorizontalOptions=\nCenter\n\n                     Orientation=\nVertical\n\n                     VerticalOptions=\nCenter\n\n            \nLabel Text=\nUsername?\n /\n\n            \nEntry Text=\n{Binding User.Username}\n /\n\n            \nLabel Text=\nPassword?\n /\n\n            \nEntry IsPassword=\nTrue\n Text=\n{Binding User.Password}\n /\n\n\n            \nButton BackgroundColor=\nTeal\n\n                    BorderRadius=\n10\n\n                    Command=\n{Binding LoginCommand}\n\n                    Text=\nEnter The App\n\n                    TextColor=\nWhite\n /\n\n        \n/StackLayout\n\n    \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThere is lots to complain about in this demonstration (including lack of encryption, storage of passwords, and a\ngenerally bad UI).  However, it serves to demonstrate the salient points for using a (perhaps pre-existing) identity\ndatabase for authentication of the users.\n\n\nUsing Azure Active Directory B2C\n\n\nCustom authentication allows you to really customize the process, but I like to reduce the amount of code I write by\nusing services or libraries.   The whole sign-in and sign-up process is ripe for this sort of reduction.  The code\nneeded for building the sign-in / sign-up process is boiler-plate code.  It also introduces problems that I have to\ndeal with going forward.  I have to store passwords and profile information, which introduces a security concern.  I\nhave to scale the database and ensure my app scales with it as my app gets popular.  Finally, I am being fairly\ninflexible and causing potential privacy concerns with my users.\n\n\nThere are a couple of services that I can use to get around these concerns.  The first is an Azure service:\n\nAzure Active Directory B2C\n.  The B2C stands for Business to Consumer.  It is a mechanism by which you can add a\nsign-in and sign-up flow to your application.  The user can enter a username or password, or, at your option, add on\nsupport for one or more social providers.  In addition, there is support for branding the sign-in process, doing email\nverification of sign-ups and automatic password resets via email.  The Azure AD B2C sign-in / sign-up process is\nprimarily a server-flow process, so we will be able to add support in our app with just one line of code.\n\n\nThe Minimal Setup of Azure AD B2C\n\n\nAzure AD is managed from the \nClassic Azure Portal\n, so start by logging in using your Azure\nSubscription credentials.\n\n\n\n\nClick the big \n+ NEW\n button in the bottom left of the screen.\n\n\nSelect \nApp Services\n -\n \nActive Directory\n -\n \nDirectory\n -\n \nCustom Create\n.\n\n\n\n\n\n\n\n\nChoose a name for the tenant, then choose a unique domain name (which will appear in the   \nonmicrosoft.com\n\n  domain) and country.  Ensure you check the \nThis is a B2C directory.\n\n\n\n\n\n\n\n\nClick the tick to create the directory.  As noted, this process will take a couple of minutes to complete.\n\n\n\n\nThis creates a new tenant for you to manage.  If you go back to your \nAzure Portal\n and click your name\n(top right corner), you will note that there is a new DIRECTORY entry for your B2C tenant.  This is where you will be\nmanaging your B2C tenant.\n\n\nIt's a good idea to pin the B2C settings blade to your dashboard or navigation pane so you can access it faster.\nTo do this:\n\n\n\n\nLog in to the \nAzure Portal\n.\n\n\nSwitch to your B2C tenant by clicking on your name, then selecting the new tenant in the drop-down.  (The portal\n    may ask you to re-confirm your ID and password)\n\n\nClick \nMore services\n in the left-hand navigation bar.\n\n\nSearch for \nB2C\n.\n\n\nClick the empty star next to \nAzure AD B2C\n.\n\n\n\n\n\n\nThis will make Azure AD B2C appear in your left hand navigation bar.  To place it on the dashboard, click on\n\nAzure AD B2C\n in the left hand navigation bar, then click the pin at the top of the \nAZURE AD B2C SETTINGS\n\nblade.\n\n\nWe also need to link the B2C tenant to an Azure subscription that can be billed.  If you see an orange banner\nacross the top of the Azure AD BC Settings, then click it to find the simple 3-step process to link the\nservice.  Once that is done, return to the the B2C tenant.\n\n\n\n\nWarn\n\n\nThe process for creating a B2C tenant may change over time.  If you find these instructions don't work,\nconsult the official documentation on \ndocs.microsoft.com\n.\n\n\n\n\nThe next job is to create an application registration within the B2C tenant:\n\n\n\n\nOpen the \nAzure AD B2C\n from your dashboard or the left hand navigation.\n\n\nIn the \nSettings\n blade, click \nApplications\n.\n\n\n\n\n\n\n\n\nClick \n+ ADD\n to add a new application.\n\n\nIn the \nNew application\n blade:\n\n\nEnter a unique name for the application.\n\n\nClick \nYes\n under \nInclude web app / web API\n.\n\n\nIn the Reply URL, enter \nhttps://yoursite.azurewebsites.net/.auth/login/aad/callback\n.\n\n\n\n\n\n\n\n\nClick \nOK\n.\n\n\n\n\nThere is no spinner or deployment here.  After approximately 5-10 seconds, the application registration will appear\nin the list.  Click the application registration to see the \nApplication ID\n:\n\n\n\n\nWe will also need an App Key.\n\n\n\n\nClick \nKeys\n.\n\n\nClick \n+ Generate Key\n.\n\n\nClick \nSave\n.\n\n\n\n\nThe new App Key will be generated and the display updated. Copy the key that has been generated before you leave\nthis blade as it cannot be re-displayed.  The next time you enter this blade, the secret will be obscured with\nno way of displaying it.  We will need the Application ID and App Key later on.\n\n\nThe next step is to create a Sign-in/Sign-up policy.  We'll create a policy for signing up with an email address\nand email confirmation, then signing in with that email address.  Close all the blades out to the Settings blade\nfor the B2C tenant, then:\n\n\n\n\nIn the \nSettings\n blade, click \nSign-up or sign-in policies\n.\n\n\nClick the \n+ Add\n button.\n\n\nGive the policy a name, like \nemailPolicy\n.\n\n\nClick \nIdentity providers\n:\n\n\nClick \nEmail signup / Local Account\n (a tick will appear next to the row).\n\n\nClick \nOK\n.\n\n\nClick \nSign-up attributes\n:\n\n\nClick \nEmail Address\n and any other fields you want to gather.\n\n\nClick \nOK\n.\n\n\nClick \nApplication claims\n:\n\n\nClick \nEmail Addresses\n and any other fields you want to provide to the application.\n\n\nClick \nOK\n\n\nClick \nCreate\n on the \nAdd policy\n blade.\n\n\nClick the policy you just created.  It will be named something like \nB2C_1_emailPolicy\n. Make a note of the\n\nMetadata Endpoint for this policy\n.\n\n\n\n\n\n\nNow that your B2C tenant is configured, you can switch back to your original tenant (by clicking on your name in\nthe top-right corner and selecting the default directory).\n\n\nTo configure the App Service \nAuthentication / Authorization\n.  Open up the \nSettings\n blade, then\n\nAuthentication / Authorization\n.  Ensure the authentication service is turned on.  Click on\n\nAzure Active Directory\n. This time, we are going to select the \nAdvanced\n option.  The \nClient ID\n is\nthe application ID of your B2C application registration, and the \nIssuer Url\n is the \nMetadata Endpoint\n\nfor your sign-up policy:\n\n\n\n\nClick \nOK\n to configure the authentication server flow, the \nSave\n to save the settings.  As before, you\ncan test your server flow by pointing your browser to \nhttps://yoursite.azurewebsites.net/.auth/login/aad\n:\n\n\n\n\nIf you have done everything right, you should be able to register an account, get the email verification code,\nand finally log in to get the happy login page.\n\n\n\n\nAll that is left to do is to configure your app for Azure Active Directory Server Flow.  We did that earlier when\ndiscussing the Enterprise Authentication flow for the mobile client.\n\n\nDrawbacks of Azure Active Directory B2C\n\n\nAzure AD B2C is great for storing your users passwords and doing the sign-up and sign-in process for you.  There\nare a couple of reasons why you wouldn't want to use Azure Active Directory B2C.\n\n\nThe most obvious one is that this is built on Azure Active Directory.  That means you won't be able to, for example,\nintegrate the Facebook, Google and Twitter identity providers by utilizing their client libraries.  You also do\nnot get access to the underlying identity provider token, so you are restricted from accessing the Graph API for\nthe individual providers.  Finally, since the AAD B2C identity provider is configured with the AAD provider, you\ncan't use both a B2C provider and a regular AAD provider.\n\n\nIf you just want a sign-up / sign-in flow, then AAD B2C is probably the best way to go.  If, however, your plans\ninclude integration with other social identity providers, you should consider using the identity providers\ndirectly or via separate configuration with the Azure App Service Authentication / Authorization.\n\n\nFinally, you cannot use a \"client-flow\" for Azure Active Directory B2C when using it in combination with Azure\nMobile Apps.  The Azure Mobile Apps will only accept a token from the ADAL library (as we described in the\nActive Directory section), and Azure Active Directory B2C requires authentication with MSAL (a newer library).\nWe can happily work with server-flow.\n\n\nUsing Third Party Tokens\n\n\nThe final method of authenticating a user we are going to look at is a process by which you use a third party\nauthentication token.   For example, you may want to authenticate via \nGitHub\n or \nmiiCard\n or using\nan authentication provider like \nAuth0\n to get some single sign-in capabilities.\n\n\nAuthentication with third party tokens works remarkably similar to the custom authentication case.  Instead of\na username and password, you pass in the token from the other provider.\n\n\nTo look at this in example form, we are going to implement Auth0 as a provider. Your first stop should be the\n\nAuth0\n web site to sign up for a developer account. Once you have done that:\n\n\n\n\nClick the \n+ NEW CLIENT\n button in the \nDashboard\n.\n\n\nGive your app a name, then click \nNative\n and then \nCREATE\n.\n\n\n\n\n\n\n\n\nClick the \nXamarin\n icon to get the Xamarin Quickstart.\n\n\nClick \nSettings\n.\n\n\nEnter the callback URL in the \nAllowed Callback URLs\n.  The callback URL will be something like\n  \nhttps://_youraccount_.auth0.com/mobile\n and will be listed in the Quickstart page.\n\n\nScroll down to the bottom of the page and click \nSAVE CHANGES\n.\n\n\nMake a note of the Client ID of the application.  You will need it later.\n\n\nClick \nConnections\n.\n\n\nTurn on any connections that you want to use.  For this example, ensure you turn on the\n\nUsername-Password-Authentication\n and a couple of social providers.\n\n\n\n\nNow that the Auth0 service is configured, we can turn our attention to the mobile client.  The Xamarin.Auth0Client\nis a component, so right-click the \nComponents\n node of a platform project and select \nGet More Components...\n.\nIn the dialog, find the \nAuth0 SDK\n, then click \nAdd to App\n.\n\n\nFor our iOS application, we are going to integrate Auth0 into the \nServices\\iOSLoginProvider.cs\n:\n\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginAuth0Async();\n\n            var zumoPayload = new JObject();\n            zumoPayload[\naccess_token\n] = accessToken;\n            await client.LoginAsync(\nauth0\n, zumoPayload);\n        }\n\n        public UIViewController RootView =\n UIApplication.SharedApplication.KeyWindow.RootViewController;\n\n        public async Task\nstring\n LoginAuth0Async()\n        {\n            var auth0 = new Auth0.SDK.Auth0Client(\n                \nshellmonger.auth0.com\n,\n                \nlmFp5jXnwPpD9lQIYwgwwPmFeofuLpYq\n);\n            var user = await auth0.LoginAsync(RootView, scope: \nopenid email\n);\n            return user.IdToken;\n        }\n\n\n\n\nThe parameters for the constructor to the \nAuth0Client\n are your Auth0 domain and client ID.  You can retrieve these\nfrom the Auth0 management page for your app.  Note that I am requesting the email address.  This will become a part\nof my ZUMO token when I create it.\n\n\nSwitching our attention to our \nBackend\n project, we need a new custom authentication controller.  This is located\nin \nControllers\\Auth0Controller.cs\n:\n\n\nusing System;\nusing System.Diagnostics;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Web.Http;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Login;\n\nnamespace Backend.Controllers\n{\n    [Route(\n.auth/login/auth0\n)]\n    public class Auth0Controller : ApiController\n    {\n        private JwtSecurityTokenHandler tokenHandler;\n        private string clientID, domain;\n        private string signingKey, audience, issuer;\n\n        public Auth0Controller()\n        {\n            // Information for the incoming Auth0 Token\n            domain = Environment.GetEnvironmentVariable(\nAUTH0_DOMAIN\n);\n            clientID = Environment.GetEnvironmentVariable(\nAUTH0_CLIENTID\n);\n\n            // Information for the outgoing ZUMO Token\n            signingKey = Environment.GetEnvironmentVariable(\nWEBSITE_AUTH_SIGNING_KEY\n);\n            var website = Environment.GetEnvironmentVariable(\nWEBSITE_HOSTNAME\n);\n            audience = $\nhttps://{website}/\n;\n            issuer = $\nhttps://{website}/\n;\n\n            // Token Handler\n            tokenHandler = new JwtSecurityTokenHandler();\n        }\n\n        [HttpPost]\n        public IHttpActionResult Post([FromBody] Auth0User body)\n        {\n            if (body == null || body.access_token == null || body.access_token.Length == 0)\n            {\n                return BadRequest();\n            }\n\n            try\n            {\n                var token = (JwtSecurityToken)tokenHandler.ReadToken(body.access_token);\n                if (!IsValidUser(token))\n                {\n                    return Unauthorized();\n                }\n\n                var subject = token.Claims.FirstOrDefault(c =\n c.Type.Equals(\nsub\n))?.Value;\n                var email = token.Claims.FirstOrDefault(c =\n c.Type.Equals(\nemail\n))?.Value;\n                if (subject == null || email == null)\n                {\n                    return BadRequest();\n                }\n\n                var claims = new Claim[]\n                {\n                    new Claim(JwtRegisteredClaimNames.Sub, subject),\n                    new Claim(JwtRegisteredClaimNames.Email, email)\n                };\n\n                JwtSecurityToken zumoToken = AppServiceLoginHandler.CreateToken(\n                    claims, signingKey, audience, issuer, TimeSpan.FromDays(30));\n                return Ok(new LoginResult()\n                {\n                    AuthenticationToken = zumoToken.RawData,\n                    User = new LoginResultUser { UserId = email }\n                });\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\nAuth0 JWT Exception = {ex.Message}\n);\n                throw ex;\n            }\n        }\n\n        private bool IsValidUser(JwtSecurityToken token)\n        {\n            if (token == null)\n                return false;\n            var audience = token.Audiences.FirstOrDefault();\n            if (!audience.Equals(clientID))\n                return false;\n            if (!token.Issuer.Equals($\nhttps://{domain}/\n))\n                return false;\n            if (token.ValidTo.AddMinutes(5) \n DateTime.Now)\n                return false;\n            return true;\n        }\n    }\n\n    public class Auth0User\n    {\n        public string access_token { get; set; }\n    }\n}\n\n\n\n\nNote that we are reading two new environment variables.  In the Azure App Service, you can read Application Settings\nby reading the environment variable of the same name.  We need to set the \nAUTH0_CLIENTID\n to the Client ID of\nour Auth0 application, and the \nAUTH0_DOMAIN\n to the domain of our account.  Both of these values need to match\nthe settings in the client.  These are not \"secure items\".  If using the client secret (to validate the token), then\nthat would be considered secure and should only appear on the server side.\n\n\nThe validation is that the token passed in is valid (i.e. it has the right audience, issuer and expiry times).  In\naddition, you should check the validity of the token signature.  You can do this by acquiring the token secret and\nusing \ntokenHandler.ValidateToken()\n instead of \ntokenHandler.ReadToken()\n.  My new token lasts for 30 days.  The\nZUMO token that is generated in custom authentication does not have to be the same length as the original token.\nYou can make it last for as long as you like.", 
            "title": "Custom Authentication"
        }, 
        {
            "location": "/chapter2/custom/#custom-authentication", 
            "text": "For some situations, the social or enterprise flows are not valid for the mobile client.  Perhaps you want the ability\nto provide a sign-up process with a username and password rather than using a social provider.  Perhaps you want to use\nan alternate provider that is not one of the supported five providers.  Whatever the reason, Azure App Service provides\nthe ability to handle all situations.  In this section, I will look at three methods for providing a unique set of\nusernames with no connection to the social or enterprise authentication.", 
            "title": "Custom authentication"
        }, 
        {
            "location": "/chapter2/custom/#using-an-identity-database", 
            "text": "Probably the most common request is to use a custom identity database.  In general, this is desirable because you\nalready have a database of usernames and password.  However, it's probably the least desirable option because of the\nsecurity concerns that come along with this technique.  The news is rife with password leakage for very large\norganizations.  The best way to ensure you do not disclose a users password is to not have it in the first place.   Warn  I'm not going to cover the sign-up case here.  This would be an additional process and would use a regular Web API\nto insert data into the database after validation (and probably verification via email or text message).   The first thing we need to add to our project is a model for the user object.  I created the following in the  Models \nfolder of the  Backend  project:  using System.ComponentModel.DataAnnotations;\n\nnamespace Backend.Models\n{\n    public class User\n    {\n        [Key]\n        public int Id { get; set; }\n\n        public string Username { get; set; }\n\n        public string Password { get; set; }\n    }\n}  We also need to modify the  MobileServiceContext.cs  file so that the database table is included in the Entity Framework\ncontext:      public class MobileServiceContext : DbContext\n    {\n        private const string connectionStringName =  Name=MS_TableConnectionString ;\n\n        public MobileServiceContext() : base(connectionStringName)\n        {\n        }\n\n        public DbSet TodoItem  TodoItems { get; set; }\n        public DbSet User  Users { get; set; }\n\n        protected override void OnModelCreating(DbModelBuilder modelBuilder)\n        {\n            modelBuilder.Conventions.Add(\n                new AttributeToColumnAnnotationConvention TableColumnAttribute, string (\n                     ServiceTableColumn , (property, attributes) =  attributes.Single().ColumnType.ToString()));\n        }\n    }  Finally, we probably want to put some seed data into the database when it is first created so that we can test it.\nAdjust the  MobileServiceInitializer  in the  Startup.MobileApp.cs  file:          protected override void Seed(MobileServiceContext context)\n        {\n            List TodoItem  todoItems = new List TodoItem \n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  First item , Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  Second item , Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set TodoItem ().Add(todoItem);\n            }\n\n            List User  users = new List User \n            {\n                new User { Id = 1, Username =  adrian , Password =  supersecret  }\n            };\n\n            foreach (User user in users)\n            {\n                context.Set User ().Add(user);\n            }\n\n            base.Seed(context);\n        }  Note that we are storing the passwords in plain text.  This is most definitely frowned upon.  We should be using some\nsort of encryption.  This code is most definitely just for demonstration purposes.  Continuing the code on the backend,\nwe need to handle the request to authenticate from the client.  We will use a custom API controller for this; it is\nlocated in  Controllers\\CustomAuthController.cs :  using System;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Web.Http;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Login;\nusing Newtonsoft.Json;\n\nnamespace Backend.Controllers\n{\n    [Route( .auth/login/custom )]\n    public class CustomAuthController : ApiController\n    {\n        private MobileServiceContext db;\n        private string signingKey, audience, issuer;\n\n        public CustomAuthController()\n        {\n            db = new MobileServiceContext();\n            signingKey = Environment.GetEnvironmentVariable( WEBSITE_AUTH_SIGNING_KEY );\n            var website = Environment.GetEnvironmentVariable( WEBSITE_HOSTNAME );\n            audience = $ https://{website}/ ;\n            issuer = $ https://{website}/ ;\n        }\n\n        [HttpPost]\n        public IHttpActionResult Post([FromBody] User body)\n        {\n            if (body == null || body.Username == null || body.Password == null ||\n                body.Username.Length == 0 || body.Password.Length == 0)\n            {\n                return BadRequest(); ;\n            }\n\n            if (!IsValidUser(body))\n            {\n                return Unauthorized();\n            }\n\n            var claims = new Claim[]\n            {\n                new Claim(JwtRegisteredClaimNames.Sub, body.Username)\n            };\n\n            JwtSecurityToken token = AppServiceLoginHandler.CreateToken(\n                claims, signingKey, audience, issuer, TimeSpan.FromDays(30));\n            return Ok(new LoginResult()\n            {\n                AuthenticationToken = token.RawData,\n                User = new LoginResultUser { UserId = body.Username }\n            });\n        }\n\n        protected override void Dispose(bool disposing)\n        {\n            if (disposing)\n            {\n                db.Dispose();\n            }\n            base.Dispose(disposing);\n        }\n\n        private bool IsValidUser(User user)\n        {\n            return db.Users.Count(u =  u.Username.Equals(user.Username)   u.Password.Equals(user.Password))   0;\n        }\n    }\n\n    public class LoginResult\n    {\n        [JsonProperty(PropertyName =  authenticationToken )]\n        public string AuthenticationToken { get; set; }\n\n        [JsonProperty(PropertyName =  user )]\n        public LoginResultUser User { get; set; }\n    }\n\n    public class LoginResultUser\n    {\n        [JsonProperty(PropertyName =  userId )]\n        public string UserId { get; set; }\n    }\n}  There is a lot going on here:   The constructor reads the signing key and other information that we need for constructing the JWT.  Note that the\n  signing key is only available if you have the Authentication / Authorization is turned on.  The  LoginResult  and  LoginResultUser  provide the response to the client, when serialized by the JSON serializer.  The  Post()  method is where the work happens.  It verifies that you have a valid object, then checks that the\n  username and password match something in the user database.  It then constructs the JWT and returns the required\n  JSON object.  The  IsValidUser()  method actually validates the username and password provided in the request with the users in\n  the database.  This version is very simplistic.  I expect your version to at least include encryption of the password.    Warn  You must turn on Authentication / Authorization in your App Service.  Set the  Action to take when request\nis not authenticated  to  Allow Request (no action)  and do not configure any of the supported authentication\nproviders.   You can add additional claims in the token that is passed back to the client by adding additional rows to the  claims \nobject.  For example:      var claims = new Claim[]\n    {\n        new Claim(JwtRegisteredClaimNames.Sub, body.Username),\n        new Claim( foo ,  Value for Foo )\n    };  For example, you could do a custom authentication that includes group information, permissions structures, or\nadditional information about the user from the directory.  Claim names are normally three letters and the value\nis always a string.  It is normal to create a class (just like the  JwtRegisteredClaimNames ) with the strings\nin it that can be shared between the client and server projects:  public static class LocalClaimNames\n{\n    public string MainUser =   mus \n};  The only claim that  must  be present is the \"sub\" claim (referenced here by  JwtRegisteredClaimNames.Sub  claim\ntype).  The token, when encoded, must fit in a HTTP header.  For Windows systems based on IIS, the maximum size\nof a header is 16Kb.  For Linux systems based on Apache, the maximum size of a header is 8Kb.  The server will\nreturn  413 Entity Too Large  if the header is too long.  The token is also transmitted with every single\nrequest so you should make efforts to reduce the size of the token.  It is better to make two requests initially\n(one request for the token followed by an authenticated request for the extra information) than to include the\nextra information in the token.  Next, we need to wire the custom authentication controller so that it appears in the same place as all the other\nauthenticators.  We are going to access it via the  /.auth/login/custom  endpoint.  The normal ASP.NET methods can be\napplied for this.  In this project, we can enable  attribute routing :  public static void ConfigureMobileApp(IAppBuilder app)\n{\n    HttpConfiguration config = new HttpConfiguration();\n\n    new MobileAppConfiguration()\n        .AddTablesWithEntityFramework()\n        .ApplyTo(config);\n\n    // Map routes by attribute\n    config.MapHttpAttributeRoutes();\n\n    // Use Entity Framework Code First to create database tables based on your DbContext\n    Database.SetInitializer(new MobileServiceInitializer());\n\n    MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n    if (string.IsNullOrEmpty(settings.HostName))\n    {\n        app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n        {\n            SigningKey = ConfigurationManager.AppSettings[ SigningKey ],\n            ValidAudiences = new[] { ConfigurationManager.AppSettings[ ValidAudience ] },\n            ValidIssuers = new[] { ConfigurationManager.AppSettings[ ValidIssuer ] },\n            TokenHandler = config.GetAppServiceTokenHandler()\n        });\n    }\n\n    app.UseWebApi(config);\n}  At this point, we can deploy the backend to the App Service and send a suitably formed POST request to\nthe backend.  I use [Postman][19] for this purpose. The request:   A successful POST will return the token and user ID in the response:   Any other request (such as no body or a wrong username or password) should produce the right response.  If the body is\ncorrect, but the information is wrong, then a 401 Unauthorized response should be produced.  If the body is invalid, then\n400 Bad Request should be produced.   Info  The format of the response is exactly the same as the token response we saw earlier when we were discussing\nthe contents of a JWT.   We can now turn our attention to the mobile client.  Custom Authentication is always implemented using a client-flow\nmechanism. To implement this, we are going to adjust the entry page so that the username and password fields are\ndisplayed.  The gathered username and password will then be passed to a new ICloudService  LoginAsync()  method.\nAll of the UI work is done in the shared project.  To start, we need a copy of the  User.cs  model from the backend project.  Unlike Data Transfer Objects, this model is\nthe same:  namespace TaskList.Models\n{\n    public class User\n    {\n        public string Username { get; set; }\n        public string Password { get; set; }\n    }\n}  The abstraction we use for the cloud service needs to be adjusted so that we can pass the user object into the login\nmethod.  This is the  Abstractions\\ICloudService.cs  interface:  using System.Threading.Tasks;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n\n        Task LoginAsync();\n\n        Task LoginAsync(User user);\n    }\n}  I am adding a new version of the  LoginAsync()  method.  The concrete version of this method no longer has to go\nthrough the dependency service since I can use shared code.  Here is the definition of our new  LoginAsync() \nmethod in  Services\\AzureCloudService.cs :          public Task LoginAsync(User user)\n        {\n            return client.LoginAsync( custom , JObject.FromObject(user));\n        }  Finally, we need to update the view-model  ViewModels\\EntryPageViewModel.cs  so that we can store the username and\npassword in the model.  We will also update the call to the  LoginAsync()  method of the cloud service so it calls\nour new method:  using System;\nusing System.Diagnostics;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.Models;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class EntryPageViewModel : BaseViewModel\n    {\n        public EntryPageViewModel()\n        {\n            Title =  Task List ;\n            User = new Models.User { Username =  , Password =   };\n        }\n\n        Command loginCmd;\n        public Command LoginCommand =  loginCmd ?? (loginCmd = new Command(async () =  await ExecuteLoginCommand()));\n\n        public Models.User User { get; set; }\n\n        async Task ExecuteLoginCommand()\n        {\n            if (IsBusy)\n                return;\n            IsBusy = true;\n\n            try\n            {\n                var cloudService = ServiceLocator.Instance.Resolve ICloudService ();\n                await cloudService.LoginAsync(User);\n                Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ [ExecuteLoginCommand] Error = {ex.Message} );\n            }\n            finally\n            {\n                IsBusy = false;\n            }\n        }\n    }\n}  There are three new pieces here.  Firstly, we have the User property (for holding the username and password in our\nform).  Next, the constructor initializes the user object to an empty object.  Finally, the call to  LoginAsync() \npasses the user object to the cloud service.  We also need some UI changes.  Specifically, we need a couple of fields for the username and password added to the Pages\\EntryPage.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage x:Class= TaskList.Pages.EntryPage \n             xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             Title= {Binding Title} \n     ContentPage.Content \n         StackLayout HorizontalOptions= Center \n                     Orientation= Vertical \n                     VerticalOptions= Center \n             Label Text= Username?  / \n             Entry Text= {Binding User.Username}  / \n             Label Text= Password?  / \n             Entry IsPassword= True  Text= {Binding User.Password}  / \n\n             Button BackgroundColor= Teal \n                    BorderRadius= 10 \n                    Command= {Binding LoginCommand} \n                    Text= Enter The App \n                    TextColor= White  / \n         /StackLayout \n     /ContentPage.Content  /ContentPage   There is lots to complain about in this demonstration (including lack of encryption, storage of passwords, and a\ngenerally bad UI).  However, it serves to demonstrate the salient points for using a (perhaps pre-existing) identity\ndatabase for authentication of the users.", 
            "title": "Using an Identity Database."
        }, 
        {
            "location": "/chapter2/custom/#using-azure-active-directory-b2c", 
            "text": "Custom authentication allows you to really customize the process, but I like to reduce the amount of code I write by\nusing services or libraries.   The whole sign-in and sign-up process is ripe for this sort of reduction.  The code\nneeded for building the sign-in / sign-up process is boiler-plate code.  It also introduces problems that I have to\ndeal with going forward.  I have to store passwords and profile information, which introduces a security concern.  I\nhave to scale the database and ensure my app scales with it as my app gets popular.  Finally, I am being fairly\ninflexible and causing potential privacy concerns with my users.  There are a couple of services that I can use to get around these concerns.  The first is an Azure service: Azure Active Directory B2C .  The B2C stands for Business to Consumer.  It is a mechanism by which you can add a\nsign-in and sign-up flow to your application.  The user can enter a username or password, or, at your option, add on\nsupport for one or more social providers.  In addition, there is support for branding the sign-in process, doing email\nverification of sign-ups and automatic password resets via email.  The Azure AD B2C sign-in / sign-up process is\nprimarily a server-flow process, so we will be able to add support in our app with just one line of code.", 
            "title": "Using Azure Active Directory B2C"
        }, 
        {
            "location": "/chapter2/custom/#the-minimal-setup-of-azure-ad-b2c", 
            "text": "Azure AD is managed from the  Classic Azure Portal , so start by logging in using your Azure\nSubscription credentials.   Click the big  + NEW  button in the bottom left of the screen.  Select  App Services  -   Active Directory  -   Directory  -   Custom Create .     Choose a name for the tenant, then choose a unique domain name (which will appear in the    onmicrosoft.com \n  domain) and country.  Ensure you check the  This is a B2C directory.     Click the tick to create the directory.  As noted, this process will take a couple of minutes to complete.   This creates a new tenant for you to manage.  If you go back to your  Azure Portal  and click your name\n(top right corner), you will note that there is a new DIRECTORY entry for your B2C tenant.  This is where you will be\nmanaging your B2C tenant.  It's a good idea to pin the B2C settings blade to your dashboard or navigation pane so you can access it faster.\nTo do this:   Log in to the  Azure Portal .  Switch to your B2C tenant by clicking on your name, then selecting the new tenant in the drop-down.  (The portal\n    may ask you to re-confirm your ID and password)  Click  More services  in the left-hand navigation bar.  Search for  B2C .  Click the empty star next to  Azure AD B2C .    This will make Azure AD B2C appear in your left hand navigation bar.  To place it on the dashboard, click on Azure AD B2C  in the left hand navigation bar, then click the pin at the top of the  AZURE AD B2C SETTINGS \nblade.  We also need to link the B2C tenant to an Azure subscription that can be billed.  If you see an orange banner\nacross the top of the Azure AD BC Settings, then click it to find the simple 3-step process to link the\nservice.  Once that is done, return to the the B2C tenant.   Warn  The process for creating a B2C tenant may change over time.  If you find these instructions don't work,\nconsult the official documentation on  docs.microsoft.com .   The next job is to create an application registration within the B2C tenant:   Open the  Azure AD B2C  from your dashboard or the left hand navigation.  In the  Settings  blade, click  Applications .     Click  + ADD  to add a new application.  In the  New application  blade:  Enter a unique name for the application.  Click  Yes  under  Include web app / web API .  In the Reply URL, enter  https://yoursite.azurewebsites.net/.auth/login/aad/callback .     Click  OK .   There is no spinner or deployment here.  After approximately 5-10 seconds, the application registration will appear\nin the list.  Click the application registration to see the  Application ID :   We will also need an App Key.   Click  Keys .  Click  + Generate Key .  Click  Save .   The new App Key will be generated and the display updated. Copy the key that has been generated before you leave\nthis blade as it cannot be re-displayed.  The next time you enter this blade, the secret will be obscured with\nno way of displaying it.  We will need the Application ID and App Key later on.  The next step is to create a Sign-in/Sign-up policy.  We'll create a policy for signing up with an email address\nand email confirmation, then signing in with that email address.  Close all the blades out to the Settings blade\nfor the B2C tenant, then:   In the  Settings  blade, click  Sign-up or sign-in policies .  Click the  + Add  button.  Give the policy a name, like  emailPolicy .  Click  Identity providers :  Click  Email signup / Local Account  (a tick will appear next to the row).  Click  OK .  Click  Sign-up attributes :  Click  Email Address  and any other fields you want to gather.  Click  OK .  Click  Application claims :  Click  Email Addresses  and any other fields you want to provide to the application.  Click  OK  Click  Create  on the  Add policy  blade.  Click the policy you just created.  It will be named something like  B2C_1_emailPolicy . Make a note of the Metadata Endpoint for this policy .    Now that your B2C tenant is configured, you can switch back to your original tenant (by clicking on your name in\nthe top-right corner and selecting the default directory).  To configure the App Service  Authentication / Authorization .  Open up the  Settings  blade, then Authentication / Authorization .  Ensure the authentication service is turned on.  Click on Azure Active Directory . This time, we are going to select the  Advanced  option.  The  Client ID  is\nthe application ID of your B2C application registration, and the  Issuer Url  is the  Metadata Endpoint \nfor your sign-up policy:   Click  OK  to configure the authentication server flow, the  Save  to save the settings.  As before, you\ncan test your server flow by pointing your browser to  https://yoursite.azurewebsites.net/.auth/login/aad :   If you have done everything right, you should be able to register an account, get the email verification code,\nand finally log in to get the happy login page.   All that is left to do is to configure your app for Azure Active Directory Server Flow.  We did that earlier when\ndiscussing the Enterprise Authentication flow for the mobile client.", 
            "title": "The Minimal Setup of Azure AD B2C"
        }, 
        {
            "location": "/chapter2/custom/#drawbacks-of-azure-active-directory-b2c", 
            "text": "Azure AD B2C is great for storing your users passwords and doing the sign-up and sign-in process for you.  There\nare a couple of reasons why you wouldn't want to use Azure Active Directory B2C.  The most obvious one is that this is built on Azure Active Directory.  That means you won't be able to, for example,\nintegrate the Facebook, Google and Twitter identity providers by utilizing their client libraries.  You also do\nnot get access to the underlying identity provider token, so you are restricted from accessing the Graph API for\nthe individual providers.  Finally, since the AAD B2C identity provider is configured with the AAD provider, you\ncan't use both a B2C provider and a regular AAD provider.  If you just want a sign-up / sign-in flow, then AAD B2C is probably the best way to go.  If, however, your plans\ninclude integration with other social identity providers, you should consider using the identity providers\ndirectly or via separate configuration with the Azure App Service Authentication / Authorization.  Finally, you cannot use a \"client-flow\" for Azure Active Directory B2C when using it in combination with Azure\nMobile Apps.  The Azure Mobile Apps will only accept a token from the ADAL library (as we described in the\nActive Directory section), and Azure Active Directory B2C requires authentication with MSAL (a newer library).\nWe can happily work with server-flow.", 
            "title": "Drawbacks of Azure Active Directory B2C"
        }, 
        {
            "location": "/chapter2/custom/#using-third-party-tokens", 
            "text": "The final method of authenticating a user we are going to look at is a process by which you use a third party\nauthentication token.   For example, you may want to authenticate via  GitHub  or  miiCard  or using\nan authentication provider like  Auth0  to get some single sign-in capabilities.  Authentication with third party tokens works remarkably similar to the custom authentication case.  Instead of\na username and password, you pass in the token from the other provider.  To look at this in example form, we are going to implement Auth0 as a provider. Your first stop should be the Auth0  web site to sign up for a developer account. Once you have done that:   Click the  + NEW CLIENT  button in the  Dashboard .  Give your app a name, then click  Native  and then  CREATE .     Click the  Xamarin  icon to get the Xamarin Quickstart.  Click  Settings .  Enter the callback URL in the  Allowed Callback URLs .  The callback URL will be something like\n   https://_youraccount_.auth0.com/mobile  and will be listed in the Quickstart page.  Scroll down to the bottom of the page and click  SAVE CHANGES .  Make a note of the Client ID of the application.  You will need it later.  Click  Connections .  Turn on any connections that you want to use.  For this example, ensure you turn on the Username-Password-Authentication  and a couple of social providers.   Now that the Auth0 service is configured, we can turn our attention to the mobile client.  The Xamarin.Auth0Client\nis a component, so right-click the  Components  node of a platform project and select  Get More Components... .\nIn the dialog, find the  Auth0 SDK , then click  Add to App .  For our iOS application, we are going to integrate Auth0 into the  Services\\iOSLoginProvider.cs :          public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Client Flow\n            var accessToken = await LoginAuth0Async();\n\n            var zumoPayload = new JObject();\n            zumoPayload[ access_token ] = accessToken;\n            await client.LoginAsync( auth0 , zumoPayload);\n        }\n\n        public UIViewController RootView =  UIApplication.SharedApplication.KeyWindow.RootViewController;\n\n        public async Task string  LoginAuth0Async()\n        {\n            var auth0 = new Auth0.SDK.Auth0Client(\n                 shellmonger.auth0.com ,\n                 lmFp5jXnwPpD9lQIYwgwwPmFeofuLpYq );\n            var user = await auth0.LoginAsync(RootView, scope:  openid email );\n            return user.IdToken;\n        }  The parameters for the constructor to the  Auth0Client  are your Auth0 domain and client ID.  You can retrieve these\nfrom the Auth0 management page for your app.  Note that I am requesting the email address.  This will become a part\nof my ZUMO token when I create it.  Switching our attention to our  Backend  project, we need a new custom authentication controller.  This is located\nin  Controllers\\Auth0Controller.cs :  using System;\nusing System.Diagnostics;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Web.Http;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Login;\n\nnamespace Backend.Controllers\n{\n    [Route( .auth/login/auth0 )]\n    public class Auth0Controller : ApiController\n    {\n        private JwtSecurityTokenHandler tokenHandler;\n        private string clientID, domain;\n        private string signingKey, audience, issuer;\n\n        public Auth0Controller()\n        {\n            // Information for the incoming Auth0 Token\n            domain = Environment.GetEnvironmentVariable( AUTH0_DOMAIN );\n            clientID = Environment.GetEnvironmentVariable( AUTH0_CLIENTID );\n\n            // Information for the outgoing ZUMO Token\n            signingKey = Environment.GetEnvironmentVariable( WEBSITE_AUTH_SIGNING_KEY );\n            var website = Environment.GetEnvironmentVariable( WEBSITE_HOSTNAME );\n            audience = $ https://{website}/ ;\n            issuer = $ https://{website}/ ;\n\n            // Token Handler\n            tokenHandler = new JwtSecurityTokenHandler();\n        }\n\n        [HttpPost]\n        public IHttpActionResult Post([FromBody] Auth0User body)\n        {\n            if (body == null || body.access_token == null || body.access_token.Length == 0)\n            {\n                return BadRequest();\n            }\n\n            try\n            {\n                var token = (JwtSecurityToken)tokenHandler.ReadToken(body.access_token);\n                if (!IsValidUser(token))\n                {\n                    return Unauthorized();\n                }\n\n                var subject = token.Claims.FirstOrDefault(c =  c.Type.Equals( sub ))?.Value;\n                var email = token.Claims.FirstOrDefault(c =  c.Type.Equals( email ))?.Value;\n                if (subject == null || email == null)\n                {\n                    return BadRequest();\n                }\n\n                var claims = new Claim[]\n                {\n                    new Claim(JwtRegisteredClaimNames.Sub, subject),\n                    new Claim(JwtRegisteredClaimNames.Email, email)\n                };\n\n                JwtSecurityToken zumoToken = AppServiceLoginHandler.CreateToken(\n                    claims, signingKey, audience, issuer, TimeSpan.FromDays(30));\n                return Ok(new LoginResult()\n                {\n                    AuthenticationToken = zumoToken.RawData,\n                    User = new LoginResultUser { UserId = email }\n                });\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ Auth0 JWT Exception = {ex.Message} );\n                throw ex;\n            }\n        }\n\n        private bool IsValidUser(JwtSecurityToken token)\n        {\n            if (token == null)\n                return false;\n            var audience = token.Audiences.FirstOrDefault();\n            if (!audience.Equals(clientID))\n                return false;\n            if (!token.Issuer.Equals($ https://{domain}/ ))\n                return false;\n            if (token.ValidTo.AddMinutes(5)   DateTime.Now)\n                return false;\n            return true;\n        }\n    }\n\n    public class Auth0User\n    {\n        public string access_token { get; set; }\n    }\n}  Note that we are reading two new environment variables.  In the Azure App Service, you can read Application Settings\nby reading the environment variable of the same name.  We need to set the  AUTH0_CLIENTID  to the Client ID of\nour Auth0 application, and the  AUTH0_DOMAIN  to the domain of our account.  Both of these values need to match\nthe settings in the client.  These are not \"secure items\".  If using the client secret (to validate the token), then\nthat would be considered secure and should only appear on the server side.  The validation is that the token passed in is valid (i.e. it has the right audience, issuer and expiry times).  In\naddition, you should check the validity of the token signature.  You can do this by acquiring the token secret and\nusing  tokenHandler.ValidateToken()  instead of  tokenHandler.ReadToken() .  My new token lasts for 30 days.  The\nZUMO token that is generated in custom authentication does not have to be the same length as the original token.\nYou can make it last for as long as you like.", 
            "title": "Using Third Party Tokens"
        }, 
        {
            "location": "/chapter2/authorization/", 
            "text": "Claims and Authorization\n\n\nNow that we have covered all the techniques for authentication, it's time to look at authorization.  While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation.\n\n\nAuthorization is handled within the server-side project by the \n[Authorize]\n attribute.  Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not.  The Authorize attribute can also check to see if a user is in a list of users or roles.  However, there is a problem with this.  The user id is not guessable and we have no roles.  To see what I mean, run the \nBackend\n project locally and set a break point on the \nGetAllTodoItems()\n method in the \nTodoItemController\n, then run your server and your UWP application.\n\n\n\n\nTip\n\n\nOnce you have built and deployed the UWP application, it will appear in your normal Application list.  This allows you to run the application and the server at the same time on the same machine.\n\n\n\n\nOnce you have authenticated, you will be able to set a break point to take a look at \nthis.User.Identity\n:\n\n\n\n\nNote that the \nName\n property is null.  This is the property that is used when you want to authorize individual users. Expand the \nClaims\n property and then click on \nResults View\n:\n\n\n\n\nThe only claims are the ones in the token, and none of them match the \nRoleClaimType\n, so we can't use roles either. Clearly, we are going to have to do something else.\n\n\nObtaining User Claims\n\n\nAt some point you are going to need to deal with something other than the claims that are in the token passed for authentication.  Fortunately, the Authentication / Authorization feature has an endpoint for that at \n/.auth/me\n:\n\n\n\n\nOf course, the \n/.auth/me\n endpoint is not of any use if you cannot access it.  The most use of this information is gained during authorization on the server and we will cover this use later on.  However, there are reasons to pull this information on the client as well.  For example, we may want to make the List View title be our name instead of\n\"Tasks\".\n\n\n\n\nWarn\n\n\nYou can't use the /.auth/me endpoint when using custom authentication.\n\n\n\n\nSince identity provider claims can be anything, they are transferred as a list within a JSON object.  Before we can decode the JSON object, we need to define the models.  This is done in the shared \nTaskList\n project.  I've defined this in \nModels\\AppServiceIdentity.cs\n.\n\n\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace TaskList.Models\n{\n    public class AppServiceIdentity\n    {\n        [JsonProperty(PropertyName = \nid_token\n)]\n        public string IdToken { get; set; }\n\n        [JsonProperty(PropertyName = \nprovider_name\n)]\n        public string ProviderName { get; set; }\n\n        [JsonProperty(PropertyName = \nuser_id\n)]\n        public string UserId { get; set; }\n\n        [JsonProperty(PropertyName = \nuser_claims\n)]\n        public List\nUserClaim\n UserClaims { get; set; }\n    }\n\n    public class UserClaim\n    {\n        [JsonProperty(PropertyName = \ntyp\n)]\n        public string Type { get; set; }\n\n        [JsonProperty(PropertyName = \nval\n)]\n        public string Value { get; set; }\n    }\n}\n\n\n\n\nThis matches the JSON format from the \n/.auth/me\n call we did earlier.   This is going to be a part of the ICloudService as follows:\n\n\nusing System.Threading.Tasks;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n\n        Task LoginAsync();\n\n        Task LoginAsync(User user);\n\n        Task\nAppServiceIdentity\n GetIdentityAsync();\n    }\n}\n\n\n\n\nFinally, we need to actually implement the concrete version in \nAzureCloudService.cs\n:\n\n\nList\nAppServiceIdentity\n identities = null;\n\npublic async Task\nAppServiceIdentity\n GetIdentityAsync()\n{\n    if (client.CurrentUser == null || client.CurrentUser?.MobileServiceAuthenticationToken == null)\n    {\n        throw new InvalidOperationException(\nNot Authenticated\n);\n    }\n\n    if (identities == null)\n    {\n        identities = await client.InvokeApiAsync\nList\nAppServiceIdentity\n(\n/.auth/me\n);\n    }\n\n    if (identities.Count \n 0)\n        return identities[0];\n    return null;\n}\n\n\n\n\nNote that there is no reason to instantiate your own \nHttpClient()\n.  The Azure Mobile Apps SDK has a method for invoking custom API calls (as we shall see later on).  However, if you prefix the path with a slash, it will execute a HTTP GET for any API with any authentication that is currently in force.  We can leverage this to call the \n/.auth/me\n endpoint and decode the response in one line of code.  Adjust the \nExecuteRefreshCommand()\n method in the \nViewModels\\TaskListViewModel.cs\n file to take advantage of this:\n\n\nasync Task ExecuteRefreshCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var identity = await cloudService.GetIdentityAsync();\n        if (identity != null)\n        {\n            var name = identity.UserClaims.FirstOrDefault(c =\n c.Type.Equals(\nname\n)).Value;\n            Title = $\nTasks for {name}\n;\n        }\n        var list = await Table.ReadAllItemsAsync();\n        Items.ReplaceRange(list);\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert(\nItems Not Loaded\n, ex.Message, \nOK\n);\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}\n\n\n\n\nThe return value from the \nGetIdentityAsync()\n method is the first identity.  Normally, a user would only authenticate once, so this is fairly safe.  The number of claims returned depends on the identity provider and could easily number in the hundreds.  Even the default configuration for Azure Active Directory returns 18 claims.  These are easily handled using LINQ, however.  The \nType\n property holds the type.  This could be a short (common) name.  It could also be a schema name, which looks more like a URI.  The only way to know what claims are coming back for sure is to look at the \n/.auth/me\n result with something like Postman.\n\n\n\n\nWarn\n\n\nIf you are using Custom Authentication (e.g. username/password or a third-party token), then the \n/.auth/me\n    endpoint is not available to you.  You can still produce a custom API in your backend to provide this information to your client, but you are responsible for the code - it's custom, after all!\n\n\n\n\nAuthorization\n\n\nNow that we have covered all the techniques for authentication, it's time to look at authorization.  While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation.\n\n\nAuthorization is handled within the server-side project by the \n[Authorize]\n attribute.  Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not.  The Authorize attribute can also check to see if a user is in a list of users or roles.  However, there is a problem with this.  The user id is not guessable and we have no roles.  To see what I mean, run the \nBackend\n project and set a break point on the \nGetAllTodoItems()\n method in the \nTodoItemController\n, then run your server and your UWP application.\n\n\n\n\nTip\n\n\nOnce you have built and deployed the UWP application, it will appear in your normal Application list.  This allows you to run the application and the server at the same time on the same machine.  Alternatively, you can attach a Debugger to your Azure App Service within Visual Studio's Cloud Explorer.\n\n\n\n\nOnce you have authenticated, you will be able to set a break point to take a look at \nthis.User.Identity\n:\n\n\n\n\nNote that the \nName\n property is null.  This is the property that is used when you want to authorize individual users.  Expand the \nClaims\n property and then click on \nResults View\n:\n\n\n\n\nThe only claims are the ones in the token, and none of them match the \nRoleClaimType\n, so we can't use roles either.  Clearly, we are going to have to do something else.  Fortunately, we already know that we can get some information about the identity provider claims from the \n/.auth/me\n endpoint.  To get the extra information, we need to query the \nUser\n object:\n\n\nvar identity = await User.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(Request);\n\n\n\n\nThere is one \nCredentials\n class for each supported authentication technique - Azure Active Directory, Facebook, Google, Microsoft Account and Twitter.  These are in the \nMicrosoft.Azure.Mobile.Server.Authentication\n namespace.  They all follow the same pattern as the model we created for the client - there are Provider, UserId and UserClaims properties.  The token and any special information will be automatically decoded for you.  For instance, the TenantId is pulled out of the response for Azure AD.\n\n\n\n\nTip\n\n\nYou can use the AccessToken property to do Graph API lookups for most providers.  Use the Graph SDK provided by the authentication provider for this purpose; either in the mobile backend or the client.\n\n\n\n\nAdding Group Claims to the Request\n\n\nThere are times when you want to add something else to the token that is returned from Azure AD. The most common requirement is to add group information to the response so you can handle group-based authorization.\n\n\nTo add security groups to the Azure AD token:\n\n\n\n\nLog into the \nAzure Portal\n.\n\n\nClick on \nAzure Active Directory\n in your left-hand menu (you may need to search for it).\n\n\nIf you need to work in a different directory, click \nSwitch directory\n to do so.\n\n\nClick \nApp registrations\n.\n\n\nClick your WEB application.\n\n\nClick \nManifest\n at the top of the registered app blade.\n\n\n\n\nYou are now placed within a manifest editor.  Find the \ngroupMembershipClaims\n entry and change it from null to \n\"SecurityGroup\"\n.\n\n\n\n\n\n\n\n\nWhen complete, click \nSave\n.\n\n\n\n\n\n\nYou can now give the web application additional permissions:\n\n\n\n\nClose the Manifest editor.\n\n\nClick \nSettings\n.\n\n\nClick \nRequired permissions\n.\n\n\nClick \nWindows Azure Active Directory\n.\n\n\nClick \nRead directory data\n under the \nDelegated Permissions\n section.\n\n\nScroll to the bottom, click on \nDelegated Permissions\n.\n\n\nCheck the box for \nRead directory data\n.\n\n\n\n\n\n\n\n\nClick on \nSave\n.\n\n\n\n\nIf you are not an administrator of the domain, then an administrator will need to approve your request.  Now that you have configured the application to return groups as part of the claims, you should probably add a couple of groups:\n\n\n\n\nClick \nAzure Active Directory\n in the left-hand menu to return to the top-level.\n\n\nClick \nUsers and groups\n.\n\n\nClick \nAll groups\n.\n\n\nClick \n+ Add\n.\n\n\nFill in the information, then click \nCreate\n.\n\n\n\n\n\n\n\n\nClick on the new group.\n\n\n\n\n\n\n\n\nMake a note of the \nOBJECT ID\n.  The claims for groups are listed by the Object ID, so you will need this to\n   refer to the group later.\n\n\n\n\nIt's a good idea to add a couple of groups for testing purposes.  If you are using the organization directory, you will need to request the creation of a couple of groups for application roles.  The view of the groups will be shown when we get the identity of the user using \nUser.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(Request)\n:\n\n\n\n\nGroup Authorization\n\n\nNow that we have group claims in the claims list for the \n/.auth/me\n endpoint, we can move forward to do authorization\nbased on these claims.  This can be done in a relatively basic manner by implementing a method to check the claims:\n\n\nasync Task\nbool\n IsAuthorizedAsync()\n{\n    var identity = await User.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(Request);\n    var countofGroups = identity.UserClaims\n        .Where(c =\n c.Type.Equals(\ngroups\n) \n c.Value.Equals(\n01f214a9-af1f-4bdd-938f-3f16749aef0e\n))\n        .Count();\n    return (countofGroups \n 0);\n}\n\n\n\n\nThe \nUserClaims\n object is an \nIEnumerable\n that contains objects with a Type and a Value.  The Type for the group claims is \ngroups\n.  Once we have this knowledge, we can use a LINQ query to obtain a count of the claims that match the conditions we want to test.  The Value we use is the Object ID of the group.  This is available in the \nPROPERTIES\n tab of the group.\n\n\nWe can prevent a new record being added by adjusting the \nPostTodoItem()\n method:\n\n\npublic async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n{\n    if (!await IsAuthorizedAsync())\n    {\n        return Unauthorized();\n    }\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\n\n\n\nUnfortunately, most of the table controller methods do not return an \nIHttpActionResult\n, so this has limited value.  What would be better would be an \n[Authorize]\n attribute that tests the claims for us.  For instance, we should be able to do the following:\n\n\n[AuthorizeClaims(\ngroups\n, \n01f214a9-af1f-4bdd-938f-3f16749aef0e\n)]\npublic async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n{\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\n\n\n\nThe \n[AuthorizeClaims()]\n attribute does not exist, so we have to provide it ourselves:\n\n\nusing System.Linq;\nusing System.Net;\nusing System.Security.Principal;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.Filters;\nusing Microsoft.Azure.Mobile.Server.Authentication;\n\nnamespace Backend.Helpers\n{\n    public class AuthorizeClaimsAttribute : AuthorizationFilterAttribute\n    {\n        string Type { get; }\n        string Value { get; }\n\n        public AuthorizeClaimsAttribute(string type, string value)\n        {\n            Type = type;\n            Value = value;\n        }\n\n        public override async Task OnAuthorizationAsync(HttpActionContext actionContext, CancellationToken cancellationToken)\n        {\n            var request = actionContext.Request;\n            var user = actionContext.RequestContext.Principal;\n            if (user != null)\n            {\n                var identity = await user.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(request);\n                var countOfMatchingClaims = identity.UserClaims\n                    .Where(c =\n c.Type.Equals(Type) \n c.Value.Equals(Value))\n                    .Count();\n                if (countOfMatchingClaims \n 0) return;\n\n            }\n            throw new HttpResponseException(HttpStatusCode.Unauthorized);\n        }\n    }\n}\n\n\n\n\nThis is the same type of authorization filter attribute that the officially provided \nAuthorizeAttribute\n is based on.  However, the AuthorizeAttribute is synchronous.  We require an asynchronous version of the attribute, so we cannot use a sub-class of the AuthorizeAttribute.  Aside from that note, this uses virtually the same code that we used in the \nIsAythorizedAsync()\n method we developped earlier.\n\n\nWe can now use this attribute for testing any claim.  For example, our claims has the identity provider as a claim.  We can use the following:\n\n\n[AuthorizeClaims(\nhttp://schemas.microsoft.com/identity/claims/identityprovider\n, \nlive.com\n)]\n\n\n\n\n\n\nTip\n\n\nIf you want to test other claims that are not provided, you can enable the \nRead Directory Data\n permission in the Azure Active Directory permissions and do a query against the Azure Active Directory.  You should think about caching results or minting a new ZUMO token (just like we did in the custom authentication case) for performance reasons.", 
            "title": "Claims and Authorization"
        }, 
        {
            "location": "/chapter2/authorization/#claims-and-authorization", 
            "text": "Now that we have covered all the techniques for authentication, it's time to look at authorization.  While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation.  Authorization is handled within the server-side project by the  [Authorize]  attribute.  Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not.  The Authorize attribute can also check to see if a user is in a list of users or roles.  However, there is a problem with this.  The user id is not guessable and we have no roles.  To see what I mean, run the  Backend  project locally and set a break point on the  GetAllTodoItems()  method in the  TodoItemController , then run your server and your UWP application.   Tip  Once you have built and deployed the UWP application, it will appear in your normal Application list.  This allows you to run the application and the server at the same time on the same machine.   Once you have authenticated, you will be able to set a break point to take a look at  this.User.Identity :   Note that the  Name  property is null.  This is the property that is used when you want to authorize individual users. Expand the  Claims  property and then click on  Results View :   The only claims are the ones in the token, and none of them match the  RoleClaimType , so we can't use roles either. Clearly, we are going to have to do something else.", 
            "title": "Claims and Authorization"
        }, 
        {
            "location": "/chapter2/authorization/#obtaining-user-claims", 
            "text": "At some point you are going to need to deal with something other than the claims that are in the token passed for authentication.  Fortunately, the Authentication / Authorization feature has an endpoint for that at  /.auth/me :   Of course, the  /.auth/me  endpoint is not of any use if you cannot access it.  The most use of this information is gained during authorization on the server and we will cover this use later on.  However, there are reasons to pull this information on the client as well.  For example, we may want to make the List View title be our name instead of\n\"Tasks\".   Warn  You can't use the /.auth/me endpoint when using custom authentication.   Since identity provider claims can be anything, they are transferred as a list within a JSON object.  Before we can decode the JSON object, we need to define the models.  This is done in the shared  TaskList  project.  I've defined this in  Models\\AppServiceIdentity.cs .  using System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace TaskList.Models\n{\n    public class AppServiceIdentity\n    {\n        [JsonProperty(PropertyName =  id_token )]\n        public string IdToken { get; set; }\n\n        [JsonProperty(PropertyName =  provider_name )]\n        public string ProviderName { get; set; }\n\n        [JsonProperty(PropertyName =  user_id )]\n        public string UserId { get; set; }\n\n        [JsonProperty(PropertyName =  user_claims )]\n        public List UserClaim  UserClaims { get; set; }\n    }\n\n    public class UserClaim\n    {\n        [JsonProperty(PropertyName =  typ )]\n        public string Type { get; set; }\n\n        [JsonProperty(PropertyName =  val )]\n        public string Value { get; set; }\n    }\n}  This matches the JSON format from the  /.auth/me  call we did earlier.   This is going to be a part of the ICloudService as follows:  using System.Threading.Tasks;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n\n        Task LoginAsync();\n\n        Task LoginAsync(User user);\n\n        Task AppServiceIdentity  GetIdentityAsync();\n    }\n}  Finally, we need to actually implement the concrete version in  AzureCloudService.cs :  List AppServiceIdentity  identities = null;\n\npublic async Task AppServiceIdentity  GetIdentityAsync()\n{\n    if (client.CurrentUser == null || client.CurrentUser?.MobileServiceAuthenticationToken == null)\n    {\n        throw new InvalidOperationException( Not Authenticated );\n    }\n\n    if (identities == null)\n    {\n        identities = await client.InvokeApiAsync List AppServiceIdentity ( /.auth/me );\n    }\n\n    if (identities.Count   0)\n        return identities[0];\n    return null;\n}  Note that there is no reason to instantiate your own  HttpClient() .  The Azure Mobile Apps SDK has a method for invoking custom API calls (as we shall see later on).  However, if you prefix the path with a slash, it will execute a HTTP GET for any API with any authentication that is currently in force.  We can leverage this to call the  /.auth/me  endpoint and decode the response in one line of code.  Adjust the  ExecuteRefreshCommand()  method in the  ViewModels\\TaskListViewModel.cs  file to take advantage of this:  async Task ExecuteRefreshCommand()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var identity = await cloudService.GetIdentityAsync();\n        if (identity != null)\n        {\n            var name = identity.UserClaims.FirstOrDefault(c =  c.Type.Equals( name )).Value;\n            Title = $ Tasks for {name} ;\n        }\n        var list = await Table.ReadAllItemsAsync();\n        Items.ReplaceRange(list);\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert( Items Not Loaded , ex.Message,  OK );\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}  The return value from the  GetIdentityAsync()  method is the first identity.  Normally, a user would only authenticate once, so this is fairly safe.  The number of claims returned depends on the identity provider and could easily number in the hundreds.  Even the default configuration for Azure Active Directory returns 18 claims.  These are easily handled using LINQ, however.  The  Type  property holds the type.  This could be a short (common) name.  It could also be a schema name, which looks more like a URI.  The only way to know what claims are coming back for sure is to look at the  /.auth/me  result with something like Postman.   Warn  If you are using Custom Authentication (e.g. username/password or a third-party token), then the  /.auth/me     endpoint is not available to you.  You can still produce a custom API in your backend to provide this information to your client, but you are responsible for the code - it's custom, after all!", 
            "title": "Obtaining User Claims"
        }, 
        {
            "location": "/chapter2/authorization/#authorization", 
            "text": "Now that we have covered all the techniques for authentication, it's time to look at authorization.  While authentication looked at verifying that a user is who they say they are, authorization looks at if a user is allowed to do a specific operation.  Authorization is handled within the server-side project by the  [Authorize]  attribute.  Our Azure Mobile Apps backend is leveraging this to provide authorization based on whether a user is authenticated or not.  The Authorize attribute can also check to see if a user is in a list of users or roles.  However, there is a problem with this.  The user id is not guessable and we have no roles.  To see what I mean, run the  Backend  project and set a break point on the  GetAllTodoItems()  method in the  TodoItemController , then run your server and your UWP application.   Tip  Once you have built and deployed the UWP application, it will appear in your normal Application list.  This allows you to run the application and the server at the same time on the same machine.  Alternatively, you can attach a Debugger to your Azure App Service within Visual Studio's Cloud Explorer.   Once you have authenticated, you will be able to set a break point to take a look at  this.User.Identity :   Note that the  Name  property is null.  This is the property that is used when you want to authorize individual users.  Expand the  Claims  property and then click on  Results View :   The only claims are the ones in the token, and none of them match the  RoleClaimType , so we can't use roles either.  Clearly, we are going to have to do something else.  Fortunately, we already know that we can get some information about the identity provider claims from the  /.auth/me  endpoint.  To get the extra information, we need to query the  User  object:  var identity = await User.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (Request);  There is one  Credentials  class for each supported authentication technique - Azure Active Directory, Facebook, Google, Microsoft Account and Twitter.  These are in the  Microsoft.Azure.Mobile.Server.Authentication  namespace.  They all follow the same pattern as the model we created for the client - there are Provider, UserId and UserClaims properties.  The token and any special information will be automatically decoded for you.  For instance, the TenantId is pulled out of the response for Azure AD.   Tip  You can use the AccessToken property to do Graph API lookups for most providers.  Use the Graph SDK provided by the authentication provider for this purpose; either in the mobile backend or the client.", 
            "title": "Authorization"
        }, 
        {
            "location": "/chapter2/authorization/#adding-group-claims-to-the-request", 
            "text": "There are times when you want to add something else to the token that is returned from Azure AD. The most common requirement is to add group information to the response so you can handle group-based authorization.  To add security groups to the Azure AD token:   Log into the  Azure Portal .  Click on  Azure Active Directory  in your left-hand menu (you may need to search for it).  If you need to work in a different directory, click  Switch directory  to do so.  Click  App registrations .  Click your WEB application.  Click  Manifest  at the top of the registered app blade.   You are now placed within a manifest editor.  Find the  groupMembershipClaims  entry and change it from null to  \"SecurityGroup\" .     When complete, click  Save .    You can now give the web application additional permissions:   Close the Manifest editor.  Click  Settings .  Click  Required permissions .  Click  Windows Azure Active Directory .  Click  Read directory data  under the  Delegated Permissions  section.  Scroll to the bottom, click on  Delegated Permissions .  Check the box for  Read directory data .     Click on  Save .   If you are not an administrator of the domain, then an administrator will need to approve your request.  Now that you have configured the application to return groups as part of the claims, you should probably add a couple of groups:   Click  Azure Active Directory  in the left-hand menu to return to the top-level.  Click  Users and groups .  Click  All groups .  Click  + Add .  Fill in the information, then click  Create .     Click on the new group.     Make a note of the  OBJECT ID .  The claims for groups are listed by the Object ID, so you will need this to\n   refer to the group later.   It's a good idea to add a couple of groups for testing purposes.  If you are using the organization directory, you will need to request the creation of a couple of groups for application roles.  The view of the groups will be shown when we get the identity of the user using  User.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (Request) :", 
            "title": "Adding Group Claims to the Request"
        }, 
        {
            "location": "/chapter2/authorization/#group-authorization", 
            "text": "Now that we have group claims in the claims list for the  /.auth/me  endpoint, we can move forward to do authorization\nbased on these claims.  This can be done in a relatively basic manner by implementing a method to check the claims:  async Task bool  IsAuthorizedAsync()\n{\n    var identity = await User.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (Request);\n    var countofGroups = identity.UserClaims\n        .Where(c =  c.Type.Equals( groups )   c.Value.Equals( 01f214a9-af1f-4bdd-938f-3f16749aef0e ))\n        .Count();\n    return (countofGroups   0);\n}  The  UserClaims  object is an  IEnumerable  that contains objects with a Type and a Value.  The Type for the group claims is  groups .  Once we have this knowledge, we can use a LINQ query to obtain a count of the claims that match the conditions we want to test.  The Value we use is the Object ID of the group.  This is available in the  PROPERTIES  tab of the group.  We can prevent a new record being added by adjusting the  PostTodoItem()  method:  public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n{\n    if (!await IsAuthorizedAsync())\n    {\n        return Unauthorized();\n    }\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}  Unfortunately, most of the table controller methods do not return an  IHttpActionResult , so this has limited value.  What would be better would be an  [Authorize]  attribute that tests the claims for us.  For instance, we should be able to do the following:  [AuthorizeClaims( groups ,  01f214a9-af1f-4bdd-938f-3f16749aef0e )]\npublic async Task IHttpActionResult  PostTodoItem(TodoItem item)\n{\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}  The  [AuthorizeClaims()]  attribute does not exist, so we have to provide it ourselves:  using System.Linq;\nusing System.Net;\nusing System.Security.Principal;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.Filters;\nusing Microsoft.Azure.Mobile.Server.Authentication;\n\nnamespace Backend.Helpers\n{\n    public class AuthorizeClaimsAttribute : AuthorizationFilterAttribute\n    {\n        string Type { get; }\n        string Value { get; }\n\n        public AuthorizeClaimsAttribute(string type, string value)\n        {\n            Type = type;\n            Value = value;\n        }\n\n        public override async Task OnAuthorizationAsync(HttpActionContext actionContext, CancellationToken cancellationToken)\n        {\n            var request = actionContext.Request;\n            var user = actionContext.RequestContext.Principal;\n            if (user != null)\n            {\n                var identity = await user.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (request);\n                var countOfMatchingClaims = identity.UserClaims\n                    .Where(c =  c.Type.Equals(Type)   c.Value.Equals(Value))\n                    .Count();\n                if (countOfMatchingClaims   0) return;\n\n            }\n            throw new HttpResponseException(HttpStatusCode.Unauthorized);\n        }\n    }\n}  This is the same type of authorization filter attribute that the officially provided  AuthorizeAttribute  is based on.  However, the AuthorizeAttribute is synchronous.  We require an asynchronous version of the attribute, so we cannot use a sub-class of the AuthorizeAttribute.  Aside from that note, this uses virtually the same code that we used in the  IsAythorizedAsync()  method we developped earlier.  We can now use this attribute for testing any claim.  For example, our claims has the identity provider as a claim.  We can use the following:  [AuthorizeClaims( http://schemas.microsoft.com/identity/claims/identityprovider ,  live.com )]   Tip  If you want to test other claims that are not provided, you can enable the  Read Directory Data  permission in the Azure Active Directory permissions and do a query against the Azure Active Directory.  You should think about caching results or minting a new ZUMO token (just like we did in the custom authentication case) for performance reasons.", 
            "title": "Group Authorization"
        }, 
        {
            "location": "/chapter2/realworld/", 
            "text": "Caching Tokens\n\n\nYou will notice that we have to log in with every start of the application.  The token that is generated has a lifetime that is provided and controlled by the identity provider.  Some providers have a relatively short lifetime.  For example, Azure Active Directory tokens have a lifetime of 1 hour.  Others are incredibly long.  Facebook has an expiry time of 60 days.\n\n\nIrrespective of the lifespan of the token, we will want to store it securely and re-use it when we can.  Xamarin has provided a nice component, \nXamarin.Auth\n, that provides such as secure store in a cross-platform manner.  It starts with an account store:\n\n\n// For iOS:\nvar accountStore = AccountStore.Create();\n// For Android:\nvar accountStore = AccountStore.Create(Context);\n\n\n\n\nWe can then store the token with the following:\n\n\naccountStore.Save(account, \ndescriptor\n);\n\n\n\n\nThe descriptor is a string that allows us to find the token again.  The account (which is an \nAccount\n object) is uniquely identified by a key composed of the account's Username property and the descriptor.  The Account class is provided with Xamarin.Auth.  Storage is backed by the \nKeychain\n on iOS and the \nKeyStore\n on Android.\n\n\nTo get the token back, we use the following:\n\n\nvar accounts = accountStore.FindAccountsForService(\ndescriptor\n);\n\n\n\n\nWhen we receive the token back from the key store, we will want to check the expiry time to ensure the token has not expired.  As a result, there is a little bit more code to caching code than one would expect.\n\n\nLet's start with the Android version in \nTaskList.Droid\n.  As with all the other login code, we are adjusting the \nLoginAsync()\n method in \nServices\\DroidLoginProvider.cs\n:\n\n\nusing System;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Android.App;\nusing Android.Content;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\nusing TaskList.Helpers;\nusing Xamarin.Auth;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        public Context RootView { get; private set; }\n\n        public AccountStore AccountStore { get; private set; }\n\n        public void Init(Context context)\n        {\n            RootView = context;\n            AccountStore = AccountStore.Create(context);\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Check if the token is available within the key store\n            var accounts = AccountStore.FindAccountsForService(\ntasklist\n);\n            if (accounts != null)\n            {\n                foreach (var acct in accounts)\n                {\n                    string token;\n\n                    if (acct.Properties.TryGetValue(\ntoken\n, out token))\n                    {\n                        if (!IsTokenExpired(token))\n                        {\n                            client.CurrentUser = new MobileServiceUser(acct.Username);\n                            client.CurrentUser.MobileServiceAuthenticationToken = token;\n                            return;\n                        }\n                    }\n                }\n            }\n\n            // Server Flow\n            await client.LoginAsync(RootView, \naad\n);\n\n            // Store the new token within the store\n            var account = new Account(client.CurrentUser.UserId);\n            account.Properties.Add(\ntoken\n, client.CurrentUser.MobileServiceAuthenticationToken);\n            AccountStore.Save(account, \ntasklist\n);\n        }\n\n        bool IsTokenExpired(string token)\n        {\n            // Get just the JWT part of the token (without the signature).\n            var jwt = token.Split(new Char[] { '.' })[1];\n\n            // Undo the URL encoding.\n            jwt = jwt.Replace('-', '+').Replace('_', '/');\n            switch (jwt.Length % 4)\n            {\n                case 0: break;\n                case 2: jwt += \n==\n; break;\n                case 3: jwt += \n=\n; break;\n                default:\n                    throw new ArgumentException(\nThe token is not a valid Base64 string.\n);\n            }\n\n            // Convert to a JSON String\n            var bytes = Convert.FromBase64String(jwt);\n            string jsonString = UTF8Encoding.UTF8.GetString(bytes, 0, bytes.Length);\n\n            // Parse as JSON object and get the exp field value,\n            // which is the expiration date as a JavaScript primative date.\n            JObject jsonObj = JObject.Parse(jsonString);\n            var exp = Convert.ToDouble(jsonObj[\nexp\n].ToString());\n\n            // Calculate the expiration by adding the exp value (in seconds) to the\n            // base date of 1/1/1970.\n            DateTime minTime = new DateTime(1970, 1, 1, 0, 0, 0, 0, DateTimeKind.Utc);\n            var expire = minTime.AddSeconds(exp);\n            return (expire \n DateTime.UtcNow);\n        }\n    }\n}\n\n\n\n\nThere are three new pieces to this code.  The first piece is to check to see if there is an existing token in the KeyStore.  If there is, we check the expiry time and then set up the Azure Mobile Apps client with the username and token from the KeyStore.  If there isn't, we do the normal authentication process.  If the authentication process is successful, we reach the second piece, which is to store the token within the KeyStore.  If there is an existing entry, it will be overwritten.  Finally, there is a method called \nIsTokenExpired()\n whose only job is to check to see if a token is expired or not.  This same code can be used in the \nServices/iOSLoginProvider.cs\n.  The only difference is in the \nAccountStore.Create()\n call (as discussed earlier).\n\n\n\n\nUpdate Entitlements for iOS 10\n\n\nYou may notice that you are not able to use \nAccountStore.Save()\n in the iOS 10 Simulator.  A change to the iOS entitlements has caused this change.  You must add keychain access to your Entitlements.plist file, and use the Entitlements.plist file as a custom entitlements list.  \n\n\n\n\nVisual Studio for the PC doesn't provide a lot of assistance with the entitlements.  However, Visual Studio for Mac has a great editor for the entitlement, so this is one time I'd suggest going over to the Mac to do something.  Make sure you have created an Apple Developer account and created a provisioning profile.  These are pre-requisites to using the Keychain.\n\n\n\n\nRight-click the \nTaskList.iOS\n project to open the options pane and select \nOptions\n \n\n\nSelect the \niOS Bundle Signing\n menu option.\n\n\nSelect \niPhoneSimulator\n for the Platform.\n\n\nClick the \n...\n button next to \nCustom Entitlements\n.\n\n\nSelect the \nEntitlements.plist\n file, then click \nOpen\n.\n\n\nSave the properties (I used Ctrl-S for this)\n\n\nFind and open the \nEntitlements.plist\n file in the \nTaskList.iOS\n project.\n\n\nIn the \nKeychain\n sction, check the box next to \nEnable Keychain Access Groups\n.  This may require additional setup and linking to a provisioning profile.\n\n\nSave the file and re-build your project.\n\n\n\n\nXamarin.Auth only support iOS and Android.  We need to turn to an alternate library for token caching on Universal Windows.  The standard library has a package called \nPasswordVault\n that can be used identically to the \nKeyStore\n and \nKeychain\n libraries.  Here is the Universal Windows version of the same code in \nServices\\UWPLoginProvider.cs\n:\n\n\nusing System;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.UWP.Services;\nusing Windows.Security.Credentials;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public PasswordVault PasswordVault { get; private set; }\n\n        public UWPLoginProvider()\n        {\n            PasswordVault = new PasswordVault();\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Check if the token is available within the password vault\n            var acct = PasswordVault.FindAllByResource(\ntasklist\n).FirstOrDefault();\n            if (acct != null)\n            {\n                var token = PasswordVault.Retrieve(\ntasklist\n, acct.UserName).Password;\n                if (token != null \n token.Length \n 0 \n !IsTokenExpired(token))\n                {\n                    client.CurrentUser = new MobileServiceUser(acct.UserName);\n                    client.CurrentUser.MobileServiceAuthenticationToken = token;\n                    return;\n                }\n            }\n\n            // Server-Flow Version\n            await client.LoginAsync(\naad\n);\n\n            // Store the token in the password vault\n            PasswordVault.Add(new PasswordCredential(\ntasklist\n,\n                client.CurrentUser.UserId,\n                client.CurrentUser.MobileServiceAuthenticationToken));\n        }\n\n        bool IsTokenExpired(string token)\n        {\n            /* Copy code from DroidLoginProvider */\n        }\n    }\n}\n\n\n\n\nThe PasswordVault replaces the KeyStore (Android) and Keychain (iOS), but the concepts are the same.  All three\nmechanisms provide the basic functionality of storing client secrets securely.\n\n\nRefresh Tokens\n\n\nThe token cache checks the token to see if it is expired and prompts the user if the token is no longer valid.  Since the life of a token is inevitably short (maybe one hour), this will still mean that the user is prompted for new credentials most of the time.  In addition, we have an issue when the app is running for a long time.  What happens if the user leaves the app running for 2 hours?  The token we received at the start of the session will be invalid halfway through the session and we will have to restart the app in order to continue.  Both of these situations are undesirable from the point of view of the user.  Access tokens eventually expire and we need to explicitly deal with this situation.\n\n\nThe first part of the solution is to request a \nRefresh Token\n.  This is something the identity provider issues when the scope of the request includes an offline scope.  Only certain identity providers include the ability to request refresh tokens.  For server-flow:\n\n\n\n\nGoogle: Append the \"access_type=offline\" to the request.\n\n\nMicrosoft Account: Select the wl.offline_access scope in the Azure management portal.\n\n\nAzure AD: Configure Azure AD to support access to the Graph API.\n\n\n\n\nFacebook and Twitter do not provider refresh tokens.  Once you have the refresh tokens, you can simply call the refresh API in the Azure Mobile Apps SDK to refresh the token.\n\n\n\n\nInfo\n\n\nRefresh Tokens are one area that require special consideration when using Custom Authentication.  Just like with the /.auth/me endpoint, you are on your own when it comes to handling token expiry for custom authentication.\n\n\n\n\nConfiguring Refresh Tokens\n\n\nYou can add the additional information to a Google request with the following code snippet:\n\n\nclient.LoginAsync(\ngoogle\n, new Dictionary\nstring, string\n\n{\n    { \naccess_type\n, \noffline\n }\n});\n\n\n\n\nAzure Active Directory is perhaps the trickiest to configure.\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\nNavigate to your Azure Active Directory.\n\n\nClick \nApp registrations\n, then your WEB application\n\n\nClick \nKeys\n.\n\n\n\n\nEnter a friendly name.  Pick \n2 years\n in the Expiry duration\n\n\n\n\n\n\n\n\nClick \nSave\n.  The key will be generated for you.  Copy the key (you will need it below).\n\n\n\n\nGo to \nApp Services\n, then your App Service.\n\n\nClick \nResource explorer\n in the menu, then \nGo\n.\n\n\nIn the Resource Explorer, expand \nconfig\n and select \nauthsettings\n.\n\n\nClick on \nEdit\n.\n\n\nSet the clientSecret to the key you copied from above.\n\n\nSet the additionalLoginParams to \n[\"response_type=code id_token\"]\n.\n\n\n\n\n\n\n\n\nClick the \nRead/Write\n toggle button at the top of the page.\n\n\nClick the \nPUT\n button.\n\n\n\n\nThe next time the user logs into our web app side, there will be a one-time prompt to consent to graph API access.\nOnce granted, the App Service Authentication / Authorization service will start requesting and receiving refresh\ntokens.  Once you go through this process and re-authenticate, you will be able to see the refresh token in the output of the \n/.auth/me\n endpoint:\n\n\n\n\nRefresh tokens have a different expiry time to the identity token.  The refresh token theoretically lives forever,\nbut there are \"non-use expiry\" times. This varies by identity provider.\n\n\n\n\nGoogle: 6 months\n\n\nMicrosoft Account: 24 hours\n\n\nAzure Active Directory: 90 days\n\n\n\n\nIn addition, there may be other reasons why a token can be invalidated.  For instance, Google provides 25 refresh\ntokens per user.  If the user requests more than the limit, the oldest token is invalidated.  You should refer\nto the OAuth documentation for the identity provider.\n\n\nUsing Refresh Tokens\n\n\nThe Azure Mobile Apps Client SDK has a built in method for refreshing tokens for you.  It assumes that you are using\na supported identity provider (Azure Active Directory, Google or Microsoft Account), and have configured the identity\nprovider to generate the refresh token.  To refresh a token, use:\n\n\nclient.RefreshUserAsync();\n\n\n\n\n\n\nTip\n\n\nIf you get the error \"You do not have permission to view this directory or page\" when accessing the refresh\nendpoint, there are no refresh tokens for your user in the token store.  This could be because the user has\nyet to re-authenticate (causing a new refresh token to be generated), the provider is not set up to generate\nrefresh tokens or the provider does not support refresh tokens.\n\n\n\n\nWe can easily add this to the login process in the platform-specific provider.  Rather than provide the same logic\nover and over, we can extend the \nILoginProvider\n to do the base operations for us then implement the logic once\nin the \nAzureCloudService\n.  The \nAbstractions\\ILoginProvider.cs\n interface now looks like this:\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        MobileServiceUser RetrieveTokenFromSecureStore();\n\n        void StoreTokenInSecureStore(MobileServiceUser user);\n\n        Task\nMobileServiceUser\n LoginAsync(MobileServiceClient client);\n    }\n}\n\n\n\n\nSince the \nRefreshUserAsync()\n method is purely contained within the Azure Mobile Apps Client SDK and requires\nno changes between platforms, we don't need a special platform-specific version.  Each method of the interface\nis one of the primitives we have already discussed.  For example, the Android version in  \nServices\\DroidLoginProvider.cs\n\nnow looks like this:\n\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        #region ILoginProvider Interface\n        public MobileServiceUser RetrieveTokenFromSecureStore()\n        {\n            var accounts = AccountStore.FindAccountsForService(\ntasklist\n);\n            if (accounts != null)\n            {\n                foreach (var acct in accounts)\n                {\n                    string token;\n\n                    if (acct.Properties.TryGetValue(\ntoken\n, out token))\n                    {\n                        return new MobileServiceUser(acct.Username)\n                        {\n                            MobileServiceAuthenticationToken = token\n                        };\n                    }\n                }\n            }\n            return null;\n        }\n\n        public void StoreTokenInSecureStore(MobileServiceUser user)\n        {\n            var account = new Account(user.UserId);\n            account.Properties.Add(\ntoken\n, user.MobileServiceAuthenticationToken);\n            AccountStore.Save(account, \ntasklist\n);\n        }\n\n        public async Task\nMobileServiceUser\n LoginAsync(MobileServiceClient client)\n        {\n            // Server Flow\n            return await client.LoginAsync(RootView, \naad\n);\n        }\n        #endregion\n\n        public Context RootView { get; private set; }\n\n        public AccountStore AccountStore { get; private set; }\n\n        public void Init(Context context)\n        {\n            RootView = context;\n            AccountStore = AccountStore.Create(context);\n        }\n    }\n}\n\n\n\n\nThe iOS version is practically the same because we are using the common Xamarin.Auth portable library.  The\ndifference is in the methods outside of the ILoginProvider interface:\n\n\npublic UIViewController RootView =\n UIApplication.SharedApplication.KeyWindow.RootViewController;\n\npublic AccountStore AccountStore { get; private set; }\n\npublic iOSLoginProvider()\n{\n    AccountStore = AccountStore.Create();\n}\n\n\n\n\nFinally, the Universal Windows version (in \nServices\\UWPLoginProvider.cs\n) is significantly different in the\nsecure store implementation:\n\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public PasswordVault PasswordVault { get; private set; }\n\n        public UWPLoginProvider()\n        {\n            PasswordVault = new PasswordVault();\n        }\n\n        #region ILoginProvider Interface\n        public MobileServiceUser RetrieveTokenFromSecureStore()\n        {\n            try\n            {\n                // Check if the token is available within the password vault\n                var acct = PasswordVault.FindAllByResource(\ntasklist\n).FirstOrDefault();\n                if (acct != null)\n                {\n                    var token = PasswordVault.Retrieve(\ntasklist\n, acct.UserName).Password;\n                    if (token != null \n token.Length \n 0)\n                    {\n                        return new MobileServiceUser(acct.UserName)\n                        {\n                            MobileServiceAuthenticationToken = token\n                        };\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($\nError retrieving existing token: {ex.Message}\n);\n            }\n            return null;\n        }\n\n        public void StoreTokenInSecureStore(MobileServiceUser user)\n        {\n            PasswordVault.Add(new PasswordCredential(\ntasklist\n, user.UserId, user.MobileServiceAuthenticationToken));\n        }\n\n        public async Task\nMobileServiceUser\n LoginAsync(MobileServiceClient client)\n        {\n            // Server-Flow Version\n            return await client.LoginAsync(\naad\n);\n        }\n        #endregion\n    }\n}\n\n\n\n\nWe can swap out the server-flow Azure Active Directory login method with any of the client-flow, server-flow or\ncustom flows that we have been discussing thus far across all three platform-specific implementations.\n\n\nThe common flow handles all the logic for us.  This is the \nLoginAsync()\n method in the \nServices\\AzureCloudService.cs\n\nclass:\n\n\npublic async Task\nMobileServiceUser\n LoginAsync()\n{\n    var loginProvider = DependencyService.Get\nILoginProvider\n();\n\n    client.CurrentUser = loginProvider.RetrieveTokenFromSecureStore();\n    if (client.CurrentUser != null)\n    {\n        // User has previously been authenticated - try to Refresh the token\n        try\n        {\n            var refreshed = await client.RefreshUserAsync();\n            if (refreshed != null)\n            {\n                loginProvider.StoreTokenInSecureStore(refreshed);\n                return refreshed;\n            }\n        }\n        catch (Exception refreshException)\n        {\n            Debug.WriteLine($\nCould not refresh token: {refreshException.Message}\n);\n        }\n    }\n\n    if (client.CurrentUser != null \n !IsTokenExpired(client.CurrentUser.MobileServiceAuthenticationToken))\n    {\n        // User has previously been authenticated, no refresh is required\n        return client.CurrentUser;\n    }\n\n    // We need to ask for credentials at this point\n    await loginProvider.LoginAsync(client);\n    if (client.CurrentUser != null)\n    {\n        // We were able to successfully log in\n        loginProvider.StoreTokenInSecureStore(client.CurrentUser);\n    }\n    return client.CurrentUser;\n}\n\n\n\n\nFor full disclosure, I've also moved the \nIsTokenExpired()\n method from the platform-specific code to the shared project, and updated the \nICloudService.cs\n to match the new signature of \nLoginAsync()\n.  The process follows the best practices:\n\n\n\n\nCheck for a stored token - if one exists, try to refresh it.\n\n\nIf the token (that potentially just got refreshed) is not expired, continue using it.\n\n\nIf not, ask the user for credentials.\n\n\nIf we get a valid token back, store it in the secure store for next time.\n\n\n\n\nThere is another place that we must consider refresh tokens.  During a HTTP request to our mobile backend, it is possible that the token has expired since our last request.  The request will return a 401 Unauthorized response in this case.  We need to trap that and perform a login request.  The login request will either refresh the token or prompt the user for new credentials.  We can then continue with the request as before.\n\n\nThe Azure Mobile Apps SDK contains a mechanism for hooking into the HTTP workflow using a \nDelegatingHandler\n. A delegating handler is a base type for a HTTP handler that allows us to process the request and response from the HTTP client object before (and after) it finally get processed.  It's used for adding additional headers to the request or logging the request and response, for example.  We are going to use it to validate the response and re-submit the request (after login) if the request comes back as a 401 Unauthorized.\n\n\nWe start with the adjustment to the \nServices\\AzureCloudService.cs\n constructor:\n\n\npublic AzureCloudService()\n{\n    client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler());\n\n    if (Locations.AlternateLoginHost != null)\n        client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n}\n\n\n\n\nThe \nAuthenticationDelegatingHandler()\n is the new piece here.  This is the delegating handler that we are going\nto implement to handle the re-try logic.  I've placed the code in \nHelpers\\AuthenticationDelegatingHandler.cs\n:\n\n\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Helpers\n{\n    class AuthenticationDelegatingHandler : DelegatingHandler\n    {\n        protected override async Task\nHttpResponseMessage\n SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n        {\n            // Clone the request, in case we need to re-issue it\n            var clone = await CloneHttpRequestMessageAsync(request);\n            // Now do the request\n            var response = await base.SendAsync(request, cancellationToken);\n\n            if (response.StatusCode == HttpStatusCode.Unauthorized)\n            {\n                // The request resulted in a 401 Unauthorized.  We need to do a LoginAsync,\n                // which will do the Refresh if appropriate, or ask for credentials if not.\n                var user = await ServiceLocator.Instance.Resolve\nICloudService\n().LoginAsync();\n\n                // Now, retry the request with the cloned request.  The only thing we have\n                // to do is replace the X-ZUMO-AUTH header with the new auth token.\n                clone.Headers.Remove(\nX-ZUMO-AUTH\n);\n                clone.Headers.Add(\nX-ZUMO-AUTH\n, user.MobileServiceAuthenticationToken);\n                response = await base.SendAsync(clone, cancellationToken);\n            }\n\n            return response;\n        }\n\n        /// \nsummary\n\n        /// Clone a HttpRequestMessage\n        /// Credit: http://stackoverflow.com/questions/25044166/how-to-clone-a-httprequestmessage-when-the-original-request-has-content\n        /// \n/summary\n\n        /// \nparam name=\nreq\nThe request\n/param\n\n        /// \nreturns\nA copy of the request\n/returns\n\n        public static async Task\nHttpRequestMessage\n CloneHttpRequestMessageAsync(HttpRequestMessage req)\n        {\n            HttpRequestMessage clone = new HttpRequestMessage(req.Method, req.RequestUri);\n\n            // Copy the request's content (via a MemoryStream) into the cloned object\n            var ms = new MemoryStream();\n            if (req.Content != null)\n            {\n                await req.Content.CopyToAsync(ms).ConfigureAwait(false);\n                ms.Position = 0;\n                clone.Content = new StreamContent(ms);\n\n                // Copy the content headers\n                if (req.Content.Headers != null)\n                    foreach (var h in req.Content.Headers)\n                        clone.Content.Headers.Add(h.Key, h.Value);\n            }\n\n\n            clone.Version = req.Version;\n\n            foreach (KeyValuePair\nstring, object\n prop in req.Properties)\n                clone.Properties.Add(prop);\n\n            foreach (KeyValuePair\nstring, IEnumerable\nstring\n header in req.Headers)\n                clone.Headers.TryAddWithoutValidation(header.Key, header.Value);\n\n            return clone;\n        }\n    }\n}\n\n\n\n\nThere is no in-built method for cloning a \nHttpRequestMessage\n object.  Fortunately \nStack Overflow\n provided an answer that seems to work.  Running this code will now pass every single non-login request through the delegating handler.  If we get an Unauthorized at any point, the login flow (which includes an implicit refresh token) will be triggered.\n\n\n\n\nInfo\n\n\nThere are two HTTPClient objects created inside of the \nMobileServiceClient\n object. One is for all the non-login flows and it supports the delegating handlers.  However there is another one for login flows.  The one for login flows does not support delegating handlers.  This means you don't have to worry about cyclical references within the delegating handler (where a login flow triggers another login flow).\n\n\n\n\nLogging out\n\n\nThere is a dirty little secret within the Azure Mobile Apps Client SDK.  Calling \nLogoutAsync()\n does not actually invalidate the token you are using.  It simply removes it from the \nMobileServiceClient\n context.  Don't believe me?  Here is \nthe code\n:\n\n\n        /// \nsummary\n\n        /// Log a user out.\n        /// \n/summary\n\n        public Task LogoutAsync()\n        {\n            this.CurrentUser = null;\n            return Task.FromResult(0);\n        }\n\n\n\n\nWhen you actually think about it, this makes sense.  You can get logged in via five different supported identity providers via a web-flow.  In this case, you are logging your \nbrowser\n out of the identity provider.  Do you really want to log out of Facebook when you log out of your app?\n\n\nSo, how do you log out?  You should:\n\n\n\n\nCall the identity provider logout method (if appropriate).  Many identity providers don't provide this.\n\n\nInvalidate the token on the mobile backend.\n\n\nRemove the token from the local secure cache store.\n\n\nFinally, call the \nLogoutAsync()\n method on the \nMobileServiceClient\n.\n\n\n\n\nInvalidating the token on the mobile backend.\n\n\nCalling the \n/.auth/logout\n endpoint on the Azure App Service mobile backend will remove the entry on the token store.  However, it does not (currently) invalidate the token.  The token, if submitted, will still authorize the user.  The refresh token is stored in the token store. The user submitting the token will be unable to refresh the token.  Once the ZUMO token has expired (which happens an hour after it was created), the logout is complete.\n\n\nWe need to do a HTTP client call for this purpose:\n\n\n// Invalidate the token on the mobile backend\nvar authUri = new Uri($\n{client.MobileAppUri}/.auth/logout\n);\nusing (var httpClient = new HttpClient())\n{\n    httpClient.DefaultRequestHeaders.Add(\nX-ZUMO-AUTH\n, client.CurrentUser.MobileServiceAuthenticationToken);\n    await httpClient.GetAsync(authUri);\n}\n\n\n\n\nRemoving the token from the local secure cache store.\n\n\nFor this part of the process, We can add a new method to the \nILoginProvider.cs\n interface:\n\n\nvoid RemoveTokenFromSecureStore();\n\n\n\n\nFor Android and iOS, the concrete implementation looks like this:\n\n\npublic void RemoveTokenFromSecureStore()\n{\n    var accounts = AccountStore.FindAccountsForService(\ntasklist\n);\n    if (accounts != null)\n    {\n        foreach (var acct in accounts)\n        {\n            AccountStore.Delete(acct, \ntasklist\n);\n        }\n    }\n}\n\n\n\n\nFor Universal Windows, the concrete implementation is a bit different:\n\n\npublic void RemoveTokenFromSecureStore()\n{\n    try\n    {\n        // Check if the token is available within the password vault\n        var acct = PasswordVault.FindAllByResource(\ntasklist\n).FirstOrDefault();\n        if (acct != null)\n        {\n            PasswordVault.Remove(acct);\n        }\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($\nError retrieving existing token: {ex.Message}\n);\n    }\n}\n\n\n\n\nImplementing a LogoutAsync() method.\n\n\nI've added the following to the \nICloudService\n interface:\n\n\nTask LogoutAsync();\n\n\n\n\nThis has a concrete implementation in \nServices\\AzureCloudService.cs\n:\n\n\npublic async Task LogoutAsync()\n{\n    if (client.CurrentUser == null || client.CurrentUser.MobileServiceAuthenticationToken == null)\n        return;\n\n    // Log out of the identity provider (if required)\n\n    // Invalidate the token on the mobile backend\n    var authUri = new Uri($\n{client.MobileAppUri}/.auth/logout\n);\n    using (var httpClient = new HttpClient())\n    {\n        httpClient.DefaultRequestHeaders.Add(\nX-ZUMO-AUTH\n, client.CurrentUser.MobileServiceAuthenticationToken);\n        await httpClient.GetAsync(authUri);\n    }\n\n    // Remove the token from the cache\n    DependencyService.Get\nILoginProvider\n().RemoveTokenFromSecureStore();\n\n    // Remove the token from the MobileServiceClient\n    await client.LogoutAsync();\n}\n\n\n\n\nThis does three of the four providers.  If your identity provider supports an app-level logout, then you should call that where indicated.  This is probably going to be platform-specific code, so you will want to add a method to the \nILoginProvider.cs\n interface and add a concrete implementation to each platform project.\n\n\nI've also added a logout button to my \nPages\\TaskList.xaml\n (\nview code\n) and added the event handler for the logout button to the \nViewModels\\EntryPageViewModel.cs\n (\nview code\n).", 
            "title": "Tokens in Real Apps"
        }, 
        {
            "location": "/chapter2/realworld/#caching-tokens", 
            "text": "You will notice that we have to log in with every start of the application.  The token that is generated has a lifetime that is provided and controlled by the identity provider.  Some providers have a relatively short lifetime.  For example, Azure Active Directory tokens have a lifetime of 1 hour.  Others are incredibly long.  Facebook has an expiry time of 60 days.  Irrespective of the lifespan of the token, we will want to store it securely and re-use it when we can.  Xamarin has provided a nice component,  Xamarin.Auth , that provides such as secure store in a cross-platform manner.  It starts with an account store:  // For iOS:\nvar accountStore = AccountStore.Create();\n// For Android:\nvar accountStore = AccountStore.Create(Context);  We can then store the token with the following:  accountStore.Save(account,  descriptor );  The descriptor is a string that allows us to find the token again.  The account (which is an  Account  object) is uniquely identified by a key composed of the account's Username property and the descriptor.  The Account class is provided with Xamarin.Auth.  Storage is backed by the  Keychain  on iOS and the  KeyStore  on Android.  To get the token back, we use the following:  var accounts = accountStore.FindAccountsForService( descriptor );  When we receive the token back from the key store, we will want to check the expiry time to ensure the token has not expired.  As a result, there is a little bit more code to caching code than one would expect.  Let's start with the Android version in  TaskList.Droid .  As with all the other login code, we are adjusting the  LoginAsync()  method in  Services\\DroidLoginProvider.cs :  using System;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Android.App;\nusing Android.Content;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Droid.Services;\nusing TaskList.Helpers;\nusing Xamarin.Auth;\n\n[assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        public Context RootView { get; private set; }\n\n        public AccountStore AccountStore { get; private set; }\n\n        public void Init(Context context)\n        {\n            RootView = context;\n            AccountStore = AccountStore.Create(context);\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Check if the token is available within the key store\n            var accounts = AccountStore.FindAccountsForService( tasklist );\n            if (accounts != null)\n            {\n                foreach (var acct in accounts)\n                {\n                    string token;\n\n                    if (acct.Properties.TryGetValue( token , out token))\n                    {\n                        if (!IsTokenExpired(token))\n                        {\n                            client.CurrentUser = new MobileServiceUser(acct.Username);\n                            client.CurrentUser.MobileServiceAuthenticationToken = token;\n                            return;\n                        }\n                    }\n                }\n            }\n\n            // Server Flow\n            await client.LoginAsync(RootView,  aad );\n\n            // Store the new token within the store\n            var account = new Account(client.CurrentUser.UserId);\n            account.Properties.Add( token , client.CurrentUser.MobileServiceAuthenticationToken);\n            AccountStore.Save(account,  tasklist );\n        }\n\n        bool IsTokenExpired(string token)\n        {\n            // Get just the JWT part of the token (without the signature).\n            var jwt = token.Split(new Char[] { '.' })[1];\n\n            // Undo the URL encoding.\n            jwt = jwt.Replace('-', '+').Replace('_', '/');\n            switch (jwt.Length % 4)\n            {\n                case 0: break;\n                case 2: jwt +=  == ; break;\n                case 3: jwt +=  = ; break;\n                default:\n                    throw new ArgumentException( The token is not a valid Base64 string. );\n            }\n\n            // Convert to a JSON String\n            var bytes = Convert.FromBase64String(jwt);\n            string jsonString = UTF8Encoding.UTF8.GetString(bytes, 0, bytes.Length);\n\n            // Parse as JSON object and get the exp field value,\n            // which is the expiration date as a JavaScript primative date.\n            JObject jsonObj = JObject.Parse(jsonString);\n            var exp = Convert.ToDouble(jsonObj[ exp ].ToString());\n\n            // Calculate the expiration by adding the exp value (in seconds) to the\n            // base date of 1/1/1970.\n            DateTime minTime = new DateTime(1970, 1, 1, 0, 0, 0, 0, DateTimeKind.Utc);\n            var expire = minTime.AddSeconds(exp);\n            return (expire   DateTime.UtcNow);\n        }\n    }\n}  There are three new pieces to this code.  The first piece is to check to see if there is an existing token in the KeyStore.  If there is, we check the expiry time and then set up the Azure Mobile Apps client with the username and token from the KeyStore.  If there isn't, we do the normal authentication process.  If the authentication process is successful, we reach the second piece, which is to store the token within the KeyStore.  If there is an existing entry, it will be overwritten.  Finally, there is a method called  IsTokenExpired()  whose only job is to check to see if a token is expired or not.  This same code can be used in the  Services/iOSLoginProvider.cs .  The only difference is in the  AccountStore.Create()  call (as discussed earlier).   Update Entitlements for iOS 10  You may notice that you are not able to use  AccountStore.Save()  in the iOS 10 Simulator.  A change to the iOS entitlements has caused this change.  You must add keychain access to your Entitlements.plist file, and use the Entitlements.plist file as a custom entitlements list.     Visual Studio for the PC doesn't provide a lot of assistance with the entitlements.  However, Visual Studio for Mac has a great editor for the entitlement, so this is one time I'd suggest going over to the Mac to do something.  Make sure you have created an Apple Developer account and created a provisioning profile.  These are pre-requisites to using the Keychain.   Right-click the  TaskList.iOS  project to open the options pane and select  Options    Select the  iOS Bundle Signing  menu option.  Select  iPhoneSimulator  for the Platform.  Click the  ...  button next to  Custom Entitlements .  Select the  Entitlements.plist  file, then click  Open .  Save the properties (I used Ctrl-S for this)  Find and open the  Entitlements.plist  file in the  TaskList.iOS  project.  In the  Keychain  sction, check the box next to  Enable Keychain Access Groups .  This may require additional setup and linking to a provisioning profile.  Save the file and re-build your project.   Xamarin.Auth only support iOS and Android.  We need to turn to an alternate library for token caching on Universal Windows.  The standard library has a package called  PasswordVault  that can be used identically to the  KeyStore  and  Keychain  libraries.  Here is the Universal Windows version of the same code in  Services\\UWPLoginProvider.cs :  using System;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Microsoft.IdentityModel.Clients.ActiveDirectory;\nusing Microsoft.WindowsAzure.MobileServices;\nusing Newtonsoft.Json.Linq;\nusing TaskList.Abstractions;\nusing TaskList.Helpers;\nusing TaskList.UWP.Services;\nusing Windows.Security.Credentials;\n\n[assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public PasswordVault PasswordVault { get; private set; }\n\n        public UWPLoginProvider()\n        {\n            PasswordVault = new PasswordVault();\n        }\n\n        public async Task LoginAsync(MobileServiceClient client)\n        {\n            // Check if the token is available within the password vault\n            var acct = PasswordVault.FindAllByResource( tasklist ).FirstOrDefault();\n            if (acct != null)\n            {\n                var token = PasswordVault.Retrieve( tasklist , acct.UserName).Password;\n                if (token != null   token.Length   0   !IsTokenExpired(token))\n                {\n                    client.CurrentUser = new MobileServiceUser(acct.UserName);\n                    client.CurrentUser.MobileServiceAuthenticationToken = token;\n                    return;\n                }\n            }\n\n            // Server-Flow Version\n            await client.LoginAsync( aad );\n\n            // Store the token in the password vault\n            PasswordVault.Add(new PasswordCredential( tasklist ,\n                client.CurrentUser.UserId,\n                client.CurrentUser.MobileServiceAuthenticationToken));\n        }\n\n        bool IsTokenExpired(string token)\n        {\n            /* Copy code from DroidLoginProvider */\n        }\n    }\n}  The PasswordVault replaces the KeyStore (Android) and Keychain (iOS), but the concepts are the same.  All three\nmechanisms provide the basic functionality of storing client secrets securely.", 
            "title": "Caching Tokens"
        }, 
        {
            "location": "/chapter2/realworld/#refresh-tokens", 
            "text": "The token cache checks the token to see if it is expired and prompts the user if the token is no longer valid.  Since the life of a token is inevitably short (maybe one hour), this will still mean that the user is prompted for new credentials most of the time.  In addition, we have an issue when the app is running for a long time.  What happens if the user leaves the app running for 2 hours?  The token we received at the start of the session will be invalid halfway through the session and we will have to restart the app in order to continue.  Both of these situations are undesirable from the point of view of the user.  Access tokens eventually expire and we need to explicitly deal with this situation.  The first part of the solution is to request a  Refresh Token .  This is something the identity provider issues when the scope of the request includes an offline scope.  Only certain identity providers include the ability to request refresh tokens.  For server-flow:   Google: Append the \"access_type=offline\" to the request.  Microsoft Account: Select the wl.offline_access scope in the Azure management portal.  Azure AD: Configure Azure AD to support access to the Graph API.   Facebook and Twitter do not provider refresh tokens.  Once you have the refresh tokens, you can simply call the refresh API in the Azure Mobile Apps SDK to refresh the token.   Info  Refresh Tokens are one area that require special consideration when using Custom Authentication.  Just like with the /.auth/me endpoint, you are on your own when it comes to handling token expiry for custom authentication.", 
            "title": "Refresh Tokens"
        }, 
        {
            "location": "/chapter2/realworld/#configuring-refresh-tokens", 
            "text": "You can add the additional information to a Google request with the following code snippet:  client.LoginAsync( google , new Dictionary string, string \n{\n    {  access_type ,  offline  }\n});  Azure Active Directory is perhaps the trickiest to configure.   Log on to the  Azure portal .  Navigate to your Azure Active Directory.  Click  App registrations , then your WEB application  Click  Keys .   Enter a friendly name.  Pick  2 years  in the Expiry duration     Click  Save .  The key will be generated for you.  Copy the key (you will need it below).   Go to  App Services , then your App Service.  Click  Resource explorer  in the menu, then  Go .  In the Resource Explorer, expand  config  and select  authsettings .  Click on  Edit .  Set the clientSecret to the key you copied from above.  Set the additionalLoginParams to  [\"response_type=code id_token\"] .     Click the  Read/Write  toggle button at the top of the page.  Click the  PUT  button.   The next time the user logs into our web app side, there will be a one-time prompt to consent to graph API access.\nOnce granted, the App Service Authentication / Authorization service will start requesting and receiving refresh\ntokens.  Once you go through this process and re-authenticate, you will be able to see the refresh token in the output of the  /.auth/me  endpoint:   Refresh tokens have a different expiry time to the identity token.  The refresh token theoretically lives forever,\nbut there are \"non-use expiry\" times. This varies by identity provider.   Google: 6 months  Microsoft Account: 24 hours  Azure Active Directory: 90 days   In addition, there may be other reasons why a token can be invalidated.  For instance, Google provides 25 refresh\ntokens per user.  If the user requests more than the limit, the oldest token is invalidated.  You should refer\nto the OAuth documentation for the identity provider.", 
            "title": "Configuring Refresh Tokens"
        }, 
        {
            "location": "/chapter2/realworld/#using-refresh-tokens", 
            "text": "The Azure Mobile Apps Client SDK has a built in method for refreshing tokens for you.  It assumes that you are using\na supported identity provider (Azure Active Directory, Google or Microsoft Account), and have configured the identity\nprovider to generate the refresh token.  To refresh a token, use:  client.RefreshUserAsync();   Tip  If you get the error \"You do not have permission to view this directory or page\" when accessing the refresh\nendpoint, there are no refresh tokens for your user in the token store.  This could be because the user has\nyet to re-authenticate (causing a new refresh token to be generated), the provider is not set up to generate\nrefresh tokens or the provider does not support refresh tokens.   We can easily add this to the login process in the platform-specific provider.  Rather than provide the same logic\nover and over, we can extend the  ILoginProvider  to do the base operations for us then implement the logic once\nin the  AzureCloudService .  The  Abstractions\\ILoginProvider.cs  interface now looks like this:  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\n\nnamespace TaskList.Abstractions\n{\n    public interface ILoginProvider\n    {\n        MobileServiceUser RetrieveTokenFromSecureStore();\n\n        void StoreTokenInSecureStore(MobileServiceUser user);\n\n        Task MobileServiceUser  LoginAsync(MobileServiceClient client);\n    }\n}  Since the  RefreshUserAsync()  method is purely contained within the Azure Mobile Apps Client SDK and requires\nno changes between platforms, we don't need a special platform-specific version.  Each method of the interface\nis one of the primitives we have already discussed.  For example, the Android version in   Services\\DroidLoginProvider.cs \nnow looks like this:  [assembly: Xamarin.Forms.Dependency(typeof(DroidLoginProvider))]\nnamespace TaskList.Droid.Services\n{\n    public class DroidLoginProvider : ILoginProvider\n    {\n        #region ILoginProvider Interface\n        public MobileServiceUser RetrieveTokenFromSecureStore()\n        {\n            var accounts = AccountStore.FindAccountsForService( tasklist );\n            if (accounts != null)\n            {\n                foreach (var acct in accounts)\n                {\n                    string token;\n\n                    if (acct.Properties.TryGetValue( token , out token))\n                    {\n                        return new MobileServiceUser(acct.Username)\n                        {\n                            MobileServiceAuthenticationToken = token\n                        };\n                    }\n                }\n            }\n            return null;\n        }\n\n        public void StoreTokenInSecureStore(MobileServiceUser user)\n        {\n            var account = new Account(user.UserId);\n            account.Properties.Add( token , user.MobileServiceAuthenticationToken);\n            AccountStore.Save(account,  tasklist );\n        }\n\n        public async Task MobileServiceUser  LoginAsync(MobileServiceClient client)\n        {\n            // Server Flow\n            return await client.LoginAsync(RootView,  aad );\n        }\n        #endregion\n\n        public Context RootView { get; private set; }\n\n        public AccountStore AccountStore { get; private set; }\n\n        public void Init(Context context)\n        {\n            RootView = context;\n            AccountStore = AccountStore.Create(context);\n        }\n    }\n}  The iOS version is practically the same because we are using the common Xamarin.Auth portable library.  The\ndifference is in the methods outside of the ILoginProvider interface:  public UIViewController RootView =  UIApplication.SharedApplication.KeyWindow.RootViewController;\n\npublic AccountStore AccountStore { get; private set; }\n\npublic iOSLoginProvider()\n{\n    AccountStore = AccountStore.Create();\n}  Finally, the Universal Windows version (in  Services\\UWPLoginProvider.cs ) is significantly different in the\nsecure store implementation:  [assembly: Xamarin.Forms.Dependency(typeof(UWPLoginProvider))]\nnamespace TaskList.UWP.Services\n{\n    public class UWPLoginProvider : ILoginProvider\n    {\n        public PasswordVault PasswordVault { get; private set; }\n\n        public UWPLoginProvider()\n        {\n            PasswordVault = new PasswordVault();\n        }\n\n        #region ILoginProvider Interface\n        public MobileServiceUser RetrieveTokenFromSecureStore()\n        {\n            try\n            {\n                // Check if the token is available within the password vault\n                var acct = PasswordVault.FindAllByResource( tasklist ).FirstOrDefault();\n                if (acct != null)\n                {\n                    var token = PasswordVault.Retrieve( tasklist , acct.UserName).Password;\n                    if (token != null   token.Length   0)\n                    {\n                        return new MobileServiceUser(acct.UserName)\n                        {\n                            MobileServiceAuthenticationToken = token\n                        };\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                Debug.WriteLine($ Error retrieving existing token: {ex.Message} );\n            }\n            return null;\n        }\n\n        public void StoreTokenInSecureStore(MobileServiceUser user)\n        {\n            PasswordVault.Add(new PasswordCredential( tasklist , user.UserId, user.MobileServiceAuthenticationToken));\n        }\n\n        public async Task MobileServiceUser  LoginAsync(MobileServiceClient client)\n        {\n            // Server-Flow Version\n            return await client.LoginAsync( aad );\n        }\n        #endregion\n    }\n}  We can swap out the server-flow Azure Active Directory login method with any of the client-flow, server-flow or\ncustom flows that we have been discussing thus far across all three platform-specific implementations.  The common flow handles all the logic for us.  This is the  LoginAsync()  method in the  Services\\AzureCloudService.cs \nclass:  public async Task MobileServiceUser  LoginAsync()\n{\n    var loginProvider = DependencyService.Get ILoginProvider ();\n\n    client.CurrentUser = loginProvider.RetrieveTokenFromSecureStore();\n    if (client.CurrentUser != null)\n    {\n        // User has previously been authenticated - try to Refresh the token\n        try\n        {\n            var refreshed = await client.RefreshUserAsync();\n            if (refreshed != null)\n            {\n                loginProvider.StoreTokenInSecureStore(refreshed);\n                return refreshed;\n            }\n        }\n        catch (Exception refreshException)\n        {\n            Debug.WriteLine($ Could not refresh token: {refreshException.Message} );\n        }\n    }\n\n    if (client.CurrentUser != null   !IsTokenExpired(client.CurrentUser.MobileServiceAuthenticationToken))\n    {\n        // User has previously been authenticated, no refresh is required\n        return client.CurrentUser;\n    }\n\n    // We need to ask for credentials at this point\n    await loginProvider.LoginAsync(client);\n    if (client.CurrentUser != null)\n    {\n        // We were able to successfully log in\n        loginProvider.StoreTokenInSecureStore(client.CurrentUser);\n    }\n    return client.CurrentUser;\n}  For full disclosure, I've also moved the  IsTokenExpired()  method from the platform-specific code to the shared project, and updated the  ICloudService.cs  to match the new signature of  LoginAsync() .  The process follows the best practices:   Check for a stored token - if one exists, try to refresh it.  If the token (that potentially just got refreshed) is not expired, continue using it.  If not, ask the user for credentials.  If we get a valid token back, store it in the secure store for next time.   There is another place that we must consider refresh tokens.  During a HTTP request to our mobile backend, it is possible that the token has expired since our last request.  The request will return a 401 Unauthorized response in this case.  We need to trap that and perform a login request.  The login request will either refresh the token or prompt the user for new credentials.  We can then continue with the request as before.  The Azure Mobile Apps SDK contains a mechanism for hooking into the HTTP workflow using a  DelegatingHandler . A delegating handler is a base type for a HTTP handler that allows us to process the request and response from the HTTP client object before (and after) it finally get processed.  It's used for adding additional headers to the request or logging the request and response, for example.  We are going to use it to validate the response and re-submit the request (after login) if the request comes back as a 401 Unauthorized.  We start with the adjustment to the  Services\\AzureCloudService.cs  constructor:  public AzureCloudService()\n{\n    client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler());\n\n    if (Locations.AlternateLoginHost != null)\n        client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n}  The  AuthenticationDelegatingHandler()  is the new piece here.  This is the delegating handler that we are going\nto implement to handle the re-try logic.  I've placed the code in  Helpers\\AuthenticationDelegatingHandler.cs :  using System.Collections.Generic;\nusing System.IO;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Helpers\n{\n    class AuthenticationDelegatingHandler : DelegatingHandler\n    {\n        protected override async Task HttpResponseMessage  SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n        {\n            // Clone the request, in case we need to re-issue it\n            var clone = await CloneHttpRequestMessageAsync(request);\n            // Now do the request\n            var response = await base.SendAsync(request, cancellationToken);\n\n            if (response.StatusCode == HttpStatusCode.Unauthorized)\n            {\n                // The request resulted in a 401 Unauthorized.  We need to do a LoginAsync,\n                // which will do the Refresh if appropriate, or ask for credentials if not.\n                var user = await ServiceLocator.Instance.Resolve ICloudService ().LoginAsync();\n\n                // Now, retry the request with the cloned request.  The only thing we have\n                // to do is replace the X-ZUMO-AUTH header with the new auth token.\n                clone.Headers.Remove( X-ZUMO-AUTH );\n                clone.Headers.Add( X-ZUMO-AUTH , user.MobileServiceAuthenticationToken);\n                response = await base.SendAsync(clone, cancellationToken);\n            }\n\n            return response;\n        }\n\n        ///  summary \n        /// Clone a HttpRequestMessage\n        /// Credit: http://stackoverflow.com/questions/25044166/how-to-clone-a-httprequestmessage-when-the-original-request-has-content\n        ///  /summary \n        ///  param name= req The request /param \n        ///  returns A copy of the request /returns \n        public static async Task HttpRequestMessage  CloneHttpRequestMessageAsync(HttpRequestMessage req)\n        {\n            HttpRequestMessage clone = new HttpRequestMessage(req.Method, req.RequestUri);\n\n            // Copy the request's content (via a MemoryStream) into the cloned object\n            var ms = new MemoryStream();\n            if (req.Content != null)\n            {\n                await req.Content.CopyToAsync(ms).ConfigureAwait(false);\n                ms.Position = 0;\n                clone.Content = new StreamContent(ms);\n\n                // Copy the content headers\n                if (req.Content.Headers != null)\n                    foreach (var h in req.Content.Headers)\n                        clone.Content.Headers.Add(h.Key, h.Value);\n            }\n\n\n            clone.Version = req.Version;\n\n            foreach (KeyValuePair string, object  prop in req.Properties)\n                clone.Properties.Add(prop);\n\n            foreach (KeyValuePair string, IEnumerable string  header in req.Headers)\n                clone.Headers.TryAddWithoutValidation(header.Key, header.Value);\n\n            return clone;\n        }\n    }\n}  There is no in-built method for cloning a  HttpRequestMessage  object.  Fortunately  Stack Overflow  provided an answer that seems to work.  Running this code will now pass every single non-login request through the delegating handler.  If we get an Unauthorized at any point, the login flow (which includes an implicit refresh token) will be triggered.   Info  There are two HTTPClient objects created inside of the  MobileServiceClient  object. One is for all the non-login flows and it supports the delegating handlers.  However there is another one for login flows.  The one for login flows does not support delegating handlers.  This means you don't have to worry about cyclical references within the delegating handler (where a login flow triggers another login flow).", 
            "title": "Using Refresh Tokens"
        }, 
        {
            "location": "/chapter2/realworld/#logging-out", 
            "text": "There is a dirty little secret within the Azure Mobile Apps Client SDK.  Calling  LogoutAsync()  does not actually invalidate the token you are using.  It simply removes it from the  MobileServiceClient  context.  Don't believe me?  Here is  the code :          ///  summary \n        /// Log a user out.\n        ///  /summary \n        public Task LogoutAsync()\n        {\n            this.CurrentUser = null;\n            return Task.FromResult(0);\n        }  When you actually think about it, this makes sense.  You can get logged in via five different supported identity providers via a web-flow.  In this case, you are logging your  browser  out of the identity provider.  Do you really want to log out of Facebook when you log out of your app?  So, how do you log out?  You should:   Call the identity provider logout method (if appropriate).  Many identity providers don't provide this.  Invalidate the token on the mobile backend.  Remove the token from the local secure cache store.  Finally, call the  LogoutAsync()  method on the  MobileServiceClient .", 
            "title": "Logging out"
        }, 
        {
            "location": "/chapter2/realworld/#invalidating-the-token-on-the-mobile-backend", 
            "text": "Calling the  /.auth/logout  endpoint on the Azure App Service mobile backend will remove the entry on the token store.  However, it does not (currently) invalidate the token.  The token, if submitted, will still authorize the user.  The refresh token is stored in the token store. The user submitting the token will be unable to refresh the token.  Once the ZUMO token has expired (which happens an hour after it was created), the logout is complete.  We need to do a HTTP client call for this purpose:  // Invalidate the token on the mobile backend\nvar authUri = new Uri($ {client.MobileAppUri}/.auth/logout );\nusing (var httpClient = new HttpClient())\n{\n    httpClient.DefaultRequestHeaders.Add( X-ZUMO-AUTH , client.CurrentUser.MobileServiceAuthenticationToken);\n    await httpClient.GetAsync(authUri);\n}", 
            "title": "Invalidating the token on the mobile backend."
        }, 
        {
            "location": "/chapter2/realworld/#removing-the-token-from-the-local-secure-cache-store", 
            "text": "For this part of the process, We can add a new method to the  ILoginProvider.cs  interface:  void RemoveTokenFromSecureStore();  For Android and iOS, the concrete implementation looks like this:  public void RemoveTokenFromSecureStore()\n{\n    var accounts = AccountStore.FindAccountsForService( tasklist );\n    if (accounts != null)\n    {\n        foreach (var acct in accounts)\n        {\n            AccountStore.Delete(acct,  tasklist );\n        }\n    }\n}  For Universal Windows, the concrete implementation is a bit different:  public void RemoveTokenFromSecureStore()\n{\n    try\n    {\n        // Check if the token is available within the password vault\n        var acct = PasswordVault.FindAllByResource( tasklist ).FirstOrDefault();\n        if (acct != null)\n        {\n            PasswordVault.Remove(acct);\n        }\n    }\n    catch (Exception ex)\n    {\n        Debug.WriteLine($ Error retrieving existing token: {ex.Message} );\n    }\n}", 
            "title": "Removing the token from the local secure cache store."
        }, 
        {
            "location": "/chapter2/realworld/#implementing-a-logoutasync-method", 
            "text": "I've added the following to the  ICloudService  interface:  Task LogoutAsync();  This has a concrete implementation in  Services\\AzureCloudService.cs :  public async Task LogoutAsync()\n{\n    if (client.CurrentUser == null || client.CurrentUser.MobileServiceAuthenticationToken == null)\n        return;\n\n    // Log out of the identity provider (if required)\n\n    // Invalidate the token on the mobile backend\n    var authUri = new Uri($ {client.MobileAppUri}/.auth/logout );\n    using (var httpClient = new HttpClient())\n    {\n        httpClient.DefaultRequestHeaders.Add( X-ZUMO-AUTH , client.CurrentUser.MobileServiceAuthenticationToken);\n        await httpClient.GetAsync(authUri);\n    }\n\n    // Remove the token from the cache\n    DependencyService.Get ILoginProvider ().RemoveTokenFromSecureStore();\n\n    // Remove the token from the MobileServiceClient\n    await client.LogoutAsync();\n}  This does three of the four providers.  If your identity provider supports an app-level logout, then you should call that where indicated.  This is probably going to be platform-specific code, so you will want to add a method to the  ILoginProvider.cs  interface and add a concrete implementation to each platform project.  I've also added a logout button to my  Pages\\TaskList.xaml  ( view code ) and added the event handler for the logout button to the  ViewModels\\EntryPageViewModel.cs  ( view code ).", 
            "title": "Implementing a LogoutAsync() method."
        }, 
        {
            "location": "/chapter2/bestpractices/", 
            "text": "Best Practices\n\n\nWe've covered a lot of ground with authentication and authorization, so I wanted to cover some of the\nbest practices that I generally advise when thinking about this topic.\n\n\nDon't store passwords\n\n\nI can't really advise on which identity provider is best for your mobile application.  However, I can\nclearly say that delegating the security of the identity database to someone who has that as their full\ntime job is an excellent idea.\n\n\nChoosing an identity provider is not easy.  Here are my choices:\n\n\n\n\nIf you need enterprise authentication, use \nAzure Active Directory\n.\n\n\nIf you need a specific social identity provider (for example, Facebook or Google), use that.\n\n\nIf you need multiple social identity providers, \nAuth0\n is an excellent choice.\n\n\nIf you need usernames and password, use \nAzure Active Directory B2C\n.\n\n\n\n\nStoring usernames and passwords in your own database is a bad idea and should be avoided.  Avoid using Microsoft Account and Twitter authentication providers.  Their implementation has limitations that will be a concern if you ever try to share an authentication provider between two different backends or apps.\n\n\nUse the client SDK provided by the Identity Provider\n\n\nYour first step should be getting an access token from the identity provider itself.  You will see the\nmost integrated experience if you use their SDK.\n\n\n\n\nFor \nAzure Active Directory\n, that SDK is \nADAL\n.\n\n\nFor \nFacebook\n, check out \nXamarin.Facebook.iOS\n or \nXamarin.Facebook.Android\n.\n\n\nFor \nGoogle\n, check out \nGoogle APIs Core Client Library\n.\n\n\nFor \nAuth0\n, check out the \nAuth0 Xamarin Component\n.\n\n\nFor \nAzure Active Directory B2C\n, use \nADAL\n.\n\n\n\n\nSwap the Identity Provider access token for a ZUMO token.\n\n\nNever use the token provided by the identity provider for anything other than requesting access to\nthe resource.  Use a short lived token (an hour is the standard) that is minted just for the purpose\nof providing that access.  If you are not using an identity provider that is explicitly supported\nby Azure App Service, use a custom authentication provider to mint your own token.\n\n\nEnforce Security at the Server\n\n\nThere is no easy way to say this.  There are bad guys out there, and they are after your data.  You\nshould not assume that someone is using your client.  It could just be someone with a REST client.\nEnsure you enforce security on your server.  You can do this easily by using the \n[Authorize]\n\nattribute, the \n[AuthorizeClaims()]\n attribute we developed in the Authorization section or your\nown custom authorization attribute.\n\n\nMonitoring the server is just as important as enforcing security.  The Azure App Service Authentication\nservice outputs quite a bit of logging about who is logging in (and who is denied), so you can get some\ngood intelligence out of the logs when you mine them properly.\n\n\nSecurely Store Tokens\n\n\nThe decisions we make are always a trade-off between convenience and security.  One such decision is\nif we should store tokens with the app.  On the one hand, it's convenient to allow the app to remember\nthe login and to not ask us to log in again with each app start.  On the other hand, that fact opens\nup a security hole that a determined hacker can exploit.  We can have convenience while still having\nsecurity by utilizing the secure stores that each platform provides to store secrets like the access\ntoken.\n\n\nUse https only\n\n\nThis should go without saying.  Always use https communication.  Most security professionals start off\ntheir security career with learning \"Security at Rest \n Security in Transit\".  In practical terms, storing\nsecrets (like tokens) securely and using HTTPS as a transport mechanism satisfies both claims.\n\n\nDon't stop with security there though.  HTTPS is just a medium through which secure communications can\ntake place.  There are a wide range of protocols and ciphers that can be used to encrypt the traffic.\nSome are  considered less secure and not to be used.  Azure App Service provides a default set of protocols\nand ciphers to support backwards compatibility with older browsers.  You can adjust the ciphers in use\nby your App Service.  For information on this, refer to the \nAzure Documentation\n.\n\n\nHandle Expiring Tokens\n\n\nUnless you are using an identity provider that doesn't support refresh tokens (like Facebook or Twitter),\nyou should handle refresh tokens by silently calling the refresh action.  Tokens are going to expire.\nThis is a fact of the token specifications.  You need to deal with expiring tokens and act accordingly.\n\n\nIf you do need to use an identity provider that does not support refresh tokens, you are going to have\nto ask for credentials whenever the token expires.  You don't get out of determining the user experience\nwhen tokens expire just because you are using Facebook or Twitter.\n\n\nAuthenticating Your App\n\n\nMy final word on authentication has to do with authenticating your app.  I get the same request every\nweek.  How do I implement an API key for my app?  When I probe a little, I get a few reasons for this\nrequest:\n\n\n\n\nI want to ensure my app is the only one accessing my backend because the data is important.\n\n\nI don't want my users to log in as it is inconvenient.\n\n\nI want to monetize my app, and I can't do that if anyone can copy it.\n\n\n\n\nAPI keys are used by multi-tenant systems to route requests for data to the appropriate data store.  For\nexample, the very popular \nParse Server\n used to have an API key because all clients connected to the\nsame \nparse.com\n service.  Once the \nParse Server\n was open-sourced, the API key went away.  It was no\nlonger needed to route the request.  In the same way, the Azure App Service has a unique name - the URL\nof the service, so it doesn't need an API key to route the information.\n\n\nAn API key does not prevent a rogue client from accessing your data.  If you did use an API key for security,\nyou can easily get the API key for the app by putting together a \"man in the middle proxy\".  One such proxy\nis \nTelerik Fiddler\n.  One of its features is \"Security Testing\" which amounts to a man-in-the-middle\ndecryption technique.  This \nworks with the Android emulator\n as well.  For iOS, you can use \nCharles\n.\n\n\nSo, how do you authenticate your app?  Step back a moment.  What are you monetizing or protecting?  It's\nlikely the data within the mobile backend.  Protect that data by authenticating your users.   If you absolutely\nmust monetize your app, then there are ways to do it, and we will discuss those later in the book.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter2/bestpractices/#best-practices", 
            "text": "We've covered a lot of ground with authentication and authorization, so I wanted to cover some of the\nbest practices that I generally advise when thinking about this topic.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter2/bestpractices/#dont-store-passwords", 
            "text": "I can't really advise on which identity provider is best for your mobile application.  However, I can\nclearly say that delegating the security of the identity database to someone who has that as their full\ntime job is an excellent idea.  Choosing an identity provider is not easy.  Here are my choices:   If you need enterprise authentication, use  Azure Active Directory .  If you need a specific social identity provider (for example, Facebook or Google), use that.  If you need multiple social identity providers,  Auth0  is an excellent choice.  If you need usernames and password, use  Azure Active Directory B2C .   Storing usernames and passwords in your own database is a bad idea and should be avoided.  Avoid using Microsoft Account and Twitter authentication providers.  Their implementation has limitations that will be a concern if you ever try to share an authentication provider between two different backends or apps.", 
            "title": "Don't store passwords"
        }, 
        {
            "location": "/chapter2/bestpractices/#use-the-client-sdk-provided-by-the-identity-provider", 
            "text": "Your first step should be getting an access token from the identity provider itself.  You will see the\nmost integrated experience if you use their SDK.   For  Azure Active Directory , that SDK is  ADAL .  For  Facebook , check out  Xamarin.Facebook.iOS  or  Xamarin.Facebook.Android .  For  Google , check out  Google APIs Core Client Library .  For  Auth0 , check out the  Auth0 Xamarin Component .  For  Azure Active Directory B2C , use  ADAL .", 
            "title": "Use the client SDK provided by the Identity Provider"
        }, 
        {
            "location": "/chapter2/bestpractices/#swap-the-identity-provider-access-token-for-a-zumo-token", 
            "text": "Never use the token provided by the identity provider for anything other than requesting access to\nthe resource.  Use a short lived token (an hour is the standard) that is minted just for the purpose\nof providing that access.  If you are not using an identity provider that is explicitly supported\nby Azure App Service, use a custom authentication provider to mint your own token.", 
            "title": "Swap the Identity Provider access token for a ZUMO token."
        }, 
        {
            "location": "/chapter2/bestpractices/#enforce-security-at-the-server", 
            "text": "There is no easy way to say this.  There are bad guys out there, and they are after your data.  You\nshould not assume that someone is using your client.  It could just be someone with a REST client.\nEnsure you enforce security on your server.  You can do this easily by using the  [Authorize] \nattribute, the  [AuthorizeClaims()]  attribute we developed in the Authorization section or your\nown custom authorization attribute.  Monitoring the server is just as important as enforcing security.  The Azure App Service Authentication\nservice outputs quite a bit of logging about who is logging in (and who is denied), so you can get some\ngood intelligence out of the logs when you mine them properly.", 
            "title": "Enforce Security at the Server"
        }, 
        {
            "location": "/chapter2/bestpractices/#securely-store-tokens", 
            "text": "The decisions we make are always a trade-off between convenience and security.  One such decision is\nif we should store tokens with the app.  On the one hand, it's convenient to allow the app to remember\nthe login and to not ask us to log in again with each app start.  On the other hand, that fact opens\nup a security hole that a determined hacker can exploit.  We can have convenience while still having\nsecurity by utilizing the secure stores that each platform provides to store secrets like the access\ntoken.", 
            "title": "Securely Store Tokens"
        }, 
        {
            "location": "/chapter2/bestpractices/#use-https-only", 
            "text": "This should go without saying.  Always use https communication.  Most security professionals start off\ntheir security career with learning \"Security at Rest   Security in Transit\".  In practical terms, storing\nsecrets (like tokens) securely and using HTTPS as a transport mechanism satisfies both claims.  Don't stop with security there though.  HTTPS is just a medium through which secure communications can\ntake place.  There are a wide range of protocols and ciphers that can be used to encrypt the traffic.\nSome are  considered less secure and not to be used.  Azure App Service provides a default set of protocols\nand ciphers to support backwards compatibility with older browsers.  You can adjust the ciphers in use\nby your App Service.  For information on this, refer to the  Azure Documentation .", 
            "title": "Use https only"
        }, 
        {
            "location": "/chapter2/bestpractices/#handle-expiring-tokens", 
            "text": "Unless you are using an identity provider that doesn't support refresh tokens (like Facebook or Twitter),\nyou should handle refresh tokens by silently calling the refresh action.  Tokens are going to expire.\nThis is a fact of the token specifications.  You need to deal with expiring tokens and act accordingly.  If you do need to use an identity provider that does not support refresh tokens, you are going to have\nto ask for credentials whenever the token expires.  You don't get out of determining the user experience\nwhen tokens expire just because you are using Facebook or Twitter.", 
            "title": "Handle Expiring Tokens"
        }, 
        {
            "location": "/chapter2/bestpractices/#authenticating-your-app", 
            "text": "My final word on authentication has to do with authenticating your app.  I get the same request every\nweek.  How do I implement an API key for my app?  When I probe a little, I get a few reasons for this\nrequest:   I want to ensure my app is the only one accessing my backend because the data is important.  I don't want my users to log in as it is inconvenient.  I want to monetize my app, and I can't do that if anyone can copy it.   API keys are used by multi-tenant systems to route requests for data to the appropriate data store.  For\nexample, the very popular  Parse Server  used to have an API key because all clients connected to the\nsame  parse.com  service.  Once the  Parse Server  was open-sourced, the API key went away.  It was no\nlonger needed to route the request.  In the same way, the Azure App Service has a unique name - the URL\nof the service, so it doesn't need an API key to route the information.  An API key does not prevent a rogue client from accessing your data.  If you did use an API key for security,\nyou can easily get the API key for the app by putting together a \"man in the middle proxy\".  One such proxy\nis  Telerik Fiddler .  One of its features is \"Security Testing\" which amounts to a man-in-the-middle\ndecryption technique.  This  works with the Android emulator  as well.  For iOS, you can use  Charles .  So, how do you authenticate your app?  Step back a moment.  What are you monetizing or protecting?  It's\nlikely the data within the mobile backend.  Protect that data by authenticating your users.   If you absolutely\nmust monetize your app, then there are ways to do it, and we will discuss those later in the book.", 
            "title": "Authenticating Your App"
        }, 
        {
            "location": "/chapter3/dataconcepts/", 
            "text": "Data Access Concepts\n\n\nAt some point in the development of your application, you are going to want to store or retrieve data.  This could be\nas simple as a key-value store for storing personal settings, or as complex as a multi-table customer relationship\ndatabase.  The key ingredient to all these scenarios is structured data.  I say \nstructured\n here deliberately.  It\nis an important concept.  Most applications you write will require data in some sort of form that your application\nunderstands.\n\n\nIt's very tempting to use an unstructured data source (like NoSQL).  I'm a big fan of NoSQL stores since they free\nme up to concentrate on the mobile client without worrying about the data format. After all, you can store whatever\nentities you want in a NoSQL data source.  However, this is really just an example of sloppy programming.  Most\ndevelopers that use NoSQL are storing structured data in that NoSQL store to get around the problem that you actually\nhave to decide what data you want to store in the database.  For this reason, I prefer a SQL database.  It ensures\nthat I am thinking about the data format up front.  It also helps to ensure that a bad actor is not going to store\ndata I don't expect in my store.\n\n\nThis isn't to say that NoSQL doesn't have its place.  There are times when you need to store structured data where\nthe data format varies between each entity that you are storing.  This tends to be an outlier situation though.\n\n\nTables, Entities and Properties\n\n\nWhen we talk about data, we tend to talk in terms of \nTables\n, \nEntities\n and \nProperties\n.  These have equivalents\nin the SQL world (tables, rows and fields).  A \nTable\n is a collection of \nEntities\n that share a common format.  That\nformat is described in terms of \nProperties\n.  Properties are basic types (like strings, numbers, booleans and dates).\n\n\nWe create a \nTable Controller\n to expose the tables to a mobile client.  A \nTable Controller\n is a REST endpoint\nthat implements an \nOData v3\n interface.  OData is a standard interface to table data that allows the client to\nperform CRUD (create, read, update and delete) operations on the data.  In addition, it provides for a standard\nway for querying the data.  More normally, the \nTable Controller\n is accessed through the Azure Mobile Apps Client\nSDK.  We saw an example of this in Chapter 1 when we first introduced the mobile client.\n\n\nAzure Mobile Apps also deals with tables in a manner that enables offline synchronization of the data.  That means\nit must be opinionated about the data format.  Specifically,\n\n\n\n\nThere are four system properties on each entity.\n\n\nThere are limitations on relationships between tables.\n\n\nComplex types need special handling.\n\n\n\n\nLet's take each of these in turn.  We implemented this \nModel\n within the mobile client in Chapter 1:\n\n\nusing TaskList.Abstractions;\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nIt's a fairly basic model class.  Note the \nTableData\n base class.  I often say that Azure Mobile Apps implements\nan opinionated version of OData. What I mean by that is that the protocol expects certain system fields to be present\nduring the transfer.  The \nTableData\n class is a base class that implements that specification. It looks like this:\n\n\nusing System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}\n\n\n\n\nThe server side version adds another field - the \nDeleted\n boolean.  This is described in the \nITableData\n interface\nthat is provided with the Azure Mobile Apps Server SDK.\n\n\n\n\nInfo\n\n\nThe Azure Mobile Apps SDK uses \nDateTimeOffset\n instead of \nDateTime\n.  A DateTime object is time\nzone aware, and time zone definitions change over time.  The DateTimeOffset does not know anything\nabout time zones.  The DateTime representation can change depending on where you are.  The DateTimeOffset\nwill never change.  This makes it a better choice for these things.  You will see dates stored in UTC in\nyour database as a result of this.\n\n\n\n\nEach element of the TableData (and ITableData) has a purpose, nominally to deal with situations with Offline Sync.\n\n\nThe Id field\n\n\nOne of the common questions is this:  Can I use an auto-incrementing integer as an Id field?  Let's take a look at\na simple situation.  You have two clients writing to the same table.  It might look something like this:\n\n\n\n\nDevice A inserts a new record in to the database, with ID 1.  Device B is also inserting and decides to do ID 1\nas well.  This causes an immediate conflict that must be resolved.  We could fix this by requiring that new inserts\ndo not insert an ID.  However, this can cause problems in offline cases, where you may be inserting many records\nand have to refer back to them during your offline state.\n\n\nThe compromise here is to use a globally unique ID.  The \nGUID\n is a well-known algorithm and easily generated in\noffline scenarios.  This is stored as a string during transfer.\n\n\n\n\nTip\n\n\nYou can specify an Id field when creating an Entity.  However, you must ensure that it is globally unique.\n\n\n\n\nThe UpdatedAt field\n\n\nOne of the concepts that is always top of mind is ensuring that we are a good mobile citizen.  This means that we\ncare about bandwidth utilization.  Reducing transfer size is good for your users.  They use less of their data\nallowance and save time by transferring less.  One of the key components to reduce bandwidth utilization is \nIncremental\nSync\n.  With each record, we record the date it was last updated.  This is generally done for us as a database\ntrigger, so we never have to worry about setting this value.  When we synchronize our table, only the records\nthat have been updated since the last synchronization are requested.\n\n\nThe UpdatedAt field contains the date and time that the record was updated within the central (server-side)\ndatabase.  Do not maintain this field yourself.  If you need a \"last updated timestamp\" for the client-side\ninsert, update or delete methods, you must generate it yourself.\n\n\nThe Version field\n\n\nThe Version field is all about conflict detection.  Let's take two devices requesting the same table again:\n\n\n\n\nIn this diagram, Device A submits the first version of the entity.  Device B then updates the entity and posts\nit back.  This is accepted because Device B is sending the same version, so the server knows that this is an\nupdate to the latest version.  The server will send back a response with the updated version so that Device B\nknows that it has the latest version.\n\n\nLater on, Device A sends an update to the same entity.  It, however, still has version 1 of the entity.  The\nserver will reject that because of a version mismatch.\n\n\nThe Version field is maintained by the central (sever-side) SDK.  Do not maintain this field yourself.\n\n\nThe Deleted field\n\n\nWhen you are operating a service with an offline scope, you can't just delete entities.  If an entity is deleted\non Device A, it is then removed from the server.  The server does not know to send an update to that entity\nto Device B because it no longer exists.  For this reason, we never delete entities.  We use \nSoft Delete\n.\nSoft Delete is a feature whereby entities are marked as deleted by setting the Deleted flag to true.  When you\nquery the server, the deleted records are not shown unless you explicitly ask for them.  This is done as part\nof the offline sync process.\n\n\n\n\nInfo\n\n\nThe UpdatedAt and Version fields are maintained by the Azure Mobile Apps Domain Manager, generally by\ntriggers within the database.  You must not set these fields yourself.  \n\n\n\n\nThe Data Access Protocol\n\n\nGiven any particular table, there are a few endpoints that are important.  Given our TodoItem table from Chapter 1:\n\n\n\n\n\n\n\n\nOperation\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\n/tables/todoitem\n\n\nQuery the table\n\n\n\n\n\n\nGET\n\n\n/tables/todoitem/\nid\n\n\nRetrieve a single entity\n\n\n\n\n\n\nPOST\n\n\n/tables/todoitem\n\n\nAdd a new entity\n\n\n\n\n\n\nPATCH\n\n\n/tables/todoitem/\nid\n\n\nUpdate an existing entity\n\n\n\n\n\n\nDELETE\n\n\n/tables/todoitem/\nid\n\n\nDeletes an existing entity\n\n\n\n\n\n\nPOST\n\n\n/tables/todoitem/\nid\n\n\nUndelete a previously deleted entity\n\n\n\n\n\n\n\n\nWe can take a look at each of these in turn with the Azure App Service.  These can be done with Postman easily.\n\n\n\n\nInfo\n\n\nThe first request to a new Azure App Service will take some time, especially if the site has to set up the database.\n\n\n\n\nLet's start with a basic Query operation:\n\n\n\n\nWe always get an array of elements back.  These contain five system properties.  We didn't mention createdAt\nearlier - it's optional and will be maintained for you if you don't use it.  In addition, we have the fields\nthat were in our model.  If there are no elements in a table, we get an empty array.  If the table does not\nexist, we will get a \n404 Not Found\n error.\n\n\n\n\nInfo\n\n\nAny operation can also return a \n401 Unauthorized\n if you are not allowed to do the operation with the\ncurrent authentication, \n400 Bad Request\n if you supplied bad data and \n500 Internal Server Error\n if\nthe server crashed.\n\n\n\n\nWe can also do a GET for an Id:\n\n\n\n\nThe return is the entity serialized as an object.  If the Id does not exist, then a \n404 Not Found\n is\nreturned.\n\n\nAdding an item requires a POST:\n\n\n\n\nNote that you do not need to provide all the fields.  In particular, the system fields will be automatically\nfilled in for you.  Any fields with a default value will be similarly auto-created.  It is fairly easy to\ngenerate a \n400 Bad Request\n when updating or inserting data.  For example, if you submit a string when a number\nis expected or submit a malformed date, you can expect a \n400 Bad Request\n.  On success, the response has a\nLocation field in the headers:\n\n\n\n\nThis is the URI of the entity.  You can do a GET on this location to get the entity again.  Updating an entity\ninvolves sending the updated fields to the \nId\n endpoint with the changed properties:\n\n\n\n\nNote that you do not need to send the entire entity - just the properties that are changed.  The new entity is\nreturned on success.  As with the insert operation, data format errors will result in a \n400 Bad Request\n.  Note,\nhowever, that if you do not submit a \nversion\n field, no conflict handling is done and the server just accepts\nthe record.  We can fix this with server code later on by requiring a version field on updates.\n\n\nDeletion is fairly straight forward.  The main different is that it returns a \n204 No Content\n.\n\n\n\n\nThe table controller does not support soft delete out of the box.  If you have followed the sequence, the\nrecord we just deleted is gone.  You can verify this using a SQL Browser.  To enable soft delete, you need\nto adjust the domain manager in the TodoItemController:\n\n\nprotected override void Initialize(HttpControllerContext controllerContext)\n{\n    base.Initialize(controllerContext);\n    MobileServiceContext context = new MobileServiceContext();\n    DomainManager = new EntityDomainManager\nTodoItem\n(context, Request, enableSoftDelete: true);\n}\n\n\n\n\nAdding the \nenableSoftDelete\n parameter and setting it to true will enable the appropriate logic in the\ndomain manager.\n\n\n\n\nInfo\n\n\nWe haven't introduced the \nDomain Manager\n yet.  Azure Mobile Apps doesn't really care what sort of\ndata store you are using on the backend.  It proxies all requests through a class that implements the\n\nIDomainManager\n interface.  Azure Mobile Apps Server SDK supplies one such domain manager - the\n\nEntityDomainManager\n uses Entity Framework underneath for this purpose.\n\n\n\n\nGo through the same process of adding and deleting an entity.  You can see the entity by using the SQL Server Object\nExplorer in Visual Studio:\n\n\n\n\nGo to \nServer Explorer\n.\n\n\nExpand \nAzure\n and \nSQL Databases\n.\n\n\nSelect your database, then right-click and select \nOpen in SQL Server Object Explorer\n.\n\n\nYou will be prompted for your username and password.  Enter them, then click \nOK\n.\n\n\nYou may be prompted to update the firewall for SQL access.  Select \nMy Client IP\n, then \nOK\n.\n\n\nExpand your database node, then \nTables\n.\n\n\nRight-click on \ndbo.TodoItems\n and select \nView Data\n.\n\n\n\n\nI find the SQL Server Object Explorer to be relatively slow when it comes to database options.  However, it doesn't\nrequire any additional installs.  You can also use SQL Server Management Studio if you have it installed.  You will\nneed to \nupdate the firewall\n for access (something the SQL Server Object Explorer will do for you).\n\n\n\n\nNote the third record has the Deleted column set to true.  We will not see that record when we do a query.  We can\nsee the deleted records only if we use the parameter \n__includeDeleted=true\n:\n\n\n\n\nWe can now undelete that record by POSTing to the \nId\n endpoint:\n\n\n\n\nA success results in a \n201 Created\n response, with a failure resulting in a \n404 Not Found\n response (assuming the\nfailure is because the Id does not exist in the table).\n\n\nFiltering Data\n\n\nIf you have followed along, we have three entities in our table now.  We can do searches by utilizing the OData\n\n$filter\n operator as a query:\n\n\n\n\nThe \n$filter\n parameter takes an \nOData filter\n and returns the list of entities that match the search.  The Azure\nMobile Apps SDK supports quite a bit of the OData v3 specification that is supported by the \nMicrosoft.Data.OData\n\npackage, but not everything.  There are features of the OData package that are explicitly disabled because they\ndo not work in an offline context.  OData was defined as a method of transferring data between client and server\nin an online context so we can expect some things to work differently.\n\n\nWe can also select specific fields by using the \n$select\n clause:\n\n\n\n\nPaging Results\n\n\nAt some point, we are going to bump into an in-built limit of the server.  You can clearly see this by inserting\na lot of entities then querying the results.  Once the number of entities gets above 50, paging will occur.  You\ncan adjust the paging size on the server by adding an \n[EnableQuery()]\n attribute to the class.  For example, the\nfollowing will set the page size at 10:\n\n\nnamespace Chapter3.Controllers\n{\n    [EnableQuery(PageSize=10)]\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n\n\n\n\nYou cannot make the page size infinite, so you should always implement paging controls in your mobile client.\n\n\nWe can always receive the number of records that would have been sent if paging had not been in place by including\n\n$inlinecount=allpages\n with the query.  The query response turns into an object with two properties - the \nresults\n\nproperty contains the array of results.  This is the same response as we received before.  There is now another\nproperty called \ncount\n that contains the count of the records:\n\n\n\n\nWe can implement paging by using \n$top\n and \n$skip\n parameters. The \n$top\n parameter tells the server how many\nentities you want to return.  The \n$skip\n parameter tells the server how many entities to skip before it starts\ncounting.\n\n\nFor example, let's say you wanted to receive individual entities.  You could request:\n\n\n\n\n/tables/todoitem?$top=1\n$skip=0\n\n\n/tables/todoitem?$top=1\n$skip=1\n\n\n/tables/todoitem?$top=1\n$skip=2\n\n\n/tables/todoitem?$top=1\n$skip=3\n\n\n\n\nAt this point, no entities would be returned and you would know you are at the end.\n\n\n\n\nWarn\n\n\nAlthough it is tempting to suggest removing the limit on the number of entities that can be returned (so you\ncan receive all entities in one shot), it's better to implement paging.  The Azure App Service will run in a\nsmaller App Service Plan because it won't require as much memory.  You will be able to support more users and\nyour code will be more resilient to network issues that occur during transmission.\n\n\n\n\nOffline synchronization\n\n\nOne of the many reasons that developers choose the Azure Mobile Apps SDK is that it natively supports offline\nsync.  Offline sync provides a number of benefits.  It improves app responsiveness by caching server data\nlocally on the device.  It allows the app to survive network issues including little or no connectivity, and it\nallows the developer to decide when to synchronize, thus allowing the deferral of large updates to when there\nis wifi available, for example.  The Azure Mobile Apps SDKs provide incremental sync (thereby ensuring the minimal\namount of mobile data is used), optimistic concurrency and conflict resolution.\n\n\nTo do this, Azure Mobile Apps provides a SQLite based backing store for data persistence on the mobile client.  You\ndon't have to use SQLite, but it's built in and there are very few reasons to not use it.  If you are using iOS,\nthe implementation is based on Core Data (which is itself based on SQLite).\n\n\nWhen you perform changes to an offline table, a \nSync Context\n is created along side the offline table. One of the\nelements of this sync context is an \nOperation Queue\n.  This is an ordered list of Create, Update and Delete\noperations against the offline table.  When you \nPUSH\n the Sync Context, the list of creates, updates and Deletes\nare sent one by one to the Azure App Service, which then executes them as if they were done online.  Nothing is\nsent to the Azure App Service until your call to \nPUSH\n.\n\n\nTo retrieve entities, your mobile client will perform a \nPULL\n against a query.  The query is based on the filter\nthat we reviewed earlier.  By default, all properties of all entities are pulled down.  An \nImplicit Push\n happens\nif there are entities in the operation queue at the time of a pull request.  If you specify a query name (which is\njust a text string) to the \nPullAsync()\n method, the mobile client will do an \nIncremental Sync\n.  In this case,\nthe latest \nUpdatedAt\n timestamp that the mobile client saw is recorded in the \nSync Context\n (and associated with\nthe query name).  This allows the pull operation to pick up where it left off.\n\n\n\n\nTip\n\n\nThe query name must be unique within a Sync Context for incremental sync to work.\n\n\n\n\nThe sync process implements \nOptimistic Concurrency\n.  With optimistic concurrency, the mobile client assumes that\nits change is valid.  Conflicts are handled only on push operations.  If the mobile client submits a record with\na \nversion\n field that does not match the server version field, the server will return a 409 or 412 response code.\n\n\n\n\nInfo\n\n\nWhat's the difference between 409 and 412?  Most of the time, you will see 412 Precondition Failed.  This\nmeans the ETag of the request did not match.  The ETag is a header that is equivalent to the version value.\n409 Conflict occurs when you don't submit an ETag but do submit a version field in the update.\n\n\n\n\nIf no version field (or ETag header) is submitted, the client entity is used for the create or update irrespective\nof the value on the server.", 
            "title": "Concepts"
        }, 
        {
            "location": "/chapter3/dataconcepts/#data-access-concepts", 
            "text": "At some point in the development of your application, you are going to want to store or retrieve data.  This could be\nas simple as a key-value store for storing personal settings, or as complex as a multi-table customer relationship\ndatabase.  The key ingredient to all these scenarios is structured data.  I say  structured  here deliberately.  It\nis an important concept.  Most applications you write will require data in some sort of form that your application\nunderstands.  It's very tempting to use an unstructured data source (like NoSQL).  I'm a big fan of NoSQL stores since they free\nme up to concentrate on the mobile client without worrying about the data format. After all, you can store whatever\nentities you want in a NoSQL data source.  However, this is really just an example of sloppy programming.  Most\ndevelopers that use NoSQL are storing structured data in that NoSQL store to get around the problem that you actually\nhave to decide what data you want to store in the database.  For this reason, I prefer a SQL database.  It ensures\nthat I am thinking about the data format up front.  It also helps to ensure that a bad actor is not going to store\ndata I don't expect in my store.  This isn't to say that NoSQL doesn't have its place.  There are times when you need to store structured data where\nthe data format varies between each entity that you are storing.  This tends to be an outlier situation though.", 
            "title": "Data Access Concepts"
        }, 
        {
            "location": "/chapter3/dataconcepts/#tables-entities-and-properties", 
            "text": "When we talk about data, we tend to talk in terms of  Tables ,  Entities  and  Properties .  These have equivalents\nin the SQL world (tables, rows and fields).  A  Table  is a collection of  Entities  that share a common format.  That\nformat is described in terms of  Properties .  Properties are basic types (like strings, numbers, booleans and dates).  We create a  Table Controller  to expose the tables to a mobile client.  A  Table Controller  is a REST endpoint\nthat implements an  OData v3  interface.  OData is a standard interface to table data that allows the client to\nperform CRUD (create, read, update and delete) operations on the data.  In addition, it provides for a standard\nway for querying the data.  More normally, the  Table Controller  is accessed through the Azure Mobile Apps Client\nSDK.  We saw an example of this in Chapter 1 when we first introduced the mobile client.  Azure Mobile Apps also deals with tables in a manner that enables offline synchronization of the data.  That means\nit must be opinionated about the data format.  Specifically,   There are four system properties on each entity.  There are limitations on relationships between tables.  Complex types need special handling.   Let's take each of these in turn.  We implemented this  Model  within the mobile client in Chapter 1:  using TaskList.Abstractions;\n\nnamespace TaskList.Models\n{\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}  It's a fairly basic model class.  Note the  TableData  base class.  I often say that Azure Mobile Apps implements\nan opinionated version of OData. What I mean by that is that the protocol expects certain system fields to be present\nduring the transfer.  The  TableData  class is a base class that implements that specification. It looks like this:  using System;\n\nnamespace TaskList.Abstractions\n{\n    public abstract class TableData\n    {\n        public string Id { get; set; }\n        public DateTimeOffset? UpdatedAt { get; set; }\n        public byte[] Version { get; set; }\n    }\n}  The server side version adds another field - the  Deleted  boolean.  This is described in the  ITableData  interface\nthat is provided with the Azure Mobile Apps Server SDK.   Info  The Azure Mobile Apps SDK uses  DateTimeOffset  instead of  DateTime .  A DateTime object is time\nzone aware, and time zone definitions change over time.  The DateTimeOffset does not know anything\nabout time zones.  The DateTime representation can change depending on where you are.  The DateTimeOffset\nwill never change.  This makes it a better choice for these things.  You will see dates stored in UTC in\nyour database as a result of this.   Each element of the TableData (and ITableData) has a purpose, nominally to deal with situations with Offline Sync.", 
            "title": "Tables, Entities and Properties"
        }, 
        {
            "location": "/chapter3/dataconcepts/#the-id-field", 
            "text": "One of the common questions is this:  Can I use an auto-incrementing integer as an Id field?  Let's take a look at\na simple situation.  You have two clients writing to the same table.  It might look something like this:   Device A inserts a new record in to the database, with ID 1.  Device B is also inserting and decides to do ID 1\nas well.  This causes an immediate conflict that must be resolved.  We could fix this by requiring that new inserts\ndo not insert an ID.  However, this can cause problems in offline cases, where you may be inserting many records\nand have to refer back to them during your offline state.  The compromise here is to use a globally unique ID.  The  GUID  is a well-known algorithm and easily generated in\noffline scenarios.  This is stored as a string during transfer.   Tip  You can specify an Id field when creating an Entity.  However, you must ensure that it is globally unique.", 
            "title": "The Id field"
        }, 
        {
            "location": "/chapter3/dataconcepts/#the-updatedat-field", 
            "text": "One of the concepts that is always top of mind is ensuring that we are a good mobile citizen.  This means that we\ncare about bandwidth utilization.  Reducing transfer size is good for your users.  They use less of their data\nallowance and save time by transferring less.  One of the key components to reduce bandwidth utilization is  Incremental\nSync .  With each record, we record the date it was last updated.  This is generally done for us as a database\ntrigger, so we never have to worry about setting this value.  When we synchronize our table, only the records\nthat have been updated since the last synchronization are requested.  The UpdatedAt field contains the date and time that the record was updated within the central (server-side)\ndatabase.  Do not maintain this field yourself.  If you need a \"last updated timestamp\" for the client-side\ninsert, update or delete methods, you must generate it yourself.", 
            "title": "The UpdatedAt field"
        }, 
        {
            "location": "/chapter3/dataconcepts/#the-version-field", 
            "text": "The Version field is all about conflict detection.  Let's take two devices requesting the same table again:   In this diagram, Device A submits the first version of the entity.  Device B then updates the entity and posts\nit back.  This is accepted because Device B is sending the same version, so the server knows that this is an\nupdate to the latest version.  The server will send back a response with the updated version so that Device B\nknows that it has the latest version.  Later on, Device A sends an update to the same entity.  It, however, still has version 1 of the entity.  The\nserver will reject that because of a version mismatch.  The Version field is maintained by the central (sever-side) SDK.  Do not maintain this field yourself.", 
            "title": "The Version field"
        }, 
        {
            "location": "/chapter3/dataconcepts/#the-deleted-field", 
            "text": "When you are operating a service with an offline scope, you can't just delete entities.  If an entity is deleted\non Device A, it is then removed from the server.  The server does not know to send an update to that entity\nto Device B because it no longer exists.  For this reason, we never delete entities.  We use  Soft Delete .\nSoft Delete is a feature whereby entities are marked as deleted by setting the Deleted flag to true.  When you\nquery the server, the deleted records are not shown unless you explicitly ask for them.  This is done as part\nof the offline sync process.   Info  The UpdatedAt and Version fields are maintained by the Azure Mobile Apps Domain Manager, generally by\ntriggers within the database.  You must not set these fields yourself.", 
            "title": "The Deleted field"
        }, 
        {
            "location": "/chapter3/dataconcepts/#the-data-access-protocol", 
            "text": "Given any particular table, there are a few endpoints that are important.  Given our TodoItem table from Chapter 1:     Operation  Endpoint  Description      GET  /tables/todoitem  Query the table    GET  /tables/todoitem/ id  Retrieve a single entity    POST  /tables/todoitem  Add a new entity    PATCH  /tables/todoitem/ id  Update an existing entity    DELETE  /tables/todoitem/ id  Deletes an existing entity    POST  /tables/todoitem/ id  Undelete a previously deleted entity     We can take a look at each of these in turn with the Azure App Service.  These can be done with Postman easily.   Info  The first request to a new Azure App Service will take some time, especially if the site has to set up the database.   Let's start with a basic Query operation:   We always get an array of elements back.  These contain five system properties.  We didn't mention createdAt\nearlier - it's optional and will be maintained for you if you don't use it.  In addition, we have the fields\nthat were in our model.  If there are no elements in a table, we get an empty array.  If the table does not\nexist, we will get a  404 Not Found  error.   Info  Any operation can also return a  401 Unauthorized  if you are not allowed to do the operation with the\ncurrent authentication,  400 Bad Request  if you supplied bad data and  500 Internal Server Error  if\nthe server crashed.   We can also do a GET for an Id:   The return is the entity serialized as an object.  If the Id does not exist, then a  404 Not Found  is\nreturned.  Adding an item requires a POST:   Note that you do not need to provide all the fields.  In particular, the system fields will be automatically\nfilled in for you.  Any fields with a default value will be similarly auto-created.  It is fairly easy to\ngenerate a  400 Bad Request  when updating or inserting data.  For example, if you submit a string when a number\nis expected or submit a malformed date, you can expect a  400 Bad Request .  On success, the response has a\nLocation field in the headers:   This is the URI of the entity.  You can do a GET on this location to get the entity again.  Updating an entity\ninvolves sending the updated fields to the  Id  endpoint with the changed properties:   Note that you do not need to send the entire entity - just the properties that are changed.  The new entity is\nreturned on success.  As with the insert operation, data format errors will result in a  400 Bad Request .  Note,\nhowever, that if you do not submit a  version  field, no conflict handling is done and the server just accepts\nthe record.  We can fix this with server code later on by requiring a version field on updates.  Deletion is fairly straight forward.  The main different is that it returns a  204 No Content .   The table controller does not support soft delete out of the box.  If you have followed the sequence, the\nrecord we just deleted is gone.  You can verify this using a SQL Browser.  To enable soft delete, you need\nto adjust the domain manager in the TodoItemController:  protected override void Initialize(HttpControllerContext controllerContext)\n{\n    base.Initialize(controllerContext);\n    MobileServiceContext context = new MobileServiceContext();\n    DomainManager = new EntityDomainManager TodoItem (context, Request, enableSoftDelete: true);\n}  Adding the  enableSoftDelete  parameter and setting it to true will enable the appropriate logic in the\ndomain manager.   Info  We haven't introduced the  Domain Manager  yet.  Azure Mobile Apps doesn't really care what sort of\ndata store you are using on the backend.  It proxies all requests through a class that implements the IDomainManager  interface.  Azure Mobile Apps Server SDK supplies one such domain manager - the EntityDomainManager  uses Entity Framework underneath for this purpose.   Go through the same process of adding and deleting an entity.  You can see the entity by using the SQL Server Object\nExplorer in Visual Studio:   Go to  Server Explorer .  Expand  Azure  and  SQL Databases .  Select your database, then right-click and select  Open in SQL Server Object Explorer .  You will be prompted for your username and password.  Enter them, then click  OK .  You may be prompted to update the firewall for SQL access.  Select  My Client IP , then  OK .  Expand your database node, then  Tables .  Right-click on  dbo.TodoItems  and select  View Data .   I find the SQL Server Object Explorer to be relatively slow when it comes to database options.  However, it doesn't\nrequire any additional installs.  You can also use SQL Server Management Studio if you have it installed.  You will\nneed to  update the firewall  for access (something the SQL Server Object Explorer will do for you).   Note the third record has the Deleted column set to true.  We will not see that record when we do a query.  We can\nsee the deleted records only if we use the parameter  __includeDeleted=true :   We can now undelete that record by POSTing to the  Id  endpoint:   A success results in a  201 Created  response, with a failure resulting in a  404 Not Found  response (assuming the\nfailure is because the Id does not exist in the table).", 
            "title": "The Data Access Protocol"
        }, 
        {
            "location": "/chapter3/dataconcepts/#filtering-data", 
            "text": "If you have followed along, we have three entities in our table now.  We can do searches by utilizing the OData $filter  operator as a query:   The  $filter  parameter takes an  OData filter  and returns the list of entities that match the search.  The Azure\nMobile Apps SDK supports quite a bit of the OData v3 specification that is supported by the  Microsoft.Data.OData \npackage, but not everything.  There are features of the OData package that are explicitly disabled because they\ndo not work in an offline context.  OData was defined as a method of transferring data between client and server\nin an online context so we can expect some things to work differently.  We can also select specific fields by using the  $select  clause:", 
            "title": "Filtering Data"
        }, 
        {
            "location": "/chapter3/dataconcepts/#paging-results", 
            "text": "At some point, we are going to bump into an in-built limit of the server.  You can clearly see this by inserting\na lot of entities then querying the results.  Once the number of entities gets above 50, paging will occur.  You\ncan adjust the paging size on the server by adding an  [EnableQuery()]  attribute to the class.  For example, the\nfollowing will set the page size at 10:  namespace Chapter3.Controllers\n{\n    [EnableQuery(PageSize=10)]\n    public class TodoItemController : TableController TodoItem \n    {  You cannot make the page size infinite, so you should always implement paging controls in your mobile client.  We can always receive the number of records that would have been sent if paging had not been in place by including $inlinecount=allpages  with the query.  The query response turns into an object with two properties - the  results \nproperty contains the array of results.  This is the same response as we received before.  There is now another\nproperty called  count  that contains the count of the records:   We can implement paging by using  $top  and  $skip  parameters. The  $top  parameter tells the server how many\nentities you want to return.  The  $skip  parameter tells the server how many entities to skip before it starts\ncounting.  For example, let's say you wanted to receive individual entities.  You could request:   /tables/todoitem?$top=1 $skip=0  /tables/todoitem?$top=1 $skip=1  /tables/todoitem?$top=1 $skip=2  /tables/todoitem?$top=1 $skip=3   At this point, no entities would be returned and you would know you are at the end.   Warn  Although it is tempting to suggest removing the limit on the number of entities that can be returned (so you\ncan receive all entities in one shot), it's better to implement paging.  The Azure App Service will run in a\nsmaller App Service Plan because it won't require as much memory.  You will be able to support more users and\nyour code will be more resilient to network issues that occur during transmission.", 
            "title": "Paging Results"
        }, 
        {
            "location": "/chapter3/dataconcepts/#offline-synchronization", 
            "text": "One of the many reasons that developers choose the Azure Mobile Apps SDK is that it natively supports offline\nsync.  Offline sync provides a number of benefits.  It improves app responsiveness by caching server data\nlocally on the device.  It allows the app to survive network issues including little or no connectivity, and it\nallows the developer to decide when to synchronize, thus allowing the deferral of large updates to when there\nis wifi available, for example.  The Azure Mobile Apps SDKs provide incremental sync (thereby ensuring the minimal\namount of mobile data is used), optimistic concurrency and conflict resolution.  To do this, Azure Mobile Apps provides a SQLite based backing store for data persistence on the mobile client.  You\ndon't have to use SQLite, but it's built in and there are very few reasons to not use it.  If you are using iOS,\nthe implementation is based on Core Data (which is itself based on SQLite).  When you perform changes to an offline table, a  Sync Context  is created along side the offline table. One of the\nelements of this sync context is an  Operation Queue .  This is an ordered list of Create, Update and Delete\noperations against the offline table.  When you  PUSH  the Sync Context, the list of creates, updates and Deletes\nare sent one by one to the Azure App Service, which then executes them as if they were done online.  Nothing is\nsent to the Azure App Service until your call to  PUSH .  To retrieve entities, your mobile client will perform a  PULL  against a query.  The query is based on the filter\nthat we reviewed earlier.  By default, all properties of all entities are pulled down.  An  Implicit Push  happens\nif there are entities in the operation queue at the time of a pull request.  If you specify a query name (which is\njust a text string) to the  PullAsync()  method, the mobile client will do an  Incremental Sync .  In this case,\nthe latest  UpdatedAt  timestamp that the mobile client saw is recorded in the  Sync Context  (and associated with\nthe query name).  This allows the pull operation to pick up where it left off.   Tip  The query name must be unique within a Sync Context for incremental sync to work.   The sync process implements  Optimistic Concurrency .  With optimistic concurrency, the mobile client assumes that\nits change is valid.  Conflicts are handled only on push operations.  If the mobile client submits a record with\na  version  field that does not match the server version field, the server will return a 409 or 412 response code.   Info  What's the difference between 409 and 412?  Most of the time, you will see 412 Precondition Failed.  This\nmeans the ETag of the request did not match.  The ETag is a header that is equivalent to the version value.\n409 Conflict occurs when you don't submit an ETag but do submit a version field in the update.   If no version field (or ETag header) is submitted, the client entity is used for the create or update irrespective\nof the value on the server.", 
            "title": "Offline synchronization"
        }, 
        {
            "location": "/chapter3/server/", 
            "text": "Implementing Table Controllers\n\n\nThe central component for providing a table endpoint on the Azure App Service side of things occurs in your\nbackend project.  You must implement a Table Controller.  This is a specialized version of an ApiController\nthat has some similarities with an ODataController.  However, it has its own base class and the Azure Mobile\nApps SDK simplifies the process of making them.\n\n\nImplementing Code First Migrations\n\n\nBefore we can get started with adding another table controller, we have to deal with modifications to our\ndatabase schema.  The default code for Azure Mobile Apps will deploy the configured schema \nonly if the\ndatabase is empty\n.  If the database is not empty, you have to do extra work.  This extra work involves\nupdating the schema of your database.\n\n\nThere are two methods of doing this.  The first we will consider is Code First Migrations.  With code first\nmigrations, we construct a code file whenever we need to change the database.  Don't worry - it's done for\nus.  Later on, we will consider Database First.  With database first, we adjust the database schema\nmanually, then write C# models to reflect that change.\n\n\nNothing causes more headaches in an Azure Mobile Apps backend than \ncode-first migrations\n.  A code-first\nmigration is simply a set of configuration commands that updates the database to support the new\ndatabase model.  If you try to publish this application, you will see an \nInvalidOperationException\n\nand your service will likely crash.  If you manage to trap the error, it will say \nThe model backing\nthe 'MobileServiceContext' context has changed since the database was created. Consider using Code First\nMigrations to update the database.\n  That's fairly specific and is common to all applications based\non Entity Framework that use code-first models.\n\n\n\n\nTip\n\n\nThere is an alternative here called \nDatabase First Models\n.  In this alternative, you create the\ndatabase first then create the models to match.  However, Azure Mobile Apps requires specific configuration\nof mobile tables that you will need to take care of.  See \nthe section\n on\n\nusing existing SQL tables\n later on for details.\n\n\n\n\nThe first step is to enable migrations.  Go to \nView\n -\n \nOther Windows\n -\n \nPackage Manager Console\n.\nThis window will generally appear in the same place as your Output and Error List windows at the bottom\nof the screen.  Type \nenable-migrations\n in it:\n\n\n\n\nCheck out the \nMigrations\n folder that has just been created to hold the code-first migrations.  An initial\n\nConfiguration.cs\n object will be added to this folder that describes the basic configuration.  We also need\nto create an Initial migration that represents the current state of the database.  We can do this with the\ncommand \nadd-migration Initial\n.\n\n\n\n\n\n\nTip\n\n\nIf you have multiple projects in your solution, you may need to add \n-Project _projectname_\n.  For example,\n\nadd-migration -project Chapter3 Initial\n.  This applies to all the migration commands you execute in the\nPackage Manager Console.\n\n\n\n\nThe initial migration creates a few files in the \nMigrations\n folder that represent the current state of\naffairs for the database. These easily recognized by a combination of the current date and the name of the\nmigration.  Code First Migrations can be applied manually or automatically.  I personally prefer the automatic\nmethod.  To implement automated Code First Migrations, edit the \nApp_Start\\Startup.MobileApp.cs\n file:\n\n\npublic static void ConfigureMobileApp(IAppBuilder app)\n{\n    var httpConfig = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(httpConfig);\n\n    // Automatic Code First Migrations\n    var migrator = new DbMigrator(new Migrations.Configuration());\n    migrator.Update();\n\n    app.UseWebApi(httpConfig);\n}\n\n\n\n\nWe have replaced the \nDbInitializer()\n (which was the method that created the database for us) with the\nautomatic database migrator code.  There is one issue that will cause some problems.  We are no longer using\na database initializer.  The database initializer is the single line of code we just replaced that creates\nthe database tables with the appropriate triggers. This means that the special system columns will no longer\nbe wired up to update their values automatically. We can fix that by configuring the SqlGenerator in\n\nMigrations\\Configuration.cs\n:\n\n\npublic Configuration()\n{\n    AutomaticMigrationsEnabled = false;\n    SetSqlGenerator(\nSystem.Data.SqlClient\n, new EntityTableSqlGenerator());\n}\n\n\n\n\nSince we are not using a database initializer, our seed data has also gone.  You may as well delete the\n\nMobileServiceInitializer\n class in the \nApp_Start\\Startup.MobileApp.cs\n as it isn't doing anything\nany more.  You can move the seed data to the \nMigrations\\Configuration.cs\n file though:\n\n\nnamespace Chapter3.Migrations\n{\n    using System;\n    using System.Collections.Generic;\n    using System.Data.Entity.Migrations;\n    using DataObjects;\n    using Microsoft.Azure.Mobile.Server.Tables;\n\n    internal sealed class Configuration : DbMigrationsConfiguration\nChapter3.Models.MobileServiceContext\n\n    {\n        public Configuration()\n        {\n            AutomaticMigrationsEnabled = false;\n            SetSqlGenerator(\nSystem.Data.SqlClient\n, new EntityTableSqlGenerator());\n        }\n\n        protected override void Seed(Chapter3.Models.MobileServiceContext context)\n        {\n            List\nTodoItem\n todoItems = new List\nTodoItem\n\n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nFirst item\n, Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text = \nSecond item\n, Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set\nTodoItem\n().Add(todoItem);\n            }\n\n            base.Seed(context);\n        }\n    }\n}\n\n\n\n\nThe contents of the \nSeed\n method are a cut-and-paste from the \nMobileServiceInitializer\n version.\n\n\nIf all goes well, you can clear your database (or delete it and re-create it), then publish this project\nand do a GET of the \n/tables/todoitem\n endpoint.  You should still see your data.  You should do a little\nmore investigation however.\n\n\n\n\nOpen the database in the \nSQL Server Object Explorer\n.\n\n\nExpand the \nTables\n node.\n\n\nRight-click on the \ndbo.__MigrationHistory\n table and select \nView Data\n.\n\n\n\n\n\n\nThere should be one row with a name that indicates it is the initial migration.\n\n\nThe final step is to apply the migration to the local system.  In the Package Manager Console, enter\nthe command \nupdate-database\n to apply the existing migration.\n\n\nAdding a SQL Table Controller\n\n\nBefore we can use a table controller, we need to add one.  This has three steps:\n\n\n\n\nCreate a Data Transfer Object (DTO)\n\n\nCreate a Table Controller\n\n\nCreate a Code-First Migration\n\n\n\n\nCreating a Data Transfer Object\n\n\nA Data Transfer Object (or DTO as it is commonly known) is the wire representation of the model for your\ntable.  It must inherit from a concrete implementation of \nITableData\n.  The Azure Mobile Apps SDK includes\n\nEntityData\n for this reason.  EntityData is a concrete implementation that works with Entity Framework.\n\n\n\n\nWarn\n\n\nYou can't just assume EntityData will work with other data stores.  There are Entity Framework specific\nattributes decorating the properties for EntityData that will likely be different for other stores.\n\n\n\n\nThe default Azure Mobile Apps project that is supplied with the Azure SDK provides a folder for storing\nDTOs called \nDataObjects\n.  Let's create a DTO by right-clicking on the \nDataObjects\n folder, then\nusing \nAdd\n -\n \nClass...\n:\n\n\nusing System;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Example : EntityData\n    {\n        public string StringField { get; set; }\n        public int IntField { get; set; }\n        public double DoubleField { get; set; }\n        public DateTimeOffset DateTimeField { get; set; }\n    }\n}\n\n\n\n\n\n\nTip\n\n\nDon't call your model \nSomethingDTO\n.  This ends up as a \n/tables/somethingDTO\n endpoint and a \nSomethingDTO\n\ntable in your database.  Just call it \nSomething\n.  All the names will then line up properly.\n\n\n\n\nI've included several field types, including a complex type.  The basic requirement for a field is that it\nmust be serialized into a simple JSON type during transfer between the server and the mobile client.  Complex\ntypes (that is, any type that can be serialized to an object or array) will always require special handling\nand may not be able to be used at all.\n\n\nCreate a Table Controller\n\n\nVisual Studio for Windows (with the Azure SDK) provides some help in creating a table controller.  Right-click on the \nControllers\n node and select \nAdd\n -\n \nController...\n.\n\n\n\n\nVisual Studio provides scaffolding for a new table controller.  Select it and then click on \nAdd\n.\n\n\n\n\nThe dialog asks for the model (which is actually a DTO) and the data context (which is already created).\nOnce you select the model, the controller name is created for you.  You can change it if you like, but\nit's common practice to not do this.\n\n\nOnce the scaffolding is finished, you can look at your newly created table controller.  We do want to do one change.  We want to enable soft delete so that our table controller supports offline sync scenarios properly.  To do this, go into the \nInitialize()\n method and change the constructor of the \nEntityDomainManager\n.  The completed table controller looks like this:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Microsoft.Azure.Mobile.Server;\nusing Chapter3.DataObjects;\nusing Chapter3.Models;\n\nnamespace Chapter3.Controllers\n{\n    public class ExampleController : TableController\nExample\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nExample\n(context, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/Example\n        public IQueryable\nExample\n GetAllExample()\n        {\n            return Query();\n        }\n\n        // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nExample\n GetExample(string id)\n        {\n            return Lookup(id);\n        }\n\n        // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task\nExample\n PatchExample(string id, Delta\nExample\n patch)\n        {\n             return UpdateAsync(id, patch);\n        }\n\n        // POST tables/Example\n        public async Task\nIHttpActionResult\n PostExample(Example item)\n        {\n            Example current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteExample(string id)\n        {\n             return DeleteAsync(id);\n        }\n    }\n}\n\n\n\n\nCreating a Code-First Migration\n\n\nYou must add a code first migration to update the database when it is published. Use the \nadd-migration\n command in the Package Manager Console.  The \nadd-migration\n command will request a name - it just has to be unique, but it's a good idea to make the name descriptive:\n\n\n\n\nYou should also use \nupdate-database\n to apply the change to the local database (if any):\n\n\n\n\nOnce this is done, you can publish the project.  Right-click on the project and select \nPublish...\n.  Once the project is published, you should be able to send a query to the \n/tables/example\n endpoint using Postman and get an empty array.  You should also be able to insert, update and delete entities as you can with the \nTodoItem\n table.\n\n\nHandling Publish Failures\n\n\nSometimes, the publish fails.  It seems that whenever I start with code-first migrations, my publish fails.  I get a nice error screen, but no actual error.  At least half the time, the problem is not my code-first migration, but something else.  For instance, one of the things I tend to do is update my NuGet packages. This inevitably breaks something.\n\n\nFortunately, once the error message is known, it's generally trivial to correct the error.  You can turn custom error messages off (and thus expose the original error message) by editing the Web.config file.  Locate the \nsystem.web\n section and add the \ncustomErrors mode=\"Off\"/\n line:\n\n\n  \nsystem.web\n\n    \nhttpRuntime targetFramework=\n4.6\n /\n\n    \ncompilation debug=\ntrue\n targetFramework=\n4.6\n /\n\n    \ncustomErrors mode=\nOff\n /\n\n  \n/system.web\n\n\n\n\n\nThen republish your project and the response from the server is much more informative.\n\n\nTurning on Diagnostic Logs\n\n\nYou can log all the SQL statements that Entity Framework executes on your behalf by adding a Database Log.  Edit the \nModels\\MobileServiceContext.cs\n file:\n\n\npublic class MobileServiceContext : DbContext\n{\n    private const string connectionStringName = \nName=MS_TableConnectionString\n;\n\n    public MobileServiceContext() : base(connectionStringName)\n    {\n        Database.Log = s =\n WriteLog(s);\n    }\n\n    public void WriteLog(string msg)\n    {\n        System.Diagnostics.Debug.WriteLine(msg);\n    }\n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {\n        modelBuilder.Conventions.Add(\n            new AttributeToColumnAnnotationConvention\nTableColumnAttribute, string\n(\n                \nServiceTableColumn\n, (property, attributes) =\n attributes.Single().ColumnType.ToString()));\n    }\n\n    public DbSet\nDataObjects.TodoItem\n TodoItems { get; set; }\n    public DbSet\nDataObjects.Example\n Examples { get; set; }\n}\n\n\n\n\nYou have to use a real method.  System.Diagnostics.Debug is removed from the context when DEBUG is not defined, so you can't just use it directly.   Using an interim method works around that problem. Azure App Service captures the output from the console and places it into the log viewer for you.\n\n\nTo turn on diagnostic logging:\n\n\n\n\nLog in to the \nAzure Portal\n.\n\n\nClick on \nApp Services\n then your App Service.\n\n\nFind \nDiagnostic Logs\n in the list of settings (you can use the search box).\n\n\nTurn on \nApplication Logging (Filesystem)\n with a level of \nVerbose\n.\n\n\nClick on \nSave\n.\n\n\n\n\nTo view the diagnostic logs in the portal, find \nLog Stream\n in the list of settings (again, you can\nuse the search box).  You can also get the diagnostic logs within Visual Studio.\n\n\n\n\nOpen the \nServer Explorer\n.\n\n\nExpand \nAzure\n, \nApp Service\n, your resource group.\n\n\nRight-click on the your App Service and select \nView Streaming Logs\n.\n\n\n\n\n\n\nUsing an existing SQL Table\n\n\nThere are times when a \"Database First\" approach is needed.  If you are trying to expose an existing\nSQL database table, for example, you want to use a \"Database First\" approach.  You may also like the\nseparation of the database table from the mobile system columns.\n\n\n\n\nWarn\n\n\nThe Database First and Code First approaches to Entity Framework are mutually exclusive.  You need\nto decide which one you want to use and stick with it.\n\n\n\n\nTo use \"Database First\", you first set up the database.  Then you create a Model and update the DbContext object.  For example, our \nExample\n model from before can be represented by the following database schema:\n\n\nCREATE TABLE [dbo].[Examples] (\n    -- This must be a string suitable for a GUID\n    [Id]            NVARCHAR (128)     NOT NULL,\n\n    -- These are the system properties\n    [Version]       ROWVERSION         NOT NULL,\n    [CreatedAt]     DATETIMEOFFSET (7) NOT NULL,\n    [UpdatedAt]     DATETIMEOFFSET (7) NULL,\n    [Deleted]       BIT                NOT NULL\n\n    -- These are the properties of our DTO not included in EntityFramework\n    [StringField]   NVARCHAR (MAX)     NULL,\n    [IntField]      INT                NOT NULL,\n    [DoubleField]   FLOAT (53)         NOT NULL,\n    [DateTimeField] DATETIMEOFFSET (7) NOT NULL,\n);\n\nCREATE CLUSTERED INDEX [IX_CreatedAt]\n    ON [dbo].[Examples]([CreatedAt] ASC);\n\nALTER TABLE [dbo].[Examples]\n    ADD CONSTRAINT [PK_dbo.Examples] PRIMARY KEY NONCLUSTERED ([Id] ASC);\n\nCREATE TRIGGER [TR_dbo_Examples_InsertUpdateDelete] ON [dbo].[Examples]\n    AFTER INSERT, UPDATE, DELETE AS\n    BEGIN\n        UPDATE [dbo].[Examples]\n            SET [dbo].[Examples].[UpdatedAt] = CONVERT(DATETIMEOFFSET, SYSUTCDATETIME())\n            FROM INSERTED WHERE inserted.[Id] = [dbo].[Examples].[Id]\n    END;\n\n\n\n\nThe system properties are added to the schema.  We can (and do) use a trigger to update the UpdatedAt\ncolumn when the data is updated.  Placing this logic within the SQL Server schema definition means\nwe can use this SQL table outside of the mobile context and it will still work for the mobile application.\n\n\n\n\nInfo\n\n\nWe use the CreatedAt field to create a clustered index.  In older versions of SQL Server, this was\nrequired to increase performance on the table as a whole.  It may not be required now and may be removed\nin future versions of Azure Mobile Apps.\n\n\n\n\nIf you can update an existing table to match this schema, then you should do so.  Note that the Id field is\nnot set by default.  If you want to set a default, set it to \nNEWID()\n:\n\n\nALTER TABLE [dbo].[Examples]\n    ALTER COLUMN [Id] SET DEFAULT CONVERT(NVARCHAR(128), NEWID());\n\n\n\n\nOnce you have set up the database and created the models, you must also turn off the database initializer.\nThis is done in \nApp_Start\\Startup.MobileApp.cs\n:\n\n\npublic static void ConfigureMobileApp(IAppBuilder app)\n{\n    var httpConfig = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(httpConfig);\n\n    // Automatic Code First Migrations\n    // var migrator = new DbMigrator(new Migrations.Configuration());\n    // migrator.Update();\n\n    // Database First\n    Database.SetInitializer\nMobileDbContext\n(null);\n\n    app.UseWebApi(httpConfig);\n}\n\n\n\n\nThere are a lot of times when the existing table is not suitable for this sort of schema adjustment.  The\nprimary reason will be that the existing table has an auto-incrementing integer Id column and you can't\nchange that.  Quite often, I see developers needing to integrate the SQL schema of an existing application\nlike a Customer Relationship Management system with this constraint, for example.\n\n\nLet's take an example.  Suppose you have a table called [dbo].[TodoItems].  This is fairly basic and\ndefined like this:\n\n\nCREATE TABLE [dbo].[TodoItems] (\n  [id]       BIGINT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n  [UserId]   NVARCHAR(255) NOT NULL,\n  [Title]    NVARCHAR(255) NOT NULL,\n  [Complete] BIT\n);\n\n\n\n\nThis is the sort of SQL table that is very common within existing web applications, for instance.  We have\nan auto-incrementing id column and some fields.  It isn't complex (no relationships, for example), so we\ndon't have to worry about \nreferential integrity\n of the table.\n\n\n\n\nTip\n\n\nI recommend placing the \"mobile views\" that we will discuss below in a separate schema (for example,\n\n[mobile]\n) so that they don't interfere with your existing database schema.\n\n\n\n\nLet's take a look at creating a VIEW of the data that is \"mobile ready\".  This is an Entity Relationship\nDiagram of what we are going to do:\n\n\n\n\nwe are going to create a separate table for the system properties, called \n[mobile].[SysProps_TodoItems]\n.\nThis will hold the five fields that Azure Mobile Apps requires, plus a reference to the TodoItem id:\n\n\nCREATE TABLE [mobile].[SysProps_TodoItems] (\n  [Id]        NVARCHAR(128) CONSTRAINT [DF_todoitem_id] DEFAULT (CONVERT([NVARCHAR](255),NEWID(),(0))) NOT NULL,\n  [CreatedAt] DATETIMEOFFSET(7) CONSTRAINT [DF_todoitem_createdAt] DEFAULT (CONVERT([DATETIMEOFFSET](7),SYSUTCDATETIME(),(0))) NOT NULL,\n  [UpdatedAt] DATETIMEOFFSET(7) NULL,\n  [Version]   ROWVERSION NOT NULL,\n  [Deleted]   BIT DEFAULT ((0)) NOT NULL,\n  [Item_Id]   BIGINT NOT NULL\n  PRIMARY KEY NONCLUSTERED ([id] ASC)\n);\n\n\n\n\nThen we will create a SQL VIEW that maps to this:\n\n\nCREATE VIEW [mobile].[TodoItems] AS\nSELECT\n    [mobile].[SysProps_TodoItems].[Id],\n    [mobile].[SysProps_TodoItems].[CreatedAt],\n    [mobile].[SysProps_TodoItems].[UpdatedAt],\n    [mobile].[SysProps_TodoItems].[Version],\n    [mobile].[SysProps_TodoItems].[Deleted],\n    [mobile].[SysProps_TodoItems].[Item_Id],\n    [dbo].[TodoItems].[UserId],\n    [dbo].[TodoItems].[Title],\n    [dbo].[TodoItems].[Complete]\nFROM\n    [dbo].[TodoItems],\n    [mobile].[SysProps_TodoItems]\nWHERE\n    [dbo].[TodoItems].[id] = [mobile].[SysProps_TodoItems].[Item_Id];\n\n\n\n\nThis view is still only a combination of the two tables.  I want specific logic so that when a row\nis changed (inserted, updated or deleted) in the \n[dbo].[TodoItems]\n table, a similar change is made\nto the \n[mobile].[SysProps_TodoItems]\n table.  This is achieved through the use of triggers:\n\n\nCREATE TRIGGER\n    [dbo].[TRG_TodoItem_Insert]\nON\n    [dbo].[TodoItems]\nAFTER\n    INSERT\nAS BEGIN\n    INSERT INTO [mobile].[SysProps_TodoItems] ([Item_Id], [UpdatedAt])\n    SELECT inserted.id, CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME())\n    FROM inserted\nEND\nGO\n\nCREATE TRIGGER\n    [dbo].[TRG_TodoItem_Update]\nON\n    [dbo].[TodoItems]\nAFTER\n    UPDATE\nAS BEGIN\n    UPDATE\n        [mobile].[SysProps_TodoItems]\n    SET\n        [UpdatedAt] = CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME())\n    FROM\n        INSERTED\n    WHERE\n        INSERTED.id = [mobile].[SysProps_TodoItems].[Item_Id]\nEND\nGO\n\nCREATE TRIGGER\n    [dbo].[TRG_TodoItem_Delete]\nON\n    [dbo].[TodoItems]\nAFTER\n    DELETE\nAS BEGIN\n    DELETE FROM [mobile].[SysProps_TodoItems]\n    WHERE [Item_Id] IN (select deleted.id from deleted)\nEND\nGO\n\n\n\n\n\n\nTip\n\n\nYou may want to set the \nDeleted\n flag to 1 (or true) in the Delete trigger.  This will enable soft\ndelete on the system properties table, ensuring that mobile clients remove the row from their offline\ncache.\n\n\n\n\nSimilarly, when changes are made by the mobile backend to the VIEW, we need to propagate those changes\nto the underlying tables.  This is also done with triggers:\n\n\nCREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Insert]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    INSERT\nAS BEGIN\n    DECLARE @userid AS NVARCHAR(255)\n    SELECT @userid = inserted.UserId FROM inserted\n    DECLARE @title AS NVARCHAR(255)\n    SELECT @title = inserted.Title FROM inserted\n    DECLARE @complete AS BIT\n    SELECT @complete = inserted.Complete FROM inserted\n\n\n    INSERT INTO\n        [dbo].[TodoItems] ([UserId], [Title], [Complete])\n    VALUES\n        (@userid, @title, @complete)\n\n    IF UPDATE(Id) BEGIN\n        DECLARE @itemid AS BIGINT\n        SELECT @itemid = @@identity\n        DECLARE @id AS NVARCHAR(255)\n        SELECT @id = inserted.Id FROM inserted\n        UPDATE [mobile].[SysProps_TodoItems] SET [Id] = @id WHERE [item_id] = @itemid\n    END\nEND;\nGO\n\nCREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Update]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    UPDATE\nAS BEGIN\n    DECLARE @id AS NVARCHAR(255)\n    SELECT @id = inserted.id FROM inserted\n    DECLARE @itemid AS BIGINT\n    SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\n\n    IF UPDATE(UserId) BEGIN\n        DECLARE @userid AS NVARCHAR(255)\n        SELECT @userid = inserted.UserId FROM inserted\n        UPDATE [dbo].[TodoItems] SET [UserId] = @userid WHERE [id] = @itemid\n    END\n    IF UPDATE(Title) BEGIN\n        DECLARE @title AS NVARCHAR(255)\n        SELECT @title = inserted.Title FROM inserted\n        UPDATE [dbo].[TodoItems] SET [Title] = @title WHERE [id] = @itemid\n    END\n    IF UPDATE(Complete) BEGIN\n        DECLARE @complete AS BIT\n        SELECT @complete = inserted.Complete FROM inserted\n        UPDATE [dbo].[TodoItems] SET [Complete] = @complete WHERE [id] = @itemid\n    END\n    IF UPDATE(deleted) BEGIN\n        DECLARE @deleted AS BIT\n        SELECT @deleted = inserted.deleted FROM inserted\n        UPDATE [mobile].[SysProps_TodoItems] SET [deleted] = @deleted WHERE [item_id] = @itemid\n    END\nEND\nGO\n\nCREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Delete]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    DELETE\nAS BEGIN\n    DECLARE @id AS NVARCHAR(255)\n    SELECT @id = deleted.id FROM deleted\n    DECLARE @itemid AS BIGINT\n    SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\n\n    DELETE FROM [dbo].[TodoItems] WHERE [id] = @itemid\n    DELETE FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\nEND\nGO\n\n\n\n\nA standard SQL view is read-only.  We made the view read/write by using triggers to trap the\ncall and replace it with two calls instead.\n\n\n\n\nWarn\n\n\nThis version does not support soft delete.  If you wish to support soft delete, set the\n\nDeleted\n flag in the `[mobile].[SysProps_TodoItems] table instead of deleting the row.\n\n\n\n\nChanging the Mobile Schema\n\n\nWe stored our mobile representation of the table in a different schema.  Azure Mobile Apps looks for tables (and views) in the \n[dbo]\n schema by default.  There are two places you can modify the schema that Entity Framework uses to access the database.  The first is by changing the default schema that Entity Framework uses.  This will affect all the tables that we are exposing via a table controller.  This is done in the \nModels\\MobileDbContext.cs\n class:\n\n\nprotected override void OnModelCreating(DbModelBuilder modelBuilder)\n{\n    modelBuilder.HasDefaultSchema(\nmobile\n);\n    modelBuilder.Conventions.Add(\n        new AttributeToColumnAnnotationConvention\nTableColumnAttribute, string\n(\n            \nServiceTableColumn\n,\n            (property, attributes) =\n attributes.Single().ColumnType.ToString()\n        )\n    );\n}\n\n\n\n\nWe can also do this on the model as an annotation.  For example, here is the TodoItem model suitably adjusted:\n\n\nusing Microsoft.Azure.Mobile.Server;\nusing Newtonsoft.Json;\nusing System.ComponentModel.DataAnnotations.Schema;\n\nnamespace Backend.DataObjects\n{\n    [Table(\nTodoItems\n, Schema=\nmobile\n)]\n    public class TodoItem : EntityData\n    {\n        public string UserId { get; set; }\n\n        public string Title { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\n\n\nInfo\n\n\nEntity Framework adds an \ns\n onto the end of the model to create the table name.  If you have a model named \nTodoItem\n, the table is called \nTodoItems\n in the database.  You can use this same annotation to adjust the table name if it is not to your liking.\n\n\n\n\nBest Practices\n\n\nHere is a summary of what I consider best practices for table controllers:\n\n\n\n\nUse Code First when you are in a green-field database situation.\n\n\nUse Database First when you have to integrate an existing database.\n\n\nOnly expose the data that you need for the mobile client.\n\n\nSet up Automatic Code First Migrations as early as possible.\n\n\nThink about what happens to existing clients when you update the schema.\n\n\n\n\nThis last point is an important one.  Let's say you have a v1 mobile client of your awesome mobile\napplication.  This works with v1 mobile backend.  You've set up the schema and ensured that everything\nworks.  Then you want to release v2 mobile client.  It has a new feature and needs some extra data.\nSo you update to v2 mobile backend that provides that data.\n\n\nYou need to ensure that the schema changes provided by the v2 mobile backend are optional.  In other\nwords, the v1 mobile client can continue to work against the v2 mobile backend.  You should test this\ncase.  Users do not upgrade instantly.  In fact, many users don't upgrade at all.  At every release\nof your app, you are going to have to ensure that ALL versions of your mobile client work against\nyour mobile backend.", 
            "title": "Implementing Table Controllers"
        }, 
        {
            "location": "/chapter3/server/#implementing-table-controllers", 
            "text": "The central component for providing a table endpoint on the Azure App Service side of things occurs in your\nbackend project.  You must implement a Table Controller.  This is a specialized version of an ApiController\nthat has some similarities with an ODataController.  However, it has its own base class and the Azure Mobile\nApps SDK simplifies the process of making them.", 
            "title": "Implementing Table Controllers"
        }, 
        {
            "location": "/chapter3/server/#implementing-code-first-migrations", 
            "text": "Before we can get started with adding another table controller, we have to deal with modifications to our\ndatabase schema.  The default code for Azure Mobile Apps will deploy the configured schema  only if the\ndatabase is empty .  If the database is not empty, you have to do extra work.  This extra work involves\nupdating the schema of your database.  There are two methods of doing this.  The first we will consider is Code First Migrations.  With code first\nmigrations, we construct a code file whenever we need to change the database.  Don't worry - it's done for\nus.  Later on, we will consider Database First.  With database first, we adjust the database schema\nmanually, then write C# models to reflect that change.  Nothing causes more headaches in an Azure Mobile Apps backend than  code-first migrations .  A code-first\nmigration is simply a set of configuration commands that updates the database to support the new\ndatabase model.  If you try to publish this application, you will see an  InvalidOperationException \nand your service will likely crash.  If you manage to trap the error, it will say  The model backing\nthe 'MobileServiceContext' context has changed since the database was created. Consider using Code First\nMigrations to update the database.   That's fairly specific and is common to all applications based\non Entity Framework that use code-first models.   Tip  There is an alternative here called  Database First Models .  In this alternative, you create the\ndatabase first then create the models to match.  However, Azure Mobile Apps requires specific configuration\nof mobile tables that you will need to take care of.  See  the section  on using existing SQL tables  later on for details.   The first step is to enable migrations.  Go to  View  -   Other Windows  -   Package Manager Console .\nThis window will generally appear in the same place as your Output and Error List windows at the bottom\nof the screen.  Type  enable-migrations  in it:   Check out the  Migrations  folder that has just been created to hold the code-first migrations.  An initial Configuration.cs  object will be added to this folder that describes the basic configuration.  We also need\nto create an Initial migration that represents the current state of the database.  We can do this with the\ncommand  add-migration Initial .    Tip  If you have multiple projects in your solution, you may need to add  -Project _projectname_ .  For example, add-migration -project Chapter3 Initial .  This applies to all the migration commands you execute in the\nPackage Manager Console.   The initial migration creates a few files in the  Migrations  folder that represent the current state of\naffairs for the database. These easily recognized by a combination of the current date and the name of the\nmigration.  Code First Migrations can be applied manually or automatically.  I personally prefer the automatic\nmethod.  To implement automated Code First Migrations, edit the  App_Start\\Startup.MobileApp.cs  file:  public static void ConfigureMobileApp(IAppBuilder app)\n{\n    var httpConfig = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(httpConfig);\n\n    // Automatic Code First Migrations\n    var migrator = new DbMigrator(new Migrations.Configuration());\n    migrator.Update();\n\n    app.UseWebApi(httpConfig);\n}  We have replaced the  DbInitializer()  (which was the method that created the database for us) with the\nautomatic database migrator code.  There is one issue that will cause some problems.  We are no longer using\na database initializer.  The database initializer is the single line of code we just replaced that creates\nthe database tables with the appropriate triggers. This means that the special system columns will no longer\nbe wired up to update their values automatically. We can fix that by configuring the SqlGenerator in Migrations\\Configuration.cs :  public Configuration()\n{\n    AutomaticMigrationsEnabled = false;\n    SetSqlGenerator( System.Data.SqlClient , new EntityTableSqlGenerator());\n}  Since we are not using a database initializer, our seed data has also gone.  You may as well delete the MobileServiceInitializer  class in the  App_Start\\Startup.MobileApp.cs  as it isn't doing anything\nany more.  You can move the seed data to the  Migrations\\Configuration.cs  file though:  namespace Chapter3.Migrations\n{\n    using System;\n    using System.Collections.Generic;\n    using System.Data.Entity.Migrations;\n    using DataObjects;\n    using Microsoft.Azure.Mobile.Server.Tables;\n\n    internal sealed class Configuration : DbMigrationsConfiguration Chapter3.Models.MobileServiceContext \n    {\n        public Configuration()\n        {\n            AutomaticMigrationsEnabled = false;\n            SetSqlGenerator( System.Data.SqlClient , new EntityTableSqlGenerator());\n        }\n\n        protected override void Seed(Chapter3.Models.MobileServiceContext context)\n        {\n            List TodoItem  todoItems = new List TodoItem \n            {\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  First item , Complete = false },\n                new TodoItem { Id = Guid.NewGuid().ToString(), Text =  Second item , Complete = false }\n            };\n\n            foreach (TodoItem todoItem in todoItems)\n            {\n                context.Set TodoItem ().Add(todoItem);\n            }\n\n            base.Seed(context);\n        }\n    }\n}  The contents of the  Seed  method are a cut-and-paste from the  MobileServiceInitializer  version.  If all goes well, you can clear your database (or delete it and re-create it), then publish this project\nand do a GET of the  /tables/todoitem  endpoint.  You should still see your data.  You should do a little\nmore investigation however.   Open the database in the  SQL Server Object Explorer .  Expand the  Tables  node.  Right-click on the  dbo.__MigrationHistory  table and select  View Data .    There should be one row with a name that indicates it is the initial migration.  The final step is to apply the migration to the local system.  In the Package Manager Console, enter\nthe command  update-database  to apply the existing migration.", 
            "title": "Implementing Code First Migrations"
        }, 
        {
            "location": "/chapter3/server/#adding-a-sql-table-controller", 
            "text": "Before we can use a table controller, we need to add one.  This has three steps:   Create a Data Transfer Object (DTO)  Create a Table Controller  Create a Code-First Migration", 
            "title": "Adding a SQL Table Controller"
        }, 
        {
            "location": "/chapter3/server/#creating-a-data-transfer-object", 
            "text": "A Data Transfer Object (or DTO as it is commonly known) is the wire representation of the model for your\ntable.  It must inherit from a concrete implementation of  ITableData .  The Azure Mobile Apps SDK includes EntityData  for this reason.  EntityData is a concrete implementation that works with Entity Framework.   Warn  You can't just assume EntityData will work with other data stores.  There are Entity Framework specific\nattributes decorating the properties for EntityData that will likely be different for other stores.   The default Azure Mobile Apps project that is supplied with the Azure SDK provides a folder for storing\nDTOs called  DataObjects .  Let's create a DTO by right-clicking on the  DataObjects  folder, then\nusing  Add  -   Class... :  using System;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Example : EntityData\n    {\n        public string StringField { get; set; }\n        public int IntField { get; set; }\n        public double DoubleField { get; set; }\n        public DateTimeOffset DateTimeField { get; set; }\n    }\n}   Tip  Don't call your model  SomethingDTO .  This ends up as a  /tables/somethingDTO  endpoint and a  SomethingDTO \ntable in your database.  Just call it  Something .  All the names will then line up properly.   I've included several field types, including a complex type.  The basic requirement for a field is that it\nmust be serialized into a simple JSON type during transfer between the server and the mobile client.  Complex\ntypes (that is, any type that can be serialized to an object or array) will always require special handling\nand may not be able to be used at all.", 
            "title": "Creating a Data Transfer Object"
        }, 
        {
            "location": "/chapter3/server/#create-a-table-controller", 
            "text": "Visual Studio for Windows (with the Azure SDK) provides some help in creating a table controller.  Right-click on the  Controllers  node and select  Add  -   Controller... .   Visual Studio provides scaffolding for a new table controller.  Select it and then click on  Add .   The dialog asks for the model (which is actually a DTO) and the data context (which is already created).\nOnce you select the model, the controller name is created for you.  You can change it if you like, but\nit's common practice to not do this.  Once the scaffolding is finished, you can look at your newly created table controller.  We do want to do one change.  We want to enable soft delete so that our table controller supports offline sync scenarios properly.  To do this, go into the  Initialize()  method and change the constructor of the  EntityDomainManager .  The completed table controller looks like this:  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Microsoft.Azure.Mobile.Server;\nusing Chapter3.DataObjects;\nusing Chapter3.Models;\n\nnamespace Chapter3.Controllers\n{\n    public class ExampleController : TableController Example \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager Example (context, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/Example\n        public IQueryable Example  GetAllExample()\n        {\n            return Query();\n        }\n\n        // GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult Example  GetExample(string id)\n        {\n            return Lookup(id);\n        }\n\n        // PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task Example  PatchExample(string id, Delta Example  patch)\n        {\n             return UpdateAsync(id, patch);\n        }\n\n        // POST tables/Example\n        public async Task IHttpActionResult  PostExample(Example item)\n        {\n            Example current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteExample(string id)\n        {\n             return DeleteAsync(id);\n        }\n    }\n}", 
            "title": "Create a Table Controller"
        }, 
        {
            "location": "/chapter3/server/#creating-a-code-first-migration", 
            "text": "You must add a code first migration to update the database when it is published. Use the  add-migration  command in the Package Manager Console.  The  add-migration  command will request a name - it just has to be unique, but it's a good idea to make the name descriptive:   You should also use  update-database  to apply the change to the local database (if any):   Once this is done, you can publish the project.  Right-click on the project and select  Publish... .  Once the project is published, you should be able to send a query to the  /tables/example  endpoint using Postman and get an empty array.  You should also be able to insert, update and delete entities as you can with the  TodoItem  table.", 
            "title": "Creating a Code-First Migration"
        }, 
        {
            "location": "/chapter3/server/#handling-publish-failures", 
            "text": "Sometimes, the publish fails.  It seems that whenever I start with code-first migrations, my publish fails.  I get a nice error screen, but no actual error.  At least half the time, the problem is not my code-first migration, but something else.  For instance, one of the things I tend to do is update my NuGet packages. This inevitably breaks something.  Fortunately, once the error message is known, it's generally trivial to correct the error.  You can turn custom error messages off (and thus expose the original error message) by editing the Web.config file.  Locate the  system.web  section and add the  customErrors mode=\"Off\"/  line:     system.web \n     httpRuntime targetFramework= 4.6  / \n     compilation debug= true  targetFramework= 4.6  / \n     customErrors mode= Off  / \n   /system.web   Then republish your project and the response from the server is much more informative.", 
            "title": "Handling Publish Failures"
        }, 
        {
            "location": "/chapter3/server/#turning-on-diagnostic-logs", 
            "text": "You can log all the SQL statements that Entity Framework executes on your behalf by adding a Database Log.  Edit the  Models\\MobileServiceContext.cs  file:  public class MobileServiceContext : DbContext\n{\n    private const string connectionStringName =  Name=MS_TableConnectionString ;\n\n    public MobileServiceContext() : base(connectionStringName)\n    {\n        Database.Log = s =  WriteLog(s);\n    }\n\n    public void WriteLog(string msg)\n    {\n        System.Diagnostics.Debug.WriteLine(msg);\n    }\n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {\n        modelBuilder.Conventions.Add(\n            new AttributeToColumnAnnotationConvention TableColumnAttribute, string (\n                 ServiceTableColumn , (property, attributes) =  attributes.Single().ColumnType.ToString()));\n    }\n\n    public DbSet DataObjects.TodoItem  TodoItems { get; set; }\n    public DbSet DataObjects.Example  Examples { get; set; }\n}  You have to use a real method.  System.Diagnostics.Debug is removed from the context when DEBUG is not defined, so you can't just use it directly.   Using an interim method works around that problem. Azure App Service captures the output from the console and places it into the log viewer for you.  To turn on diagnostic logging:   Log in to the  Azure Portal .  Click on  App Services  then your App Service.  Find  Diagnostic Logs  in the list of settings (you can use the search box).  Turn on  Application Logging (Filesystem)  with a level of  Verbose .  Click on  Save .   To view the diagnostic logs in the portal, find  Log Stream  in the list of settings (again, you can\nuse the search box).  You can also get the diagnostic logs within Visual Studio.   Open the  Server Explorer .  Expand  Azure ,  App Service , your resource group.  Right-click on the your App Service and select  View Streaming Logs .", 
            "title": "Turning on Diagnostic Logs"
        }, 
        {
            "location": "/chapter3/server/#using-an-existing-sql-table", 
            "text": "There are times when a \"Database First\" approach is needed.  If you are trying to expose an existing\nSQL database table, for example, you want to use a \"Database First\" approach.  You may also like the\nseparation of the database table from the mobile system columns.   Warn  The Database First and Code First approaches to Entity Framework are mutually exclusive.  You need\nto decide which one you want to use and stick with it.   To use \"Database First\", you first set up the database.  Then you create a Model and update the DbContext object.  For example, our  Example  model from before can be represented by the following database schema:  CREATE TABLE [dbo].[Examples] (\n    -- This must be a string suitable for a GUID\n    [Id]            NVARCHAR (128)     NOT NULL,\n\n    -- These are the system properties\n    [Version]       ROWVERSION         NOT NULL,\n    [CreatedAt]     DATETIMEOFFSET (7) NOT NULL,\n    [UpdatedAt]     DATETIMEOFFSET (7) NULL,\n    [Deleted]       BIT                NOT NULL\n\n    -- These are the properties of our DTO not included in EntityFramework\n    [StringField]   NVARCHAR (MAX)     NULL,\n    [IntField]      INT                NOT NULL,\n    [DoubleField]   FLOAT (53)         NOT NULL,\n    [DateTimeField] DATETIMEOFFSET (7) NOT NULL,\n);\n\nCREATE CLUSTERED INDEX [IX_CreatedAt]\n    ON [dbo].[Examples]([CreatedAt] ASC);\n\nALTER TABLE [dbo].[Examples]\n    ADD CONSTRAINT [PK_dbo.Examples] PRIMARY KEY NONCLUSTERED ([Id] ASC);\n\nCREATE TRIGGER [TR_dbo_Examples_InsertUpdateDelete] ON [dbo].[Examples]\n    AFTER INSERT, UPDATE, DELETE AS\n    BEGIN\n        UPDATE [dbo].[Examples]\n            SET [dbo].[Examples].[UpdatedAt] = CONVERT(DATETIMEOFFSET, SYSUTCDATETIME())\n            FROM INSERTED WHERE inserted.[Id] = [dbo].[Examples].[Id]\n    END;  The system properties are added to the schema.  We can (and do) use a trigger to update the UpdatedAt\ncolumn when the data is updated.  Placing this logic within the SQL Server schema definition means\nwe can use this SQL table outside of the mobile context and it will still work for the mobile application.   Info  We use the CreatedAt field to create a clustered index.  In older versions of SQL Server, this was\nrequired to increase performance on the table as a whole.  It may not be required now and may be removed\nin future versions of Azure Mobile Apps.   If you can update an existing table to match this schema, then you should do so.  Note that the Id field is\nnot set by default.  If you want to set a default, set it to  NEWID() :  ALTER TABLE [dbo].[Examples]\n    ALTER COLUMN [Id] SET DEFAULT CONVERT(NVARCHAR(128), NEWID());  Once you have set up the database and created the models, you must also turn off the database initializer.\nThis is done in  App_Start\\Startup.MobileApp.cs :  public static void ConfigureMobileApp(IAppBuilder app)\n{\n    var httpConfig = new HttpConfiguration();\n    var mobileConfig = new MobileAppConfiguration();\n\n    mobileConfig\n        .AddTablesWithEntityFramework()\n        .ApplyTo(httpConfig);\n\n    // Automatic Code First Migrations\n    // var migrator = new DbMigrator(new Migrations.Configuration());\n    // migrator.Update();\n\n    // Database First\n    Database.SetInitializer MobileDbContext (null);\n\n    app.UseWebApi(httpConfig);\n}  There are a lot of times when the existing table is not suitable for this sort of schema adjustment.  The\nprimary reason will be that the existing table has an auto-incrementing integer Id column and you can't\nchange that.  Quite often, I see developers needing to integrate the SQL schema of an existing application\nlike a Customer Relationship Management system with this constraint, for example.  Let's take an example.  Suppose you have a table called [dbo].[TodoItems].  This is fairly basic and\ndefined like this:  CREATE TABLE [dbo].[TodoItems] (\n  [id]       BIGINT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n  [UserId]   NVARCHAR(255) NOT NULL,\n  [Title]    NVARCHAR(255) NOT NULL,\n  [Complete] BIT\n);  This is the sort of SQL table that is very common within existing web applications, for instance.  We have\nan auto-incrementing id column and some fields.  It isn't complex (no relationships, for example), so we\ndon't have to worry about  referential integrity  of the table.   Tip  I recommend placing the \"mobile views\" that we will discuss below in a separate schema (for example, [mobile] ) so that they don't interfere with your existing database schema.   Let's take a look at creating a VIEW of the data that is \"mobile ready\".  This is an Entity Relationship\nDiagram of what we are going to do:   we are going to create a separate table for the system properties, called  [mobile].[SysProps_TodoItems] .\nThis will hold the five fields that Azure Mobile Apps requires, plus a reference to the TodoItem id:  CREATE TABLE [mobile].[SysProps_TodoItems] (\n  [Id]        NVARCHAR(128) CONSTRAINT [DF_todoitem_id] DEFAULT (CONVERT([NVARCHAR](255),NEWID(),(0))) NOT NULL,\n  [CreatedAt] DATETIMEOFFSET(7) CONSTRAINT [DF_todoitem_createdAt] DEFAULT (CONVERT([DATETIMEOFFSET](7),SYSUTCDATETIME(),(0))) NOT NULL,\n  [UpdatedAt] DATETIMEOFFSET(7) NULL,\n  [Version]   ROWVERSION NOT NULL,\n  [Deleted]   BIT DEFAULT ((0)) NOT NULL,\n  [Item_Id]   BIGINT NOT NULL\n  PRIMARY KEY NONCLUSTERED ([id] ASC)\n);  Then we will create a SQL VIEW that maps to this:  CREATE VIEW [mobile].[TodoItems] AS\nSELECT\n    [mobile].[SysProps_TodoItems].[Id],\n    [mobile].[SysProps_TodoItems].[CreatedAt],\n    [mobile].[SysProps_TodoItems].[UpdatedAt],\n    [mobile].[SysProps_TodoItems].[Version],\n    [mobile].[SysProps_TodoItems].[Deleted],\n    [mobile].[SysProps_TodoItems].[Item_Id],\n    [dbo].[TodoItems].[UserId],\n    [dbo].[TodoItems].[Title],\n    [dbo].[TodoItems].[Complete]\nFROM\n    [dbo].[TodoItems],\n    [mobile].[SysProps_TodoItems]\nWHERE\n    [dbo].[TodoItems].[id] = [mobile].[SysProps_TodoItems].[Item_Id];  This view is still only a combination of the two tables.  I want specific logic so that when a row\nis changed (inserted, updated or deleted) in the  [dbo].[TodoItems]  table, a similar change is made\nto the  [mobile].[SysProps_TodoItems]  table.  This is achieved through the use of triggers:  CREATE TRIGGER\n    [dbo].[TRG_TodoItem_Insert]\nON\n    [dbo].[TodoItems]\nAFTER\n    INSERT\nAS BEGIN\n    INSERT INTO [mobile].[SysProps_TodoItems] ([Item_Id], [UpdatedAt])\n    SELECT inserted.id, CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME())\n    FROM inserted\nEND\nGO\n\nCREATE TRIGGER\n    [dbo].[TRG_TodoItem_Update]\nON\n    [dbo].[TodoItems]\nAFTER\n    UPDATE\nAS BEGIN\n    UPDATE\n        [mobile].[SysProps_TodoItems]\n    SET\n        [UpdatedAt] = CONVERT(DATETIMEOFFSET(7), SYSUTCDATETIME())\n    FROM\n        INSERTED\n    WHERE\n        INSERTED.id = [mobile].[SysProps_TodoItems].[Item_Id]\nEND\nGO\n\nCREATE TRIGGER\n    [dbo].[TRG_TodoItem_Delete]\nON\n    [dbo].[TodoItems]\nAFTER\n    DELETE\nAS BEGIN\n    DELETE FROM [mobile].[SysProps_TodoItems]\n    WHERE [Item_Id] IN (select deleted.id from deleted)\nEND\nGO   Tip  You may want to set the  Deleted  flag to 1 (or true) in the Delete trigger.  This will enable soft\ndelete on the system properties table, ensuring that mobile clients remove the row from their offline\ncache.   Similarly, when changes are made by the mobile backend to the VIEW, we need to propagate those changes\nto the underlying tables.  This is also done with triggers:  CREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Insert]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    INSERT\nAS BEGIN\n    DECLARE @userid AS NVARCHAR(255)\n    SELECT @userid = inserted.UserId FROM inserted\n    DECLARE @title AS NVARCHAR(255)\n    SELECT @title = inserted.Title FROM inserted\n    DECLARE @complete AS BIT\n    SELECT @complete = inserted.Complete FROM inserted\n\n\n    INSERT INTO\n        [dbo].[TodoItems] ([UserId], [Title], [Complete])\n    VALUES\n        (@userid, @title, @complete)\n\n    IF UPDATE(Id) BEGIN\n        DECLARE @itemid AS BIGINT\n        SELECT @itemid = @@identity\n        DECLARE @id AS NVARCHAR(255)\n        SELECT @id = inserted.Id FROM inserted\n        UPDATE [mobile].[SysProps_TodoItems] SET [Id] = @id WHERE [item_id] = @itemid\n    END\nEND;\nGO\n\nCREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Update]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    UPDATE\nAS BEGIN\n    DECLARE @id AS NVARCHAR(255)\n    SELECT @id = inserted.id FROM inserted\n    DECLARE @itemid AS BIGINT\n    SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\n\n    IF UPDATE(UserId) BEGIN\n        DECLARE @userid AS NVARCHAR(255)\n        SELECT @userid = inserted.UserId FROM inserted\n        UPDATE [dbo].[TodoItems] SET [UserId] = @userid WHERE [id] = @itemid\n    END\n    IF UPDATE(Title) BEGIN\n        DECLARE @title AS NVARCHAR(255)\n        SELECT @title = inserted.Title FROM inserted\n        UPDATE [dbo].[TodoItems] SET [Title] = @title WHERE [id] = @itemid\n    END\n    IF UPDATE(Complete) BEGIN\n        DECLARE @complete AS BIT\n        SELECT @complete = inserted.Complete FROM inserted\n        UPDATE [dbo].[TodoItems] SET [Complete] = @complete WHERE [id] = @itemid\n    END\n    IF UPDATE(deleted) BEGIN\n        DECLARE @deleted AS BIT\n        SELECT @deleted = inserted.deleted FROM inserted\n        UPDATE [mobile].[SysProps_TodoItems] SET [deleted] = @deleted WHERE [item_id] = @itemid\n    END\nEND\nGO\n\nCREATE TRIGGER\n    [mobile].[TRG_Mobile_TodoItem_Delete]\nON\n    [mobile].[TodoItems]\nINSTEAD OF\n    DELETE\nAS BEGIN\n    DECLARE @id AS NVARCHAR(255)\n    SELECT @id = deleted.id FROM deleted\n    DECLARE @itemid AS BIGINT\n    SELECT @itemid = [item_id] FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\n\n    DELETE FROM [dbo].[TodoItems] WHERE [id] = @itemid\n    DELETE FROM [mobile].[SysProps_TodoItems] WHERE [id] = @id\nEND\nGO  A standard SQL view is read-only.  We made the view read/write by using triggers to trap the\ncall and replace it with two calls instead.   Warn  This version does not support soft delete.  If you wish to support soft delete, set the Deleted  flag in the `[mobile].[SysProps_TodoItems] table instead of deleting the row.", 
            "title": "Using an existing SQL Table"
        }, 
        {
            "location": "/chapter3/server/#changing-the-mobile-schema", 
            "text": "We stored our mobile representation of the table in a different schema.  Azure Mobile Apps looks for tables (and views) in the  [dbo]  schema by default.  There are two places you can modify the schema that Entity Framework uses to access the database.  The first is by changing the default schema that Entity Framework uses.  This will affect all the tables that we are exposing via a table controller.  This is done in the  Models\\MobileDbContext.cs  class:  protected override void OnModelCreating(DbModelBuilder modelBuilder)\n{\n    modelBuilder.HasDefaultSchema( mobile );\n    modelBuilder.Conventions.Add(\n        new AttributeToColumnAnnotationConvention TableColumnAttribute, string (\n             ServiceTableColumn ,\n            (property, attributes) =  attributes.Single().ColumnType.ToString()\n        )\n    );\n}  We can also do this on the model as an annotation.  For example, here is the TodoItem model suitably adjusted:  using Microsoft.Azure.Mobile.Server;\nusing Newtonsoft.Json;\nusing System.ComponentModel.DataAnnotations.Schema;\n\nnamespace Backend.DataObjects\n{\n    [Table( TodoItems , Schema= mobile )]\n    public class TodoItem : EntityData\n    {\n        public string UserId { get; set; }\n\n        public string Title { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}   Info  Entity Framework adds an  s  onto the end of the model to create the table name.  If you have a model named  TodoItem , the table is called  TodoItems  in the database.  You can use this same annotation to adjust the table name if it is not to your liking.", 
            "title": "Changing the Mobile Schema"
        }, 
        {
            "location": "/chapter3/server/#best-practices", 
            "text": "Here is a summary of what I consider best practices for table controllers:   Use Code First when you are in a green-field database situation.  Use Database First when you have to integrate an existing database.  Only expose the data that you need for the mobile client.  Set up Automatic Code First Migrations as early as possible.  Think about what happens to existing clients when you update the schema.   This last point is an important one.  Let's say you have a v1 mobile client of your awesome mobile\napplication.  This works with v1 mobile backend.  You've set up the schema and ensured that everything\nworks.  Then you want to release v2 mobile client.  It has a new feature and needs some extra data.\nSo you update to v2 mobile backend that provides that data.  You need to ensure that the schema changes provided by the v2 mobile backend are optional.  In other\nwords, the v1 mobile client can continue to work against the v2 mobile backend.  You should test this\ncase.  Users do not upgrade instantly.  In fact, many users don't upgrade at all.  At every release\nof your app, you are going to have to ensure that ALL versions of your mobile client work against\nyour mobile backend.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter3/projection/", 
            "text": "Projecting a Data Set\n\n\nWe have thus far looked at what it takes to project a whole SQL database table into the mobile world.  We\ncan easily do both pre-existing and greenfield databases with code-first and database-first methodologies.\nThe next logical thing is to see what we can do to adjust the transfer of data.  How do we filter and\ntransform the data as requests are sent to the server.\n\n\nThere are two places where adjustment of the transfer is accomplished.  I recommend spending time on the\nserver adjusting the table controller so that security policies are assured.  The set of data that a\nmobile client can see should be the complete set of data that the user of that mobile device is allowed\nto see.  We can then adjust the view of that data at the client.\n\n\nFor example, let's say that the user is a sales person.  They are allowed to see the information for\ntheir accounts, but only wants to see the records for the accounts that have planned to visit within\nthe next week.  We would place the limitation on what records they can see on the server, but place\nthe date range manipulation on the client.\n\n\nIn this section, we will look at all the things one can do on in the table controller on the server.\n\n\nBasics of Projection\n\n\nThere are four basic things we will want to do with table controllers:\n\n\nFilters\n adjust the data that the requesting user can see.  We would normally apply a filter to all\nmethods EXCEPT the \nCreate\n or \nInsert\n method.  This is the most common adjustment that is coded in\nthe table controller as filtering is the key to enforcing security policy.\n\n\nTransforms\n adjust the data that is being sent to the table controller before it is stored.  It is\nused in two areas.  First, it is used to automatically inject necessary fields for ensuring the security\nfilters can be applied.  For instance, if we wish to have a per-user data store (where a user can only\nsee their own records), then we will need to store the user ID of the requesting user.  Secondly, it is\nused to insert point-in-time lookups into a record.  For instance, if we wish to record the current price\nof an item at the time the record was inserted into the table, we would do this with a transform.\n\n\nValidations\n do not adjust the data.  Validations ensure that the data is correct according to the\nserver model.  Your data may, for example, store an age indirectly by storing the year of birth.  It's\nhighly unlikely that you will want to support the entire range of possible years.  You definitely don't\nwant to support years in the future.\n\n\nFinally, \nHooks\n allow another piece of code to be triggered either before or after the request has been\nprocessed.  For example, we may wish to send a push notification on a valid insert, or kick off an order\nprocessing work flow when a record is updated with an approval to ship.  We won't be covering hooks in\nthis chapter as we have a whole chapter on \ncustomized requests\n later on.\n\n\nProjection Recipes\n\n\nThere are a few \"standard\" projects we see all the time and these are great ways to learn how to do\nprojections.\n\n\nPer-User Data\n\n\nThe first projection that pretty much everyone implements is the \nPer-User Data\n projection.  In this recipe, we want the user to only see records that they have inserted.  For example, let's update our TodoItem table to support per-user data.  This involves three parts:\n\n\n\n\nA \nFilter\n that limits data to only the logged in user.\n\n\nA \nTransform\n that updates an inserted record with the logged in user.\n\n\nA \nValidation\n that ensures an updated or deleted record is owned by the user.\n\n\n\n\nThe logged in user is available as the User object, but you have to cast it to a \nClaimsPrinicipal\n to\naccess the claims that are sent inside the identity token.  I tend to use a public property as an\nimplementation:\n\n\npublic string UserId\n{\n    get\n    {\n        var principal = this.User as ClaimsPrincipal;\n        return principal.FindFirst(ClaimTypes.NameIdentifier).Value;\n    }\n}\n\n\n\n\n\n\nTip\n\n\nIt's generally a good idea to use the SID as the user ID for the authenticated user in security\napplications.  The user can change the email address or username associated with the account, but\nthe SID never changes.\n\n\n\n\nWe need an extra property in the \nDataObjects\\TodoItem.cs\n class (in the \nBackend\n project) to hold\nthe extra security claim that we will be adding later:\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string UserId { get; set; }\n\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nRemember to do a code-first migration if you are doing this on an existing service.  Let's take a look\nat the \nPostTodoItem()\n first.  This requires the \nTransform\n to ensure the UserId field is filled\nin.  We've already defined the \nUserId\n field, so we can inject that in the inbound object:\n\n\n// POST tables/TodoItem\npublic async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n{\n    item.UserId = UserId;\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\n\n\n\nTransforms tend to be short.  This is deliberate.  We don't want any of our code in a table controller\nto do too much work.  The heavy lifting is done by the database, with the ASP.NET table controller being\na conduit for translating requests into responses.  This allows us to support more users on less virtual\nhardware.\n\n\nThe \nFilter\n is a relatively simple affair in this case.  We ensure that the only records returned\nare those that belong to the user.  For example, here is a simplistic filter applied to the \nGetAll\n\nmethod:\n\n\n// GET tables/TodoItem\npublic IQueryable\nTodoItem\n GetAllTodoItems()\n{\n    return Query().Where(item =\n item.UserId.Equals(UserId));\n}\n\n\n\n\nThe \nQuery()\n and \nLookup(id).Queryable\n methods return \nIQueryable\n objects.  The IQueryable is used\nto represent a query, so we can alter it with LINQ.  A filter is merely a LINQ expression to limit the\nrecords being returned.  There might be another filter sent by the client, in which case this filter will\nbe tacked on the end of the request.  For instance, let's say that the client requests only records where\n\nComplete == False\n.  When this comes through the \nGetAllTodoItems()\n method, the resulting SQL code will\nlook something like this:\n\n\nSELECT * FROM [dbo].[TodoItems]\n    WHERE (Complete = false) AND (UserId = @0);\n\n\n\n\nThe \n@0\n parameter will be replaced by the users SID.\n\n\n\n\nWarn\n\n\nIf a user is not logged in (i.e. you forgot to add the \n[Authorize]\n attribute), the User object will\nbe null and the server will produce a 500 Internal Server Error back to the client.\n\n\n\n\nThis can get a little unwieldy for complex filters, however.  Since the filters are applied in two\ndifferent places (and are generally used for validation as well), I like to abstract them into a\n\nLINQ extension method\n.  Create a class in \nExtensions\\TodoItemExtensions.cs\n (you will have to create\nthe \nExtensions\n directory) with the following contents:\n\n\nusing System.Linq;\nusing Chapter3.DataObjects;\n\nnamespace Chapter3.Extensions\n{\n    public static class TodoItemExtensions\n    {\n        public static IQueryable\nTodoItem\n PerUserFilter(this IQueryable\nTodoItem\n query, string userid)\n        {\n            return query.Where(item =\n item.UserId.Equals(userid));\n        }\n    }\n}\n\n\n\n\nWe can use this to simplify our filters and make them more readable:\n\n\n// GET tables/TodoItem\npublic IQueryable\nTodoItem\n GetAllTodoItems()\n{\n    return Query().PerUserFilter(UserId);\n}\n\n// GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic SingleResult\nTodoItem\n GetTodoItem(string id)\n{\n    return new SingleResult\nTodoItem\n(Lookup(id).Queryable.PerUserFilter(UserId));\n}\n\n\n\n\nNote that we need to apply the filter we have written to both the Get methods.\n\n\nWhen we look at the \nDelete\n and \nPatch\n methods, we only have to validate that the UserId owns the\nid we are updating.  For that, I write a custom validation method.  This method is in the table controller:\n\n\nprivate void ValidateOwner(string id)\n{\n    var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault\nTodoItem\n();\n    if (result == null)\n    {\n        throw new HttpResponseException(HttpStatusCode.NotFound);\n    }\n}\n\n\n\n\nThe validation method must throw an appropriate \nHttpResponseException\n if the validation fails.  It's common\nto return a 404 Not Found error rather than a 403 Forbidden error for security reasons.  Returning a\n403 Forbidden error confirms that the Id exists, which is a data leakage.  Returning a 404 Not Found\nerror means that a rogue client cannot tell the difference between \"I can't access the record\" and\n\"the record doesn't exist\".  We can use this validation method in each method that requires it:\n\n\n// PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch)\n{\n    ValidateOwner(id);\n    return UpdateAsync(id, patch);\n}\n\n// DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic Task DeleteTodoItem(string id)\n{\n    ValidateOwner(id);\n    return DeleteAsync(id);\n}\n\n\n\n\nAlthough this is a very basic example of a filtered table, we can see three different techniques:\n\n\n\n\nA \nFilter\n implemented as a LINQ query in an \nIQueryable\n extension method, applied to both \nGet\n methods.\n\n\nA \nTransform\n implemented inside the \nPost\n method.\n\n\nA \nValidation\n implemented using the filter as a method in the table controller, applied to \nPatch\n and \nDelete\n methods.\n\n\n\n\nI set up a copy of the TaskList from Chapter 2 with Azure Active Directory Client Flow for authentication.\nWe can look at the SQL table contents after we log in with this client and add a new task:\n\n\n\n\nNote that the UserId is the security ID of the authenticated user.  The security ID is a stable ID for\nthe user.  The security ID does not change if the user changes their username or email address.\n\n\nPer-Group Data\n\n\nLet's say we have a mobile client that a sales person uses to enter data about sales.  He might be able\nto pick from a list of industries that the account is in.  The enterprise may further organize those\nindustries as groups, with several people within the organization able to view the accounts associated\nwith a specific industry.\n\n\nIn this case:\n\n\n\n\nWe want to limit the mobile client to only view accounts for groups to which he belongs.\n\n\nWe want to allow the mobile client to submit new accounts for any group to which he belongs.\n\n\nUpdates and Deletes should not adjust the group field.\n\n\n\n\nThe limit will be implemented as a filter.  The update and insert methods will require a validation method (comparing the submitted group with the list of groups to which the user belongs).\n\n\nWe have another table in \nDataObjects\\Example.cs\n.  Let's extend it to support a GroupId:\n\n\nusing System;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Example : EntityData\n    {\n        public string GroupId { get; set; }\n        public string StringField { get; set; }\n        public int IntField { get; set; }\n        public double DoubleField { get; set; }\n        public DateTimeOffset DateTimeField { get; set; }\n    }\n}\n\n\n\n\nWe'll store the group ID in the additional field.  Don't forget to use a code-first migration to update the database.\nEach of the validations and filters requires a list of the groups the user belongs to.  This can be achieved using\na claim lookup:\n\n\n/// \nsummary\n\n/// Get the list of groups from the claims\n/// \n/summary\n\n/// \nreturns\nThe list of groups\n/returns\n\npublic async Task\nList\nstring\n GetGroups()\n{\n    var creds = await User.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(Request);\n    return creds.UserClaims\n        .Where(claim =\n claim.Type.Equals(\ngroups\n))\n        .Select(claim =\n claim.Value)\n        .ToList();\n}\n\n\n\n\n\n\nTip\n\n\nIf you are using claims as part of your security model, you should add the claims that you are using to the identity token that is used for authentication.  You can do this with custom authentication by calling LoginAsync() twice - once for the standard login method and the second time to adjust the token through the custom auth.\n\n\n\n\nOur filter is defined as a LINQ extension (just like in the per-user filter) in \nExtensions\\ExampleExtensions.cs\n:\n\n\nusing System.Collections.Generic;\nusing System.Linq;\nusing Chapter3.DataObjects;\n\nnamespace Chapter3.Extensions\n{\n    public static class ExampleExtensions\n    {\n        public static IQueryable\nExample\n PerGroupFilter(this IQueryable\nExample\n query, List\nstring\n groups)\n        {\n            return query.Where(item =\n groups.Contains(item.GroupId));\n        }\n    }\n}\n\n\n\n\nWe can use this LINQ extension on the retrieval methods:\n\n\n// GET tables/Example\npublic async Task\nIQueryable\nExample\n GetAllExample()\n{\n    var groups = await GetGroups();\n    return Query().PerGroupFilter(groups);\n}\n\n// GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic async Task\nSingleResult\nExample\n GetExample(string id)\n{\n    var groups = await GetGroups();\n    return new SingleResult\nExample\n(Lookup(id).Queryable.PerGroupFilter(groups));\n}\n\n\n\n\nWe have to convert each of the methods to a async method so that we can check the groups.  Retrieving the group\nlist for a user is an async method and this trickles down to the method being called.\n\n\nThe validation method is also an async method (for the same reason).  The Post and Patch methods look like\nthis:\n\n\n/// \nsummary\n\n/// Validator to determine if the provided group is in the list of groups\n/// \n/summary\n\n/// \nparam name=\ngroup\nThe group name\n/param\n\npublic async Task ValidateGroup(string group)\n{\n    var groups = await GetGroups();\n    if (!groups.Contains(group))\n    {\n        throw new HttpResponseException(HttpStatusCode.BadRequest);\n    }\n}\n\n// PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic async Task\nExample\n PatchExample(string id, Delta\nExample\n patch)\n{\n    await ValidateGroup(patch.GetEntity().GroupId);\n    return await UpdateAsync(id, patch);\n}\n\n// POST tables/Example\npublic async Task\nIHttpActionResult\n PostExample(Example item)\n{\n    await ValidateGroup(item.GroupId);\n    Example current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\n\n\n\nIt's appropriate to throw a 400 Bad Request for a validation error in this case as the user is\nauthenticated at this point.  The user is not being exposed by the response.\n\n\nThere is no transform in this recipe as the group ID is being sent on each update request.\n\n\nFriends Data\n\n\nOne of the common social patterns is a \"friends feed\". We can post to our feed and we can see\nboth our messages and our friends messages. In this recipe, we will have three tables.  The\nfirst is the \nUsers\n table with the following model:\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class User : EntityData\n    {\n        public string EmailAddress { get; set; }\n        public string Name { get; set; }\n    }\n}\n\n\n\n\n\n\nTip\n\n\nDon't forget to add a \nDbSet\n for each table to the \nMobileServiceContext\n to add the table.\n\n\n\n\nIn our application, we will update the \nUsers\n table via a \ncustom authentication controller\n.\nAfter we have logged in via Azure Active Directory, we call the \nInvokeApiAsync()\n method\nto call the custom authentication controller and get a new token with some extra information\nin it.  We'll cover custom authentication controllers in a later chapter.\n\n\nusing System;\nusing System.Data.Entity.Migrations;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Security.Principal;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing Chapter3.DataObjects;\nusing Chapter3.Models;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Login;\nusing Newtonsoft.Json;\n\nnamespace Chapter3.Controllers\n{\n    [Authorize]\n    [Route(\nauth/login/custom\n)]\n    public class CustomAuthController : ApiController\n    {\n        MobileServiceContext dbContext;\n\n        public CustomAuthController()\n        {\n            dbContext = new MobileServiceContext();\n\n            string website = Environment.GetEnvironmentVariable(\nWEBSITE_HOSTNAME\n);\n            Audience = $\nhttps://{website}/\n;\n            Issuer = $\nhttps://{website}/\n;\n            SigningKey = Environment.GetEnvironmentVariable(\nWEBSITE_AUTH_SIGNING_KEY\n);\n        }\n\n        public string Audience { get; set; }\n        public string Issuer { get; set; }\n        public string SigningKey { get; set; }\n\n        [HttpPost]\n        public async Task\nIHttpActionResult\n Post()\n        {\n            var creds = await User.GetAppServiceIdentityAsync\nAzureActiveDirectoryCredentials\n(Request);\n            var sid = ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n            var email = creds.UserClaims\n                .FirstOrDefault(claim =\n claim.Type.EndsWith(\nemailaddress\n))\n                .Value;\n            var name = creds.UserClaims\n                .FirstOrDefault(claim =\n claim.Type.EndsWith(\nname\n))\n                .Value;\n\n            // Insert the record information into the database\n            User user = new User()\n            {\n                Id = sid,\n                Name = name,\n                EmailAddress = email\n            };\n            dbContext.Users.AddOrUpdate(user);\n            dbContext.SaveChanges();\n\n            // Mind a new token based on the old one plus the new information\n            var newClaims = new Claim[]\n            {\n                new Claim(JwtRegisteredClaimNames.Sub, sid),\n                new Claim(JwtRegisteredClaimNames.Email, email),\n                new Claim(\nname\n, name)\n            };\n            JwtSecurityToken token = AppServiceLoginHandler.CreateToken(\n                newClaims, SigningKey, Audience, Issuer, TimeSpan.FromDays(30));\n\n            // Return the token and user ID to the client\n            return Ok(new LoginResult()\n            {\n                AuthenticationToken = token.RawData,\n                UserId = sid\n            });\n        }\n    }\n\n    public class LoginResult\n    {\n        [JsonProperty(PropertyName = \nauthenticationToken\n)]\n        public string AuthenticationToken { get; set; }\n\n        [JsonProperty(PropertyName = \nuser_id\n)]\n        public string UserId { get; set; }\n    }\n}\n\n\n\n\nThe actual database update is done by Entity Framework.  The ASP.NET service is just\na regular ASP.NET service using Entity Framework, so all the same facilities are\navailable as that configuration.  In this case, we take the provided token (which\nis the same token that Azure Active Directory client-flow returns) and return a\nmodified token that includes a couple of extra fields.  During this process, we\nupdate the database by adding or inserting (also known as upserting) a record with\nthe Id field set to the security ID and the additional fields we need.\n\n\nI need a table to implement the \"friends\" relationship.  My friends are not stored\nin Azure Active Directory.  If I were using a social provider, I could use the\nfriends feed from that social provider by doing a Graph API lookup.  In this case,\nI'm going to use the following model:\n\n\nnamespace Chapter3.DataObjects\n{\n    public class Friend\n    {\n        public string UserId { get; set; }\n        public string FriendId { get; set; }\n    }\n}\n\n\n\n\nThis is not based on \nEntityData\n because I am not going to expose this table to the mobile client.  It's purely for determining what records I am going to show to the mobile client.  In this example, I will maintain the data in this table manually. A \"real\" application would have some sort of custom workflow to add friends and get the friends to approve the connection.\n\n\nTo get the list of \"friends I can see\", I will request a list of the \nFriendId\n field where the \nUserId\n is my UserId.\n\n\nThe final table in the trio is the \nMessages\n table.  This will be downloaded to the\nmobile client so it has to be based on the \nEntityData\n base class.  In addition, the\ninserts for this table are going to look a lot like a per-user table.  I need the\n\nUserId\n field to properly maintain the security model.\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n    }\n}\n\n\n\n\nLet's think about the security model we want to implement in the \nMessages\n table controller:\n\n\n\n\nA \nfilter\n will allow the viewing of the users own data or any data that has an association in the Friends table.\n\n\nA \ntransform\n will set the owner of the record to my UserId\n\n\nWe will remove the ability to update or delete records since this is a write-once read-many table.\n\n\n\n\nLet's look at the \nPostMessage()\n method for the \nMessageController\n first.  This is\npractically identical to the \nPostTodoItem()\n method in the per-user recipe:\n\n\npublic string UserId =\n ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n// POST tables/Message\npublic async Task\nIHttpActionResult\n PostMessage(Message item)\n{\n    item.UserId = UserId;\n    Message current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\n\n\n\nThe filter is a little harder.  We are going to use the \nFluent Syntax\n for LINQ to provide the right logic. The Fluent Syntax is also known as \"Query Syntax\" or the \"declarative syntax\", depending on the author.  The extension method looks like this:\n\n\nusing Chapter3.DataObjects;\nusing System.Data.Entity;\nusing System.Linq;\n\nnamespace Chapter3.Extensions\n{\n    public static class MessageExtensions\n    {\n        public static IQueryable\nMessage\n OwnedByFriends(this IQueryable\nMessage\n query, DbSet\nFriend\n friends, string userId)\n        {\n            var myPosts = from m in query\n                          let fr = (from f in friends where f.FriendId == userId select f.UserId)\n                          where m.UserId == userId || fr.Contains(m.UserId)\n                          select m;\n            return myPosts;\n        }\n    }\n}\n\n\n\n\nThe LINQ query selects the messages from the \nMessages\n table where the author is either the mobile client user or the mobile client\nuser is listed as a friend of the author.  My query methods in the table controller now look similar to the per-user data:\n\n\npublic string UserId =\n ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n// GET tables/Message\npublic IQueryable\nMessage\n GetAllMessage()\n{\n    return Query().OwnedByFriends(context.Friends, UserId);\n}\n\n// GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic SingleResult\nMessage\n GetMessage(string id)\n{\n    return new SingleResult\nMessage\n(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId));\n}\n\n\n\n\nUsing LINQPad to test LINQ Queries\n\n\nI tend to struggle with LINQ queries.  Fortunately, there are a plethora of blogs, tutorials and tools out there to assist.  One\nof my favorite tools is \nLINQPad\n.  LINQPad gives you an interactive playground for testing your LINQ queries.  In this case, I\ncreated a new query with the following contents:\n\n\npublic class Message {\n   public string UserId;\n   public string Text;\n}\n\npublic class Friend {\n   public string UserId;\n   public string FollowerId;\n}\n\nvoid Main()\n{\n\n\nList\nFriend\n friends = new List\nFriend\n() {\n   new Friend() { UserId = \nadrian\n, FollowerId = \ndonna\n },\n   new Friend() { UserId = \nfabio\n, FollowerId = \ndonna\n },\n   new Friend() { UserId = \nfabio\n, FollowerId = \nadrian\n }\n};\n\nList\nMessage\n messages = new List\nMessage\n() {\n   new Message() { UserId = \nadrian\n, Text = \nmessage 1\n },\n   new Message() { UserId = \ndonna\n, Text = \nmessage 2\n },\n   new Message() { UserId = \nfabio\n, Text = \nmessage 3\n }\n};\nvar user = \nfabio\n;\n\nvar q = from m in messages\n        let fr = (from f in friends where f.FollowerId == user select f.UserId)\n        where m.UserId == user || fr.Contains(m.UserId)\n        select m;\n\nq.Dump();\n\n}\n\n\n\n\nI have a friends table and a messages table.  User \"donna\" should be able to see three messages, user \"adrian\"\nshould be able to see two messages and user \"fabio\" should be able to see just one message.  By changing the\nvalue of the \nuser\n variable, I can test the query I am writing.  I don't need to set up a database (although\nLINQPad supports that as well).\n\n\n\n\nBest Practices\n\n\nThere are a number of best practices that I think are important in developing table controllers:\n\n\n\n\nOptimize Operations\n\n\n\n\nYou should always optimize the CRUD operations that are implemented in a table controller.  This means limiting\n  the code so that only \nfilters\n, \ntransforms\n and \nvalidators\n are used.  You can use \nhooks\n as an\n  asynchronous way to handle custom code if something else needs to happen when a mobile client inserts, updates\n  or deletes a record.  (We will be delving into hooks during the custom code chapter later on).\n\n\nYou should \nNOT\n insert custom code into a table controller that runs synchronously.\n\n\n\n\nImplement Security Policy with Filters\n\n\n\n\nThe mobile backend should be concerned with security.  What can the connecting user see?  Use \nfilters\n to\n  ensure that the connecting user can only see the data that they are allowed to see.  There are several examples\n  of bad filters.  For example, if a user normally wants to see the last 7 days worth of messages, but is allowed\n  to see all messages.  I would implement this particular case as a client-side filter as it has nothing to do\n  with security.\n\n\n\n\nUse LINQ Extension Methods\n\n\n\n\nLINQ extension methods can be used to great effect to make your CRUD methods more readable.  I love readable\n  code.  For example, consider the following two code snippets from the last recipe:\n\n\npublic IQueryable\nMessage\n GetAllMessage()\n{\n  // Recipe #1\n  return Query().OwnedByFriends(context.Friends, UserId);\n\n  // Recipe #2\n  return from m in Query()\n    let fr = (from f in context.Friends where f.FriendId == UserId select f.UserId)\n    where m.UserId == UserId || fr.Contains(m.UserId)\n    select m;\n}\n\n\n\n\nThe first recipe makes the intent of the filter very clear.  I have to work at understanding the specific implementation\n  of the second method.", 
            "title": "Data Projection and Queries"
        }, 
        {
            "location": "/chapter3/projection/#projecting-a-data-set", 
            "text": "We have thus far looked at what it takes to project a whole SQL database table into the mobile world.  We\ncan easily do both pre-existing and greenfield databases with code-first and database-first methodologies.\nThe next logical thing is to see what we can do to adjust the transfer of data.  How do we filter and\ntransform the data as requests are sent to the server.  There are two places where adjustment of the transfer is accomplished.  I recommend spending time on the\nserver adjusting the table controller so that security policies are assured.  The set of data that a\nmobile client can see should be the complete set of data that the user of that mobile device is allowed\nto see.  We can then adjust the view of that data at the client.  For example, let's say that the user is a sales person.  They are allowed to see the information for\ntheir accounts, but only wants to see the records for the accounts that have planned to visit within\nthe next week.  We would place the limitation on what records they can see on the server, but place\nthe date range manipulation on the client.  In this section, we will look at all the things one can do on in the table controller on the server.", 
            "title": "Projecting a Data Set"
        }, 
        {
            "location": "/chapter3/projection/#basics-of-projection", 
            "text": "There are four basic things we will want to do with table controllers:  Filters  adjust the data that the requesting user can see.  We would normally apply a filter to all\nmethods EXCEPT the  Create  or  Insert  method.  This is the most common adjustment that is coded in\nthe table controller as filtering is the key to enforcing security policy.  Transforms  adjust the data that is being sent to the table controller before it is stored.  It is\nused in two areas.  First, it is used to automatically inject necessary fields for ensuring the security\nfilters can be applied.  For instance, if we wish to have a per-user data store (where a user can only\nsee their own records), then we will need to store the user ID of the requesting user.  Secondly, it is\nused to insert point-in-time lookups into a record.  For instance, if we wish to record the current price\nof an item at the time the record was inserted into the table, we would do this with a transform.  Validations  do not adjust the data.  Validations ensure that the data is correct according to the\nserver model.  Your data may, for example, store an age indirectly by storing the year of birth.  It's\nhighly unlikely that you will want to support the entire range of possible years.  You definitely don't\nwant to support years in the future.  Finally,  Hooks  allow another piece of code to be triggered either before or after the request has been\nprocessed.  For example, we may wish to send a push notification on a valid insert, or kick off an order\nprocessing work flow when a record is updated with an approval to ship.  We won't be covering hooks in\nthis chapter as we have a whole chapter on  customized requests  later on.", 
            "title": "Basics of Projection"
        }, 
        {
            "location": "/chapter3/projection/#projection-recipes", 
            "text": "There are a few \"standard\" projects we see all the time and these are great ways to learn how to do\nprojections.", 
            "title": "Projection Recipes"
        }, 
        {
            "location": "/chapter3/projection/#per-user-data", 
            "text": "The first projection that pretty much everyone implements is the  Per-User Data  projection.  In this recipe, we want the user to only see records that they have inserted.  For example, let's update our TodoItem table to support per-user data.  This involves three parts:   A  Filter  that limits data to only the logged in user.  A  Transform  that updates an inserted record with the logged in user.  A  Validation  that ensures an updated or deleted record is owned by the user.   The logged in user is available as the User object, but you have to cast it to a  ClaimsPrinicipal  to\naccess the claims that are sent inside the identity token.  I tend to use a public property as an\nimplementation:  public string UserId\n{\n    get\n    {\n        var principal = this.User as ClaimsPrincipal;\n        return principal.FindFirst(ClaimTypes.NameIdentifier).Value;\n    }\n}   Tip  It's generally a good idea to use the SID as the user ID for the authenticated user in security\napplications.  The user can change the email address or username associated with the account, but\nthe SID never changes.   We need an extra property in the  DataObjects\\TodoItem.cs  class (in the  Backend  project) to hold\nthe extra security claim that we will be adding later:  using Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class TodoItem : EntityData\n    {\n        public string UserId { get; set; }\n\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n    }\n}  Remember to do a code-first migration if you are doing this on an existing service.  Let's take a look\nat the  PostTodoItem()  first.  This requires the  Transform  to ensure the UserId field is filled\nin.  We've already defined the  UserId  field, so we can inject that in the inbound object:  // POST tables/TodoItem\npublic async Task IHttpActionResult  PostTodoItem(TodoItem item)\n{\n    item.UserId = UserId;\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}  Transforms tend to be short.  This is deliberate.  We don't want any of our code in a table controller\nto do too much work.  The heavy lifting is done by the database, with the ASP.NET table controller being\na conduit for translating requests into responses.  This allows us to support more users on less virtual\nhardware.  The  Filter  is a relatively simple affair in this case.  We ensure that the only records returned\nare those that belong to the user.  For example, here is a simplistic filter applied to the  GetAll \nmethod:  // GET tables/TodoItem\npublic IQueryable TodoItem  GetAllTodoItems()\n{\n    return Query().Where(item =  item.UserId.Equals(UserId));\n}  The  Query()  and  Lookup(id).Queryable  methods return  IQueryable  objects.  The IQueryable is used\nto represent a query, so we can alter it with LINQ.  A filter is merely a LINQ expression to limit the\nrecords being returned.  There might be another filter sent by the client, in which case this filter will\nbe tacked on the end of the request.  For instance, let's say that the client requests only records where Complete == False .  When this comes through the  GetAllTodoItems()  method, the resulting SQL code will\nlook something like this:  SELECT * FROM [dbo].[TodoItems]\n    WHERE (Complete = false) AND (UserId = @0);  The  @0  parameter will be replaced by the users SID.   Warn  If a user is not logged in (i.e. you forgot to add the  [Authorize]  attribute), the User object will\nbe null and the server will produce a 500 Internal Server Error back to the client.   This can get a little unwieldy for complex filters, however.  Since the filters are applied in two\ndifferent places (and are generally used for validation as well), I like to abstract them into a LINQ extension method .  Create a class in  Extensions\\TodoItemExtensions.cs  (you will have to create\nthe  Extensions  directory) with the following contents:  using System.Linq;\nusing Chapter3.DataObjects;\n\nnamespace Chapter3.Extensions\n{\n    public static class TodoItemExtensions\n    {\n        public static IQueryable TodoItem  PerUserFilter(this IQueryable TodoItem  query, string userid)\n        {\n            return query.Where(item =  item.UserId.Equals(userid));\n        }\n    }\n}  We can use this to simplify our filters and make them more readable:  // GET tables/TodoItem\npublic IQueryable TodoItem  GetAllTodoItems()\n{\n    return Query().PerUserFilter(UserId);\n}\n\n// GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic SingleResult TodoItem  GetTodoItem(string id)\n{\n    return new SingleResult TodoItem (Lookup(id).Queryable.PerUserFilter(UserId));\n}  Note that we need to apply the filter we have written to both the Get methods.  When we look at the  Delete  and  Patch  methods, we only have to validate that the UserId owns the\nid we are updating.  For that, I write a custom validation method.  This method is in the table controller:  private void ValidateOwner(string id)\n{\n    var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault TodoItem ();\n    if (result == null)\n    {\n        throw new HttpResponseException(HttpStatusCode.NotFound);\n    }\n}  The validation method must throw an appropriate  HttpResponseException  if the validation fails.  It's common\nto return a 404 Not Found error rather than a 403 Forbidden error for security reasons.  Returning a\n403 Forbidden error confirms that the Id exists, which is a data leakage.  Returning a 404 Not Found\nerror means that a rogue client cannot tell the difference between \"I can't access the record\" and\n\"the record doesn't exist\".  We can use this validation method in each method that requires it:  // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch)\n{\n    ValidateOwner(id);\n    return UpdateAsync(id, patch);\n}\n\n// DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic Task DeleteTodoItem(string id)\n{\n    ValidateOwner(id);\n    return DeleteAsync(id);\n}  Although this is a very basic example of a filtered table, we can see three different techniques:   A  Filter  implemented as a LINQ query in an  IQueryable  extension method, applied to both  Get  methods.  A  Transform  implemented inside the  Post  method.  A  Validation  implemented using the filter as a method in the table controller, applied to  Patch  and  Delete  methods.   I set up a copy of the TaskList from Chapter 2 with Azure Active Directory Client Flow for authentication.\nWe can look at the SQL table contents after we log in with this client and add a new task:   Note that the UserId is the security ID of the authenticated user.  The security ID is a stable ID for\nthe user.  The security ID does not change if the user changes their username or email address.", 
            "title": "Per-User Data"
        }, 
        {
            "location": "/chapter3/projection/#per-group-data", 
            "text": "Let's say we have a mobile client that a sales person uses to enter data about sales.  He might be able\nto pick from a list of industries that the account is in.  The enterprise may further organize those\nindustries as groups, with several people within the organization able to view the accounts associated\nwith a specific industry.  In this case:   We want to limit the mobile client to only view accounts for groups to which he belongs.  We want to allow the mobile client to submit new accounts for any group to which he belongs.  Updates and Deletes should not adjust the group field.   The limit will be implemented as a filter.  The update and insert methods will require a validation method (comparing the submitted group with the list of groups to which the user belongs).  We have another table in  DataObjects\\Example.cs .  Let's extend it to support a GroupId:  using System;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Example : EntityData\n    {\n        public string GroupId { get; set; }\n        public string StringField { get; set; }\n        public int IntField { get; set; }\n        public double DoubleField { get; set; }\n        public DateTimeOffset DateTimeField { get; set; }\n    }\n}  We'll store the group ID in the additional field.  Don't forget to use a code-first migration to update the database.\nEach of the validations and filters requires a list of the groups the user belongs to.  This can be achieved using\na claim lookup:  ///  summary \n/// Get the list of groups from the claims\n///  /summary \n///  returns The list of groups /returns \npublic async Task List string  GetGroups()\n{\n    var creds = await User.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (Request);\n    return creds.UserClaims\n        .Where(claim =  claim.Type.Equals( groups ))\n        .Select(claim =  claim.Value)\n        .ToList();\n}   Tip  If you are using claims as part of your security model, you should add the claims that you are using to the identity token that is used for authentication.  You can do this with custom authentication by calling LoginAsync() twice - once for the standard login method and the second time to adjust the token through the custom auth.   Our filter is defined as a LINQ extension (just like in the per-user filter) in  Extensions\\ExampleExtensions.cs :  using System.Collections.Generic;\nusing System.Linq;\nusing Chapter3.DataObjects;\n\nnamespace Chapter3.Extensions\n{\n    public static class ExampleExtensions\n    {\n        public static IQueryable Example  PerGroupFilter(this IQueryable Example  query, List string  groups)\n        {\n            return query.Where(item =  groups.Contains(item.GroupId));\n        }\n    }\n}  We can use this LINQ extension on the retrieval methods:  // GET tables/Example\npublic async Task IQueryable Example  GetAllExample()\n{\n    var groups = await GetGroups();\n    return Query().PerGroupFilter(groups);\n}\n\n// GET tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic async Task SingleResult Example  GetExample(string id)\n{\n    var groups = await GetGroups();\n    return new SingleResult Example (Lookup(id).Queryable.PerGroupFilter(groups));\n}  We have to convert each of the methods to a async method so that we can check the groups.  Retrieving the group\nlist for a user is an async method and this trickles down to the method being called.  The validation method is also an async method (for the same reason).  The Post and Patch methods look like\nthis:  ///  summary \n/// Validator to determine if the provided group is in the list of groups\n///  /summary \n///  param name= group The group name /param \npublic async Task ValidateGroup(string group)\n{\n    var groups = await GetGroups();\n    if (!groups.Contains(group))\n    {\n        throw new HttpResponseException(HttpStatusCode.BadRequest);\n    }\n}\n\n// PATCH tables/Example/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic async Task Example  PatchExample(string id, Delta Example  patch)\n{\n    await ValidateGroup(patch.GetEntity().GroupId);\n    return await UpdateAsync(id, patch);\n}\n\n// POST tables/Example\npublic async Task IHttpActionResult  PostExample(Example item)\n{\n    await ValidateGroup(item.GroupId);\n    Example current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}  It's appropriate to throw a 400 Bad Request for a validation error in this case as the user is\nauthenticated at this point.  The user is not being exposed by the response.  There is no transform in this recipe as the group ID is being sent on each update request.", 
            "title": "Per-Group Data"
        }, 
        {
            "location": "/chapter3/projection/#friends-data", 
            "text": "One of the common social patterns is a \"friends feed\". We can post to our feed and we can see\nboth our messages and our friends messages. In this recipe, we will have three tables.  The\nfirst is the  Users  table with the following model:  using Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class User : EntityData\n    {\n        public string EmailAddress { get; set; }\n        public string Name { get; set; }\n    }\n}   Tip  Don't forget to add a  DbSet  for each table to the  MobileServiceContext  to add the table.   In our application, we will update the  Users  table via a  custom authentication controller .\nAfter we have logged in via Azure Active Directory, we call the  InvokeApiAsync()  method\nto call the custom authentication controller and get a new token with some extra information\nin it.  We'll cover custom authentication controllers in a later chapter.  using System;\nusing System.Data.Entity.Migrations;\nusing System.IdentityModel.Tokens;\nusing System.Linq;\nusing System.Security.Claims;\nusing System.Security.Principal;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing Chapter3.DataObjects;\nusing Chapter3.Models;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Login;\nusing Newtonsoft.Json;\n\nnamespace Chapter3.Controllers\n{\n    [Authorize]\n    [Route( auth/login/custom )]\n    public class CustomAuthController : ApiController\n    {\n        MobileServiceContext dbContext;\n\n        public CustomAuthController()\n        {\n            dbContext = new MobileServiceContext();\n\n            string website = Environment.GetEnvironmentVariable( WEBSITE_HOSTNAME );\n            Audience = $ https://{website}/ ;\n            Issuer = $ https://{website}/ ;\n            SigningKey = Environment.GetEnvironmentVariable( WEBSITE_AUTH_SIGNING_KEY );\n        }\n\n        public string Audience { get; set; }\n        public string Issuer { get; set; }\n        public string SigningKey { get; set; }\n\n        [HttpPost]\n        public async Task IHttpActionResult  Post()\n        {\n            var creds = await User.GetAppServiceIdentityAsync AzureActiveDirectoryCredentials (Request);\n            var sid = ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n            var email = creds.UserClaims\n                .FirstOrDefault(claim =  claim.Type.EndsWith( emailaddress ))\n                .Value;\n            var name = creds.UserClaims\n                .FirstOrDefault(claim =  claim.Type.EndsWith( name ))\n                .Value;\n\n            // Insert the record information into the database\n            User user = new User()\n            {\n                Id = sid,\n                Name = name,\n                EmailAddress = email\n            };\n            dbContext.Users.AddOrUpdate(user);\n            dbContext.SaveChanges();\n\n            // Mind a new token based on the old one plus the new information\n            var newClaims = new Claim[]\n            {\n                new Claim(JwtRegisteredClaimNames.Sub, sid),\n                new Claim(JwtRegisteredClaimNames.Email, email),\n                new Claim( name , name)\n            };\n            JwtSecurityToken token = AppServiceLoginHandler.CreateToken(\n                newClaims, SigningKey, Audience, Issuer, TimeSpan.FromDays(30));\n\n            // Return the token and user ID to the client\n            return Ok(new LoginResult()\n            {\n                AuthenticationToken = token.RawData,\n                UserId = sid\n            });\n        }\n    }\n\n    public class LoginResult\n    {\n        [JsonProperty(PropertyName =  authenticationToken )]\n        public string AuthenticationToken { get; set; }\n\n        [JsonProperty(PropertyName =  user_id )]\n        public string UserId { get; set; }\n    }\n}  The actual database update is done by Entity Framework.  The ASP.NET service is just\na regular ASP.NET service using Entity Framework, so all the same facilities are\navailable as that configuration.  In this case, we take the provided token (which\nis the same token that Azure Active Directory client-flow returns) and return a\nmodified token that includes a couple of extra fields.  During this process, we\nupdate the database by adding or inserting (also known as upserting) a record with\nthe Id field set to the security ID and the additional fields we need.  I need a table to implement the \"friends\" relationship.  My friends are not stored\nin Azure Active Directory.  If I were using a social provider, I could use the\nfriends feed from that social provider by doing a Graph API lookup.  In this case,\nI'm going to use the following model:  namespace Chapter3.DataObjects\n{\n    public class Friend\n    {\n        public string UserId { get; set; }\n        public string FriendId { get; set; }\n    }\n}  This is not based on  EntityData  because I am not going to expose this table to the mobile client.  It's purely for determining what records I am going to show to the mobile client.  In this example, I will maintain the data in this table manually. A \"real\" application would have some sort of custom workflow to add friends and get the friends to approve the connection.  To get the list of \"friends I can see\", I will request a list of the  FriendId  field where the  UserId  is my UserId.  The final table in the trio is the  Messages  table.  This will be downloaded to the\nmobile client so it has to be based on the  EntityData  base class.  In addition, the\ninserts for this table are going to look a lot like a per-user table.  I need the UserId  field to properly maintain the security model.  using Microsoft.Azure.Mobile.Server;\n\nnamespace Chapter3.DataObjects\n{\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n    }\n}  Let's think about the security model we want to implement in the  Messages  table controller:   A  filter  will allow the viewing of the users own data or any data that has an association in the Friends table.  A  transform  will set the owner of the record to my UserId  We will remove the ability to update or delete records since this is a write-once read-many table.   Let's look at the  PostMessage()  method for the  MessageController  first.  This is\npractically identical to the  PostTodoItem()  method in the per-user recipe:  public string UserId =  ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n// POST tables/Message\npublic async Task IHttpActionResult  PostMessage(Message item)\n{\n    item.UserId = UserId;\n    Message current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}  The filter is a little harder.  We are going to use the  Fluent Syntax  for LINQ to provide the right logic. The Fluent Syntax is also known as \"Query Syntax\" or the \"declarative syntax\", depending on the author.  The extension method looks like this:  using Chapter3.DataObjects;\nusing System.Data.Entity;\nusing System.Linq;\n\nnamespace Chapter3.Extensions\n{\n    public static class MessageExtensions\n    {\n        public static IQueryable Message  OwnedByFriends(this IQueryable Message  query, DbSet Friend  friends, string userId)\n        {\n            var myPosts = from m in query\n                          let fr = (from f in friends where f.FriendId == userId select f.UserId)\n                          where m.UserId == userId || fr.Contains(m.UserId)\n                          select m;\n            return myPosts;\n        }\n    }\n}  The LINQ query selects the messages from the  Messages  table where the author is either the mobile client user or the mobile client\nuser is listed as a friend of the author.  My query methods in the table controller now look similar to the per-user data:  public string UserId =  ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n// GET tables/Message\npublic IQueryable Message  GetAllMessage()\n{\n    return Query().OwnedByFriends(context.Friends, UserId);\n}\n\n// GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959\npublic SingleResult Message  GetMessage(string id)\n{\n    return new SingleResult Message (Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId));\n}", 
            "title": "Friends Data"
        }, 
        {
            "location": "/chapter3/projection/#using-linqpad-to-test-linq-queries", 
            "text": "I tend to struggle with LINQ queries.  Fortunately, there are a plethora of blogs, tutorials and tools out there to assist.  One\nof my favorite tools is  LINQPad .  LINQPad gives you an interactive playground for testing your LINQ queries.  In this case, I\ncreated a new query with the following contents:  public class Message {\n   public string UserId;\n   public string Text;\n}\n\npublic class Friend {\n   public string UserId;\n   public string FollowerId;\n}\n\nvoid Main()\n{\n\n\nList Friend  friends = new List Friend () {\n   new Friend() { UserId =  adrian , FollowerId =  donna  },\n   new Friend() { UserId =  fabio , FollowerId =  donna  },\n   new Friend() { UserId =  fabio , FollowerId =  adrian  }\n};\n\nList Message  messages = new List Message () {\n   new Message() { UserId =  adrian , Text =  message 1  },\n   new Message() { UserId =  donna , Text =  message 2  },\n   new Message() { UserId =  fabio , Text =  message 3  }\n};\nvar user =  fabio ;\n\nvar q = from m in messages\n        let fr = (from f in friends where f.FollowerId == user select f.UserId)\n        where m.UserId == user || fr.Contains(m.UserId)\n        select m;\n\nq.Dump();\n\n}  I have a friends table and a messages table.  User \"donna\" should be able to see three messages, user \"adrian\"\nshould be able to see two messages and user \"fabio\" should be able to see just one message.  By changing the\nvalue of the  user  variable, I can test the query I am writing.  I don't need to set up a database (although\nLINQPad supports that as well).", 
            "title": "Using LINQPad to test LINQ Queries"
        }, 
        {
            "location": "/chapter3/projection/#best-practices", 
            "text": "There are a number of best practices that I think are important in developing table controllers:   Optimize Operations   You should always optimize the CRUD operations that are implemented in a table controller.  This means limiting\n  the code so that only  filters ,  transforms  and  validators  are used.  You can use  hooks  as an\n  asynchronous way to handle custom code if something else needs to happen when a mobile client inserts, updates\n  or deletes a record.  (We will be delving into hooks during the custom code chapter later on).  You should  NOT  insert custom code into a table controller that runs synchronously.   Implement Security Policy with Filters   The mobile backend should be concerned with security.  What can the connecting user see?  Use  filters  to\n  ensure that the connecting user can only see the data that they are allowed to see.  There are several examples\n  of bad filters.  For example, if a user normally wants to see the last 7 days worth of messages, but is allowed\n  to see all messages.  I would implement this particular case as a client-side filter as it has nothing to do\n  with security.   Use LINQ Extension Methods   LINQ extension methods can be used to great effect to make your CRUD methods more readable.  I love readable\n  code.  For example, consider the following two code snippets from the last recipe:  public IQueryable Message  GetAllMessage()\n{\n  // Recipe #1\n  return Query().OwnedByFriends(context.Friends, UserId);\n\n  // Recipe #2\n  return from m in Query()\n    let fr = (from f in context.Friends where f.FriendId == UserId select f.UserId)\n    where m.UserId == UserId || fr.Contains(m.UserId)\n    select m;\n}  The first recipe makes the intent of the filter very clear.  I have to work at understanding the specific implementation\n  of the second method.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter3/client/", 
            "text": "Handling Data in Mobile Clients\n\n\nPretty much any non-trivial mobile client will require access to data.  Although we have already brushed on handling data within the client, this section will go deeper into the data handling aspects on the client.  We will cover how to get and process data, how to deal with performance and reliability and some of the quirks that one must deal with when dealing with offline data.\n\n\nAn Online Client\n\n\nWe've already seen an example of an online client in our online \nTaskList\n project.  There is a method for obtaining a reference to an online table:\n\n\nvar table = client.GetTable\nModel\n();\n\n\n\n\nThis method relies on the fact that the table name is the same as the model.  One must have a consistent naming scheme - the model on the server, table controller on the server, model on the client and table on the client must all be based on the same root name.  This is definitely a best practice.  You can produce an un-typed table:\n\n\nvar table = client.GetTable(\ntodoitem\n);\n\n\n\n\nThis version of the method returns an untyped table.  Whereas a typed table is based on a concrete model, an untyped table is based on a JSON object.  This allows one to access data when the model is unknown or hard to represent in a model.  You should never use an untyped table unless there is no other way of achieving whatever operation you need.\n\n\nAll tables implement the \nIMobileServiceTable\n interface:\n\n\n\n\nReadAsync()\n performs reads against the table.\n\n\nLookupAsync()\n reads a single record in the table, identified by its id.\n\n\nInsertAsync()\n inserts a new record into the table.\n\n\nUpdateAsync()\n updates an existing record in the table.\n\n\nDeleteAsync()\n deletes a record in the table.\n\n\nUndeleteAsync()\n un-deletes a deleted record (if soft-delete is turned on).\n\n\n\n\nWhen developing my interface, I tend to wrap my table interface into another class.  This isn't because I like wrapping classes.  Rather it is because the return values from many of the methods are not compatible with the general patterns used when working with a UI.  For instance, the ReadAsync() method returns an \nIEnumerable\n type.  However, the standard list management in Xamarin and UWP applications use an \nObservableCollection\n instead.  One has to do a conversion from one to the other.\n\n\nLet's look at a standard table wrapper:\n\n\nusing System.Collections.Generic;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable\nT\n : ICloudTable\nT\n where T : TableData\n    {\n        IMobileServiceTable\nT\n table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.table = client.GetTable\nT\n();\n        }\n\n        #region ICloudTable interface\n        public async Task\nT\n CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task\nT\n UpsertItemAsync(T item)\n        {\n            return (item.Id == null) ?\n                await CreateItemAsync(item) :\n                await UpdateItemAsync(item);\n        }\n\n        public async Task DeleteItemAsync(T item)\n            =\n await table.DeleteAsync(item);\n\n        public async Task\nICollection\nT\n ReadAllItemsAsync()\n            =\n await table.ToListAsync();\n\n        public async Task\nT\n ReadItemAsync(string id)\n            =\n await table.LookupAsync(id);\n\n        public async Task\nT\n UpdateItemAsync(T item)\n        {\n            await table.UpdateAsync(item);\n            return item;\n        }\n        #endregion\n    }\n}\n\n\n\n\nThis is the \nAzureCloudTable\n class that our task list has been using thus far.  It's actually got a few bugs in it.  Let's go over them.\n\n\nProbably the most egregious bug is that the \nReadAllItemsAsync()\n method does not handle paging.  If you have more than 50 items, then the \nToListAsync()\n method will do a single GET operation and then return the results.  The Azure Mobile Apps Server SDK implements enforced paging.  This protects two things.  Firstly, the client cannot tie up the UI thread and cause a significant delay in the responsiveness of the app.  More importantly, a rogue client cannot tie up the server for a long period thus helping with dealing with denial of service attacks.  Paging is a good thing.\n\n\nTo test this:\n\n\n\n\nInsert over 50 records into the \nTodoItems\n table in your database using a SQL client.\n\n\nPut a break point at the \nItems.ReplaceRange(list);\n (line 78 approximately) in \nViewModels\\TaskListViewModel.cs\n.\n\n\nRun the UWP project.\n\n\n\n\n\n\nNote that even though there are more than 50 records, you will only see 50 records in the list.  There are multiple ways to fix this and it depends on your final expectation.  In the class of \"probably not what we want\", we can keep on reading records until there are no more records to read. This is the simplest to implement.  In the \nServices\\AzureCloudTable.cs\n file, replace the \nReadAllItemsAsync()\n method with the following:\n\n\npublic async Task\nICollection\nT\n ReadAllItemsAsync()\n{\n    List\nT\n allItems = new List\nT\n();\n\n    var pageSize = 50;\n    var hasMore = true;\n    while (hasMore)\n    {\n        var pageOfItems = await table.Skip(allItems.Count).Take(pageSize).ToListAsync();\n        if (pageOfItems.Count \n 0)\n        {\n            allItems.AddRange(pageOfItems);\n        }\n        else\n        {\n            hasMore = false;\n        }\n    }\n    return allItems;\n}\n\n\n\n\nThis code will always make a minimum of 2 requests if there is any data.  If you have 75 records, three requests will be made - the first will bring down 50 records, the second 25 records and the third no records.  Why not stop at the second request?  We expect this code to run on a live system.  The OData subsystem is allowed to return less than the requested value and it will do so for a variety of reasons.  For example, it may be configured with a maximum transfer size and the records won't fit into the transfer buffer.  The only way of knowing for sure that you have received all the records is to do a request and be told there is no more.\n\n\nThis code could be simplified quite a bit.  The reason I am not doing so is that this is not how you would want to do the transfer of items in a real application.  Doing this will tie up the UI thread of your application for quite a while as the \nAzureCloudTable\n downloads all the data.  Consider if there were thousands of entries?  This method would be problematic very quickly.\n\n\nThe alternative is to incrementally load the data as it is needed.  This means that your UI thread will pause as the data is loaded, but the resulting UI will be less memory hungry and overall more responsive.  We start by adjusting our \nAbstractions\\ICloudTable.cs\n to add a method signature for returning paged data:\n\n\npublic interface ICloudTable\nT\n where T : TableData\n{\n    Task\nT\n CreateItemAsync(T item);\n    Task\nT\n ReadItemAsync(string id);\n    Task\nT\n UpdateItemAsync(T item);\n    Task\nT\n UpsertItemAsync(T item);\n    Task DeleteItemAsync(T item);\n    Task\nICollection\nT\n ReadAllItemsAsync();\n    Task\nICollection\nT\n ReadItemsAsync(int start, int count);\n}\n\n\n\n\nThe \nReadItemsAsync()\n method is our new method here. The concrete implementation uses \n.Skip()\n and \n.Take()\n to return just the data that is required:\n\n\npublic async Task\nICollection\nT\n ReadItemsAsync(int start, int count)\n{\n    return await table.Skip(start).Take(count).ToListAsync();\n}\n\n\n\n\nNow that we have a method for paging through the contents of our table, we need to be able to wire that up to our \nListView\n.  Xamarin Forms has a concept called \nBehaviors\n that lets us add functionality to user interface controls without having to completely re-write them or sub-class them.  We can use a behavior to implement a reusable paging control for a ListView.  Xamarin provides a sample for this called \nEventToCommandBehavior\n (along with an \nexplanation\n). We are going to be using the \nItemAppearing\n event and that event uses the \nItemVisibilityEventArgs\n as a parameter.  We need a converter for the EventToCommandBehavior class (in \nConverters\\ItemVisibilityConverter.cs\n):\n\n\nusing System;\nusing System.Globalization;\nusing Xamarin.Forms;\n\nnamespace TaskList.Converters\n{\n    public class ItemVisibilityConverter : IValueConverter\n    {\n        public object Convert(object value, Type targetType, object parameter, CultureInfo culture)\n        {\n            var eventArgs = value as ItemVisibilityEventArgs;\n            return eventArgs.Item;\n        }\n\n        public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture)\n        {\n            throw new NotImplementedException();\n        }\n    }\n}\n\n\n\n\nThis is wired up with some XAML code in \nPages\\TaskList.xaml.cs\n.  There are two pieces.  Firstly, we must define the ItemVisibilityConverter that we just wrote.  This is done at the top of the file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage x:Class=\nTaskList.Pages.TaskList\n\n             xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             xmlns:behaviors=\nclr-namespace:TaskList.Behaviors;assembly=TaskList\n\n             xmlns:converters=\nclr-namespace:TaskList.Converters;assembly=TaskList\n\n             Title=\n{Binding Title}\n\n\n    \nContentPage.Resources\n\n        \nResourceDictionary\n\n            \nconverters:ItemVisibilityConverter x:Key=\nItemVisibilityConverter\n /\n\n        \n/ResourceDictionary\n\n    \n/ContentPage.Resources\n\n\n\n\n\nNext, we must define the behavior for the ListView:\n\n\nListView CachingStrategy=\nRecycleElement\n\n            IsPullToRefreshEnabled=\nTrue\n\n            IsRefreshing=\n{Binding IsBusy,\n                                    Mode=OneWay}\n\n            ItemsSource=\n{Binding Items}\n\n            RefreshCommand=\n{Binding RefreshCommand}\n\n            RowHeight=\n50\n\n            SelectedItem=\n{Binding SelectedItem,\n                                    Mode=TwoWay}\n\n    \nListView.Behaviors\n\n        \nbehaviors:EventToCommandBehavior Command=\n{Binding LoadMoreCommand}\n\n                                                Converter=\n{StaticResource ItemVisibilityConverter}\n\n                                                EventName=\nItemAppearing\n /\n\n    \n/ListView.Behaviors\n\n\n\n\n\nFinally, we need to add a new command to our \nTaskListViewModel\n to load more items.  This involves firstly defining the new command:\n\n\npublic TaskListViewModel()\n{\n    CloudTable = CloudService.GetTable\nTodoItem\n();\n\n    Title = \nTask List\n;\n\n    RefreshCommand = new Command(async () =\n await Refresh());\n    AddNewItemCommand = new Command(async () =\n await AddNewItem());\n    LogoutCommand = new Command(async () =\n await Logout());\n    LoadMoreCommand = new Command\nTodoItem\n (async (TodoItem item) =\n await LoadMore(item));\n\n    // Subscribe to events from the Task Detail Page\n    MessagingCenter.Subscribe\nTaskDetailViewModel\n(this, \nItemsChanged\n, async (sender) =\n\n    {\n        await Refresh();\n    });\n\n    // Execute the refresh command\n    RefreshCommand.Execute(null);\n}\n\npublic ICommand LoadMoreCommand { get; }\n\n\n\n\nWe also need to define the actual command code:\n\n\nbool hasMoreItems = true;\n\nasync Task LoadMore(TodoItem item)\n{\n    if (IsBusy)\n    {\n        Debug.WriteLine($\nLoadMore: bailing because IsBusy = true\n);\n        return;\n    }\n\n    // If we are not displaying the last one in the list, then return.\n    if (!Items.Last().Id.Equals(item.Id))\n    {\n        Debug.WriteLine($\nLoadMore: bailing because this id is not the last id in the list\n);\n        return;\n    }\n\n    // If we don't have more items, return\n    if (!hasMoreItems)\n    {\n        Debug.WriteLine($\nLoadMore: bailing because we don't have any more items\n);\n        return;\n    }\n\n    IsBusy = true;\n    try\n    {\n        var list = await CloudTable.ReadItemsAsync(Items.Count, 20);\n        if (list.Count \n 0)\n        {\n            Debug.WriteLine($\nLoadMore: got {list.Count} more items\n);\n            Items.AddRange(list);\n        }\n        else\n        {\n            Debug.WriteLine($\nLoadMore: no more items: setting hasMoreItems= false\n);\n            hasMoreItems = false;\n        }\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert(\nLoadMore Failed\n, ex.Message, \nOK\n);\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}\n\n\n\n\nI've added a whole bunch of debug output because this command is called a lot, so I can scroll back through the output window instead of setting a breakpoint and clicking Continue a lot.\n\n\nAs the UI displays each cell, it calls our command.  The command figures out if the record being displayed is the last one in the list.  If it is, it asks for more records.  Once no more records are available, it sets the flag \nhasMoreItems\n to false so it can short-circuit the network request.\n\n\n\n\nTip\n\n\nBe careful when using \nOrderBy()\n with online data.  Earlier pages may change, causing duplicated data or missed data in your result set.  There is little you can do about this other than pulling down all the data ordered by the \nCreatedAt\n field (which is the default).\n\n\n\n\nFinally, our current implementation of the \nRefresh()\n method loads all the items.  We need to adjust it to only load the first page:\n\n\nasync Task Refresh()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var identity = await CloudService.GetIdentityAsync();\n        if (identity != null)\n        {\n            var name = identity.UserClaims.FirstOrDefault(c =\n c.Type.Equals(\nname\n)).Value;\n            Title = $\nTasks for {name}\n;\n        }\n        var list = await CloudTable.ReadItemsAsync(0, 20);\n        Items.ReplaceRange(list);\n        hasMoreItems = true;\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert(\nItems Not Loaded\n, ex.Message, \nOK\n);\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}\n\n\n\n\nWe've done two things here.\n\n\n\n\nWe have altered the first request so that only the first 20 records are retrieved.\n\n\nWe have set \nhasMoreItems\n to true so that the \nLoadMore()\n command will do network requests again.\n\n\n\n\n\n\nInfo\n\n\nThe Azure Mobile Apps SDK also has a collection class that can be used: \nMobileServiceCollection\nT,T\n.  This handles the paging for you and implements the \nICollectionChanged\n event.  For simple cases where you don't want to do on-demand loading, you can use the \nMobileServiceCollection\nT,T\n collection class.\n\n\n\n\nQuery Support in Online Clients\n\n\nWhen using an online client, you can also use an OData query to look for records.  The following code snippet, for example, will only return records that are incomplete:\n\n\nreturn await table\n    .Where(item =\n item.Complete == false)\n    .ToListAsync()\n\n\n\n\nThis is a standard LINQ query.  Just as the LINQ query was used to adjust the SQL that is generated in the server-side code, the LINQ query here is used to adjust the OData query that is generated to call the server.  This particular query will generate the follow HTTP call:\n\n\nGET /tables/todoitem?$filter=(complete+eq+false) HTTP/1.1\n\n\n\n\nLINQ queries are very useful in dealing with online data.  In general they should be formatted in a particular order so that the best possible format across the Internet to your backend.\n\n\ntable                           // start with the table reference\n    .Where(filter)              // filter the results\n    .Select(filter)             // Only return certain fields\n    .Skip(start).Take(count)    // paging support\n    .ToListAsync()              // convert to something we can use\n\n\n\n\nThis format allows you to be very specific about what you want to return from the server, thus allowing you to balance the optimization of bandwidth with the responsiveness of the UI.\n\n\nAn Offline Client\n\n\nAnother method of optimizing bandwidth utilization with an added benefit of providing a resilient data connection is to use an offline sync database.   The Azure Mobile Apps Client SDK has a built-in offline mode that allows for the common requirements of bandwidth optimization, connection resliency, and performance optimization.\n\n\n\n\nAn offline capable table uses \nIncremental Sync\n to only bring down new records from the server.\n\n\nConnection detection allows you to defer requests until after a connection is available.\n\n\nIn-built \nConflict Resolution\n provides a robust mechanism for handling changes while your client was offline.\n\n\nSince the data is always local, the UI will be much more responsive to changes.\n\n\n\n\nAll of this comes at a cost.  We need to maintain an offline sync database for the tables you wish to provide offline, which takes up memory.  It may result in large data transfers (especially in the cases of the first sync operation and rapidly changing tables).  Finally, there is more complexity.  We have to deal with the offline table maintenance and conflict resolution patterns.\n\n\n\n\nAzure Mobile Apps v3.0\n\n\nAzure Mobile Apps introduced a breaking change in how SQLite was used in v3.0 of the Client SDK.  The breaking change ensured support for Android Nougat.  This book covers the v3.1 release of Azure Mobile Apps.  In general, always start a project using the latest generally available release of the SDK from NuGet.\n\n\n\n\nThere are three stages to using an offline client:\n\n\n\n\nInitialize the local SQLite database before use.\n\n\nSet up push / pull logic to maintain the contents of the SQLite database.\n\n\nConfigure appropriate conflict resolution.\n\n\n\n\nThe actual code for your mobile client that deals with tables remains identical to the online case.\n\n\nTo effect the offline sync, the Azure Mobile Apps client SDK keeps a table in the local SQLite database called the \nOperations Queue\n.  An entry is placed in the operations queue whenever a modification is done to a sync table.  These modifications are collapsed.  Later modifications to the same record in a sync table will be collapsed into a single update when transmitted to the mobile backend.\n\n\nConfiguring the Local SQLite database\n\n\nWe must initialize the local database before we can use it.  This happens on start up and the Azure Mobile Apps SDK knows enough of the models to understand the basics of maintaining the database.  Initializing the database is deceptively simple.  Install the \nMicrosoft.Azure.Mobile.Client.SQLiteStore\n package to all the client projects.\n\n\n\n\nTip\n\n\nThe \nMicrosoft.Azure.Mobile.Client\n package v3.0.2 drastically improved the startup experience for offline sync. Make sure you are using it.  If you are using an earlier version, additional steps are required for initializing your offline cache.\n\n\n\n\nIn the \nServices\\AzureCloudService.cs\n file, add the following method:\n\n\n    #region Offline Sync Initialization\n    async Task InitializeAsync()\n    {\n        // Short circuit - local database is already initialized\n        if (Client.SyncContext.IsInitialized)\n            return;\n\n        // Create a reference to the local sqlite store\n        var store = new MobileServiceSQLiteStore(\nofflinecache.db\n);\n\n        // Define the database schema\n        store.DefineTable\nTodoItem\n();\n\n        // Actually create the store and update the schema\n        await Client.SyncContext.InitializeAsync(store);\n    }\n    #endregion\n\n\n\n\n\n\nTip\n\n\nThe \nMicrosoft.Azure.Mobile.Client\n package does not support Android API versions earlier than API 19.  To set this, right-click on the \nTaskList.Droid\n project and ensure the \nMinimum Android version to target\n is set to a minimum of API level 19.\n\n\n\n\nWe need to ensure the initialization is carried out before we use the local database. The best place to do this is\nin the \nGetTable\n method:\n\n\n    /// \nsummary\n\n    /// Returns a link to the specific table.\n    /// \n/summary\n\n    /// \ntypeparam name=\nT\nThe model\n/typeparam\n\n    /// \nreturns\nThe table reference\n/returns\n\n    public async Task\nICloudTable\nT\n GetTableAsync\nT\n() where T : TableData\n    {\n        await InitializeAsync();\n        return new AzureCloudTable\nT\n(Client);\n    }\n\n\n\n\nNote that we made the routine async during this process.  Adjust the \nICloudService\n interface and the calls to \nGetTable\n in the rest of the code to compensate for this.\n\n\nUpdating the Sync tables\n\n\nWe also need to add some routines to our \nICloudTable\n and \nAzureCloudTable\n classes to effect a synchronization. The first method is added to the  \nICloudTable\n interface and pulls data from the mobile backend:\n\n\n    public interface ICloudTable\nT\n where T : TableData\n    {\n        Task\nT\n CreateItemAsync(T item);\n        Task\nT\n ReadItemAsync(string id);\n        Task\nT\n UpdateItemAsync(T item);\n        Task\nT\n UpsertItemAsync(T item);\n        Task DeleteItemAsync(T item);\n        Task\nICollection\nT\n ReadAllItemsAsync();\n        Task\nICollection\nT\n ReadItemsAsync(int start, int count);\n        Task PullAsync();\n    }\n\n\n\n\nThe new method is \nPullAsync()\n which will do the \"pull data from the server\" operation.  The \nAzureCloudTable\n class has the concrete implementation:\n\n\n        IMobileServiceSyncTable\nT\n table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            table = client.GetSyncTable\nT\n();\n        }\n\n        public async Task PullAsync()\n        {\n            string queryName = $\nincsync_{typeof(T).Name}\n;\n            await table.PullAsync(queryName, table.CreateQuery());\n        }\n\n\n\n\nNote that we have changed the call to get the table reference.  It's now a \nSyncTable\n.  You can test to see if the sync table has been defined with the following:\n\n\nif (client.IsSyncTable(typeof(T).Name)) {\n    // There is a sync table defined\n}\n\n\n\n\nThis can be used to have a single \nAzureCloudTable\n implementation for both the online and offline capabilities.  The concrete implementation of the \nPullAsync()\n method implements incremental sync.  If we set the queryName to \nnull\n, then the entire table is pulled across each time.  By setting a queryName, the last updated time for the table is stored and associated with the queryName.  The last updated time is used to request only records that have changed since the last updated time when doing the pull from the backend.\n\n\nThe push of the operations queue up to the mobile backend is handled by a single call in the \nAzureCloudService\n class:\n\n\n    public async Task SyncOfflineCacheAsync()\n    {\n        await InitializeAsync();\n\n        // Push the Operations Queue to the mobile backend\n        await Client.SyncContext.PushAsync();\n\n        // Pull each sync table\n        var taskTable = await GetTableAsync\nTodoItem\n(); await taskTable.PullAsync();\n    }\n\n\n\n\nnote that if the mobile client tries to pull data while there are pending operations in the operations queue, the Azure Mobile Apps client SDK will perform an implicit push.  You can check the state of the operations queue with:\n\n\nif (Client.SyncContext.PendingOperations \n 0) {\n    // There are pending operations\n}\n\n\n\n\nThe only thing left to do now is to decide when to synchronize the local database on the mobile device.  In this example, I am going to synchronize the database during the refresh command of the \nTaskListViewModel\n and on saving or deleting an item in the \nTaskDetailViewModel\n.  Each synchronization will be called with the following:\n\n\nawait CloudService.SyncOfflineCacheAsync();\n\n\n\n\n\n\nInfo\n\n\nThe offline sync cache automatically handles paging of results during the transfer for you so you never have to worry about it.\n\n\n\n\nAdditional Steps on Universal Windows\n\n\nIf you are compiling the Universal Windows (UWP) project, there is an extra step.  You must Add a reference for SQLite to the project:\n\n\n\n\nOpen the \nTaskList.UWP\n project.\n\n\nRight-click on the \nReferences\n node, then select \nAdd Reference\n. \n\n\nSelect \nUniversal Windows\n -\n \nExtensions\n. \n\n\nPlace a check mark next to the \nSQLite for Universal Windows\n and \nVisual C++ 2015 Runtime for Universal Windows\n components.\n\n\n\n\n\n\nIf you don't do this, you will get the rather cryptic error message \"Unable to set temporary directory\" when running the\napplication.\n\n\nDetecting Connection State\n\n\nXamarin has a rather cool plugin called the Connectivity Plugin for testing connectivity state.  We can install it by installing the \nXam.Plugin.Connectivity\n NuGet package.  Once that is installed, we can update our \nSyncOfflineCacheAsync()\n method to use it:\n\n\n    public async Task SyncOfflineCacheAsync()\n    {\n        await InitializeAsync();\n\n        if (!(await CrossConnectivity.Current.IsRemoteReachable(Client.MobileAppUri.Host, 443)))\n        {\n            Debug.WriteLine($\nCannot connect to {Client.MobileAppUri} right now - offline\n);\n            return;\n        }\n\n        // Push the Operations Queue to the mobile backend\n        await Client.SyncContext.PushAsync();\n\n        // Pull each sync table\n        var taskTable = await GetTableAsync\nTodoItem\n(); await taskTable.PullAsync();\n    }\n\n\n\n\nThe Connectivity Plugin (which is accessible through \nCrossConnectivity.Current\n) has other parameters that allow the mobile client to test for specific types of connectivity.  For example, let's say you only wanted to pull one of the tables over wifi.  You could do this as follows:\n\n\nvar connections = CrossConnectivity.Current.ConnectionTypes;\nif (connections.Contains(ConnectionType.Wifi)) {\n    var largeTable = await GetTableAsync\nLargeModel\n();\n    largeTable.PullAsync();\n}\n\n\n\n\nThe Connectivity plugin when paired with the Azure Mobile Apps Client SDK gives you a lot of flexibility in deciding what to sync and over what type of connection you should sync.\n\n\nHandling Conflict Resolution\n\n\nWhen the mobile client submits a modification to the mobile backend, it's generally done as a two-step process:\n\n\n\n\nA \"pre-condition check\" is done, comparing the Version to the ETag of the record (which is derived from the Version of the record).\n\n\nThe actual request is done, where the Version of the mobile client is compared to the Version of the record on the mobile backend.\n\n\n\n\nIf the version of the record being submitted is different from the version on the server, the test will fail.  In the case of the first check, a \n412 Precondition Failed\n will be returned for the modification.  If the second test fails, a \n409 Conflict\n response is returned.  Normally, you will see the \n412 Precondition Failed\n response, but you should be prepared for either response.\n\n\nBoth responses indicate a conflict that needs to be resolved before continuing.  Programatically, when doing a modification in an online table (such as an Insert, Update, Delete), you should be capture the \nMobileServicePreconditionFailedException\n Exception to handle the conflict.  In an offline sync table, you should capture the \nMobileServicePushFailedException\n during the \nPushAsync()\n operation.  This will potentially have an array of conflicts for you to deal with.\n\n\nFor the online case, the code would look like this:\n\n\ntry\n{\n    await CloudService.InsertAsync(item);\n}\ncatch (MobileServicePreconditionFailedException\nTodoItem\n ex)\n{\n    await ResolveConflictAsync(item, ex.Item);\n}\n\n\n\n\nFor the offline case, the code would look like this:\n\n\ntry\n{\n    await CloudService.PushAsync();\n}\ncatch (MobileServicePushFailedException ex)\n{\n    if (ex.PushResult != null)\n    {\n        foreach (var error in ex.PushResult.Errors)\n        {\n            await ResolveConflictAsync(error);\n        }\n    }\n}\n\n\n\n\nIn both cases, the \nResolveConflictAsync()\n method is called for each conflict in turn.  Resolving the conflict involves\ndeciding between the two options or making corrections to the item.\n\n\n\n\nIf you wish to keep the client copy, set the client Version property to the server Version value and re-submit.\n\n\nIf you wish to keep the server copy, discard the update.\n\n\nIf you wish to do something else, create a new copy of the record, set the Version to be the same as the server Version, then re-submit.\n\n\n\n\n\n\nInfo\n\n\nIf the model on your mobile client does not have a Version field, the policy is \"last write wins\".\n\n\n\n\nThe online case is relatively simple as you are going to be ignoring the old request and creating a new request.  Here is some template code that implements both client-wins and server-wins strategies for the offline case:\n\n\nsync Task ResolveConflictAsync(MobileServiceTableOperationError error)\n{\n    var serverItem = error.Result.ToObject\nT\n();\n    var localItem = error.Item.ToObject\nT\n();\n\n    // Note that you need to implement the public override Equals(TodoItem item)\n    // method in the Model for this to work\n    if (serverItem.Equals(localItem))\n    {\n        // Items are the same, so ignore the conflict\n        await error.CancelAndDiscardItemAsync();\n        return;\n    }\n\n    // Client Always Wins\n    localItem.Version = serverItem.Version;\n    await error.UpdateOperationAsync(JObject.FromObject(localItem));\n\n    // Server Always Wins\n    // await error.CancelAndDiscardItemAsync();\n}\n\n\n\n\nYou could also ask the user as an option.  Finally, you could do some processing.  For example, let's say that you wanted to keep the local version of the Text, but keep the server version of Complete:\n\n\nlocalItem.Complete = serverItem.Complete;\nlocalItem.Version = serverItem.Version;\nawait error.UpdateOperationAsync(JObject.FromObject(serverItem));\n\n\n\n\nThere are many ways to resolve a conflict.  You should consider your requirements carefully.  Asking the user should always be the last resort for conflict resolution.  In the majority of applications, most users will click the \"keep my version\" button to resolve conflicts, so the UI for resolving conflicts should do more than just ask the user to decide between a server and a client version.\n\n\nQuery Management\n\n\nRecord selection is based on two factors:\n\n\n\n\nSecurity policy is enforced at the server.\n\n\nUser preference is enabled at the client.\n\n\n\n\nWe've already discussed security policy in depth in \nthe last section\n.  We've also discussed\nuser queries for online usage.  We just use LINQ to affect a change in the query sent to the server.  However,\nwhat about offline cases? There are situations where you want to keep a smaller subset of the data that you are\nallowed to see for offline usage.  A common request, for example, is to have the last X days of records available\noffline.\n\n\nIn our \nPullAsync()\n call for the table, we use \ntable.CreateQuery()\n to create a query that is designed to get\nall the records available to the user from the server.  This is not always appropriate and can be adjusted.  Let's\nadjust it to only obtain the records for our TodoItem table where either of the following conditions apply:\n\n\n\n\nThe record has been updated within the last day.\n\n\nThe task is incomplete.\n\n\n\n\nOnce the task is marked completed, it will remain in the cache for 1 day, then be removed automatically.  You can\nstill obtain the task while online.  The \ntable.CreateQuery()\n method produces an \nIMobileServiceTableQuery\n\nobject.  This can be dealt with via LINQ:\n\n\nvar queryName = $\nincsync:r:{typeof(T).Name}\n;\nvar query = table.CreateQuery()\n    .Where(r =\n !r.Complete || r.UpdatedAt \n DateTimeOffset.Now.AddDays(-1));\nawait table.PullAsync(queryName, query);\n\n\n\n\nNote that I am using a different query name in this case.  The maximum value of UpdatedAt for the records is stored\nand associated with the query name.  If the same query name is used, then only the records updated since the stored\ndate will be retrieved.  If you change the query, then change the query name.\n\n\nAnother common request is to restrict the properties that are in the local cache.  For instance, maybe you have\na particularly large text blob that you want to make available online, but not offline:\n\n\nvar queryName = $\nincsync:s:{typeof(T).Name}\n;\nvar query = table.CreateQuery()\n    .Select(r =\n new { r.Text, r.Complete, r.UpdatedAt, r.Version });\nawait table.PullAsync(queryName, query);\n\n\n\n\nYou can also use the Fluent syntax:\n\n\nvar query =\n    from r in table.CreateQuery()\n    select new { r.Text, r.Complete, r.UpdatedAt, r.Version };\n\n\n\n\nYou should always construct the object including the \nUpdatedAt\n and \nVersion\n properties.  \nUpdatedAt\n is used\nfor incremental sync and \nVersion\n is used for conflict resolution.\n\n\nBoth of these cases use standard LINQ syntax to adjust the query being sent to the mobile backend in exactly the\nsame way that we adjusted the query when we were doing online searches.  An offline \"pull\" is exactly the same\nas an online \"query\".\n\n\n\n\nTip\n\n\nThere are times when you want to download two different queries.  Avoid this if at all possible as it will\ncause additional requests to the backend that are un-necessary.  Construct your query such that all records\nthat you want to download to the offline cache are requested at once.\n\n\n\n\nDealing with Historical Data\n\n\nLet's continue our example with a small extension.  Let's say you want to have the last 7 days worth of records\navailable offline, but you still want the ability to do a historical search.  In this case, you can create two\ntable references to the same table - one online for historical searches and one offline for the normal use case:\n\n\nvar table = client.GetSyncTable\nMessage\n();\nvar historicalTable = client.GetTable\nMessage\n();\n\n\n\n\nIn this case, the \ntable\n reference is used to access the offline data.   However, you could implement a search\ncapability that does a query against the \nhistoricalTable\n instead.  They both point to the same table.  In one\ncase, the server is referenced (and only available online) and in the other, the local cache is referenced\n(and available offline).\n\n\nPurging the Local Cache\n\n\nIt will be inevitable that you will want to clear the local cache at some point.\n\n\n\n\nYou have just changed the model and underlying data and need to re-establish a baseline.\n\n\nYou only cache newer data and want to remove historical data.\n\n\nThings got corrupt for some reason and you need to refresh everything.\n\n\n\n\nWhatever the reason, clearing the cache is one of those infrequent things that is going to be a necessity.  There\nare three forms of this operation:\n\n\nDeleting the backing store\n\n\nThe major requirement during development is that you want to delete the SQLite file that backs the offline cache.  It's likely that the model evolves over time during development.  Add test data that can sometimes cause things to go wrong and you have a recipe for bugs that are only there because you are developing.  If you suspect bad data in the offline cache, the first thing you want to do is start afresh.\n\n\nEach platform stores the offline cache in a different place and the mechanism for removing it is different in each case.  For Universal Windows projects, the offline cache is stored in the Local AppData folder for the application. This is a dedicated area that each Universal Windows app has access to for anything from temporary files to settings and cache files.  To find the location of the file, open the \nTaskList.UWP\n project and open the \nPackage.appxmanifest\n file.  Go to the \nPackaging\n tab.\n\n\n\n\nNote the long \nPackage family name\n field.  Your backing file is in your home directory under \nAppData\\Local\\Packages\\{family_name}\\LocalState\n.  You specified the name of the file when you created the store.\n\n\nYou need to find the \nPackage Name\n for Android.  Right-click on the \nTaskList.Droid\n project and select \nProperties\n, then select the \nAndroid Manifest\n tab.\n\n\n\n\nThe database will be located in \n/data/data/{package_name}/files\n directory on the emulator.  Google has provided utilities for handling developer connections to devices (including emulators).  In this case, we can use the \nadb\n utility.  First, start your emulator of choice through the \nTools\n -\n \nAndroid\n -\n \nAndroid Emulator Manager\n menu option.  Highlight the emulator you have been using, then click \nStart\n.  Ensure the emulator is fully started before continuing.  The \nadb\n utility can be accessed through Visual Studio using \nTools\n -\n \nAndroid\n -\n \nAndroid Adb Command Prompt\n.  You will be able to use \nadb\n commands from there.  Use \nadb devices\n to find the device and \nadb connect\n to connect to the device.\n\n\nThis opens up a Linux-like shell onto the Android device.  You can use normal Linux commands to move around.   You can remove the entire private data area for your package using the following:\n\n\n**root@donatello:/#** cd /data/data/Tasklist.Droid.TaskList.Droid\n**root@donatollo:/#** find . -name tasklist.db -print | xargs rm\n\n\n\n\nThe database will normally be in the \nfiles\n directory.  Use \nexit\n to close the shell prompt on the Android device.  Each disk image file is independent.  You must remove the database file on each emulator individually.\n\n\n\n\nTip\n\n\nYou can use the same \nadb\n commands to connect to a real Android device connected via USB.  Ensure \nUSB Debugging\n is\nenabled on the device.  Use \nadb devices\n to find the device.  For more information, see \nthe Android documentation\n.\n\n\n\n\nThe iOS Simulator does not use an image files.  Instead, it stores files on your Mac disk in \n~/Library/Developer/CoreSimulator/Devices\n.  There is a file called \ndevice_set.plist\n that contains the list of devices that are defined and their location.  It is most easy to find a specific device.  For example, if you are testing on the iPhone 6x simulator:\n\n\n$ grep -B 1 'iPhone-6s\n' device_set.plist\n\nstring\nA3536AA4-0678-43CC-BA21-DD997B89778A\n/string\n\n\nkey\ncom.apple.CoreSimulator.SimDeviceType.iPhone-6s\n/key\n\n--\n\nstring\n83D08BC0-2F9A-4479-ABBD-A69858819E93\n/string\n\n\nkey\ncom.apple.CoreSimulator.SimDeviceType.iPhone-6s\n/key\n\n--\n\nstring\nECAE441D-93F8-4D7A-BF14-7FA2D11BC152\n/string\n\n\nkey\ncom.apple.CoreSimulator.SimDeviceType.iPhone-6s\n/key\n\n\n\n\n\nEach one of these corresponds to a different OS version.  You can find the ordering like this:\n\n\n$ grep SimRuntime.iOS device_set.plist\n\nkey\ncom.apple.CoreSimulator.SimRuntime.iOS-9-1\n/key\n\n\nkey\ncom.apple.CoreSimulator.SimRuntime.iOS-9-2\n/key\n\n\nkey\ncom.apple.CoreSimulator.SimRuntime.iOS-9-3\n/key\n\n\n\n\n\nMy simulator is an iPhone 6s running iOS 9.3, so I can see the GUID is the third one: \nECAE441D-93F8-4D7A-BF14-7FA2D11BC152\n.  This GUID is a directory in the same directory as the \ndevice_set.plist\n file.  You can use normal UNIX style commands to remove the backing store:\n\n\n$ cd ECAE441D-93F8-4D7A-BF14-7FA2D11BC152\n$ find . -name 'tasklist.db' -print | xargs rm\n\n\n\n\nYou can also use the normal Finder utilities to search for and remove the database file for your app.\n\n\nPurging Records from the Offline Cache\n\n\nThe \nIMobileServiceSyncTable\n interface also includes a capability for purging records that are stored in the offline sync by query.  This is done in code like this:\n\n\nvar lowerBound = DateTimeOffset.Now.AddDays(-7);\nvar query = syncTable.CreateQuery().Where(item =\n item.UpdatedAt \n lowerBound);\nvar force = true;\nawait syncTable.PurgeAsync(\nincsync_Tag\n, query, force);\n\n\n\n\nThere are four parameters for the \nPurgeAsync\n call:\n\n\n\n\nA query name.\n\n\nAn OData query.\n\n\nA boolean for forcing the operation.\n\n\nA cancellation token (for the async operation).\n\n\n\n\nEach incremental sync query has a unique name that is specified during the \nPullAsync()\n method call.  If you use the same name during the \nPurgeAsync()\n call, then the date associated with the incremental sync query is reset, causing a full refresh of the data.  This allows you to do a \"purge and refresh\" operation.  If you don't want this to happen, set the query name to null.\n\n\nThe OData query is a similar query format to the incremental sync query that we used with \nPullAsync()\n.  In this case, it selects the records that should be purged from the offline sync cache.  If we wished to purge everything, we could just use \nsyncTable.CreateQuery()\n. If we want to purge only certain records, then we can adjust the query with a \n.Where()\n LINQ query.  In the example above, records that have not been updated within the last 7 days are purged.\n\n\nFinally, the \nPurgeAsync()\n call will fail (and generate an exception) if there are any operations pending in the operations queue.  If we specify \nforce = true\n, then the operations queue check is bypassed and pending operations in the operations queue are flushed without being uploaded.  It is important that this option is used only when absolutely required.  You can leave your database in an inconsistent state if you expect referential integrity between different tables.  Use \nSyncContext.PushAsync()\n to push the operations queue to the remote server before calling \nPurgeAsync()\n.   If you use \nforce = true\n, then also specify a query name to reset the incremental sync state.\n\n\nSchema changes in the Offline Cache\n\n\nDuring development, it's very likely that you will want to change the schema of the data being stored on the client. Unfortunately, there isn't a really good way of dealing with this situation.  Here is my solution:\n\n\n\n\nCreate a constant within your mobile client containing a schema version number.  The actual value (text or int) doesn't matter.  You should never have the same schema version number twice though.  Change this constant whenever you change the schema.\n\n\nRead a file from the mobile device with the schema version in it.  If the file exists and the read value is different from the constant you created in step 1, then you are \"refreshing the cache\"\n\n\nIf you are refreshing the cache, delete the offline cache file, then write the current schema version number to the file that you read in step 2.\n\n\nSet up your offline cache, define your tables, etc.\n\n\nIf you are \"refreshing the cache\", do a full \nPullAsync()\n on all tables to populate the cache.\n\n\n\n\nYou obviously want to avoid this process as much as possible.  One possible solution is to use the \nVersionTrackingPlugin\n by Colby Williams to detect when the schema has been updated.  The following code can be added to automatically delete the offline cache when the version of the app changes:\n\n\n// Add an isRemoteDatabaseSchemaChanged boolean somewhere\nif (CrossVersionTracking.Current.IsFirstLaunchForBuild)\n{\n    isRemoteDatabaseSchemaChanged = true;\n\n    // Drop SQLite Store file to overcome remote-db schema changes\n    File.Delete(Constants.SqliteStorePath);\n}\n\n\n\n\nDebugging the Offline Cache\n\n\nOne of the most difficult parts of the offline cache is that it is opaque - you can't really see what is going on.  Fortunately, the\n\nSQLiteStore\n that is used is relatively straight forward to sub-class so we can add logging to it.  The following helper method can\nbe substituted for a \nSQLiteStore\n in any code.\n\n\npublic class MobileServiceSQLiteStoreWithLogging : MobileServiceSQLiteStore\n{\n    private bool logResults;\n    private bool logParameters;\n\n    public MobileServiceSQLiteStoreWithLogging(string fileName, bool logResults = false, bool logParameters = false)\n        : base(fileName)\n    {\n        this.logResults = logResults;\n        this.logParameters = logParameters;\n    }\n\n    protected override IList\nNewtonsoft.Json.Linq.JObject\n ExecuteQuery(string tableName, string sql, IDictionary\nstring, object\n parameters)\n    {\n        Console.WriteLine (sql);\n\n        if(logParameters)\n            PrintDictionary (parameters);\n\n        var result = base.ExecuteQuery(tableName, sql, parameters);\n\n        if (logResults \n result != null)\n        {\n            foreach (var token in result)\n                Console.WriteLine (token);\n        }\n\n        return result;\n    }\n\n    protected override void ExecuteNonQuery(string sql, IDictionary\nstring, object\n parameters)\n    {\n        Console.WriteLine (sql);\n\n        if(logParameters)\n            PrintDictionary (parameters);\n\n        base.ExecuteNonQuery(sql, parameters);\n    }\n\n    private void PrintDictionary(IDictionary\nstring,object\n dictionary)\n    {\n        if (dictionary == null)\n            return;\n\n        foreach (var pair in dictionary)\n            Console.WriteLine (\n{0}:{1}\n, pair.Key, pair.Value);\n    }\n}\n\npublic class LoggingHandler : DelegatingHandler\n{\n    private bool logRequestResponseBody;\n\n    public LoggingHandler(bool logRequestResponseBody = false)\n    {\n        this.logRequestResponseBody = logRequestResponseBody;\n    }\n\n    protected override async Task\nHttpResponseMessage\n SendAsync(HttpRequestMessage request, System.Threading.CancellationToken cancellationToken)\n    {\n        Console.WriteLine(\nRequest: {0} {1}\n, request.Method, request.RequestUri.ToString());\n\n        if (logRequestResponseBody \n request.Content != null)\n        {\n            var requestContent = await request.Content.ReadAsStringAsync ();\n            Console.WriteLine (requestContent);\n        }\n\n        var response = await base.SendAsync(request, cancellationToken);\n\n        Console.WriteLine (\nResponse: {0}\n, response.StatusCode);\n\n        if (logRequestResponseBody)\n        {\n            var responseContent = await response.Content.ReadAsStringAsync ();\n            Console.WriteLine (responseContent);\n        }\n\n        return response;\n    }\n}\n\n\n\n\nUsing this class will print all the SQL commands that are executed against the SQLite store.  Ensure you are capturing the console somewhere so that you can see the debug messages as you are running your application.", 
            "title": "The Mobile Client"
        }, 
        {
            "location": "/chapter3/client/#handling-data-in-mobile-clients", 
            "text": "Pretty much any non-trivial mobile client will require access to data.  Although we have already brushed on handling data within the client, this section will go deeper into the data handling aspects on the client.  We will cover how to get and process data, how to deal with performance and reliability and some of the quirks that one must deal with when dealing with offline data.", 
            "title": "Handling Data in Mobile Clients"
        }, 
        {
            "location": "/chapter3/client/#an-online-client", 
            "text": "We've already seen an example of an online client in our online  TaskList  project.  There is a method for obtaining a reference to an online table:  var table = client.GetTable Model ();  This method relies on the fact that the table name is the same as the model.  One must have a consistent naming scheme - the model on the server, table controller on the server, model on the client and table on the client must all be based on the same root name.  This is definitely a best practice.  You can produce an un-typed table:  var table = client.GetTable( todoitem );  This version of the method returns an untyped table.  Whereas a typed table is based on a concrete model, an untyped table is based on a JSON object.  This allows one to access data when the model is unknown or hard to represent in a model.  You should never use an untyped table unless there is no other way of achieving whatever operation you need.  All tables implement the  IMobileServiceTable  interface:   ReadAsync()  performs reads against the table.  LookupAsync()  reads a single record in the table, identified by its id.  InsertAsync()  inserts a new record into the table.  UpdateAsync()  updates an existing record in the table.  DeleteAsync()  deletes a record in the table.  UndeleteAsync()  un-deletes a deleted record (if soft-delete is turned on).   When developing my interface, I tend to wrap my table interface into another class.  This isn't because I like wrapping classes.  Rather it is because the return values from many of the methods are not compatible with the general patterns used when working with a UI.  For instance, the ReadAsync() method returns an  IEnumerable  type.  However, the standard list management in Xamarin and UWP applications use an  ObservableCollection  instead.  One has to do a conversion from one to the other.  Let's look at a standard table wrapper:  using System.Collections.Generic;\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class AzureCloudTable T  : ICloudTable T  where T : TableData\n    {\n        IMobileServiceTable T  table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            this.table = client.GetTable T ();\n        }\n\n        #region ICloudTable interface\n        public async Task T  CreateItemAsync(T item)\n        {\n            await table.InsertAsync(item);\n            return item;\n        }\n\n        public async Task T  UpsertItemAsync(T item)\n        {\n            return (item.Id == null) ?\n                await CreateItemAsync(item) :\n                await UpdateItemAsync(item);\n        }\n\n        public async Task DeleteItemAsync(T item)\n            =  await table.DeleteAsync(item);\n\n        public async Task ICollection T  ReadAllItemsAsync()\n            =  await table.ToListAsync();\n\n        public async Task T  ReadItemAsync(string id)\n            =  await table.LookupAsync(id);\n\n        public async Task T  UpdateItemAsync(T item)\n        {\n            await table.UpdateAsync(item);\n            return item;\n        }\n        #endregion\n    }\n}  This is the  AzureCloudTable  class that our task list has been using thus far.  It's actually got a few bugs in it.  Let's go over them.  Probably the most egregious bug is that the  ReadAllItemsAsync()  method does not handle paging.  If you have more than 50 items, then the  ToListAsync()  method will do a single GET operation and then return the results.  The Azure Mobile Apps Server SDK implements enforced paging.  This protects two things.  Firstly, the client cannot tie up the UI thread and cause a significant delay in the responsiveness of the app.  More importantly, a rogue client cannot tie up the server for a long period thus helping with dealing with denial of service attacks.  Paging is a good thing.  To test this:   Insert over 50 records into the  TodoItems  table in your database using a SQL client.  Put a break point at the  Items.ReplaceRange(list);  (line 78 approximately) in  ViewModels\\TaskListViewModel.cs .  Run the UWP project.    Note that even though there are more than 50 records, you will only see 50 records in the list.  There are multiple ways to fix this and it depends on your final expectation.  In the class of \"probably not what we want\", we can keep on reading records until there are no more records to read. This is the simplest to implement.  In the  Services\\AzureCloudTable.cs  file, replace the  ReadAllItemsAsync()  method with the following:  public async Task ICollection T  ReadAllItemsAsync()\n{\n    List T  allItems = new List T ();\n\n    var pageSize = 50;\n    var hasMore = true;\n    while (hasMore)\n    {\n        var pageOfItems = await table.Skip(allItems.Count).Take(pageSize).ToListAsync();\n        if (pageOfItems.Count   0)\n        {\n            allItems.AddRange(pageOfItems);\n        }\n        else\n        {\n            hasMore = false;\n        }\n    }\n    return allItems;\n}  This code will always make a minimum of 2 requests if there is any data.  If you have 75 records, three requests will be made - the first will bring down 50 records, the second 25 records and the third no records.  Why not stop at the second request?  We expect this code to run on a live system.  The OData subsystem is allowed to return less than the requested value and it will do so for a variety of reasons.  For example, it may be configured with a maximum transfer size and the records won't fit into the transfer buffer.  The only way of knowing for sure that you have received all the records is to do a request and be told there is no more.  This code could be simplified quite a bit.  The reason I am not doing so is that this is not how you would want to do the transfer of items in a real application.  Doing this will tie up the UI thread of your application for quite a while as the  AzureCloudTable  downloads all the data.  Consider if there were thousands of entries?  This method would be problematic very quickly.  The alternative is to incrementally load the data as it is needed.  This means that your UI thread will pause as the data is loaded, but the resulting UI will be less memory hungry and overall more responsive.  We start by adjusting our  Abstractions\\ICloudTable.cs  to add a method signature for returning paged data:  public interface ICloudTable T  where T : TableData\n{\n    Task T  CreateItemAsync(T item);\n    Task T  ReadItemAsync(string id);\n    Task T  UpdateItemAsync(T item);\n    Task T  UpsertItemAsync(T item);\n    Task DeleteItemAsync(T item);\n    Task ICollection T  ReadAllItemsAsync();\n    Task ICollection T  ReadItemsAsync(int start, int count);\n}  The  ReadItemsAsync()  method is our new method here. The concrete implementation uses  .Skip()  and  .Take()  to return just the data that is required:  public async Task ICollection T  ReadItemsAsync(int start, int count)\n{\n    return await table.Skip(start).Take(count).ToListAsync();\n}  Now that we have a method for paging through the contents of our table, we need to be able to wire that up to our  ListView .  Xamarin Forms has a concept called  Behaviors  that lets us add functionality to user interface controls without having to completely re-write them or sub-class them.  We can use a behavior to implement a reusable paging control for a ListView.  Xamarin provides a sample for this called  EventToCommandBehavior  (along with an  explanation ). We are going to be using the  ItemAppearing  event and that event uses the  ItemVisibilityEventArgs  as a parameter.  We need a converter for the EventToCommandBehavior class (in  Converters\\ItemVisibilityConverter.cs ):  using System;\nusing System.Globalization;\nusing Xamarin.Forms;\n\nnamespace TaskList.Converters\n{\n    public class ItemVisibilityConverter : IValueConverter\n    {\n        public object Convert(object value, Type targetType, object parameter, CultureInfo culture)\n        {\n            var eventArgs = value as ItemVisibilityEventArgs;\n            return eventArgs.Item;\n        }\n\n        public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture)\n        {\n            throw new NotImplementedException();\n        }\n    }\n}  This is wired up with some XAML code in  Pages\\TaskList.xaml.cs .  There are two pieces.  Firstly, we must define the ItemVisibilityConverter that we just wrote.  This is done at the top of the file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage x:Class= TaskList.Pages.TaskList \n             xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             xmlns:behaviors= clr-namespace:TaskList.Behaviors;assembly=TaskList \n             xmlns:converters= clr-namespace:TaskList.Converters;assembly=TaskList \n             Title= {Binding Title} \n\n     ContentPage.Resources \n         ResourceDictionary \n             converters:ItemVisibilityConverter x:Key= ItemVisibilityConverter  / \n         /ResourceDictionary \n     /ContentPage.Resources   Next, we must define the behavior for the ListView:  ListView CachingStrategy= RecycleElement \n            IsPullToRefreshEnabled= True \n            IsRefreshing= {Binding IsBusy,\n                                    Mode=OneWay} \n            ItemsSource= {Binding Items} \n            RefreshCommand= {Binding RefreshCommand} \n            RowHeight= 50 \n            SelectedItem= {Binding SelectedItem,\n                                    Mode=TwoWay} \n     ListView.Behaviors \n         behaviors:EventToCommandBehavior Command= {Binding LoadMoreCommand} \n                                                Converter= {StaticResource ItemVisibilityConverter} \n                                                EventName= ItemAppearing  / \n     /ListView.Behaviors   Finally, we need to add a new command to our  TaskListViewModel  to load more items.  This involves firstly defining the new command:  public TaskListViewModel()\n{\n    CloudTable = CloudService.GetTable TodoItem ();\n\n    Title =  Task List ;\n\n    RefreshCommand = new Command(async () =  await Refresh());\n    AddNewItemCommand = new Command(async () =  await AddNewItem());\n    LogoutCommand = new Command(async () =  await Logout());\n    LoadMoreCommand = new Command TodoItem  (async (TodoItem item) =  await LoadMore(item));\n\n    // Subscribe to events from the Task Detail Page\n    MessagingCenter.Subscribe TaskDetailViewModel (this,  ItemsChanged , async (sender) = \n    {\n        await Refresh();\n    });\n\n    // Execute the refresh command\n    RefreshCommand.Execute(null);\n}\n\npublic ICommand LoadMoreCommand { get; }  We also need to define the actual command code:  bool hasMoreItems = true;\n\nasync Task LoadMore(TodoItem item)\n{\n    if (IsBusy)\n    {\n        Debug.WriteLine($ LoadMore: bailing because IsBusy = true );\n        return;\n    }\n\n    // If we are not displaying the last one in the list, then return.\n    if (!Items.Last().Id.Equals(item.Id))\n    {\n        Debug.WriteLine($ LoadMore: bailing because this id is not the last id in the list );\n        return;\n    }\n\n    // If we don't have more items, return\n    if (!hasMoreItems)\n    {\n        Debug.WriteLine($ LoadMore: bailing because we don't have any more items );\n        return;\n    }\n\n    IsBusy = true;\n    try\n    {\n        var list = await CloudTable.ReadItemsAsync(Items.Count, 20);\n        if (list.Count   0)\n        {\n            Debug.WriteLine($ LoadMore: got {list.Count} more items );\n            Items.AddRange(list);\n        }\n        else\n        {\n            Debug.WriteLine($ LoadMore: no more items: setting hasMoreItems= false );\n            hasMoreItems = false;\n        }\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert( LoadMore Failed , ex.Message,  OK );\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}  I've added a whole bunch of debug output because this command is called a lot, so I can scroll back through the output window instead of setting a breakpoint and clicking Continue a lot.  As the UI displays each cell, it calls our command.  The command figures out if the record being displayed is the last one in the list.  If it is, it asks for more records.  Once no more records are available, it sets the flag  hasMoreItems  to false so it can short-circuit the network request.   Tip  Be careful when using  OrderBy()  with online data.  Earlier pages may change, causing duplicated data or missed data in your result set.  There is little you can do about this other than pulling down all the data ordered by the  CreatedAt  field (which is the default).   Finally, our current implementation of the  Refresh()  method loads all the items.  We need to adjust it to only load the first page:  async Task Refresh()\n{\n    if (IsBusy)\n        return;\n    IsBusy = true;\n\n    try\n    {\n        var identity = await CloudService.GetIdentityAsync();\n        if (identity != null)\n        {\n            var name = identity.UserClaims.FirstOrDefault(c =  c.Type.Equals( name )).Value;\n            Title = $ Tasks for {name} ;\n        }\n        var list = await CloudTable.ReadItemsAsync(0, 20);\n        Items.ReplaceRange(list);\n        hasMoreItems = true;\n    }\n    catch (Exception ex)\n    {\n        await Application.Current.MainPage.DisplayAlert( Items Not Loaded , ex.Message,  OK );\n    }\n    finally\n    {\n        IsBusy = false;\n    }\n}  We've done two things here.   We have altered the first request so that only the first 20 records are retrieved.  We have set  hasMoreItems  to true so that the  LoadMore()  command will do network requests again.    Info  The Azure Mobile Apps SDK also has a collection class that can be used:  MobileServiceCollection T,T .  This handles the paging for you and implements the  ICollectionChanged  event.  For simple cases where you don't want to do on-demand loading, you can use the  MobileServiceCollection T,T  collection class.", 
            "title": "An Online Client"
        }, 
        {
            "location": "/chapter3/client/#query-support-in-online-clients", 
            "text": "When using an online client, you can also use an OData query to look for records.  The following code snippet, for example, will only return records that are incomplete:  return await table\n    .Where(item =  item.Complete == false)\n    .ToListAsync()  This is a standard LINQ query.  Just as the LINQ query was used to adjust the SQL that is generated in the server-side code, the LINQ query here is used to adjust the OData query that is generated to call the server.  This particular query will generate the follow HTTP call:  GET /tables/todoitem?$filter=(complete+eq+false) HTTP/1.1  LINQ queries are very useful in dealing with online data.  In general they should be formatted in a particular order so that the best possible format across the Internet to your backend.  table                           // start with the table reference\n    .Where(filter)              // filter the results\n    .Select(filter)             // Only return certain fields\n    .Skip(start).Take(count)    // paging support\n    .ToListAsync()              // convert to something we can use  This format allows you to be very specific about what you want to return from the server, thus allowing you to balance the optimization of bandwidth with the responsiveness of the UI.", 
            "title": "Query Support in Online Clients"
        }, 
        {
            "location": "/chapter3/client/#an-offline-client", 
            "text": "Another method of optimizing bandwidth utilization with an added benefit of providing a resilient data connection is to use an offline sync database.   The Azure Mobile Apps Client SDK has a built-in offline mode that allows for the common requirements of bandwidth optimization, connection resliency, and performance optimization.   An offline capable table uses  Incremental Sync  to only bring down new records from the server.  Connection detection allows you to defer requests until after a connection is available.  In-built  Conflict Resolution  provides a robust mechanism for handling changes while your client was offline.  Since the data is always local, the UI will be much more responsive to changes.   All of this comes at a cost.  We need to maintain an offline sync database for the tables you wish to provide offline, which takes up memory.  It may result in large data transfers (especially in the cases of the first sync operation and rapidly changing tables).  Finally, there is more complexity.  We have to deal with the offline table maintenance and conflict resolution patterns.   Azure Mobile Apps v3.0  Azure Mobile Apps introduced a breaking change in how SQLite was used in v3.0 of the Client SDK.  The breaking change ensured support for Android Nougat.  This book covers the v3.1 release of Azure Mobile Apps.  In general, always start a project using the latest generally available release of the SDK from NuGet.   There are three stages to using an offline client:   Initialize the local SQLite database before use.  Set up push / pull logic to maintain the contents of the SQLite database.  Configure appropriate conflict resolution.   The actual code for your mobile client that deals with tables remains identical to the online case.  To effect the offline sync, the Azure Mobile Apps client SDK keeps a table in the local SQLite database called the  Operations Queue .  An entry is placed in the operations queue whenever a modification is done to a sync table.  These modifications are collapsed.  Later modifications to the same record in a sync table will be collapsed into a single update when transmitted to the mobile backend.", 
            "title": "An Offline Client"
        }, 
        {
            "location": "/chapter3/client/#configuring-the-local-sqlite-database", 
            "text": "We must initialize the local database before we can use it.  This happens on start up and the Azure Mobile Apps SDK knows enough of the models to understand the basics of maintaining the database.  Initializing the database is deceptively simple.  Install the  Microsoft.Azure.Mobile.Client.SQLiteStore  package to all the client projects.   Tip  The  Microsoft.Azure.Mobile.Client  package v3.0.2 drastically improved the startup experience for offline sync. Make sure you are using it.  If you are using an earlier version, additional steps are required for initializing your offline cache.   In the  Services\\AzureCloudService.cs  file, add the following method:      #region Offline Sync Initialization\n    async Task InitializeAsync()\n    {\n        // Short circuit - local database is already initialized\n        if (Client.SyncContext.IsInitialized)\n            return;\n\n        // Create a reference to the local sqlite store\n        var store = new MobileServiceSQLiteStore( offlinecache.db );\n\n        // Define the database schema\n        store.DefineTable TodoItem ();\n\n        // Actually create the store and update the schema\n        await Client.SyncContext.InitializeAsync(store);\n    }\n    #endregion   Tip  The  Microsoft.Azure.Mobile.Client  package does not support Android API versions earlier than API 19.  To set this, right-click on the  TaskList.Droid  project and ensure the  Minimum Android version to target  is set to a minimum of API level 19.   We need to ensure the initialization is carried out before we use the local database. The best place to do this is\nin the  GetTable  method:      ///  summary \n    /// Returns a link to the specific table.\n    ///  /summary \n    ///  typeparam name= T The model /typeparam \n    ///  returns The table reference /returns \n    public async Task ICloudTable T  GetTableAsync T () where T : TableData\n    {\n        await InitializeAsync();\n        return new AzureCloudTable T (Client);\n    }  Note that we made the routine async during this process.  Adjust the  ICloudService  interface and the calls to  GetTable  in the rest of the code to compensate for this.", 
            "title": "Configuring the Local SQLite database"
        }, 
        {
            "location": "/chapter3/client/#updating-the-sync-tables", 
            "text": "We also need to add some routines to our  ICloudTable  and  AzureCloudTable  classes to effect a synchronization. The first method is added to the   ICloudTable  interface and pulls data from the mobile backend:      public interface ICloudTable T  where T : TableData\n    {\n        Task T  CreateItemAsync(T item);\n        Task T  ReadItemAsync(string id);\n        Task T  UpdateItemAsync(T item);\n        Task T  UpsertItemAsync(T item);\n        Task DeleteItemAsync(T item);\n        Task ICollection T  ReadAllItemsAsync();\n        Task ICollection T  ReadItemsAsync(int start, int count);\n        Task PullAsync();\n    }  The new method is  PullAsync()  which will do the \"pull data from the server\" operation.  The  AzureCloudTable  class has the concrete implementation:          IMobileServiceSyncTable T  table;\n\n        public AzureCloudTable(MobileServiceClient client)\n        {\n            table = client.GetSyncTable T ();\n        }\n\n        public async Task PullAsync()\n        {\n            string queryName = $ incsync_{typeof(T).Name} ;\n            await table.PullAsync(queryName, table.CreateQuery());\n        }  Note that we have changed the call to get the table reference.  It's now a  SyncTable .  You can test to see if the sync table has been defined with the following:  if (client.IsSyncTable(typeof(T).Name)) {\n    // There is a sync table defined\n}  This can be used to have a single  AzureCloudTable  implementation for both the online and offline capabilities.  The concrete implementation of the  PullAsync()  method implements incremental sync.  If we set the queryName to  null , then the entire table is pulled across each time.  By setting a queryName, the last updated time for the table is stored and associated with the queryName.  The last updated time is used to request only records that have changed since the last updated time when doing the pull from the backend.  The push of the operations queue up to the mobile backend is handled by a single call in the  AzureCloudService  class:      public async Task SyncOfflineCacheAsync()\n    {\n        await InitializeAsync();\n\n        // Push the Operations Queue to the mobile backend\n        await Client.SyncContext.PushAsync();\n\n        // Pull each sync table\n        var taskTable = await GetTableAsync TodoItem (); await taskTable.PullAsync();\n    }  note that if the mobile client tries to pull data while there are pending operations in the operations queue, the Azure Mobile Apps client SDK will perform an implicit push.  You can check the state of the operations queue with:  if (Client.SyncContext.PendingOperations   0) {\n    // There are pending operations\n}  The only thing left to do now is to decide when to synchronize the local database on the mobile device.  In this example, I am going to synchronize the database during the refresh command of the  TaskListViewModel  and on saving or deleting an item in the  TaskDetailViewModel .  Each synchronization will be called with the following:  await CloudService.SyncOfflineCacheAsync();   Info  The offline sync cache automatically handles paging of results during the transfer for you so you never have to worry about it.", 
            "title": "Updating the Sync tables"
        }, 
        {
            "location": "/chapter3/client/#additional-steps-on-universal-windows", 
            "text": "If you are compiling the Universal Windows (UWP) project, there is an extra step.  You must Add a reference for SQLite to the project:   Open the  TaskList.UWP  project.  Right-click on the  References  node, then select  Add Reference .   Select  Universal Windows  -   Extensions .   Place a check mark next to the  SQLite for Universal Windows  and  Visual C++ 2015 Runtime for Universal Windows  components.    If you don't do this, you will get the rather cryptic error message \"Unable to set temporary directory\" when running the\napplication.", 
            "title": "Additional Steps on Universal Windows"
        }, 
        {
            "location": "/chapter3/client/#detecting-connection-state", 
            "text": "Xamarin has a rather cool plugin called the Connectivity Plugin for testing connectivity state.  We can install it by installing the  Xam.Plugin.Connectivity  NuGet package.  Once that is installed, we can update our  SyncOfflineCacheAsync()  method to use it:      public async Task SyncOfflineCacheAsync()\n    {\n        await InitializeAsync();\n\n        if (!(await CrossConnectivity.Current.IsRemoteReachable(Client.MobileAppUri.Host, 443)))\n        {\n            Debug.WriteLine($ Cannot connect to {Client.MobileAppUri} right now - offline );\n            return;\n        }\n\n        // Push the Operations Queue to the mobile backend\n        await Client.SyncContext.PushAsync();\n\n        // Pull each sync table\n        var taskTable = await GetTableAsync TodoItem (); await taskTable.PullAsync();\n    }  The Connectivity Plugin (which is accessible through  CrossConnectivity.Current ) has other parameters that allow the mobile client to test for specific types of connectivity.  For example, let's say you only wanted to pull one of the tables over wifi.  You could do this as follows:  var connections = CrossConnectivity.Current.ConnectionTypes;\nif (connections.Contains(ConnectionType.Wifi)) {\n    var largeTable = await GetTableAsync LargeModel ();\n    largeTable.PullAsync();\n}  The Connectivity plugin when paired with the Azure Mobile Apps Client SDK gives you a lot of flexibility in deciding what to sync and over what type of connection you should sync.", 
            "title": "Detecting Connection State"
        }, 
        {
            "location": "/chapter3/client/#handling-conflict-resolution", 
            "text": "When the mobile client submits a modification to the mobile backend, it's generally done as a two-step process:   A \"pre-condition check\" is done, comparing the Version to the ETag of the record (which is derived from the Version of the record).  The actual request is done, where the Version of the mobile client is compared to the Version of the record on the mobile backend.   If the version of the record being submitted is different from the version on the server, the test will fail.  In the case of the first check, a  412 Precondition Failed  will be returned for the modification.  If the second test fails, a  409 Conflict  response is returned.  Normally, you will see the  412 Precondition Failed  response, but you should be prepared for either response.  Both responses indicate a conflict that needs to be resolved before continuing.  Programatically, when doing a modification in an online table (such as an Insert, Update, Delete), you should be capture the  MobileServicePreconditionFailedException  Exception to handle the conflict.  In an offline sync table, you should capture the  MobileServicePushFailedException  during the  PushAsync()  operation.  This will potentially have an array of conflicts for you to deal with.  For the online case, the code would look like this:  try\n{\n    await CloudService.InsertAsync(item);\n}\ncatch (MobileServicePreconditionFailedException TodoItem  ex)\n{\n    await ResolveConflictAsync(item, ex.Item);\n}  For the offline case, the code would look like this:  try\n{\n    await CloudService.PushAsync();\n}\ncatch (MobileServicePushFailedException ex)\n{\n    if (ex.PushResult != null)\n    {\n        foreach (var error in ex.PushResult.Errors)\n        {\n            await ResolveConflictAsync(error);\n        }\n    }\n}  In both cases, the  ResolveConflictAsync()  method is called for each conflict in turn.  Resolving the conflict involves\ndeciding between the two options or making corrections to the item.   If you wish to keep the client copy, set the client Version property to the server Version value and re-submit.  If you wish to keep the server copy, discard the update.  If you wish to do something else, create a new copy of the record, set the Version to be the same as the server Version, then re-submit.    Info  If the model on your mobile client does not have a Version field, the policy is \"last write wins\".   The online case is relatively simple as you are going to be ignoring the old request and creating a new request.  Here is some template code that implements both client-wins and server-wins strategies for the offline case:  sync Task ResolveConflictAsync(MobileServiceTableOperationError error)\n{\n    var serverItem = error.Result.ToObject T ();\n    var localItem = error.Item.ToObject T ();\n\n    // Note that you need to implement the public override Equals(TodoItem item)\n    // method in the Model for this to work\n    if (serverItem.Equals(localItem))\n    {\n        // Items are the same, so ignore the conflict\n        await error.CancelAndDiscardItemAsync();\n        return;\n    }\n\n    // Client Always Wins\n    localItem.Version = serverItem.Version;\n    await error.UpdateOperationAsync(JObject.FromObject(localItem));\n\n    // Server Always Wins\n    // await error.CancelAndDiscardItemAsync();\n}  You could also ask the user as an option.  Finally, you could do some processing.  For example, let's say that you wanted to keep the local version of the Text, but keep the server version of Complete:  localItem.Complete = serverItem.Complete;\nlocalItem.Version = serverItem.Version;\nawait error.UpdateOperationAsync(JObject.FromObject(serverItem));  There are many ways to resolve a conflict.  You should consider your requirements carefully.  Asking the user should always be the last resort for conflict resolution.  In the majority of applications, most users will click the \"keep my version\" button to resolve conflicts, so the UI for resolving conflicts should do more than just ask the user to decide between a server and a client version.", 
            "title": "Handling Conflict Resolution"
        }, 
        {
            "location": "/chapter3/client/#query-management", 
            "text": "Record selection is based on two factors:   Security policy is enforced at the server.  User preference is enabled at the client.   We've already discussed security policy in depth in  the last section .  We've also discussed\nuser queries for online usage.  We just use LINQ to affect a change in the query sent to the server.  However,\nwhat about offline cases? There are situations where you want to keep a smaller subset of the data that you are\nallowed to see for offline usage.  A common request, for example, is to have the last X days of records available\noffline.  In our  PullAsync()  call for the table, we use  table.CreateQuery()  to create a query that is designed to get\nall the records available to the user from the server.  This is not always appropriate and can be adjusted.  Let's\nadjust it to only obtain the records for our TodoItem table where either of the following conditions apply:   The record has been updated within the last day.  The task is incomplete.   Once the task is marked completed, it will remain in the cache for 1 day, then be removed automatically.  You can\nstill obtain the task while online.  The  table.CreateQuery()  method produces an  IMobileServiceTableQuery \nobject.  This can be dealt with via LINQ:  var queryName = $ incsync:r:{typeof(T).Name} ;\nvar query = table.CreateQuery()\n    .Where(r =  !r.Complete || r.UpdatedAt   DateTimeOffset.Now.AddDays(-1));\nawait table.PullAsync(queryName, query);  Note that I am using a different query name in this case.  The maximum value of UpdatedAt for the records is stored\nand associated with the query name.  If the same query name is used, then only the records updated since the stored\ndate will be retrieved.  If you change the query, then change the query name.  Another common request is to restrict the properties that are in the local cache.  For instance, maybe you have\na particularly large text blob that you want to make available online, but not offline:  var queryName = $ incsync:s:{typeof(T).Name} ;\nvar query = table.CreateQuery()\n    .Select(r =  new { r.Text, r.Complete, r.UpdatedAt, r.Version });\nawait table.PullAsync(queryName, query);  You can also use the Fluent syntax:  var query =\n    from r in table.CreateQuery()\n    select new { r.Text, r.Complete, r.UpdatedAt, r.Version };  You should always construct the object including the  UpdatedAt  and  Version  properties.   UpdatedAt  is used\nfor incremental sync and  Version  is used for conflict resolution.  Both of these cases use standard LINQ syntax to adjust the query being sent to the mobile backend in exactly the\nsame way that we adjusted the query when we were doing online searches.  An offline \"pull\" is exactly the same\nas an online \"query\".   Tip  There are times when you want to download two different queries.  Avoid this if at all possible as it will\ncause additional requests to the backend that are un-necessary.  Construct your query such that all records\nthat you want to download to the offline cache are requested at once.", 
            "title": "Query Management"
        }, 
        {
            "location": "/chapter3/client/#dealing-with-historical-data", 
            "text": "Let's continue our example with a small extension.  Let's say you want to have the last 7 days worth of records\navailable offline, but you still want the ability to do a historical search.  In this case, you can create two\ntable references to the same table - one online for historical searches and one offline for the normal use case:  var table = client.GetSyncTable Message ();\nvar historicalTable = client.GetTable Message ();  In this case, the  table  reference is used to access the offline data.   However, you could implement a search\ncapability that does a query against the  historicalTable  instead.  They both point to the same table.  In one\ncase, the server is referenced (and only available online) and in the other, the local cache is referenced\n(and available offline).", 
            "title": "Dealing with Historical Data"
        }, 
        {
            "location": "/chapter3/client/#purging-the-local-cache", 
            "text": "It will be inevitable that you will want to clear the local cache at some point.   You have just changed the model and underlying data and need to re-establish a baseline.  You only cache newer data and want to remove historical data.  Things got corrupt for some reason and you need to refresh everything.   Whatever the reason, clearing the cache is one of those infrequent things that is going to be a necessity.  There\nare three forms of this operation:", 
            "title": "Purging the Local Cache"
        }, 
        {
            "location": "/chapter3/client/#deleting-the-backing-store", 
            "text": "The major requirement during development is that you want to delete the SQLite file that backs the offline cache.  It's likely that the model evolves over time during development.  Add test data that can sometimes cause things to go wrong and you have a recipe for bugs that are only there because you are developing.  If you suspect bad data in the offline cache, the first thing you want to do is start afresh.  Each platform stores the offline cache in a different place and the mechanism for removing it is different in each case.  For Universal Windows projects, the offline cache is stored in the Local AppData folder for the application. This is a dedicated area that each Universal Windows app has access to for anything from temporary files to settings and cache files.  To find the location of the file, open the  TaskList.UWP  project and open the  Package.appxmanifest  file.  Go to the  Packaging  tab.   Note the long  Package family name  field.  Your backing file is in your home directory under  AppData\\Local\\Packages\\{family_name}\\LocalState .  You specified the name of the file when you created the store.  You need to find the  Package Name  for Android.  Right-click on the  TaskList.Droid  project and select  Properties , then select the  Android Manifest  tab.   The database will be located in  /data/data/{package_name}/files  directory on the emulator.  Google has provided utilities for handling developer connections to devices (including emulators).  In this case, we can use the  adb  utility.  First, start your emulator of choice through the  Tools  -   Android  -   Android Emulator Manager  menu option.  Highlight the emulator you have been using, then click  Start .  Ensure the emulator is fully started before continuing.  The  adb  utility can be accessed through Visual Studio using  Tools  -   Android  -   Android Adb Command Prompt .  You will be able to use  adb  commands from there.  Use  adb devices  to find the device and  adb connect  to connect to the device.  This opens up a Linux-like shell onto the Android device.  You can use normal Linux commands to move around.   You can remove the entire private data area for your package using the following:  **root@donatello:/#** cd /data/data/Tasklist.Droid.TaskList.Droid\n**root@donatollo:/#** find . -name tasklist.db -print | xargs rm  The database will normally be in the  files  directory.  Use  exit  to close the shell prompt on the Android device.  Each disk image file is independent.  You must remove the database file on each emulator individually.   Tip  You can use the same  adb  commands to connect to a real Android device connected via USB.  Ensure  USB Debugging  is\nenabled on the device.  Use  adb devices  to find the device.  For more information, see  the Android documentation .   The iOS Simulator does not use an image files.  Instead, it stores files on your Mac disk in  ~/Library/Developer/CoreSimulator/Devices .  There is a file called  device_set.plist  that contains the list of devices that are defined and their location.  It is most easy to find a specific device.  For example, if you are testing on the iPhone 6x simulator:  $ grep -B 1 'iPhone-6s ' device_set.plist string A3536AA4-0678-43CC-BA21-DD997B89778A /string  key com.apple.CoreSimulator.SimDeviceType.iPhone-6s /key \n-- string 83D08BC0-2F9A-4479-ABBD-A69858819E93 /string  key com.apple.CoreSimulator.SimDeviceType.iPhone-6s /key \n-- string ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 /string  key com.apple.CoreSimulator.SimDeviceType.iPhone-6s /key   Each one of these corresponds to a different OS version.  You can find the ordering like this:  $ grep SimRuntime.iOS device_set.plist key com.apple.CoreSimulator.SimRuntime.iOS-9-1 /key  key com.apple.CoreSimulator.SimRuntime.iOS-9-2 /key  key com.apple.CoreSimulator.SimRuntime.iOS-9-3 /key   My simulator is an iPhone 6s running iOS 9.3, so I can see the GUID is the third one:  ECAE441D-93F8-4D7A-BF14-7FA2D11BC152 .  This GUID is a directory in the same directory as the  device_set.plist  file.  You can use normal UNIX style commands to remove the backing store:  $ cd ECAE441D-93F8-4D7A-BF14-7FA2D11BC152\n$ find . -name 'tasklist.db' -print | xargs rm  You can also use the normal Finder utilities to search for and remove the database file for your app.", 
            "title": "Deleting the backing store"
        }, 
        {
            "location": "/chapter3/client/#purging-records-from-the-offline-cache", 
            "text": "The  IMobileServiceSyncTable  interface also includes a capability for purging records that are stored in the offline sync by query.  This is done in code like this:  var lowerBound = DateTimeOffset.Now.AddDays(-7);\nvar query = syncTable.CreateQuery().Where(item =  item.UpdatedAt   lowerBound);\nvar force = true;\nawait syncTable.PurgeAsync( incsync_Tag , query, force);  There are four parameters for the  PurgeAsync  call:   A query name.  An OData query.  A boolean for forcing the operation.  A cancellation token (for the async operation).   Each incremental sync query has a unique name that is specified during the  PullAsync()  method call.  If you use the same name during the  PurgeAsync()  call, then the date associated with the incremental sync query is reset, causing a full refresh of the data.  This allows you to do a \"purge and refresh\" operation.  If you don't want this to happen, set the query name to null.  The OData query is a similar query format to the incremental sync query that we used with  PullAsync() .  In this case, it selects the records that should be purged from the offline sync cache.  If we wished to purge everything, we could just use  syncTable.CreateQuery() . If we want to purge only certain records, then we can adjust the query with a  .Where()  LINQ query.  In the example above, records that have not been updated within the last 7 days are purged.  Finally, the  PurgeAsync()  call will fail (and generate an exception) if there are any operations pending in the operations queue.  If we specify  force = true , then the operations queue check is bypassed and pending operations in the operations queue are flushed without being uploaded.  It is important that this option is used only when absolutely required.  You can leave your database in an inconsistent state if you expect referential integrity between different tables.  Use  SyncContext.PushAsync()  to push the operations queue to the remote server before calling  PurgeAsync() .   If you use  force = true , then also specify a query name to reset the incremental sync state.", 
            "title": "Purging Records from the Offline Cache"
        }, 
        {
            "location": "/chapter3/client/#schema-changes-in-the-offline-cache", 
            "text": "During development, it's very likely that you will want to change the schema of the data being stored on the client. Unfortunately, there isn't a really good way of dealing with this situation.  Here is my solution:   Create a constant within your mobile client containing a schema version number.  The actual value (text or int) doesn't matter.  You should never have the same schema version number twice though.  Change this constant whenever you change the schema.  Read a file from the mobile device with the schema version in it.  If the file exists and the read value is different from the constant you created in step 1, then you are \"refreshing the cache\"  If you are refreshing the cache, delete the offline cache file, then write the current schema version number to the file that you read in step 2.  Set up your offline cache, define your tables, etc.  If you are \"refreshing the cache\", do a full  PullAsync()  on all tables to populate the cache.   You obviously want to avoid this process as much as possible.  One possible solution is to use the  VersionTrackingPlugin  by Colby Williams to detect when the schema has been updated.  The following code can be added to automatically delete the offline cache when the version of the app changes:  // Add an isRemoteDatabaseSchemaChanged boolean somewhere\nif (CrossVersionTracking.Current.IsFirstLaunchForBuild)\n{\n    isRemoteDatabaseSchemaChanged = true;\n\n    // Drop SQLite Store file to overcome remote-db schema changes\n    File.Delete(Constants.SqliteStorePath);\n}", 
            "title": "Schema changes in the Offline Cache"
        }, 
        {
            "location": "/chapter3/client/#debugging-the-offline-cache", 
            "text": "One of the most difficult parts of the offline cache is that it is opaque - you can't really see what is going on.  Fortunately, the SQLiteStore  that is used is relatively straight forward to sub-class so we can add logging to it.  The following helper method can\nbe substituted for a  SQLiteStore  in any code.  public class MobileServiceSQLiteStoreWithLogging : MobileServiceSQLiteStore\n{\n    private bool logResults;\n    private bool logParameters;\n\n    public MobileServiceSQLiteStoreWithLogging(string fileName, bool logResults = false, bool logParameters = false)\n        : base(fileName)\n    {\n        this.logResults = logResults;\n        this.logParameters = logParameters;\n    }\n\n    protected override IList Newtonsoft.Json.Linq.JObject  ExecuteQuery(string tableName, string sql, IDictionary string, object  parameters)\n    {\n        Console.WriteLine (sql);\n\n        if(logParameters)\n            PrintDictionary (parameters);\n\n        var result = base.ExecuteQuery(tableName, sql, parameters);\n\n        if (logResults   result != null)\n        {\n            foreach (var token in result)\n                Console.WriteLine (token);\n        }\n\n        return result;\n    }\n\n    protected override void ExecuteNonQuery(string sql, IDictionary string, object  parameters)\n    {\n        Console.WriteLine (sql);\n\n        if(logParameters)\n            PrintDictionary (parameters);\n\n        base.ExecuteNonQuery(sql, parameters);\n    }\n\n    private void PrintDictionary(IDictionary string,object  dictionary)\n    {\n        if (dictionary == null)\n            return;\n\n        foreach (var pair in dictionary)\n            Console.WriteLine ( {0}:{1} , pair.Key, pair.Value);\n    }\n}\n\npublic class LoggingHandler : DelegatingHandler\n{\n    private bool logRequestResponseBody;\n\n    public LoggingHandler(bool logRequestResponseBody = false)\n    {\n        this.logRequestResponseBody = logRequestResponseBody;\n    }\n\n    protected override async Task HttpResponseMessage  SendAsync(HttpRequestMessage request, System.Threading.CancellationToken cancellationToken)\n    {\n        Console.WriteLine( Request: {0} {1} , request.Method, request.RequestUri.ToString());\n\n        if (logRequestResponseBody   request.Content != null)\n        {\n            var requestContent = await request.Content.ReadAsStringAsync ();\n            Console.WriteLine (requestContent);\n        }\n\n        var response = await base.SendAsync(request, cancellationToken);\n\n        Console.WriteLine ( Response: {0} , response.StatusCode);\n\n        if (logRequestResponseBody)\n        {\n            var responseContent = await response.Content.ReadAsStringAsync ();\n            Console.WriteLine (responseContent);\n        }\n\n        return response;\n    }\n}  Using this class will print all the SQL commands that are executed against the SQLite store.  Ensure you are capturing the console somewhere so that you can see the debug messages as you are running your application.", 
            "title": "Debugging the Offline Cache"
        }, 
        {
            "location": "/chapter3/relationships/", 
            "text": "Relationships\n\n\nOne of the biggest benefits to using a SQL database over a NoSQL store is relationships between entities.  Relationships provide the ability to normalize the data, allowing you to store the minimal amount of data for a specific use case on the mobile device.  This reduces bandwidth usage and memory usage on the device.  Relationships are a good thing.\n\n\nUnfortunately, relationships between tables are hard when one is working within an offline context.  This is primarily caused by the need for resilience.  Because we can do many updates to the tables on the offline client, the transactions that update the tables need to be co-ordinated.  This is practically impossible in an offline context where one of the\ngoals in bandwidth performance.\n\n\nAzure Mobile Apps, when used in an offline context, has an operations table.  As you do each operation against a table, an entry is made in the operations table.  The operations table is then replayed in order to the mobile backend to effect changes in the remote database.  However, this also has the effect that we do not have transactions to allow the updating of multiple tables within the database at the same time.  Each record in each table is updated individually.  The push process that offline sync uses has major ramifications for how relationships between tables work.  Specifically, only 1-way relationships will work in an offline sync world.\n\n\n\n\n1-Way Relationships\n\n\nYou can define relationships in Entity Framework with or without a virtual back-reference.  Relationships with the virtual back-reference are known as 2-way relationships (because you can get back to the original model).  Relationships with only a forward reference (and no knowledge of the original model) are said to have a 1-way relationship.  A database model with only 1-way relationships can generally be represented with a tree\nstructure.\n\n\n\n\nLet's take a quick example.  We've been using the \"task list\" scenario for our testing thus far.  Let's say that each task could be assigned a tag from a list of tags.   We can use a 1-way 1:1 relationship between the tasks and the tags. To do that, we would store the Id of the tag in the task model.  If, however, we could attach many tags to a single task, that would be a 1:Many relationship.\n\n\n1:1 Relationships\n\n\nLet's take a look at our task list example, from the perspective of the models on the server side:\n\n\nusing Microsoft.Azure.Mobile.Server;\nusing System.ComponentModel.DataAnnotations.Schema;\n\nnamespace ComplexTypes.DataObjects\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n\n        #region Relationships\n        public string TagId { get; set; }\n\n        [ForeignKey(\nTagId\n)]\n        public Tag Tag { get; set; }\n        #endregion\n    }\n}\n\n\n\n\n1:1 relationships are defined using a foreign key in the SQL database.  We can use Entity Framework to define the foreign\nkey relationship easily.  In this case, our \nTodoItem\n model will, have a TagId that contains the Id field of the tag.  I\nalso created a pair of table controllers for these models in the normal manner.  Finally, I've created some records using\nthe \nSeed()\n method within the \nApp_Start\\Startup.MobileApp.cs\n file to give us some test data.\n\n\nIf we take a look at records through Postman, we will get the following:\n\n\n\n\nNote that the first item has a reference to a tag, by virtue of the TagId.  The second item does not have a tag assigned,\nso the value of TagId is null.\n\n\nWhen we implement the client, we are going to download these tables independently.  The linkage and relationships between\nthe tables is lost when going from the backend to the offline client.  We have to link them together ourselves.  This is\nwhy the \"1-way\" relationship is necessary.  In a 2-way relationship, a tag and task would have to be created at the same\ntime as part of an SQL transaction.  In a 1-way relationship, the tag can be created THEN the task that has the relationship\nis created\n\n\n\n\nTip\n\n\nWhen you think of all the mobile applications you own, you will realize that 1-way relationships are the normal state of\naffairs.  Very few data models for mobile apps actually require a two-way relationship.\n\n\n\n\nWhen you are developing the mobile client, the \nTag\n is removed from the model:\n\n\nusing TaskList.Helpers;\n\nnamespace TaskList.Models\n{\n    public class Tag : TableData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n\n        public string TagId { get; set; }\n    }\n}\n\n\n\n\nOne can easily retrieve the tag information with a LINQ query on the Tag table:\n\n\nvar tag = tagTable.FirstOrDefault(tag =\n tag.Id.Equals(task.TagId)).Value;\n\n\n\n\nThere are a couple of rules you must follow within your client code:\n\n\n\n\nYou need to ensure that you create a tag before associating that tag with a task.\n\n\nYou need to store the TagId with the task, not the \nTag\n object (as you would normally do within Entity Framework).\n\n\n\n\nThe former is generally handled for you.  If you \nInsertAsync\n a tag in offline mode, it will be placed into the\noperations queue prior to anything that uses it.  Since the operations queue is processed in order, the tag will\nbe sent to the backend prior to any record updates that would use it.\n\n\n1:Many Relationships\n\n\nWhat if we had a list of messages and wanted to assign more than one tag to each record?  In this case, we would\nneed a 1:Many relationship.  Setting up a 1:Many relationship on the backend again relies on Entity Framework syntax:\n\n\nusing Microsoft.Azure.Mobile.Server;\nusing System.Collections.Generic;\n\nnamespace TaskList.Models\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n        public virtual ICollection\nTag\n Tags { get; set; }\n    }\n}\n\n\n\n\nWe can seed some information into this database to simulate relationships with a standard initializer:\n\n\npublic class MobileServiceInitializer : CreateDatabaseIfNotExists\nMobileServiceContext\n\n{\n    protected override void Seed(MobileServiceContext context)\n    {\n        List\nTag\n tags = new List\nTag\n\n        {\n            new Tag { Id = NewGuid(), TagName = \nTag-1\n },\n            new Tag { Id = NewGuid(), TagName = \nTag-2\n },\n            new Tag { Id = NewGuid(), TagName = \nTag-3\n }\n        };\n        context.Set\nTag\n().AddRange(tags);\n\n        List\nMessage\n messages = new List\nMessage\n\n        {\n            new Message { Id = NewGuid(), Text = \nMessage-1\n, Tags = tags },\n            new Message { Id = NewGuid(), Text = \nmessage-2\n, Tags = new List\nTag\n() }\n        };\n        context.Set\nMessage\n().AddRange(messages);\n\n        base.Seed(context);\n    }\n\n    private string NewGuid()\n    {\n        return Guid.NewGuid().ToString();\n    }\n}\n\n\n\n\nIf we use Postman to do a GET /tables/Message, we get the following:\n\n\n\n\nNote that the \ntags\n field is not even produced.  The Azure Mobile Apps Server SDK depends on multiple frameworks.  It\nuses Entity Framework for data access, for instance.  It also uses the standard Microsoft OData server to translate\nOData queries into results.  This has a side effect that the rules of the Microsoft OData server must be followed.\nSpecifically, this means that collections, such as the list of tags, will not be produced unless you explicitly expand\nthem.  You can do this on the URL by using the \n$expand\n parameter:\n\n\nGET /tables/Message?$expand=tags\n\n\n\n\nThis will result in the following output:\n\n\n\n\nNote that the tags are fully expanded and embedded in the object.  This has some serious consequences that must be\nconsidered:\n\n\n\n\nThe table becomes read-only through the controller.  Inserts, Updates and Deletes must have special code handling.\n\n\nThe data takes up more space on the client as the tags will be duplicated whenever used.\n\n\nYou cannot use offline sync since the data model is no longer flat.\n\n\n\n\nIn addition to these three points, you need to be able to add the \n$expand\n property to the request.  This is not done\nby the SDK.  You can automatically add the \n$expand\n property on the server-side by using an attribute for the purpose:\n\n\nusing System;\nusing System.Linq;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.Filters;\n\nnamespace Chapter3.Extensions\n{\n    [AttributeUsage(AttributeTargets.Method, AllowMultiple = true)]\n    public class ExpandPropertyAttribute : ActionFilterAttribute\n    {\n        string propertyName;\n\n        public ExpandPropertyAttribute(string propertyName)\n        {\n            this.propertyName = propertyName;\n        }\n\n        public override void OnActionExecuting(HttpActionContext actionContext)\n        {\n            base.OnActionExecuting(actionContext);\n            var uriBuilder = new UriBuilder(actionContext.Request.RequestUri);\n            var queryParams = uriBuilder.Query.TrimStart('?').Split(new[] { '\n' }, StringSplitOptions.RemoveEmptyEntries).ToList();\n            int expandIndex = -1;\n            for (var i = 0; i \n queryParams.Count; i++)\n            {\n                if (queryParams[i].StartsWith(\n$expand\n, StringComparison.Ordinal))\n                {\n                    expandIndex = i;\n                    break;\n                }\n            }\n\n            if (expandIndex \n 0)\n            {\n                queryParams.Add(\n$expand=\n + this.propertyName);\n            }\n            else\n            {\n                queryParams[expandIndex] = queryParams[expandIndex] + \n,\n + propertyName;\n            }\n\n            uriBuilder.Query = string.Join(\n, queryParams);\n            actionContext.Request.RequestUri = uriBuilder.Uri;\n        }\n    }\n}\n\n\n\n\nThis is used in the controller:\n\n\npublic class MessageController : TableController\nMessage\n\n{\n    private MobileServiceContext context;\n\n    protected override void Initialize(HttpControllerContext controllerContext)\n    {\n        base.Initialize(controllerContext);\n        context = new MobileServiceContext();\n        DomainManager = new EntityDomainManager\nMessage\n(context, Request);\n    }\n\n    public string UserId =\n ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n    // GET tables/Message\n    [ExpandProperty(\ntags\n)]\n    public IQueryable\nMessage\n GetAllMessage()\n    {\n        return Query();\n        //return Query().OwnedByFriends(context.Friends, UserId);\n    }\n\n    // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959\n    [ExpandProperty(\ntags\n)]\n    public SingleResult\nMessage\n GetMessage(string id)\n    {\n        return new SingleResult\nMessage\n(Lookup(id).Queryable);\n        //return new SingleResult\nMessage\n(Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId));\n    }\n\n    // POST tables/Message\n    public async Task\nIHttpActionResult\n PostMessageAsync(Message item)\n    {\n        item.UserId = UserId;\n        Message current = await InsertAsync(item);\n        return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n    }\n}\n\n\n\n\nGiven this is not recommended, what are the alternatives?  There are two alternatives. The first alternative is to\nuse a third table that joins the Messages and Tags together in a loose manner:\n\n\nusing Microsoft.Azure.Mobile.Server;\nusing System.Collections.Generic;\n\nnamespace TaskList.Models\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n    }\n\n    public class MessageTag : EntityData\n    {\n        public string MessageId { get; set; }\n        public string TagId { get; set; }\n    }\n}\n\n\n\n\nWe can now create a \nMessageTagController\n to retrieve the information.  This data can be stored offline since the model\nis now flat.  Obtaining the list of tags for a message is a single LINQ query:\n\n\nvar tags = from tag in tagTable\n           let tl = (from mt in messageTags where mt.MessageId == messageId select mt.TagId)\n           where tl.Contains(tag.Id)\n           select tag;\n\n\n\n\nYou may recognize this LINQ query as it is very similar to the LINQ query used for the friends filter.  A similar query\ncan be used to find the messages associated with a tag:\n\n\nvar msgs = from message in messageTable\n           let tl = (from mt in messageTags where mt.TagId == tagId select mt.MessageId)\n           where tl.Contains(message.Id)\n           orderby message.CreatedAtRoute\n           select message;\n\n\n\n\nThe second alternative is to use an alternative DomainManager that implements the relationships for you.  We will cover this\nin the next section.", 
            "title": "Relationships"
        }, 
        {
            "location": "/chapter3/relationships/#relationships", 
            "text": "One of the biggest benefits to using a SQL database over a NoSQL store is relationships between entities.  Relationships provide the ability to normalize the data, allowing you to store the minimal amount of data for a specific use case on the mobile device.  This reduces bandwidth usage and memory usage on the device.  Relationships are a good thing.  Unfortunately, relationships between tables are hard when one is working within an offline context.  This is primarily caused by the need for resilience.  Because we can do many updates to the tables on the offline client, the transactions that update the tables need to be co-ordinated.  This is practically impossible in an offline context where one of the\ngoals in bandwidth performance.  Azure Mobile Apps, when used in an offline context, has an operations table.  As you do each operation against a table, an entry is made in the operations table.  The operations table is then replayed in order to the mobile backend to effect changes in the remote database.  However, this also has the effect that we do not have transactions to allow the updating of multiple tables within the database at the same time.  Each record in each table is updated individually.  The push process that offline sync uses has major ramifications for how relationships between tables work.  Specifically, only 1-way relationships will work in an offline sync world.   1-Way Relationships  You can define relationships in Entity Framework with or without a virtual back-reference.  Relationships with the virtual back-reference are known as 2-way relationships (because you can get back to the original model).  Relationships with only a forward reference (and no knowledge of the original model) are said to have a 1-way relationship.  A database model with only 1-way relationships can generally be represented with a tree\nstructure.   Let's take a quick example.  We've been using the \"task list\" scenario for our testing thus far.  Let's say that each task could be assigned a tag from a list of tags.   We can use a 1-way 1:1 relationship between the tasks and the tags. To do that, we would store the Id of the tag in the task model.  If, however, we could attach many tags to a single task, that would be a 1:Many relationship.", 
            "title": "Relationships"
        }, 
        {
            "location": "/chapter3/relationships/#11-relationships", 
            "text": "Let's take a look at our task list example, from the perspective of the models on the server side:  using Microsoft.Azure.Mobile.Server;\nusing System.ComponentModel.DataAnnotations.Schema;\n\nnamespace ComplexTypes.DataObjects\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class TodoItem : EntityData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n\n        #region Relationships\n        public string TagId { get; set; }\n\n        [ForeignKey( TagId )]\n        public Tag Tag { get; set; }\n        #endregion\n    }\n}  1:1 relationships are defined using a foreign key in the SQL database.  We can use Entity Framework to define the foreign\nkey relationship easily.  In this case, our  TodoItem  model will, have a TagId that contains the Id field of the tag.  I\nalso created a pair of table controllers for these models in the normal manner.  Finally, I've created some records using\nthe  Seed()  method within the  App_Start\\Startup.MobileApp.cs  file to give us some test data.  If we take a look at records through Postman, we will get the following:   Note that the first item has a reference to a tag, by virtue of the TagId.  The second item does not have a tag assigned,\nso the value of TagId is null.  When we implement the client, we are going to download these tables independently.  The linkage and relationships between\nthe tables is lost when going from the backend to the offline client.  We have to link them together ourselves.  This is\nwhy the \"1-way\" relationship is necessary.  In a 2-way relationship, a tag and task would have to be created at the same\ntime as part of an SQL transaction.  In a 1-way relationship, the tag can be created THEN the task that has the relationship\nis created   Tip  When you think of all the mobile applications you own, you will realize that 1-way relationships are the normal state of\naffairs.  Very few data models for mobile apps actually require a two-way relationship.   When you are developing the mobile client, the  Tag  is removed from the model:  using TaskList.Helpers;\n\nnamespace TaskList.Models\n{\n    public class Tag : TableData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class TodoItem : TableData\n    {\n        public string Text { get; set; }\n\n        public bool Complete { get; set; }\n\n        public string TagId { get; set; }\n    }\n}  One can easily retrieve the tag information with a LINQ query on the Tag table:  var tag = tagTable.FirstOrDefault(tag =  tag.Id.Equals(task.TagId)).Value;  There are a couple of rules you must follow within your client code:   You need to ensure that you create a tag before associating that tag with a task.  You need to store the TagId with the task, not the  Tag  object (as you would normally do within Entity Framework).   The former is generally handled for you.  If you  InsertAsync  a tag in offline mode, it will be placed into the\noperations queue prior to anything that uses it.  Since the operations queue is processed in order, the tag will\nbe sent to the backend prior to any record updates that would use it.", 
            "title": "1:1 Relationships"
        }, 
        {
            "location": "/chapter3/relationships/#1many-relationships", 
            "text": "What if we had a list of messages and wanted to assign more than one tag to each record?  In this case, we would\nneed a 1:Many relationship.  Setting up a 1:Many relationship on the backend again relies on Entity Framework syntax:  using Microsoft.Azure.Mobile.Server;\nusing System.Collections.Generic;\n\nnamespace TaskList.Models\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n        public virtual ICollection Tag  Tags { get; set; }\n    }\n}  We can seed some information into this database to simulate relationships with a standard initializer:  public class MobileServiceInitializer : CreateDatabaseIfNotExists MobileServiceContext \n{\n    protected override void Seed(MobileServiceContext context)\n    {\n        List Tag  tags = new List Tag \n        {\n            new Tag { Id = NewGuid(), TagName =  Tag-1  },\n            new Tag { Id = NewGuid(), TagName =  Tag-2  },\n            new Tag { Id = NewGuid(), TagName =  Tag-3  }\n        };\n        context.Set Tag ().AddRange(tags);\n\n        List Message  messages = new List Message \n        {\n            new Message { Id = NewGuid(), Text =  Message-1 , Tags = tags },\n            new Message { Id = NewGuid(), Text =  message-2 , Tags = new List Tag () }\n        };\n        context.Set Message ().AddRange(messages);\n\n        base.Seed(context);\n    }\n\n    private string NewGuid()\n    {\n        return Guid.NewGuid().ToString();\n    }\n}  If we use Postman to do a GET /tables/Message, we get the following:   Note that the  tags  field is not even produced.  The Azure Mobile Apps Server SDK depends on multiple frameworks.  It\nuses Entity Framework for data access, for instance.  It also uses the standard Microsoft OData server to translate\nOData queries into results.  This has a side effect that the rules of the Microsoft OData server must be followed.\nSpecifically, this means that collections, such as the list of tags, will not be produced unless you explicitly expand\nthem.  You can do this on the URL by using the  $expand  parameter:  GET /tables/Message?$expand=tags  This will result in the following output:   Note that the tags are fully expanded and embedded in the object.  This has some serious consequences that must be\nconsidered:   The table becomes read-only through the controller.  Inserts, Updates and Deletes must have special code handling.  The data takes up more space on the client as the tags will be duplicated whenever used.  You cannot use offline sync since the data model is no longer flat.   In addition to these three points, you need to be able to add the  $expand  property to the request.  This is not done\nby the SDK.  You can automatically add the  $expand  property on the server-side by using an attribute for the purpose:  using System;\nusing System.Linq;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.Filters;\n\nnamespace Chapter3.Extensions\n{\n    [AttributeUsage(AttributeTargets.Method, AllowMultiple = true)]\n    public class ExpandPropertyAttribute : ActionFilterAttribute\n    {\n        string propertyName;\n\n        public ExpandPropertyAttribute(string propertyName)\n        {\n            this.propertyName = propertyName;\n        }\n\n        public override void OnActionExecuting(HttpActionContext actionContext)\n        {\n            base.OnActionExecuting(actionContext);\n            var uriBuilder = new UriBuilder(actionContext.Request.RequestUri);\n            var queryParams = uriBuilder.Query.TrimStart('?').Split(new[] { ' ' }, StringSplitOptions.RemoveEmptyEntries).ToList();\n            int expandIndex = -1;\n            for (var i = 0; i   queryParams.Count; i++)\n            {\n                if (queryParams[i].StartsWith( $expand , StringComparison.Ordinal))\n                {\n                    expandIndex = i;\n                    break;\n                }\n            }\n\n            if (expandIndex   0)\n            {\n                queryParams.Add( $expand=  + this.propertyName);\n            }\n            else\n            {\n                queryParams[expandIndex] = queryParams[expandIndex] +  ,  + propertyName;\n            }\n\n            uriBuilder.Query = string.Join( , queryParams);\n            actionContext.Request.RequestUri = uriBuilder.Uri;\n        }\n    }\n}  This is used in the controller:  public class MessageController : TableController Message \n{\n    private MobileServiceContext context;\n\n    protected override void Initialize(HttpControllerContext controllerContext)\n    {\n        base.Initialize(controllerContext);\n        context = new MobileServiceContext();\n        DomainManager = new EntityDomainManager Message (context, Request);\n    }\n\n    public string UserId =  ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n    // GET tables/Message\n    [ExpandProperty( tags )]\n    public IQueryable Message  GetAllMessage()\n    {\n        return Query();\n        //return Query().OwnedByFriends(context.Friends, UserId);\n    }\n\n    // GET tables/Message/48D68C86-6EA6-4C25-AA33-223FC9A27959\n    [ExpandProperty( tags )]\n    public SingleResult Message  GetMessage(string id)\n    {\n        return new SingleResult Message (Lookup(id).Queryable);\n        //return new SingleResult Message (Lookup(id).Queryable.OwnedByFriends(context.Friends, UserId));\n    }\n\n    // POST tables/Message\n    public async Task IHttpActionResult  PostMessageAsync(Message item)\n    {\n        item.UserId = UserId;\n        Message current = await InsertAsync(item);\n        return CreatedAtRoute( Tables , new { id = current.Id }, current);\n    }\n}  Given this is not recommended, what are the alternatives?  There are two alternatives. The first alternative is to\nuse a third table that joins the Messages and Tags together in a loose manner:  using Microsoft.Azure.Mobile.Server;\nusing System.Collections.Generic;\n\nnamespace TaskList.Models\n{\n    public class Tag : EntityData\n    {\n        public string TagName { get; set; }\n    }\n\n    public class Message : EntityData\n    {\n        public string UserId { get; set; }\n        public string Text { get; set; }\n    }\n\n    public class MessageTag : EntityData\n    {\n        public string MessageId { get; set; }\n        public string TagId { get; set; }\n    }\n}  We can now create a  MessageTagController  to retrieve the information.  This data can be stored offline since the model\nis now flat.  Obtaining the list of tags for a message is a single LINQ query:  var tags = from tag in tagTable\n           let tl = (from mt in messageTags where mt.MessageId == messageId select mt.TagId)\n           where tl.Contains(tag.Id)\n           select tag;  You may recognize this LINQ query as it is very similar to the LINQ query used for the friends filter.  A similar query\ncan be used to find the messages associated with a tag:  var msgs = from message in messageTable\n           let tl = (from mt in messageTags where mt.TagId == tagId select mt.MessageId)\n           where tl.Contains(message.Id)\n           orderby message.CreatedAtRoute\n           select message;  The second alternative is to use an alternative DomainManager that implements the relationships for you.  We will cover this\nin the next section.", 
            "title": "1:Many Relationships"
        }, 
        {
            "location": "/chapter3/domainmgr/", 
            "text": "The Domain Manager\n\n\nAs a request comes in to the mobile backend, it is processed through several layers.  First, ASP.NET\nprocesses the request, handling things like Authentication and Authorization.  It is then processed\nthrough the \nMicrosoft.Web.Http.OData\n controller, which compiles the requested query.  Then it is \npassed to the Domain Manager, which is responsible for converting the request into a response.  The\nresponse is then passed back up the stack to be finally given back to the mobile client.\n\n\nThe Domain Manager is a central part of this process.  It is a class that implements the \nIDomainManager\n\ninterface:\n\n\nnamespace Microsoft.Azure.Mobile.Server.Tables\n{\n    public interface IDomainManager\nTData\n where TData : class, ITableData\n    {\n        IQueryable\nTData\n Query();\n        SingleResult\nTData\n Lookup(string id);\n        Task\nIEnumerable\nTData\n QueryAsync(ODataQueryOptions query);\n        Task\nSingleResult\nTData\n LookupAsync(string id);\n        Task\nTData\n InsertAsync(TData data);\n        Task\nTData\n UpdateAsync(string id, Delta\nTData\n patch);\n        Task\nTData\n ReplaceAsync(string id, TData data);\n        Task\nbool\n DeleteAsync(string id);\n    }\n}\n\n\n\n\nThis looks deceptively simple.  Just 8 methods.  In reality, this is anything but simple.  The major issue\nthat a prospective domain manager implementor has to grapple with is the translation of an \nIQueryable\n into\nsomething that the backend data source can understand.\n\n\nLet's take a look at a couple of domain managers that solve specific problems that crop up from time to time\nduring development.  It should be noted that \nNEITHER\n of these domain managers are recommended as a generalized\nsolution.  They both have significant caveats to their use and you should understand those caveats before\nembarking on integrating them.\n\n\nExisting Table Relationships with the MappedEntityDomainManager\n\n\nOne of the key areas that is weak when using the default \nEntityDomainManager\n is handling existing tables.  The\ngenerally accepted method of dealing with relationships is through loose coupling and manual relationship management\nin the client. Relationships are core to the SQL database world and we sometimes want to project those relationships \ninto the mobile client, allowing the backend to preserve any relationships that have been configured while still \nusing the standard offline client capabilities.  If you have existing SQL relationships, you can use a combination\nof \nAutoMapper\n and the \nMappedEntityDomainManager\n.  \n\n\nThe \nMappedEntityDomainManager\n is an abstract \nIDomainManager\n implementation targetting SQL as the backend store where\nthere is not a 1:1 mapping between the data object (DTO) exposed through the TableController and the domain model managed \nby Entity Framework.  If there is a 1:1 mapping, use \nEntityDomainManager\n.  The \nMappedEntityDomainManager\n uses \n\nAutoMapper\n to map between the DTO and the domain model.  It assumes that AutoMapper has already been initialized with \nappropriate mappings that map from DTO to domain model and from the domain model to the DTO.\n\n\nLet's take a small example.  If I am producing an enterprise mobile app that field engineers can use - the ones\nthat, for example, visit your house to install cable.  I can define an Entity Framework model map as follows:\n\n\n[Table(\nCustomers\n)]\npublic class Customer\n{\n    public Customer()\n    {\n        this.Jobs = new HashSet\nJob\n();\n    }\n\n    [StringLength(50)]\n    public string Id { get; set; }\n\n    [StringLength(50)]\n    public string FullName { get; set; }\n\n    [StringLength(250)]\n    public string Address { get; set; \n\n    public decimal? Latitude { get; set; }\n\n    public decimal? Longitude { get; set; }\n\n    public ICollection\nJob\n Jobs { get; set; }\n}\n\n[Table(\nEquipment\n)]\npublic class Equipment : ITableData\n{\n    public Equipment()\n    {\n        this.Jobs = new HashSet\nJob\n();\n    }\n\n    [StringLength(50)]\n    public string Name { get; set; }\n\n    [StringLength(28)]\n    public string Asset { get; set; }\n\n    [StringLength(250)]\n    public string Description { get; set; }\n\n    #region ITableData\n    public DateTimeOffset? CreatedAt { get; set; }\n    public bool Deleted { get; set; }\n\n    [DatabaseGenerated(DatabaseGeneratedOption.Computed)]\n    public DateTimeOffset? UpdatedAt { get; set; }\n\n    [Timestamp]\n    public byte[] Version { get; set; }\n    #endregion\n\n    public ICollection\nJob\n Jobs { get; set; }\n}\n\n[Table(\nJobs\n)]\npublic class Job : ITableData\n{\n    public Job()\n    {\n        this.Equipments = new HashSet\nEquipment\n();\n    }\n\n    [StringLength(50)]\n    public string CustomerId { get; set; }\n\n    [StringLength(50)]\n    public string AgentId { get; set; }\n\n    public DateTimeOffset? StartTime { get; set; }\n\n    public DateTimeOffset? EndTime { get; set; }\n\n    [StringLength(50)]\n    public string Status { get; set; }\n\n    [StringLength(250)]\n    public string Description { get; set; }\n\n    #region ITableData\n    public DateTimeOffset? CreatedAt { get; set; }\n    public bool Deleted { get; set; }\n\n    [DatabaseGenerated(DatabaseGeneratedOption.Computed)]\n    public DateTimeOffset? UpdatedAt { get; set; }\n\n    [Timestamp]\n    public byte[] Version { get; set; }\n    #endregion\n\n    public ICollection\nEquipment\n Equipments { get; set; }\n}\n\n\n\n\nThis is the representation of the tables within the database.  They don't have to map to what the client\nrequires.  We can wire up the relationships in the normal \nEntity Framework way\n, within the \nDbContext\n:\n\n\npublic partial class ExistingDbContext : DbContext\n{\n    public FieldDbContext() : base(\nname=MS_TableConnectionString\n)\n    {            \n    }\n\n    public virtual DbSet\nCustomer\n Customers { get; set; }\n    public virtual DbSet\nEquipment\n Equipments { get; set; }\n    public virtual DbSet\nJob\n Jobs { get; set; } \n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {            \n        modelBuilder.HasDefaultSchema(\ndbo\n);\n        modelBuilder.Conventions.Add(\n            new AttributeToColumnAnnotationConvention\nTableColumnAttribute, string\n(\n            \nServiceTableColumn\n, (property, attributes) =\n attributes.Single().ColumnType.ToString()));\n\n        modelBuilder.Entity\nCustomer\n().Property(e =\n e.Address).IsUnicode(false);\n        modelBuilder.Entity\nCustomer\n().Property(e =\n e.FullName).IsUnicode(false);\n        modelBuilder.Entity\nCustomer\n().Property(e =\n e.Id).IsUnicode(false);\n        modelBuilder.Entity\nCustomer\n().Property(e =\n e.Latitude).HasPrecision(9, 6);\n        modelBuilder.Entity\nCustomer\n().Property(e =\n e.Longitude).HasPrecision(9, 6);\n\n        modelBuilder.Entity\nEquipment\n().Property(e =\n e.Asset).IsUnicode(false);\n        modelBuilder.Entity\nEquipment\n().Property(e =\n e.Description).IsUnicode(false);\n        modelBuilder.Entity\nEquipment\n().Property(e =\n e.Name).IsUnicode(false);\n        modelBuilder.Entity\nEquipment\n().Property(e =\n e.Id).IsUnicode(false);\n\n        modelBuilder.Entity\nJob\n().Property(e =\n e.CustomerId).IsUnicode(false);\n        modelBuilder.Entity\nJob\n().Property(e =\n e.AgentId).IsUnicode(false);\n        modelBuilder.Entity\nJob\n().Property(e =\n e.Id).IsUnicode(false);\n        modelBuilder.Entity\nJob\n().Property(e =\n e.Status).IsUnicode(false);\n        modelBuilder.Entity\nJob\n().Property(e =\n e.Description).IsUnicode(false);\n\n        modelBuilder.Entity\nEquipment\n()\n            .HasMany(e =\n e.Jobs)\n            .WithMany(e =\n e.Equipments)\n            .Map(m =\n m.ToTable(\nEquipmentIds\n).MapLeftKey(\nEquipmentId\n).MapRightKey(\nJobId\n));\n    }\n}\n\n\n\n\nWe can see the relationship (a Many:Many relationship) at the end of the \nmodelBuilder\n within\nthe DbContext.  The 1:Many and 1:1 relationships are handled within the models themselves, per the\nnormal Entity Framework methods. This is pure Entity Framework thus far - we have defined the\nstructure of the database.\n\n\nTo translate this into a mobile client, we need to define Data Transfer Objects.  These don't\nhave to be the same shape as the models that Entity Framework is using.  For example:\n\n\npublic class CustomerDTO\n{\n    public string FullName { get; set; }\n    public string Address { get; set; \n    public decimal? Latitude { get; set; }\n    public decimal? Longitude { get; set; }\n}\n\npublic class EquipmentDTO\n{\n    public string Name { get; set; }\n    public string Asset { get; set; }\n    public string Description { get; set; }\n}\n\npublic class JobDTO : EntityData\n{\n    public string AgentId { get; set; }\n    public DateTimeOffset? StartTime { get; set; }\n    public DateTimeOffset? EndTime { get; set; }\n    public string Status { get; set; }\n    public string Description { get; set; }  \n\n    public virtual CustomerDTO Customer { get; set; }\n    public virtual List\nEquipmentDTO\n Equipments { get; set; }\n}\n\n\n\n\nNote that the DTOs are similar, but definitely not the same.  They don't have the same relationships between\nthe records, for example.  \nMappedEntityDomainManager\n requires that AutoMapper is already configured and\ninitialized, so that's the next step.  Set up an AutoMapper configuration in the \nApp_Start\n directory:\n\n\nusing AutoMapper;\nusing FieldEngineer.Service.DataObjects;\nusing FieldEngineer.Service.Models;\n\nnamespace FieldEngineer.Service\n{\n    public class AutomapperConfiguration\n    {\n        public static void CreateMapping(IConfiguration cfg)\n        {\n            // Apply some name changes from the entity to the DTO\n            cfg.CreateMap\nJob, JobDTO\n()                \n                .ForMember(jobDTO =\n jobDTO.Equipments, map =\n map.MapFrom(job =\n job.Equipments));\n\n            // For incoming requests, ignore the relationships\n            cfg.CreateMap\nJobDTO, Job\n()                                            \n                .ForMember(job =\n job.Customer, map =\n map.Ignore())\n                .ForMember(job =\n job.Equipments, map =\n map.Ignore());\n\n            cfg.CreateMap\nCustomer, CustomerDTO\n();            \n            cfg.CreateMap\nEquipment, EquipmentDTO\n();\n        }\n    }\n}\n\n\n\n\nYou will also need to initialize the AutoMapper - this can be done where you also configure the Azure Mobile Apps:\n\n\nusing System;\nusing System.Web.Http;\nusing AutoMapper;\nusing Microsoft.WindowsAzure.Mobile.Service;\n\nnamespace FieldEngineer.Service\n{\n    public static class WebApiConfig\n    {\n        public static void Register()\n        {\n            // Use this class to set configuration options for your mobile service\n            ConfigOptions options = new ConfigOptions();\n\n            // Use this class to set WebAPI configuration options\n            HttpConfiguration config = ServiceConfig.Initialize(new ConfigBuilder(options));\n\n            // To display errors in the browser during development, uncomment the following\n            // line. Comment it out again when you deploy your service for production use.\n            config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always;\n\n            // This is the line that initializes AutoMapper\n            Mapper.Initialize(cfg =\n { AutomapperConfiguration.CreateMapping(cfg); });                                \n        }\n    }\n}\n\n\n\n\nFinally, we can create a controller that allows the receipt and update of jobs:\n\n\nnamespace FieldEngineer.Service.Controllers\n{\n    [Authorize]  \n    public class JobController : TableController\nJobDTO\n\n    {      \n        private FieldDbContext context;        \n\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            this.context = new FieldDbContext();\n\n            this.DomainManager = new DefaultMappedEntityDomainManager\nJobDTO,Job\n(this.context, Request, Services);            \n        }\n\n        [ExpandProperty(\nCustomer\n)]\n        [ExpandProperty(\nEquipments\n)]\n        public async Task\nIQueryable\nJobDTO\n GetAllJobs()\n        {                        \n            var jobs = this.context.Jobs\n                .Include(\nCustomer\n)\n                .Include(\nEquipments\n)\n                .Project().To\nJobDTO\n();            \n            return jobs;\n        }\n\n        [ExpandProperty(\nCustomer\n)]\n        [ExpandProperty(\nEquipments\n)]\n        public SingleResult\nJobDTO\n GetJob(string id)\n        {\n            return this.Lookup(id);\n        }\n\n        public async Task\nJobDTO\n PatchJob(string id, Delta\nJobDTO\n patch)\n        {\n            return await this.UpdateAsync(id, patch);                   \n        }        \n    }\n}\n\n\n\n\nWe are using \n[ExpandProperty]\n to expand the Customer and Equipment data so that it is transferred with the Job object.\nThe \nMappedEntityDomainManager\n is an abstract type, so we have to create a concrete implementation.  Fortunately, most\nof the work is done for us.  There are already concrete versions of most of the methods we require (like insert, delete\nand lookup). The \nMappedEntityDomainManager\n needs help to deal with replacements nor optimistic concurrency - features \nwe want.  We can use the \nDefaultMappedEntityDomainManager\nto handle this for us:\n\n\nnamespace FieldEngineerLite.Service.Helpers\n{\n    public class DefaultMappedEntityDomainManager\nTData, TModel\n\n            : MappedEntityDomainManager\nTData, TModel\n\n        where TData : class, ITableData\n        where TModel : class, ITableData\n    {\n        public DefaultMappedEntityDomainManager(DbContext context, HttpRequestMessage request, ApiServices services)\n            : base(context, request, services)\n        {            \n        }\n\n        public override Task\nbool\n DeleteAsync(string id)\n        {\n            return this.DeleteItemAsync(id);\n        }\n\n        public override Task\nTData\n UpdateAsync(string id, Delta\nTData\n patch)\n        {\n            return this.UpdateEntityAsync(patch, id);\n        }\n\n        public override SingleResult\nTData\n Lookup(string id)\n        {\n            return this.LookupEntity(model =\n model.Id == id);\n        }\n\n        protected override void SetOriginalVersion(TModel model, byte[] version)\n        {            \n            this.Context.Entry(model).OriginalValues[\nVersion\n] = version;\n        }\n    }\n}\n\n\n\n\nThe primary thing that the \nDefaultMappedEntityDomainManager\n does that the original doesn't is in the \nSetOriginalVersion\n\nmethod.  This causes the model to be updated with a new version, allowing for conflict detection in the domain manager.\n\n\nIf we move now to the models on the mobile client, we see some fairly standard models:\n\n\npublic class Customer\n{\n    public string Id { get; set; }\n    public string FullName { get; set; }\n    public string Address { get; set; \n    public decimal? Latitude { get; set; }\n    public decimal? Longitude { get; set; }\n}\n\npublic class Equipment\n{\n    public string Id { get; set; }\n    public string Name { get; set; }\n    public string Asset { get; set; }\n    public string Description { get; set; }\n}\n\npublic class Job\n{\n    public const string CompleteStatus = \nCompleted\n;\n    public const string InProgressStatus = \nOn Site\n;\n    public const string PendingStatus = \nNot Started\n;\n\n    public string Id { get; set; }\n    public string AgentId { get; set; }\n    public DateTimeOffset? StartTime { get; set; }\n    public DateTimeOffSet? EndTime { get; set; }\n    public string Status { get; set; }\n    public string Description { get; set; }\n\n    public Customer Customer { get; set; }\n    public List\nEquipment\n Equipments { get; set; }\n\n    [Version]\n    public string Version { get; set; }\n}\n\n\n\n\nIn this case, we only need to sync the Job table, so we can define it in the \nInitializeAsync()\n method on the client:\n\n\npublic async Task InitializeAsync()\n{\n        var store = new MobileServiceSQLiteStore(\nlocaldata.db\n);\n        store.DefineTable\nJob\n();\n        await MobileService.SyncContext.InitializeAsync(store);\n}\n\n\n\n\nYou can use \nGetSyncTable\nJob\n()\n to get a reference to the table and deal with it as you normally would.  I'd expect in this\ncase that the Customer and Equipment would be handled elsewhere - maybe a separate web application that customer service\nagents use, for example.\n\n\nSo, what are the caveats?  The first is that the Job, Customer and Equipment data all comes down as one record.  This has a \nside effect of ensuring that the Customer and Equipment data is read-only.  You can only update the information in the Job\ntable.  This is also a very time consuming process to set up properly.  Automapper is known as a fairly picky piece of \nsoftware to integrate, so extra time must be allotted to make it work correctly.\n\n\nIn the end, I prefer handling tables individually and handling relationship management on the mobile client manually.  This\ncauses more code on the mobile client but makes the server much simpler by avoiding most of the complexity of relationships.\n\n\nNoSQL Storage with the StorageDomainManager\n\n\nWhat if you don't want to use a SQL backend for your service?  Relationships between entities are not that important in the mobile client and Azure Table Storage costs significantly less than SQL Azure.  There are always trade-offs between various storage providers.  A Domain Manager enables you to swap out the storage for one of your own choosing.  Azure Mobile Apps provides a domain manager for Azure Table Storage.  Azure Table Storage is Microsoft's NOSQL key/attribute store.  It has a schemaless design, which (at least theoretically) enables you to adapt your data models as the application evolves without having to worry about the schema.\n\n\nTo see this in action, let's rework the existing data store (which has tags and todoitems as tables) to use Table Storage.  First up, we need to set up a suitable environment.  This involves:\n\n\n\n\nCreate a Resource Group\n\n\nCreate an Azure App Service\n\n\nSet up authentication on the Azure App Service\n\n\nCreate a Storage Account\n\n\nLink the Storage Account to the Azure App Service.\n\n\n\n\nWe've already covered the first three items in previous chapters.  The important element here is that we do not create a SQL database.  We are going to be using Table Storage instead so we don't need it.  To create a Storage Account:\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\nClick the big \n+ NEW\n button in the top left corner.\n\n\nClick \nData + Storage\n, then \nStorage account\n.\n\n\nFill in the form:\n\n\nThe name can only contain letters and numbers and must be unique.  A GUID without the dashes is a good choice.\n\n\nThe \nDeployment model\n should be set to \nResource manager\n.\n\n\nThe \nAccount kind\n should be set to \nGeneral purpose\n.\n\n\nThe \nPerformance\n should be set to \nStandard\n for this example.\n\n\nThe \nReplication\n should be set to \nLocally-redundant storage (LRS)\n.\n\n\nSet the \nResource group\n to your existing resource group.\n\n\nSet the \nLocation\n to the same location as your App Service.\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nJust like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. We have selected the slowest performance and least redundant options here to keep the cost down on your service.\n\n\n\n\nWarn\n\n\nThere is no \"free\" option for Azure Storage.  You pay by the kilobyte depending on the performance and redundancy selected.\n\n\n\n\nOnce the Azure Storage account is deployed, you can link the storage account to your App Service:\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick  \nData Connections\n under the \nMOBILE\n section in the settings menu.\n\n\nClick \n+ ADD\n\n\nIn the \nAdd data connection\n blade:\n\n\nSet the Type to \nStorage\n.\n\n\nClick the \nStorage\n link.\n\n\nIn the \nStorage Account\n selector, click the storage account you just created.\n\n\nClick the \nConnection string\n.\n\n\nIn the \nConnection string\n selector, make a note of the \nName\n field.\n\n\nClick \nOK\n.\n\n\nClick \nOK\n to close the \nAdd data connection\n blade.\n\n\n\n\n\n\n\n\nClick on the \nApplication Settings\n menu option, then scroll down to the \nConnection Strings\n section.  Note that the portal has created the connection string as an App Setting for you with the right value:\n\n\nDefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=\nkey1\n\n\n\n\n\nThe key is the access key for the storage.  When a storage account is created, two keys are also created.  If you re-generate the storage access keys, remember to update your connection string.  By default, the connection string is called \nMS_AzureStorageAccountConnectionString\n and we will use that throughout.\n\n\nNow that our resources are set up, let's look at the Backend project.  This started off as a standard Azure Mobile Apps template.  The template assumes you are going to use SQL Azure, so there is quite a bit of work to convert the provided template to use Azure Table Storage.  Let's start with the \nApp_Start\\Startup.MobileApp.cs\n file.  There is no Entity Framework, so that needs to be stripped out:\n\n\nusing System.Configuration;\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Owin;\n\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            HttpConfiguration config = new HttpConfiguration();\n\n            new MobileAppConfiguration()\n                .AddTables()\n                .ApplyTo(config);\n\n            app.UseWebApi(config);\n        }\n    }\n}\n\n\n\n\nWe've made three changes:\n\n\n\n\nWe've removed the database seeding.\n\n\nWe've removed the database initializer.\n\n\nWe've changed \nAddTablesWithEntityFramework()\n to \nAddTables()\n.\n\n\n\n\nThere is extra work needed with Entity Framework.  Since we aren't using it, we don't need the additional work.  We do, however, need to create the ASP.NET routes to the table controllers.\n\n\n\n\nTip\n\n\nYou must add the \nMicrosoft.Azure.Mobile.Server.Storage\n package from NuGet. \n\n\n\n\nLet's move onto the DataObjects.  These are very similar:\n\n\nnamespace Backend.DataObjects\n{\n    public class TodoItem : StorageData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}\n\n\n\n\nEach storage implementation will likely need their own implementation of the \nITableData\n interface.  The \nStorageData\n class performs the same duties as the \nEntityData\n class for Entity Framework based backends.\n\n\n\n\nTip\n\n\nYou can remove the \nModels\n directory and the \nDbContext\n for the project.  These are only needed when working with Entity Framework.\n\n\n\n\nThe Azure Table Storage SDK is completely async driven.  Fortunately, the domain manager specification (codified in the definition of \nIDomainManager\n) allows both async and synchronous usage.  This does require a change to our controller:\n\n\nusing System.Collections.Generic;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing System.Web.Http.OData.Query;\nusing Backend.DataObjects;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        const string connectionString = \nMS_AzureStorageAccountConnectionString\n;\n        const string tableName = \nTodoItem\n;\n\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            DomainManager = new StorageDomainManager\nTodoItem\n(connectionString, tableName, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/TodoItem\n        public async Task\nIEnumerable\nTodoItem\n GetAllTodoItemsAsync(ODataQueryOptions query)\n        {\n            return await QueryAsync(query);\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task\nSingleResult\nTodoItem\n GetTodoItemAsync(string id)\n        {\n            return await LookupAsync(id);\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task\nTodoItem\n PatchTodoItemAsync(string id, Delta\nTodoItem\n patch)\n        {\n            return await UpdateAsync(id, patch);\n        }\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItemAsync(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task DeleteTodoItemAsync(string id)\n        {\n            await DeleteAsync(id);\n        }\n    }\n}\n\n\n\n\nNote how we instantiate the storage domain controller.  This requires a connection string and the name of the table.  We have created the connection string in the portal, but we have not exposed that connection string to our ASP.NET application. We need to edit the \nWeb.config\n file as well:\n\n\nconnectionStrings\n\n    \nadd name=\nMS_AzureStorageAccountConnectionString\n connectionString=\nUseDevelopmentStorage=true\n/\n\n\n/connectionStrings\n\n\n\n\n\nThis will be overwritten by the connection string in the portal.  If you are running the service locally, you can use this\nsetting with the Azure Storage Emulator.  If you don't add this line to the \nWeb.config\n, you will not be able to run this\nserver locally.\n\n\nThis backend can now be published and we can work with Postman to test it out.  For instance, let's try adding a simple test\nof getting some data:\n\n\n\n\nThis is exactly the same response as we got when we don't have any data from the Entity Framework version.  Let's add a \nrecord:\n\n\n\n\nThere are a couple of things to note here.  Firstly, the Id must be specified.  It also must be of a specific form.  There are two numbers.  The first is a partition key and the second is a row key.  Tables are partitioned to support load balancing across storage notes.  It can be anything you wish it to be.  Similarly, the row key is unique within a partition.  We can use this information to generate a suitable Id if one is not provided:\n\n\n// POST tables/TodoItem\npublic async Task\nIHttpActionResult\n PostTodoItemAsync(TodoItem item)\n{\n    if (item.Id == null || item.Id == \n'',''\n)\n    {\n        item.Id = GenerateUniqueId();\n    }\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n}\n\nprivate string GenerateUniqueId()\n{\n    var partitionId = \n1\n;\n    var rowId = Guid.NewGuid().ToString(\nN\n);\n    return $\n'{partitionId}','{rowId}'\n;\n}\n\n\n\n\nThe value \n'',''\n is the default value of the Id column.  However, that is not good enough to get a successful store.  This code\ngenerates a unique identifier in the right format.  A single partition is reasonable for most applications.  If you intend on\nstoring massive amounts of data, the data partitioning scheme will require some consideration (just like any other NoSQL application).\n\n\n\n\nTip\n\n\nYou will need a copy of the \nGenerateUniqueId()\n method if you generate unique identifiers for your records within \nyour mobile client.  The partition key and row key are returned as part of the record.  \n\n\n\n\nYou can use the \nCloud Explorer\n if you wish to see the data stored in Azure Table Storage.  Expand the \nStorage Accounts\n node,\nthen expand the appropriate nodes: your storage account, \nTables\n, \nTodoItem\n.  You can open the table editor or delete the table\nfrom there.  \n\n\n\n\nWithin the table editor, you can right-click on any row to delete or edit values.  This will update the time stamp and etag, ensuring\nthat your mobile clients are updated as well.\n\n\nLimitations of the Azure Storage Domain Manager\n\n\nThere are, of course, caveats to working in Azure Mobile Apps with Azure Table Storage.   There are three major caveats that you should\nbe aware of.\n\n\n\n\nThere are no relationships possible with Azure Table Storage.\n\n\nOnly \n$filter\n, \n$top\n and \n$select\n are supported in the URI.\n\n\nOffline sync only supports flat objects.\n\n\n\n\nLet's take a look at each in turn.\n\n\nNo relationhips\n\n\nYou have to work at relationships and relationships between entities are severely restricted in Entity Framework.  However, they are\npossible.  Not so with Azure Table Storage.  The NoSQL store has no concept of a relationship of any description.  This is not a major\ncaveat since your mobile client similarly has no notion of relationships.  Just treat every table as distinct.\n\n\nLimited support for OData query options\n\n\nOnly \n$filter\n, \n$top\n and \n$select\n are \nsupported by the OData interface\n.  Since the Azure Table Storage Domain Manager passes the \nincoming OData query to the Storage driver intact, this limitation is passed on to the OData interface for Azure Mobile Apps.  Specifically, \nthis means paging is handled differently.  With the \nEntityDomainManager\n, paging was accomplished by using \n$skip\n and \n$top\n to get more\nrecords until zero records were returned.  With the \nStorageDomainManager\n, a \nLink\n header is returned when there are more records.\n\n\n\n\nThe \nLink\n header contains the URI that you need to retrieve to get the next page of the results.  This has implications for how you \nreceive more than 50 records.\n\n\nOffline sync only supports \"flat\" objects\n\n\nOne of the common reasons for using NoSQL stores is that you can store pretty much any document you wish.  You just have to have a JSON\nrepresentation of the object cross the wire.  If you have complex objects stored in Azure Table Storage, they won't be able to be stored\nin the offline cache.  The offline cache is based on SQLite and inherits the limitations of that resource.  In particular, this means no\ncomplex types.\n\n\nUsing a NoSQL store seems like a great idea.  However, the limitations of the platform make Azure Table Storage a poor choice for this\nparticular function.  The Azure Table Storage is ill-suited to the demands of a mobile client backend.\n\n\nOne of the great uses of the Azure Table Storage Domain Manager is to see how you can write your own domain manager.  The \ncode\n for\nthe domain manager (and the ITableData interface) is relatively simple since it passes through the OData query to Azure Storage.  This\nallows you to see what is truly involved in writing a domain manager.", 
            "title": "The Domain Manager"
        }, 
        {
            "location": "/chapter3/domainmgr/#the-domain-manager", 
            "text": "As a request comes in to the mobile backend, it is processed through several layers.  First, ASP.NET\nprocesses the request, handling things like Authentication and Authorization.  It is then processed\nthrough the  Microsoft.Web.Http.OData  controller, which compiles the requested query.  Then it is \npassed to the Domain Manager, which is responsible for converting the request into a response.  The\nresponse is then passed back up the stack to be finally given back to the mobile client.  The Domain Manager is a central part of this process.  It is a class that implements the  IDomainManager \ninterface:  namespace Microsoft.Azure.Mobile.Server.Tables\n{\n    public interface IDomainManager TData  where TData : class, ITableData\n    {\n        IQueryable TData  Query();\n        SingleResult TData  Lookup(string id);\n        Task IEnumerable TData  QueryAsync(ODataQueryOptions query);\n        Task SingleResult TData  LookupAsync(string id);\n        Task TData  InsertAsync(TData data);\n        Task TData  UpdateAsync(string id, Delta TData  patch);\n        Task TData  ReplaceAsync(string id, TData data);\n        Task bool  DeleteAsync(string id);\n    }\n}  This looks deceptively simple.  Just 8 methods.  In reality, this is anything but simple.  The major issue\nthat a prospective domain manager implementor has to grapple with is the translation of an  IQueryable  into\nsomething that the backend data source can understand.  Let's take a look at a couple of domain managers that solve specific problems that crop up from time to time\nduring development.  It should be noted that  NEITHER  of these domain managers are recommended as a generalized\nsolution.  They both have significant caveats to their use and you should understand those caveats before\nembarking on integrating them.", 
            "title": "The Domain Manager"
        }, 
        {
            "location": "/chapter3/domainmgr/#existing-table-relationships-with-the-mappedentitydomainmanager", 
            "text": "One of the key areas that is weak when using the default  EntityDomainManager  is handling existing tables.  The\ngenerally accepted method of dealing with relationships is through loose coupling and manual relationship management\nin the client. Relationships are core to the SQL database world and we sometimes want to project those relationships \ninto the mobile client, allowing the backend to preserve any relationships that have been configured while still \nusing the standard offline client capabilities.  If you have existing SQL relationships, you can use a combination\nof  AutoMapper  and the  MappedEntityDomainManager .    The  MappedEntityDomainManager  is an abstract  IDomainManager  implementation targetting SQL as the backend store where\nthere is not a 1:1 mapping between the data object (DTO) exposed through the TableController and the domain model managed \nby Entity Framework.  If there is a 1:1 mapping, use  EntityDomainManager .  The  MappedEntityDomainManager  uses  AutoMapper  to map between the DTO and the domain model.  It assumes that AutoMapper has already been initialized with \nappropriate mappings that map from DTO to domain model and from the domain model to the DTO.  Let's take a small example.  If I am producing an enterprise mobile app that field engineers can use - the ones\nthat, for example, visit your house to install cable.  I can define an Entity Framework model map as follows:  [Table( Customers )]\npublic class Customer\n{\n    public Customer()\n    {\n        this.Jobs = new HashSet Job ();\n    }\n\n    [StringLength(50)]\n    public string Id { get; set; }\n\n    [StringLength(50)]\n    public string FullName { get; set; }\n\n    [StringLength(250)]\n    public string Address { get; set; \n\n    public decimal? Latitude { get; set; }\n\n    public decimal? Longitude { get; set; }\n\n    public ICollection Job  Jobs { get; set; }\n}\n\n[Table( Equipment )]\npublic class Equipment : ITableData\n{\n    public Equipment()\n    {\n        this.Jobs = new HashSet Job ();\n    }\n\n    [StringLength(50)]\n    public string Name { get; set; }\n\n    [StringLength(28)]\n    public string Asset { get; set; }\n\n    [StringLength(250)]\n    public string Description { get; set; }\n\n    #region ITableData\n    public DateTimeOffset? CreatedAt { get; set; }\n    public bool Deleted { get; set; }\n\n    [DatabaseGenerated(DatabaseGeneratedOption.Computed)]\n    public DateTimeOffset? UpdatedAt { get; set; }\n\n    [Timestamp]\n    public byte[] Version { get; set; }\n    #endregion\n\n    public ICollection Job  Jobs { get; set; }\n}\n\n[Table( Jobs )]\npublic class Job : ITableData\n{\n    public Job()\n    {\n        this.Equipments = new HashSet Equipment ();\n    }\n\n    [StringLength(50)]\n    public string CustomerId { get; set; }\n\n    [StringLength(50)]\n    public string AgentId { get; set; }\n\n    public DateTimeOffset? StartTime { get; set; }\n\n    public DateTimeOffset? EndTime { get; set; }\n\n    [StringLength(50)]\n    public string Status { get; set; }\n\n    [StringLength(250)]\n    public string Description { get; set; }\n\n    #region ITableData\n    public DateTimeOffset? CreatedAt { get; set; }\n    public bool Deleted { get; set; }\n\n    [DatabaseGenerated(DatabaseGeneratedOption.Computed)]\n    public DateTimeOffset? UpdatedAt { get; set; }\n\n    [Timestamp]\n    public byte[] Version { get; set; }\n    #endregion\n\n    public ICollection Equipment  Equipments { get; set; }\n}  This is the representation of the tables within the database.  They don't have to map to what the client\nrequires.  We can wire up the relationships in the normal  Entity Framework way , within the  DbContext :  public partial class ExistingDbContext : DbContext\n{\n    public FieldDbContext() : base( name=MS_TableConnectionString )\n    {            \n    }\n\n    public virtual DbSet Customer  Customers { get; set; }\n    public virtual DbSet Equipment  Equipments { get; set; }\n    public virtual DbSet Job  Jobs { get; set; } \n\n    protected override void OnModelCreating(DbModelBuilder modelBuilder)\n    {            \n        modelBuilder.HasDefaultSchema( dbo );\n        modelBuilder.Conventions.Add(\n            new AttributeToColumnAnnotationConvention TableColumnAttribute, string (\n             ServiceTableColumn , (property, attributes) =  attributes.Single().ColumnType.ToString()));\n\n        modelBuilder.Entity Customer ().Property(e =  e.Address).IsUnicode(false);\n        modelBuilder.Entity Customer ().Property(e =  e.FullName).IsUnicode(false);\n        modelBuilder.Entity Customer ().Property(e =  e.Id).IsUnicode(false);\n        modelBuilder.Entity Customer ().Property(e =  e.Latitude).HasPrecision(9, 6);\n        modelBuilder.Entity Customer ().Property(e =  e.Longitude).HasPrecision(9, 6);\n\n        modelBuilder.Entity Equipment ().Property(e =  e.Asset).IsUnicode(false);\n        modelBuilder.Entity Equipment ().Property(e =  e.Description).IsUnicode(false);\n        modelBuilder.Entity Equipment ().Property(e =  e.Name).IsUnicode(false);\n        modelBuilder.Entity Equipment ().Property(e =  e.Id).IsUnicode(false);\n\n        modelBuilder.Entity Job ().Property(e =  e.CustomerId).IsUnicode(false);\n        modelBuilder.Entity Job ().Property(e =  e.AgentId).IsUnicode(false);\n        modelBuilder.Entity Job ().Property(e =  e.Id).IsUnicode(false);\n        modelBuilder.Entity Job ().Property(e =  e.Status).IsUnicode(false);\n        modelBuilder.Entity Job ().Property(e =  e.Description).IsUnicode(false);\n\n        modelBuilder.Entity Equipment ()\n            .HasMany(e =  e.Jobs)\n            .WithMany(e =  e.Equipments)\n            .Map(m =  m.ToTable( EquipmentIds ).MapLeftKey( EquipmentId ).MapRightKey( JobId ));\n    }\n}  We can see the relationship (a Many:Many relationship) at the end of the  modelBuilder  within\nthe DbContext.  The 1:Many and 1:1 relationships are handled within the models themselves, per the\nnormal Entity Framework methods. This is pure Entity Framework thus far - we have defined the\nstructure of the database.  To translate this into a mobile client, we need to define Data Transfer Objects.  These don't\nhave to be the same shape as the models that Entity Framework is using.  For example:  public class CustomerDTO\n{\n    public string FullName { get; set; }\n    public string Address { get; set; \n    public decimal? Latitude { get; set; }\n    public decimal? Longitude { get; set; }\n}\n\npublic class EquipmentDTO\n{\n    public string Name { get; set; }\n    public string Asset { get; set; }\n    public string Description { get; set; }\n}\n\npublic class JobDTO : EntityData\n{\n    public string AgentId { get; set; }\n    public DateTimeOffset? StartTime { get; set; }\n    public DateTimeOffset? EndTime { get; set; }\n    public string Status { get; set; }\n    public string Description { get; set; }  \n\n    public virtual CustomerDTO Customer { get; set; }\n    public virtual List EquipmentDTO  Equipments { get; set; }\n}  Note that the DTOs are similar, but definitely not the same.  They don't have the same relationships between\nthe records, for example.   MappedEntityDomainManager  requires that AutoMapper is already configured and\ninitialized, so that's the next step.  Set up an AutoMapper configuration in the  App_Start  directory:  using AutoMapper;\nusing FieldEngineer.Service.DataObjects;\nusing FieldEngineer.Service.Models;\n\nnamespace FieldEngineer.Service\n{\n    public class AutomapperConfiguration\n    {\n        public static void CreateMapping(IConfiguration cfg)\n        {\n            // Apply some name changes from the entity to the DTO\n            cfg.CreateMap Job, JobDTO ()                \n                .ForMember(jobDTO =  jobDTO.Equipments, map =  map.MapFrom(job =  job.Equipments));\n\n            // For incoming requests, ignore the relationships\n            cfg.CreateMap JobDTO, Job ()                                            \n                .ForMember(job =  job.Customer, map =  map.Ignore())\n                .ForMember(job =  job.Equipments, map =  map.Ignore());\n\n            cfg.CreateMap Customer, CustomerDTO ();            \n            cfg.CreateMap Equipment, EquipmentDTO ();\n        }\n    }\n}  You will also need to initialize the AutoMapper - this can be done where you also configure the Azure Mobile Apps:  using System;\nusing System.Web.Http;\nusing AutoMapper;\nusing Microsoft.WindowsAzure.Mobile.Service;\n\nnamespace FieldEngineer.Service\n{\n    public static class WebApiConfig\n    {\n        public static void Register()\n        {\n            // Use this class to set configuration options for your mobile service\n            ConfigOptions options = new ConfigOptions();\n\n            // Use this class to set WebAPI configuration options\n            HttpConfiguration config = ServiceConfig.Initialize(new ConfigBuilder(options));\n\n            // To display errors in the browser during development, uncomment the following\n            // line. Comment it out again when you deploy your service for production use.\n            config.IncludeErrorDetailPolicy = IncludeErrorDetailPolicy.Always;\n\n            // This is the line that initializes AutoMapper\n            Mapper.Initialize(cfg =  { AutomapperConfiguration.CreateMapping(cfg); });                                \n        }\n    }\n}  Finally, we can create a controller that allows the receipt and update of jobs:  namespace FieldEngineer.Service.Controllers\n{\n    [Authorize]  \n    public class JobController : TableController JobDTO \n    {      \n        private FieldDbContext context;        \n\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            this.context = new FieldDbContext();\n\n            this.DomainManager = new DefaultMappedEntityDomainManager JobDTO,Job (this.context, Request, Services);            \n        }\n\n        [ExpandProperty( Customer )]\n        [ExpandProperty( Equipments )]\n        public async Task IQueryable JobDTO  GetAllJobs()\n        {                        \n            var jobs = this.context.Jobs\n                .Include( Customer )\n                .Include( Equipments )\n                .Project().To JobDTO ();            \n            return jobs;\n        }\n\n        [ExpandProperty( Customer )]\n        [ExpandProperty( Equipments )]\n        public SingleResult JobDTO  GetJob(string id)\n        {\n            return this.Lookup(id);\n        }\n\n        public async Task JobDTO  PatchJob(string id, Delta JobDTO  patch)\n        {\n            return await this.UpdateAsync(id, patch);                   \n        }        \n    }\n}  We are using  [ExpandProperty]  to expand the Customer and Equipment data so that it is transferred with the Job object.\nThe  MappedEntityDomainManager  is an abstract type, so we have to create a concrete implementation.  Fortunately, most\nof the work is done for us.  There are already concrete versions of most of the methods we require (like insert, delete\nand lookup). The  MappedEntityDomainManager  needs help to deal with replacements nor optimistic concurrency - features \nwe want.  We can use the  DefaultMappedEntityDomainManager to handle this for us:  namespace FieldEngineerLite.Service.Helpers\n{\n    public class DefaultMappedEntityDomainManager TData, TModel \n            : MappedEntityDomainManager TData, TModel \n        where TData : class, ITableData\n        where TModel : class, ITableData\n    {\n        public DefaultMappedEntityDomainManager(DbContext context, HttpRequestMessage request, ApiServices services)\n            : base(context, request, services)\n        {            \n        }\n\n        public override Task bool  DeleteAsync(string id)\n        {\n            return this.DeleteItemAsync(id);\n        }\n\n        public override Task TData  UpdateAsync(string id, Delta TData  patch)\n        {\n            return this.UpdateEntityAsync(patch, id);\n        }\n\n        public override SingleResult TData  Lookup(string id)\n        {\n            return this.LookupEntity(model =  model.Id == id);\n        }\n\n        protected override void SetOriginalVersion(TModel model, byte[] version)\n        {            \n            this.Context.Entry(model).OriginalValues[ Version ] = version;\n        }\n    }\n}  The primary thing that the  DefaultMappedEntityDomainManager  does that the original doesn't is in the  SetOriginalVersion \nmethod.  This causes the model to be updated with a new version, allowing for conflict detection in the domain manager.  If we move now to the models on the mobile client, we see some fairly standard models:  public class Customer\n{\n    public string Id { get; set; }\n    public string FullName { get; set; }\n    public string Address { get; set; \n    public decimal? Latitude { get; set; }\n    public decimal? Longitude { get; set; }\n}\n\npublic class Equipment\n{\n    public string Id { get; set; }\n    public string Name { get; set; }\n    public string Asset { get; set; }\n    public string Description { get; set; }\n}\n\npublic class Job\n{\n    public const string CompleteStatus =  Completed ;\n    public const string InProgressStatus =  On Site ;\n    public const string PendingStatus =  Not Started ;\n\n    public string Id { get; set; }\n    public string AgentId { get; set; }\n    public DateTimeOffset? StartTime { get; set; }\n    public DateTimeOffSet? EndTime { get; set; }\n    public string Status { get; set; }\n    public string Description { get; set; }\n\n    public Customer Customer { get; set; }\n    public List Equipment  Equipments { get; set; }\n\n    [Version]\n    public string Version { get; set; }\n}  In this case, we only need to sync the Job table, so we can define it in the  InitializeAsync()  method on the client:  public async Task InitializeAsync()\n{\n        var store = new MobileServiceSQLiteStore( localdata.db );\n        store.DefineTable Job ();\n        await MobileService.SyncContext.InitializeAsync(store);\n}  You can use  GetSyncTable Job ()  to get a reference to the table and deal with it as you normally would.  I'd expect in this\ncase that the Customer and Equipment would be handled elsewhere - maybe a separate web application that customer service\nagents use, for example.  So, what are the caveats?  The first is that the Job, Customer and Equipment data all comes down as one record.  This has a \nside effect of ensuring that the Customer and Equipment data is read-only.  You can only update the information in the Job\ntable.  This is also a very time consuming process to set up properly.  Automapper is known as a fairly picky piece of \nsoftware to integrate, so extra time must be allotted to make it work correctly.  In the end, I prefer handling tables individually and handling relationship management on the mobile client manually.  This\ncauses more code on the mobile client but makes the server much simpler by avoiding most of the complexity of relationships.", 
            "title": "Existing Table Relationships with the MappedEntityDomainManager"
        }, 
        {
            "location": "/chapter3/domainmgr/#nosql-storage-with-the-storagedomainmanager", 
            "text": "What if you don't want to use a SQL backend for your service?  Relationships between entities are not that important in the mobile client and Azure Table Storage costs significantly less than SQL Azure.  There are always trade-offs between various storage providers.  A Domain Manager enables you to swap out the storage for one of your own choosing.  Azure Mobile Apps provides a domain manager for Azure Table Storage.  Azure Table Storage is Microsoft's NOSQL key/attribute store.  It has a schemaless design, which (at least theoretically) enables you to adapt your data models as the application evolves without having to worry about the schema.  To see this in action, let's rework the existing data store (which has tags and todoitems as tables) to use Table Storage.  First up, we need to set up a suitable environment.  This involves:   Create a Resource Group  Create an Azure App Service  Set up authentication on the Azure App Service  Create a Storage Account  Link the Storage Account to the Azure App Service.   We've already covered the first three items in previous chapters.  The important element here is that we do not create a SQL database.  We are going to be using Table Storage instead so we don't need it.  To create a Storage Account:   Log on to the  Azure portal .  Click the big  + NEW  button in the top left corner.  Click  Data + Storage , then  Storage account .  Fill in the form:  The name can only contain letters and numbers and must be unique.  A GUID without the dashes is a good choice.  The  Deployment model  should be set to  Resource manager .  The  Account kind  should be set to  General purpose .  The  Performance  should be set to  Standard  for this example.  The  Replication  should be set to  Locally-redundant storage (LRS) .  Set the  Resource group  to your existing resource group.  Set the  Location  to the same location as your App Service.    Click  Create .   Just like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. We have selected the slowest performance and least redundant options here to keep the cost down on your service.   Warn  There is no \"free\" option for Azure Storage.  You pay by the kilobyte depending on the performance and redundancy selected.   Once the Azure Storage account is deployed, you can link the storage account to your App Service:   Open your App Service in the  Azure portal .  Click   Data Connections  under the  MOBILE  section in the settings menu.  Click  + ADD  In the  Add data connection  blade:  Set the Type to  Storage .  Click the  Storage  link.  In the  Storage Account  selector, click the storage account you just created.  Click the  Connection string .  In the  Connection string  selector, make a note of the  Name  field.  Click  OK .  Click  OK  to close the  Add data connection  blade.     Click on the  Application Settings  menu option, then scroll down to the  Connection Strings  section.  Note that the portal has created the connection string as an App Setting for you with the right value:  DefaultEndpointsProtocol=https;AccountName=thebook;AccountKey= key1   The key is the access key for the storage.  When a storage account is created, two keys are also created.  If you re-generate the storage access keys, remember to update your connection string.  By default, the connection string is called  MS_AzureStorageAccountConnectionString  and we will use that throughout.  Now that our resources are set up, let's look at the Backend project.  This started off as a standard Azure Mobile Apps template.  The template assumes you are going to use SQL Azure, so there is quite a bit of work to convert the provided template to use Azure Table Storage.  Let's start with the  App_Start\\Startup.MobileApp.cs  file.  There is no Entity Framework, so that needs to be stripped out:  using System.Configuration;\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server;\nusing Microsoft.Azure.Mobile.Server.Authentication;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Owin;\n\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public static void ConfigureMobileApp(IAppBuilder app)\n        {\n            HttpConfiguration config = new HttpConfiguration();\n\n            new MobileAppConfiguration()\n                .AddTables()\n                .ApplyTo(config);\n\n            app.UseWebApi(config);\n        }\n    }\n}  We've made three changes:   We've removed the database seeding.  We've removed the database initializer.  We've changed  AddTablesWithEntityFramework()  to  AddTables() .   There is extra work needed with Entity Framework.  Since we aren't using it, we don't need the additional work.  We do, however, need to create the ASP.NET routes to the table controllers.   Tip  You must add the  Microsoft.Azure.Mobile.Server.Storage  package from NuGet.    Let's move onto the DataObjects.  These are very similar:  namespace Backend.DataObjects\n{\n    public class TodoItem : StorageData\n    {\n        public string Text { get; set; }\n        public bool Complete { get; set; }\n    }\n}  Each storage implementation will likely need their own implementation of the  ITableData  interface.  The  StorageData  class performs the same duties as the  EntityData  class for Entity Framework based backends.   Tip  You can remove the  Models  directory and the  DbContext  for the project.  These are only needed when working with Entity Framework.   The Azure Table Storage SDK is completely async driven.  Fortunately, the domain manager specification (codified in the definition of  IDomainManager ) allows both async and synchronous usage.  This does require a change to our controller:  using System.Collections.Generic;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing System.Web.Http.OData.Query;\nusing Backend.DataObjects;\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController TodoItem \n    {\n        const string connectionString =  MS_AzureStorageAccountConnectionString ;\n        const string tableName =  TodoItem ;\n\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            DomainManager = new StorageDomainManager TodoItem (connectionString, tableName, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/TodoItem\n        public async Task IEnumerable TodoItem  GetAllTodoItemsAsync(ODataQueryOptions query)\n        {\n            return await QueryAsync(query);\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task SingleResult TodoItem  GetTodoItemAsync(string id)\n        {\n            return await LookupAsync(id);\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task TodoItem  PatchTodoItemAsync(string id, Delta TodoItem  patch)\n        {\n            return await UpdateAsync(id, patch);\n        }\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItemAsync(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task DeleteTodoItemAsync(string id)\n        {\n            await DeleteAsync(id);\n        }\n    }\n}  Note how we instantiate the storage domain controller.  This requires a connection string and the name of the table.  We have created the connection string in the portal, but we have not exposed that connection string to our ASP.NET application. We need to edit the  Web.config  file as well:  connectionStrings \n     add name= MS_AzureStorageAccountConnectionString  connectionString= UseDevelopmentStorage=true /  /connectionStrings   This will be overwritten by the connection string in the portal.  If you are running the service locally, you can use this\nsetting with the Azure Storage Emulator.  If you don't add this line to the  Web.config , you will not be able to run this\nserver locally.  This backend can now be published and we can work with Postman to test it out.  For instance, let's try adding a simple test\nof getting some data:   This is exactly the same response as we got when we don't have any data from the Entity Framework version.  Let's add a \nrecord:   There are a couple of things to note here.  Firstly, the Id must be specified.  It also must be of a specific form.  There are two numbers.  The first is a partition key and the second is a row key.  Tables are partitioned to support load balancing across storage notes.  It can be anything you wish it to be.  Similarly, the row key is unique within a partition.  We can use this information to generate a suitable Id if one is not provided:  // POST tables/TodoItem\npublic async Task IHttpActionResult  PostTodoItemAsync(TodoItem item)\n{\n    if (item.Id == null || item.Id ==  '','' )\n    {\n        item.Id = GenerateUniqueId();\n    }\n    TodoItem current = await InsertAsync(item);\n    return CreatedAtRoute( Tables , new { id = current.Id }, current);\n}\n\nprivate string GenerateUniqueId()\n{\n    var partitionId =  1 ;\n    var rowId = Guid.NewGuid().ToString( N );\n    return $ '{partitionId}','{rowId}' ;\n}  The value  '',''  is the default value of the Id column.  However, that is not good enough to get a successful store.  This code\ngenerates a unique identifier in the right format.  A single partition is reasonable for most applications.  If you intend on\nstoring massive amounts of data, the data partitioning scheme will require some consideration (just like any other NoSQL application).   Tip  You will need a copy of the  GenerateUniqueId()  method if you generate unique identifiers for your records within \nyour mobile client.  The partition key and row key are returned as part of the record.     You can use the  Cloud Explorer  if you wish to see the data stored in Azure Table Storage.  Expand the  Storage Accounts  node,\nthen expand the appropriate nodes: your storage account,  Tables ,  TodoItem .  You can open the table editor or delete the table\nfrom there.     Within the table editor, you can right-click on any row to delete or edit values.  This will update the time stamp and etag, ensuring\nthat your mobile clients are updated as well.", 
            "title": "NoSQL Storage with the StorageDomainManager"
        }, 
        {
            "location": "/chapter3/domainmgr/#limitations-of-the-azure-storage-domain-manager", 
            "text": "There are, of course, caveats to working in Azure Mobile Apps with Azure Table Storage.   There are three major caveats that you should\nbe aware of.   There are no relationships possible with Azure Table Storage.  Only  $filter ,  $top  and  $select  are supported in the URI.  Offline sync only supports flat objects.   Let's take a look at each in turn.  No relationhips  You have to work at relationships and relationships between entities are severely restricted in Entity Framework.  However, they are\npossible.  Not so with Azure Table Storage.  The NoSQL store has no concept of a relationship of any description.  This is not a major\ncaveat since your mobile client similarly has no notion of relationships.  Just treat every table as distinct.  Limited support for OData query options  Only  $filter ,  $top  and  $select  are  supported by the OData interface .  Since the Azure Table Storage Domain Manager passes the \nincoming OData query to the Storage driver intact, this limitation is passed on to the OData interface for Azure Mobile Apps.  Specifically, \nthis means paging is handled differently.  With the  EntityDomainManager , paging was accomplished by using  $skip  and  $top  to get more\nrecords until zero records were returned.  With the  StorageDomainManager , a  Link  header is returned when there are more records.   The  Link  header contains the URI that you need to retrieve to get the next page of the results.  This has implications for how you \nreceive more than 50 records.  Offline sync only supports \"flat\" objects  One of the common reasons for using NoSQL stores is that you can store pretty much any document you wish.  You just have to have a JSON\nrepresentation of the object cross the wire.  If you have complex objects stored in Azure Table Storage, they won't be able to be stored\nin the offline cache.  The offline cache is based on SQLite and inherits the limitations of that resource.  In particular, this means no\ncomplex types.  Using a NoSQL store seems like a great idea.  However, the limitations of the platform make Azure Table Storage a poor choice for this\nparticular function.  The Azure Table Storage is ill-suited to the demands of a mobile client backend.  One of the great uses of the Azure Table Storage Domain Manager is to see how you can write your own domain manager.  The  code  for\nthe domain manager (and the ITableData interface) is relatively simple since it passes through the OData query to Azure Storage.  This\nallows you to see what is truly involved in writing a domain manager.", 
            "title": "Limitations of the Azure Storage Domain Manager"
        }, 
        {
            "location": "/chapter4/options/", 
            "text": "Server Side Code\n\n\nAt some point during your mobile client development, you will need to do something a little outside simple\ndata access.  We have already seen an example of this in custom authentication.  It may be that you want to\nkick off a workflow, re-size an image using server-side resources, push a notification to another user, or\ndo a complex transaction on the database.\n\n\nWhatever the reason, that time is when you want to use server side code.  The aim is simple enough.  On a\ntrigger, execute some code and do something with the result.  The trigger can be as simple as a HTTP request\nfrom your client, but could also be in response to a timer, or because something in your environment\nhappened.  The result can be sent back to the user, placed in storage, or updated in the database.  There\nreally are no rules when it comes to server side code.\n\n\nThere are, however, options for running server side code.\n\n\nClient Processing with WebAPIs and Custom APIs\n\n\nThe first of the set is the venerable \nASP.NET WebAPI\n.  Firstly, configure the ASP.NET application to allow\nattribute-based routing.  This is done in your \nStartup.MobileApp.cs\n file with this line:\n\n\n    config.MapHttpAttributeRoutes();\n\n\n\n\nAny WebAPI that you provide must be preceded by a \n[Route]\n attribute.  We saw an example of this in the\n\nCustom Authentication\n section.  In custom authentication, we were setting up an endpoint that allows\nus to validate a login request.  The attribute looked like this:\n\n\nnamespace Backend.Controllers\n{\n    [Route(\n.auth/login/custom\n)]\n    public class CustomAuthController : ApiController\n    {\n        [HttpPost]\n        public async Task\nIHttpActionResult\n Post([FromBody] UserInfo body)\n        {\n            ...\n        }\n    }\n}\n\n\n\n\nThe ASP.NET WebAPI comes with a bunch of capabilities:\n\n\n\n\nYou can use your regular ASP.NET WebAPI programming techniques.\n\n\nYou can use Entity Framework for adjusting the database.\n\n\nYou can use the \n[Authorize]\n attribute for controlling access.\n\n\n\n\nWith ASP.NET WebAPI, you are responsible for absolutely everything.  This option is great for providing\nan endpoint to your mobile client that doesn't need anything special.\n\n\nAs flexible as the ASP.NET WebAPI is, most of the time you just want to do something and get the result\nreturned.  The Custom API feature of the Azure Mobile Apps SDK is a great option because it does a lot\nof the scaffolding for you.  With a Custom API:\n\n\n\n\nYour API appears under '/api' - no exceptions.\n\n\nThe server enforces the ZUMO-API-VERSION.\n\n\nThe server emits a X-ZUMO-SERVER-VERSION header in the response.\n\n\n\n\nYou can be sure, for example, that a random web crawler is not going to call your Custom API - the web crawler\nis not going to provide the \nZUMO-API-VERSION\n header, so your code would never be touched.  The Azure\nMobile Apps Client SDK also includes a routine that assumes a Custom API.  For example, let's say you\ncreate a \nValuesController\n, then you can call this from your mobile client with the following code:\n\n\nvar result = client.InvokeApiAsync\nResultType\n('Values');\n\n\n\n\nUsing \nInvokeApiAsync\n is a good alternative because it provides the authentication automatically for you\nand uses any \nDelegatingHandler\n classes you have configured.\n\n\n\n\nTip\n\n\nYou can still use \nInvokeApiAsync()\n with an ASP.NET WebAPI.\n\n\n\n\nBackground Processing with WebJobs \n Azure Functions\n\n\nThe ASP.NET WebAPI and the Custom API provide HTTP endpoints for your mobile clients to interact with.  \nWebJobs\n\nand \nAzure Functions\n, by comparison, are primed for background tasks.  Things that WebJobs and Azure Functions\nare good at:\n\n\n\n\nImage processing or other CPU-intensive work.\n\n\nQueue processing.\n\n\nRSS aggregation.\n\n\nFile and database management.\n\n\n\n\nWebJobs\n run in the context of your site.  They use the same set of virtual machines that your website uses and\nthey share resources with your site.  That means that running memory or CPU intensive jobs can affect your\nmobile backend.\n\n\nAzure Functions\n run in a separate project and run in \"dynamic compute\".  They don't run on your virtual machines.\nRather, they pick up compute power from wherever it is available.  The scaling and lifecycle of the Function\nis handled for you by the platform.  The downside is that you have to configure it as a separate project and\nyou pay for the executions separately.\n\n\nBest Practices\n\n\nFor each thing we have defined, we have two choices.  There are reasons to use each and every one of these\noptions.  So, which do you choose.  Here are my choices:\n\n\n\n\nUse a \nCustom API\n for basic web endpoints.\n\n\nUse a \nWebAPI\n for custom authentication and anything where you care about the shape of the API.\n\n\nUse \nWebJobs\n for clean-up tasks running on a schedule.\n\n\nUse \nFunctions\n for triggered batch processing, like image or queue processing.\n\n\n\n\nMost mobile applications will be able to work with the following:\n\n\n\n\nA single \nWebJob\n for cleaning up the database deleted records.\n\n\nA \nCustom API\n for doing transaction work or triggering a batch process.\n\n\nAn \nAzure Function\n for doing the batch processing.", 
            "title": "Options for Server Code"
        }, 
        {
            "location": "/chapter4/options/#server-side-code", 
            "text": "At some point during your mobile client development, you will need to do something a little outside simple\ndata access.  We have already seen an example of this in custom authentication.  It may be that you want to\nkick off a workflow, re-size an image using server-side resources, push a notification to another user, or\ndo a complex transaction on the database.  Whatever the reason, that time is when you want to use server side code.  The aim is simple enough.  On a\ntrigger, execute some code and do something with the result.  The trigger can be as simple as a HTTP request\nfrom your client, but could also be in response to a timer, or because something in your environment\nhappened.  The result can be sent back to the user, placed in storage, or updated in the database.  There\nreally are no rules when it comes to server side code.  There are, however, options for running server side code.", 
            "title": "Server Side Code"
        }, 
        {
            "location": "/chapter4/options/#client-processing-with-webapis-and-custom-apis", 
            "text": "The first of the set is the venerable  ASP.NET WebAPI .  Firstly, configure the ASP.NET application to allow\nattribute-based routing.  This is done in your  Startup.MobileApp.cs  file with this line:      config.MapHttpAttributeRoutes();  Any WebAPI that you provide must be preceded by a  [Route]  attribute.  We saw an example of this in the Custom Authentication  section.  In custom authentication, we were setting up an endpoint that allows\nus to validate a login request.  The attribute looked like this:  namespace Backend.Controllers\n{\n    [Route( .auth/login/custom )]\n    public class CustomAuthController : ApiController\n    {\n        [HttpPost]\n        public async Task IHttpActionResult  Post([FromBody] UserInfo body)\n        {\n            ...\n        }\n    }\n}  The ASP.NET WebAPI comes with a bunch of capabilities:   You can use your regular ASP.NET WebAPI programming techniques.  You can use Entity Framework for adjusting the database.  You can use the  [Authorize]  attribute for controlling access.   With ASP.NET WebAPI, you are responsible for absolutely everything.  This option is great for providing\nan endpoint to your mobile client that doesn't need anything special.  As flexible as the ASP.NET WebAPI is, most of the time you just want to do something and get the result\nreturned.  The Custom API feature of the Azure Mobile Apps SDK is a great option because it does a lot\nof the scaffolding for you.  With a Custom API:   Your API appears under '/api' - no exceptions.  The server enforces the ZUMO-API-VERSION.  The server emits a X-ZUMO-SERVER-VERSION header in the response.   You can be sure, for example, that a random web crawler is not going to call your Custom API - the web crawler\nis not going to provide the  ZUMO-API-VERSION  header, so your code would never be touched.  The Azure\nMobile Apps Client SDK also includes a routine that assumes a Custom API.  For example, let's say you\ncreate a  ValuesController , then you can call this from your mobile client with the following code:  var result = client.InvokeApiAsync ResultType ('Values');  Using  InvokeApiAsync  is a good alternative because it provides the authentication automatically for you\nand uses any  DelegatingHandler  classes you have configured.   Tip  You can still use  InvokeApiAsync()  with an ASP.NET WebAPI.", 
            "title": "Client Processing with WebAPIs and Custom APIs"
        }, 
        {
            "location": "/chapter4/options/#background-processing-with-webjobs-azure-functions", 
            "text": "The ASP.NET WebAPI and the Custom API provide HTTP endpoints for your mobile clients to interact with.   WebJobs \nand  Azure Functions , by comparison, are primed for background tasks.  Things that WebJobs and Azure Functions\nare good at:   Image processing or other CPU-intensive work.  Queue processing.  RSS aggregation.  File and database management.   WebJobs  run in the context of your site.  They use the same set of virtual machines that your website uses and\nthey share resources with your site.  That means that running memory or CPU intensive jobs can affect your\nmobile backend.  Azure Functions  run in a separate project and run in \"dynamic compute\".  They don't run on your virtual machines.\nRather, they pick up compute power from wherever it is available.  The scaling and lifecycle of the Function\nis handled for you by the platform.  The downside is that you have to configure it as a separate project and\nyou pay for the executions separately.", 
            "title": "Background Processing with WebJobs &amp; Azure Functions"
        }, 
        {
            "location": "/chapter4/options/#best-practices", 
            "text": "For each thing we have defined, we have two choices.  There are reasons to use each and every one of these\noptions.  So, which do you choose.  Here are my choices:   Use a  Custom API  for basic web endpoints.  Use a  WebAPI  for custom authentication and anything where you care about the shape of the API.  Use  WebJobs  for clean-up tasks running on a schedule.  Use  Functions  for triggered batch processing, like image or queue processing.   Most mobile applications will be able to work with the following:   A single  WebJob  for cleaning up the database deleted records.  A  Custom API  for doing transaction work or triggering a batch process.  An  Azure Function  for doing the batch processing.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter4/custom/", 
            "text": "Custom HTTP Endpoints\n\n\nAzure Mobile Apps makes it really easy to develop basic APIs that can be used in mobile clients.  Most\ncustom APIs can be simply invoked.  Azure Mobile Apps takes care of most of the scaffolding for you. The\nserver SDK will:\n\n\n\n\nEnsure the \nZUMO-API-VERSION\n is present and valid.\n\n\nHandle serialization and deserialization of JSON.\n\n\nEnsure the API is given an appropriate URL.\n\n\n\n\nAll the custom APIs will appear under the \n/api\n endpoint.  For example, if you created a controller\ncalled \nFooController\n, it would be invoked by sending messages to \n/api/Foo\n.  This is case-insensitive,\nso you could also reference this API as \n/api/foo\n.\n\n\nConfiguring Custom APIs\n\n\nBefore anything happens, you must add the \nMapApiControllers()\n method to the \nMobileAppConfiguration()\n\ncall.  This is done in the \nConfigureMobileApp()\n method in \nApp_Start\\Startup.MobileApp.cs\n file:\n\n\n    new MobileAppConfiguration()\n        .AddTablesWithEntityFramework()     /* /tables endpoints */\n        .MapApiControllers()                /* /api endpoints */\n        .ApplyTo(config);\n\n\n\n\nThe \nMapApiControllers()\n extension method does the actual work of looking for custom APIs and mapping them\nonto the \n/api\n endpoint.\n\n\nCreating a Basic Custom API\n\n\nYou might remember that the original Azure Mobile Apps project within Visual Studio comes with a sample\ncustom API called the \nValuesController\n.  This controller did not do anything useful.  Let's re-create\nit from scratch.\n\n\n\n\nRight-click the \nControllers\n node in your backend project and use \nAdd\n -\n \nController...\n.\n\n\n\n\n\n\n\n\nSelect the \nAzure Mobile Apps Custom Controller\n, then click \nAdd\n.\n\n\nEnter the name for the controller, for example, \nValuesController\n.  Click \nAdd\n.\n\n\n\n\nThe new controller will be scaffolded for you.  When you are done, it looks like this:\n\n\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class ValuesController : ApiController\n    {\n        // GET api/Default\n        public string Get()\n        {\n            return \nHello from custom controller!\n;\n        }\n    }\n}\n\n\n\n\nIf you have not done anything that requires a backend, then you can press F5 to run the backend and use\nPostman to interact with your new custom API:\n\n\n\n\nWe still have to submit the \nZUMO-API-VERSION\n header for this to work.  Whatever my method returns will\nbe returned as JSON.  This one is not exactly exciting.  One of the things I do quite often is provide\na configuration endpoint called \n/api/config\n which returns a JSON object that I can use to configure the\nmobile client.\n\n\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing System.Collections.Generic;\nusing System;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class ConfigController : ApiController\n    {\n        private ConfigViewModel configuration;\n\n        public ConfigController()\n        {\n            Dictionary\nstring, ProviderInformation\n providers = new Dictionary\nstring, ProviderInformation\n();\n\n            AddToProviders(providers, \naad\n, \nMOBILE_AAD_CLIENT_ID\n);\n            AddToProviders(providers, \nfacebook\n, \nMOBILE_FB_CLIENT_ID\n);\n            AddToProviders(providers, \ngoogle\n, \nMOBILE_GOOGLE_CLIENT_ID\n);\n            AddToProviders(providers, \nmicrosoftaccount\n, \nMOBILE_MSA_CLIENT_ID\n);\n            AddToProviders(providers, \ntwitter\n, \nMOBILE_TWITTER_CLIENT_ID\n);\n\n            configuration = new ConfigViewModel\n            {\n                AuthProviders = providers\n            };\n        }\n\n        private void AddToProviders(Dictionary\nstring, ProviderInformation\n providers, string provider, string envVar)\n        {\n            string envVal = Environment.GetEnvironmentVariable(envVar);\n            if (envVal != null \n envVal?.Length \n 0)\n            {\n                providers.Add(provider, new ProviderInformation { ClientId = envVal });\n            }\n\n        }\n\n        [HttpGet]\n        public ConfigViewModel Get()\n        {\n            return configuration;\n        }\n    }\n\n    public class ProviderInformation\n    {\n        public string ClientId { get; set; }\n    }\n\n    public class ConfigViewModel\n    {\n        public Dictionary\nstring, ProviderInformation\n AuthProviders { get; set; }\n    }\n}\n\n\n\n\nThe constructor produces a \nConfigViewModel\n for me.  This describes the configuration object I want to send.  In\nthis case, I want to send the client ID for each authentication provider.  If the authentication provider is not\nconfigured, then the client ID is not sent.  I use the application settings to determine what is configured. The\nprimary idea behind this is to integrate all the client flows within my mobile client.  When the user wishes to\nlog in, they are presented with a menu of options and can pick which social provider they wish to use.  The\nclient-flow authentication libraries may use different client IDs than the ones that are configured into the authentication\nservice.  For example, AAD uses two client IDs - one for server-flow and one for client-flow.  As a result, this\ncontroller uses \nApplication Settings\n (which appear as environment variables to the backend) to set the\nclient IDs.  The result of calling this API from Postman looks like this:\n\n\n\n\n\n\nWarn\n\n\nOnly expose information that you would normally and reasonably embed in a mobile client.  Never transmit\nsecrets this way.  It is insecure and can put your entire authentication system at risk of hijack.\n\n\n\n\nYou can read this information using the Azure Mobile Apps Client SDK once you have a client reference, using\nthe same model classes:\n\n\nvar configuration = await client.InvokeAsync\nConfigViewModel\n(\nconfig\n, HttpMethod.Get, null);\n\n\n\n\nYou must specify a class that deserializes the JSON that is produced by your API.  If you use the same classes,\nthat is practically guaranteed.  The other methods in call are the HTTP Method (GET, POST, PATCH, DELETE, etc.)\nand the query parameters.\n\n\nHandling Parameters to a Custom API\n\n\nThe \n/api/config\n endpoint didn't require any information that is extra.  Sometimes, we need to provide\nextra information so that the right thing can be produced.  For example, consider the case of uploading\nor downloading a file to Azure Storage.  We may want to provide some extra information - the filename of\nthe file we want to upload and the permissions for the file.  Uploading and downloading files is discussed\nmore fully \nlater\n in the book and offers a fuller example of this concept.\n\n\nTo illustrate the concept clearly, let's create an API that adds two numbers together.  We would call\nthis API through HTTP like this: \nGET /api/addition?first=1\nsecond=2\n.  The first number gets added to\nthe second number and we will return the result.  If the first or the second number doesn't exist, we\nwant to produce a 400 Bad Request response rather than crashing the server.  Here is the code:\n\n\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing System.Net;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class AdditionController : ApiController\n    {\n        // GET api/Addition\n        public ResultViewModel Get(int? first, int? second)\n        {\n            if (first == null || second == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n            ResultViewModel results = new ResultViewModel\n            {\n                First = first.GetValueOrDefault(),\n                Second = second.GetValueOrDefault()\n            };\n            results.Result = results.First + results.Second;\n            return results;\n        }\n    }\n\n    public class ResultViewModel\n    {\n        public int First { get; set; }\n        public int Second { get; set; }\n        public int Result { get; set; }\n    }\n}\n\n\n\n\nIf you try this out, you will notice something rather odd.  Try doing the following URL: \n/api/addition?first=1\nsecond=2\n.\nYou will note it works as expected.  However, if you try doing the following URL: \n/api/addition?first=1\n, then you\nwill note that you get a \n404 Not Found\n.  This makes the API easy to write because you don't have to worry about\nyour code receiving bad input (most of the time).  However, you may not get the API surface that you want.  In this\ncase, I want to return a \n400 Bad Request\n instead of the normal 404 response.  I have to do a lot more work to\nsupport this case:\n\n\n    public class AdditionController : ApiController\n    {\n        // GET api/Addition\n        public ResultViewModel Get()\n        {\n            int? first = GetParameter(Request, \nfirst\n),\n                 second = GetParameter(Request, \nsecond\n);\n\n            ResultViewModel results = new ResultViewModel\n            {\n                First = first.GetValueOrDefault(),\n                Second = second.GetValueOrDefault()\n            };\n            results.Result = results.First + results.Second;\n            return results;\n        }\n\n        private int? GetParameter(HttpRequestMessage request, string name)\n        {\n            var queryParams = request.GetQueryNameValuePairs().Where(kv =\n kv.Key == name).ToList();\n            if (queryParams.Count == 0)\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n\n            int rv;\n            if (!Int32.TryParse(queryParams[0].Value, out rv))\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n            return rv;\n        }\n    }\n\n\n\n\nWhen our Get() routine does not take parameters, no particular pattern is required.  We can use the \nRequest\n\nobject to access the parameters we send.  In this case, the \nGetParameter()\n routine checks to see if there\nis a named parameter and converts it to an integer.  If the named parameter is not there or it is not numeric,\nthen a Bad Request response is sent.\n\n\nHandling POST Requests\n\n\nGET and DELETE requests take parameters on the URI.  These can be dealt with via the automatic conversion to\nmethod parameters or they can be handled via LINQ queries on the request object, as we observed in the prior\nsection.  POST requests, by contrast, allow you to submit a JSON body for processing.  This is useful when we\nwant to submit multiple JSON objects for processing.  For example, one of the common requirements we have is\nfor transactions.  When we want to submit two objects that are joined by a foreign key, we can submit them\nboth and construct a transaction in the backend with Entity Framework.\n\n\nLet's take an example.  We want to produce a music trakcing mobile app.  When we add an album to our music\ndatabase, we also want to add the tracks for that database.  This can be modeled with code first Entity\nFramework:\n\n\npublic class Track : EntityData\n{\n    public Track() {}\n\n    public string Title { get; set; }\n    public int Length { get; set; }\n\n    public virtual Album Album { get; set; }\n}\n\npublic class Album : EntityData\n{\n    public Album()\n    {\n        Tracks = new List\nTrack\n();\n    }\n\n    public string Title { get; set; }\n\n    public virtual ICollection\nTrack\n Tracks { get; set; }\n}\n\n\n\n\nThis will generate a foreign key relationship in the tables of our database.  However, as we learned in\n\nchapter 3\n, relationships are hard to implement and come with some serious caveats.  We could decouple\nthe tracks from the albums, but let's instead try to insert the data for an album all in one go.  We do\nthis by submitting the JSON for an Album to a custom controller:\n\n\n    [MobileAppController]\n    public class AlbumCustomController : ApiController\n    {\n        MobileServiceContext context;\n\n        public AlbumCustomController() : base()\n        {\n            context = new MobileServiceContext();\n        }\n\n        [HttpPost]\n        public async Task\nAlbumCustomResponse\n PostAsync([FromBody] Album newAlbum)\n        {\n            // Use a transaction to update the database\n            using (DbContextTransaction transaction = context.Database.BeginTransaction())\n            {\n                try\n                {\n                    context.Albums.Add(newAlbum);\n                    await context.SaveChangesAsync();\n                    transaction.Commit();\n                }\n                catch (Exception ex)\n                {\n                    transaction.Rollback();\n                }\n            }\n\n            // Now generate whatever output we want.\n            AlbumCustomResponse response = new AlbumCustomResponse\n            {\n                Status = 200\n            };\n            return response;\n        }\n    }\n\n\n\n\nIn this particular scenario, we would construct an album object on the mobile client side that directly corresponded\nto the JSON we want to push to the database, including the track information.  Let's take a look at a typical call:\n\n\nvar response = client.InvokeApiAsync\nAlbum,AlbumCustomResponse\n(\n    \nAlbumController\n,              // The name of the API\n    newAlbum,                       // The body of the POST\n    HttpMethod.Post,                // The HTTP Method\n    null,                           // Request Headers\n    null);                          // Parameters\n\n\n\n\nThere are \nquite a few signatures\n for the \nInvokeApiAsync\n()\n method.  This one sends an \nAlbum\n object as JSON as the\nbody of the request, and returns an \nAlbumCustomResponse\n, decoding the response as it goes.\n\n\nThe Downside of Custom APIs\n\n\nGiven that we can do transaction processing within a custom API, one might be forgiven for wondering why we don't\nuse custom APIs for processing data within our normalized SQL schema.  The problem, of course, is that custom APIs\n(including WebAPIs) can only be executed while connected to the Internet.  When the mobile device is offline, the\ncustom API cannot be executed, thus breaking our offline model.  Offline just doesn't mix with SQL relationships.\n\n\nWhen I first encountered this, I thought a great idea may be to instantiate a queue, much like the Operations Queue\nthat is used within the offline sync process.  When I want to queue up a transaction, I insert it into my own\noperations queue.  My early research used a light-weight queuing mechanism (\nDotNetMQ\n, for those interested) to\nimplement the queue.  Transactions were inserted into the queue at the appropriate time in the client.  During the\nsync process, the transactions were pushed, then the tables were pulled.  The problem is that the tables in the\noffline sync were not maintained until a pull, resulting in \"old data\".  If I updated the data in the offline cache\nas well, I produced many conflicts and inconsistent data within the offline cache.  In the end, I concluded that\nit was better to loosely couple the tables that were being used (as we discussed in \nchapter 3\n).", 
            "title": "Custom HTTP Endpoints"
        }, 
        {
            "location": "/chapter4/custom/#custom-http-endpoints", 
            "text": "Azure Mobile Apps makes it really easy to develop basic APIs that can be used in mobile clients.  Most\ncustom APIs can be simply invoked.  Azure Mobile Apps takes care of most of the scaffolding for you. The\nserver SDK will:   Ensure the  ZUMO-API-VERSION  is present and valid.  Handle serialization and deserialization of JSON.  Ensure the API is given an appropriate URL.   All the custom APIs will appear under the  /api  endpoint.  For example, if you created a controller\ncalled  FooController , it would be invoked by sending messages to  /api/Foo .  This is case-insensitive,\nso you could also reference this API as  /api/foo .", 
            "title": "Custom HTTP Endpoints"
        }, 
        {
            "location": "/chapter4/custom/#configuring-custom-apis", 
            "text": "Before anything happens, you must add the  MapApiControllers()  method to the  MobileAppConfiguration() \ncall.  This is done in the  ConfigureMobileApp()  method in  App_Start\\Startup.MobileApp.cs  file:      new MobileAppConfiguration()\n        .AddTablesWithEntityFramework()     /* /tables endpoints */\n        .MapApiControllers()                /* /api endpoints */\n        .ApplyTo(config);  The  MapApiControllers()  extension method does the actual work of looking for custom APIs and mapping them\nonto the  /api  endpoint.", 
            "title": "Configuring Custom APIs"
        }, 
        {
            "location": "/chapter4/custom/#creating-a-basic-custom-api", 
            "text": "You might remember that the original Azure Mobile Apps project within Visual Studio comes with a sample\ncustom API called the  ValuesController .  This controller did not do anything useful.  Let's re-create\nit from scratch.   Right-click the  Controllers  node in your backend project and use  Add  -   Controller... .     Select the  Azure Mobile Apps Custom Controller , then click  Add .  Enter the name for the controller, for example,  ValuesController .  Click  Add .   The new controller will be scaffolded for you.  When you are done, it looks like this:  using System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class ValuesController : ApiController\n    {\n        // GET api/Default\n        public string Get()\n        {\n            return  Hello from custom controller! ;\n        }\n    }\n}  If you have not done anything that requires a backend, then you can press F5 to run the backend and use\nPostman to interact with your new custom API:   We still have to submit the  ZUMO-API-VERSION  header for this to work.  Whatever my method returns will\nbe returned as JSON.  This one is not exactly exciting.  One of the things I do quite often is provide\na configuration endpoint called  /api/config  which returns a JSON object that I can use to configure the\nmobile client.  using System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing System.Collections.Generic;\nusing System;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class ConfigController : ApiController\n    {\n        private ConfigViewModel configuration;\n\n        public ConfigController()\n        {\n            Dictionary string, ProviderInformation  providers = new Dictionary string, ProviderInformation ();\n\n            AddToProviders(providers,  aad ,  MOBILE_AAD_CLIENT_ID );\n            AddToProviders(providers,  facebook ,  MOBILE_FB_CLIENT_ID );\n            AddToProviders(providers,  google ,  MOBILE_GOOGLE_CLIENT_ID );\n            AddToProviders(providers,  microsoftaccount ,  MOBILE_MSA_CLIENT_ID );\n            AddToProviders(providers,  twitter ,  MOBILE_TWITTER_CLIENT_ID );\n\n            configuration = new ConfigViewModel\n            {\n                AuthProviders = providers\n            };\n        }\n\n        private void AddToProviders(Dictionary string, ProviderInformation  providers, string provider, string envVar)\n        {\n            string envVal = Environment.GetEnvironmentVariable(envVar);\n            if (envVal != null   envVal?.Length   0)\n            {\n                providers.Add(provider, new ProviderInformation { ClientId = envVal });\n            }\n\n        }\n\n        [HttpGet]\n        public ConfigViewModel Get()\n        {\n            return configuration;\n        }\n    }\n\n    public class ProviderInformation\n    {\n        public string ClientId { get; set; }\n    }\n\n    public class ConfigViewModel\n    {\n        public Dictionary string, ProviderInformation  AuthProviders { get; set; }\n    }\n}  The constructor produces a  ConfigViewModel  for me.  This describes the configuration object I want to send.  In\nthis case, I want to send the client ID for each authentication provider.  If the authentication provider is not\nconfigured, then the client ID is not sent.  I use the application settings to determine what is configured. The\nprimary idea behind this is to integrate all the client flows within my mobile client.  When the user wishes to\nlog in, they are presented with a menu of options and can pick which social provider they wish to use.  The\nclient-flow authentication libraries may use different client IDs than the ones that are configured into the authentication\nservice.  For example, AAD uses two client IDs - one for server-flow and one for client-flow.  As a result, this\ncontroller uses  Application Settings  (which appear as environment variables to the backend) to set the\nclient IDs.  The result of calling this API from Postman looks like this:    Warn  Only expose information that you would normally and reasonably embed in a mobile client.  Never transmit\nsecrets this way.  It is insecure and can put your entire authentication system at risk of hijack.   You can read this information using the Azure Mobile Apps Client SDK once you have a client reference, using\nthe same model classes:  var configuration = await client.InvokeAsync ConfigViewModel ( config , HttpMethod.Get, null);  You must specify a class that deserializes the JSON that is produced by your API.  If you use the same classes,\nthat is practically guaranteed.  The other methods in call are the HTTP Method (GET, POST, PATCH, DELETE, etc.)\nand the query parameters.", 
            "title": "Creating a Basic Custom API"
        }, 
        {
            "location": "/chapter4/custom/#handling-parameters-to-a-custom-api", 
            "text": "The  /api/config  endpoint didn't require any information that is extra.  Sometimes, we need to provide\nextra information so that the right thing can be produced.  For example, consider the case of uploading\nor downloading a file to Azure Storage.  We may want to provide some extra information - the filename of\nthe file we want to upload and the permissions for the file.  Uploading and downloading files is discussed\nmore fully  later  in the book and offers a fuller example of this concept.  To illustrate the concept clearly, let's create an API that adds two numbers together.  We would call\nthis API through HTTP like this:  GET /api/addition?first=1 second=2 .  The first number gets added to\nthe second number and we will return the result.  If the first or the second number doesn't exist, we\nwant to produce a 400 Bad Request response rather than crashing the server.  Here is the code:  using System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing System.Net;\n\nnamespace Backend.Controllers\n{\n    [MobileAppController]\n    public class AdditionController : ApiController\n    {\n        // GET api/Addition\n        public ResultViewModel Get(int? first, int? second)\n        {\n            if (first == null || second == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n            ResultViewModel results = new ResultViewModel\n            {\n                First = first.GetValueOrDefault(),\n                Second = second.GetValueOrDefault()\n            };\n            results.Result = results.First + results.Second;\n            return results;\n        }\n    }\n\n    public class ResultViewModel\n    {\n        public int First { get; set; }\n        public int Second { get; set; }\n        public int Result { get; set; }\n    }\n}  If you try this out, you will notice something rather odd.  Try doing the following URL:  /api/addition?first=1 second=2 .\nYou will note it works as expected.  However, if you try doing the following URL:  /api/addition?first=1 , then you\nwill note that you get a  404 Not Found .  This makes the API easy to write because you don't have to worry about\nyour code receiving bad input (most of the time).  However, you may not get the API surface that you want.  In this\ncase, I want to return a  400 Bad Request  instead of the normal 404 response.  I have to do a lot more work to\nsupport this case:      public class AdditionController : ApiController\n    {\n        // GET api/Addition\n        public ResultViewModel Get()\n        {\n            int? first = GetParameter(Request,  first ),\n                 second = GetParameter(Request,  second );\n\n            ResultViewModel results = new ResultViewModel\n            {\n                First = first.GetValueOrDefault(),\n                Second = second.GetValueOrDefault()\n            };\n            results.Result = results.First + results.Second;\n            return results;\n        }\n\n        private int? GetParameter(HttpRequestMessage request, string name)\n        {\n            var queryParams = request.GetQueryNameValuePairs().Where(kv =  kv.Key == name).ToList();\n            if (queryParams.Count == 0)\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n\n            int rv;\n            if (!Int32.TryParse(queryParams[0].Value, out rv))\n            {\n                throw new HttpResponseException(HttpStatusCode.BadRequest);\n            }\n            return rv;\n        }\n    }  When our Get() routine does not take parameters, no particular pattern is required.  We can use the  Request \nobject to access the parameters we send.  In this case, the  GetParameter()  routine checks to see if there\nis a named parameter and converts it to an integer.  If the named parameter is not there or it is not numeric,\nthen a Bad Request response is sent.", 
            "title": "Handling Parameters to a Custom API"
        }, 
        {
            "location": "/chapter4/custom/#handling-post-requests", 
            "text": "GET and DELETE requests take parameters on the URI.  These can be dealt with via the automatic conversion to\nmethod parameters or they can be handled via LINQ queries on the request object, as we observed in the prior\nsection.  POST requests, by contrast, allow you to submit a JSON body for processing.  This is useful when we\nwant to submit multiple JSON objects for processing.  For example, one of the common requirements we have is\nfor transactions.  When we want to submit two objects that are joined by a foreign key, we can submit them\nboth and construct a transaction in the backend with Entity Framework.  Let's take an example.  We want to produce a music trakcing mobile app.  When we add an album to our music\ndatabase, we also want to add the tracks for that database.  This can be modeled with code first Entity\nFramework:  public class Track : EntityData\n{\n    public Track() {}\n\n    public string Title { get; set; }\n    public int Length { get; set; }\n\n    public virtual Album Album { get; set; }\n}\n\npublic class Album : EntityData\n{\n    public Album()\n    {\n        Tracks = new List Track ();\n    }\n\n    public string Title { get; set; }\n\n    public virtual ICollection Track  Tracks { get; set; }\n}  This will generate a foreign key relationship in the tables of our database.  However, as we learned in chapter 3 , relationships are hard to implement and come with some serious caveats.  We could decouple\nthe tracks from the albums, but let's instead try to insert the data for an album all in one go.  We do\nthis by submitting the JSON for an Album to a custom controller:      [MobileAppController]\n    public class AlbumCustomController : ApiController\n    {\n        MobileServiceContext context;\n\n        public AlbumCustomController() : base()\n        {\n            context = new MobileServiceContext();\n        }\n\n        [HttpPost]\n        public async Task AlbumCustomResponse  PostAsync([FromBody] Album newAlbum)\n        {\n            // Use a transaction to update the database\n            using (DbContextTransaction transaction = context.Database.BeginTransaction())\n            {\n                try\n                {\n                    context.Albums.Add(newAlbum);\n                    await context.SaveChangesAsync();\n                    transaction.Commit();\n                }\n                catch (Exception ex)\n                {\n                    transaction.Rollback();\n                }\n            }\n\n            // Now generate whatever output we want.\n            AlbumCustomResponse response = new AlbumCustomResponse\n            {\n                Status = 200\n            };\n            return response;\n        }\n    }  In this particular scenario, we would construct an album object on the mobile client side that directly corresponded\nto the JSON we want to push to the database, including the track information.  Let's take a look at a typical call:  var response = client.InvokeApiAsync Album,AlbumCustomResponse (\n     AlbumController ,              // The name of the API\n    newAlbum,                       // The body of the POST\n    HttpMethod.Post,                // The HTTP Method\n    null,                           // Request Headers\n    null);                          // Parameters  There are  quite a few signatures  for the  InvokeApiAsync ()  method.  This one sends an  Album  object as JSON as the\nbody of the request, and returns an  AlbumCustomResponse , decoding the response as it goes.", 
            "title": "Handling POST Requests"
        }, 
        {
            "location": "/chapter4/custom/#the-downside-of-custom-apis", 
            "text": "Given that we can do transaction processing within a custom API, one might be forgiven for wondering why we don't\nuse custom APIs for processing data within our normalized SQL schema.  The problem, of course, is that custom APIs\n(including WebAPIs) can only be executed while connected to the Internet.  When the mobile device is offline, the\ncustom API cannot be executed, thus breaking our offline model.  Offline just doesn't mix with SQL relationships.  When I first encountered this, I thought a great idea may be to instantiate a queue, much like the Operations Queue\nthat is used within the offline sync process.  When I want to queue up a transaction, I insert it into my own\noperations queue.  My early research used a light-weight queuing mechanism ( DotNetMQ , for those interested) to\nimplement the queue.  Transactions were inserted into the queue at the appropriate time in the client.  During the\nsync process, the transactions were pushed, then the tables were pulled.  The problem is that the tables in the\noffline sync were not maintained until a pull, resulting in \"old data\".  If I updated the data in the offline cache\nas well, I produced many conflicts and inconsistent data within the offline cache.  In the end, I concluded that\nit was better to loosely couple the tables that were being used (as we discussed in  chapter 3 ).", 
            "title": "The Downside of Custom APIs"
        }, 
        {
            "location": "/chapter4/webjobs/", 
            "text": "I'd like to think that my mobile backend is self-managing.   Just like the App Service it is running on, it cleans up\nafter itself, always works and never needs a backup or maintenance.  Alas, some things can't be encoded in a client to\nserver transaction.  For example, in \nchapter 3\n, we discussed offline synchronization and the need for soft delete.\nThe soft delete process just marks the records as deleted.  In a busy database, those deleted records still take up\nspace and resources during searches.  One of the maintenance tasks we need to do is to clean up the database.\n\n\nThere is almost always a need for \nbackend processing\n that is not related to the client-server communication.  That is\nwhere \nAzure Functions\n and \nWebJobs\n come in.  They handle the custom processing that is not initiated by a mobile\nclient.  There are several examples aside from the aforementioned soft delete cleanup:\n\n\n\n\nYou \nupload files\n and want to process them before letting other users download them.\n\n\nYou want to do \nsentiment analysis\n or other machine learning on incoming database records.\n\n\nYou need to do workflows like order fulfillment.\n\n\nYou need to handle data securely and cannot transmit it to the mobile client for processing.\n\n\n\n\nIn all these cases, the initiation may come from the mobile client or it may be scheduled.  However, the defining\ncharacteristic of these requirements is that the code must be run asynchronously (the mobile client is not waiting\nfor the result) and may take longer than the time for a normal request.\n\n\nYou can consider Azure Functions as \"WebJobs as a Service\".  WebJobs run on the same set of virtual machines that\nare running your mobile backend.  They have access to the same resources (like CPU and memory), so they can interfere\nwith the running of your app.  Azure Functions can run this way as well, but they really take off when running in\nDynamic Compute mode (which is the default).  In Dynamic Compute, they run in spare compute power and potentially\non a completely different set of virtual machines, so they don't interfere with your mobile backend.\n\n\nI recommend WebJobs for clean-up or maintenance tasks - things like cleaning up the database on a regular basis.\nEvent-driven tasks should be handled by Azure Functions.\n\n\nA Database Clean Up WebJob\n\n\nAs an example WebJob, let's implement the database clean-up process as a WebJob.  This will run on a regular\nbasis (say, once a day) and during processing, it will delete all records in our TodoItem table that are\ndeleted where the updatedAt field is older than 7 days.  The SQL command to run is this:\n\n\nDELETE FROM\n    [dbo].[TodoItems]\nWHERE\n    [deleted] = 1 AND [updatedAt] \n DATEADD(day, -7, SYSDATETIMEOFFSET())\nGO\n\n\n\n\nA WebJob is always a separate project from your mobile backend.  To create a WebJob:\n\n\n\n\nRight click the solution.\n\n\nChoose \nAdd\n -\n \nNew Item...\n.\n\n\n\n\nSearch for \nWebJob\n.\n\n\n\n\n\n\n\n\nSelect \nAzure WebJob\n with a \nVisual C#\n language.\n\n\n\n\nEnter a suitable name for the WebJob and click \nAdd\n.\n\n\n\n\nOnce the scaffolding has finished and the package restore is done, you will see a \nProgram.cs\n file.  A WebJob\nis just a console application running with the WebJobs SDK.  By default, the WebJob will wait around for a\ntrigger.  We don't have a trigger since we are going to run this code each and every time the scheduler runs\nit.  As a result, our code in Program.cs is relatively simple:\n\n\nusing System;\nusing System.Configuration;\nusing System.Data.SqlClient;\nusing System.Diagnostics;\n\nnamespace CleanupDatabaseWebJob\n{\n    class Program\n    {\n        static void Main()\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[\nMS_TableConnectionString\n].ConnectionString;\n\n            using (SqlConnection sqlConnection = new SqlConnection(connectionString))\n            {\n                using (SqlCommand sqlCommand = sqlConnection.CreateCommand())\n                {\n                    Console.WriteLine(\n[CleanupDatabaseWebJob] Initiating SQL Connection\n);\n                    sqlConnection.Open();\n\n                    Console.WriteLine(\n[CleanupDatabaseWebJob] Executing SQL Statement\n);\n                    sqlCommand.CommandText = \nDELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] \n DATEADD(day, -7, SYSDATETIMEOFFSET())\n;\n                    var rowsAffected = sqlCommand.ExecuteNonQuery();\n                    Console.WriteLine($\n[CleanupDatabaseWebJob] {rowsAffected} rows deleted.\n);\n\n                    sqlConnection.Close();\n                }\n            }\n        }\n    }\n}\n\n\n\n\nDebug messages that you want captured in the logs must be output with \nConsole.WriteLine\n.  The \nDebug\n\nchannel is not captured in the logs.\n\n\nYou must also define the connection string in the \nApp.config\n file of the WebJob project.  The scaffolding\ncreates two connection strings, but they are not normally defined in Azure App Service.  The following\ndefines the \nMS_TableConnectionString\n that is used by the mobile backend:\n\n\n  \nconnectionStrings\n\n    \nadd name=\nMS_TableConnectionString\n connectionString=\n /\n\n    \nadd name=\nAzureWebJobsDashboard\n connectionString=\n/\n\n    \nadd name=\nAzureWebJobsStorage\n connectionString=\n/\n\n  \n/connectionStrings\n\n\n\n\n\nThis is also defined (in the same way) in the \nWeb.config\n file for the mobile backend.  Note the two\nextra connection strings.  These are for running WebJobs, so you always need a connected storage account\nto run WebJobs.  WebJobs will create a number of blob containers in your storage account.  These containers\nall start with \nazure-jobs\n or \nazure-webjobs\n.  You should not mess with these in any way as they are\nrequired for WebJobs functionality.   To set up the connection string, create a storage account. Once you have\ncreated the storage account:\n\n\n\n\nOpen the Storage account blade.\n\n\nClick \nAccess keys\n in the menu.\n\n\nRight click the triple dot on the same line as \nkey1\n and choose \nView connection string\n.\n\n\nCopy the connection string, then click \nOK\n.\n\n\nClose the Storage account blade.\n\n\nOpen the App Service blade for your mobile backend.\n\n\nClick \nApplication settings\n in the menu.\n\n\nScroll down to the \nConnection Strings\n section and add a new connection string:\n\n\nEnter \nAzureWebJobsDashboard\n for the connection string name.\n\n\nPaste the connection string you copied earlier into the value.\n\n\nSelect \nCustom\n for the type of connection string.\n\n\n\n\n\n\nRepeat this process to create the \nAzureWebJobsStorage\n connection string.\n\n\nClick \nSave\n at the top of the blade.\n\n\n\n\nTo deploy a WebJob, we need to link it to our mobile backend:\n\n\n\n\nRight click the \nBackend\n project.\n\n\n\n\nSelect \nAdd\n then \nExisting Project as Azure WebJob\n.\n\n\n\n\n\n\n\n\nFill in the \nProject name\n and \nWebJob name\n (if you named your project differently).\n\n\n\n\nIn \nWebJob run mode\n, select \nRun on Demand\n.\n\n\n\n\nThis creates two files:\n\n\n\n\nThe \nwebjobs-list.json\n in the Backend project.\n\n\nThe \nwebjob-publish-settings.json\n in the WebJob project.\n\n\n\n\nVisual Studio 2017 does not provide for running WebJobs on a scheduled basis.  You need to edit the \nwebjob-publish-settings.json\n file.  For instance, to configure the WebJob as a scheduled job running once a day at 3am, use the following:\n\n\n{\n  \n$schema\n: \nhttp://schemastore.org/schemas/json/webjob-publish-settings.json\n,\n  \nwebJobName\n: \nCleanupDatabaseWebJob\n,\n  \nstartTime\n: \n2016-10-15T03:00:00-08:00\n,\n  \nendTime\n: \n2016-10-16T04:00:00-08:00\n,\n  \njobRecurrenceFrequency\n: \nDay\n,\n  \ninterval\n: 1,\n  \nrunMode\n: \nScheduled\n\n}\n\n\n\n\nThe limits for the ending time ensures that your WebJob doesn't run forever.  You may have a WebJob that runs\na complicated report each night to consolidate a lot of data into something that can be downloaded by your\nmobile client.  These reports can sometimes run for hours.  You should place reasonable limits on your WebJob\nto ensure that they don't affect the operation of your site.\n\n\nRight click your \nBackend\n project and select \nPublish...\n\n\n\n\nWarn\n\n\nWebJobs use the \nApp_Data\n area.  Ensure your publish profile does not delete files in the \nApp_Data\n area.\n\n\n\n\nSince you have published before, the dialog will be on the \nPreview\n tab.  Click on the \nPreview\n button\nto generate a view of what would happen:\n\n\n\n\nThe WebJob has been bundled with your mobile backend for publication.  If you log into the portal, there are\nseveral things you can do.  Go to your mobile backend.  In the menu, select \nWebJobs\n to see your configured\nWebJob.  You can trigger a run of the WebJob independently of the schedule that you configured when you linked\nthe WebJob to the mobile backend.  You can also view the logs for the WebJob.\n\n\n\n\nInfo\n\n\nYou will see an additional resource group when you created a scheduler WebJob.  This holds the Azure\nScheduler resource that you will use.  Azure Scheduler has a free tier which includes up to 3,600\njob executions per month and 5 jobs.  Ensure you delete the scheduler if you are not using it.\n\n\n\n\nAn Image Resizing WebJob\n\n\nScheduled WebJobs are great for report generation and maintenance tasks, but WebJobs can also be run continuously.\nIn a continuous mode, they wait for some trigger and then execute the code on that.  This is where the WebJobs\nSDK comes in.\n\n\nLet's take an example.  In our \nrecipes\n section, we have a method of uploading a photo to blob storage.\nHowever, we don't have any in-built methods of controlling what is uploaded.  The Azure Storage SDK will allow\nany file, so the user can upload a file that is not an image, or they can upload an image that is too big.\nWe can do something about this by running the following logic when a file is uploaded to the \nuserdata\n area:\n\n\n\n\nIf the file is an image:\n\n\nResize the image to 800px x 600px.\n\n\nStore the new file in the \npublicdata\n area.\n\n\n\n\n\n\nDelete the file from the \nuserdata\n area.\n\n\n\n\nThe monitoring is done lazily and depends on lots of factors (including how busy the storage account is).  It\nmay be several minutes before the service notices that a file is available.  To accomplish this, I am going to\ndo the processing of the file in one trigger function and the deletion in another, joining the two together via\nan Azure Storage queue.  My first step is to adjust the WebJob \nApp.config\n to include the storage connection\nstring:\n\n\n  \nconnectionStrings\n\n    \nadd name=\nMS_AzureStorageAccountConnectionString\n, connectionString=\n /\n\n    \nadd name=\nAzureWebJobsDashboard\n connectionString=\n/\n\n    \nadd name=\nAzureWebJobsStorage\n connectionString=\n/\n\n  \n/connectionStrings\n\n\n\n\n\nMy main program is in \nProgram.cs\n as follows:\n\n\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\nusing Microsoft.WindowsAzure.Storage.Queue;\nusing System.Configuration;\n\nnamespace ImageResizeWebJob\n{\n    class Program\n    {\n        static void Main()\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[\nMS_AzureStorageAccountConnectionString\n].ConnectionString;\n            CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n            CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n            CloudQueueClient queueClient = storageAccount.CreateCloudQueueClient();\n\n            // Ensure the userdata area exists\n            CloudBlobContainer userdata = blobClient.GetContainerReference(\nuserdata\n);\n            userdata.CreateIfNotExists();\n\n            // Ensure the publicdata area exists\n            CloudBlobContainer publicdata = blobClient.GetContainerReference(\npublicdata\n);\n            publicdata.CreateIfNotExists();\n\n            // Ensure the delete queue exists\n            CloudQueue queue = queueClient.GetQueueReference(\ndelete\n);\n            queue.CreateIfNotExists();\n\n            // Start running the WebJob\n            var host = new JobHost();\n            host.RunAndBlock();\n        }\n    }\n}\n\n\n\n\nWe must ensure that the areas we are monitoring to trigger activity exist before the WebJob starts processing.\nIf we don't, there is nothing to trigger.  The WebJob will not find the appropriate hooks for monitoring and\nthe WebJob will likely not run.\n\n\n\n\nTip\n\n\nFor more information on creating Azure Storage, see the \nrecipes\n section of this chapter.\n\n\n\n\nThe actual trigger methods (called \"functions\") are located in the \nFunctions\n class in the default namespace\nof the WebJob.   All trigger methods are static methods with the following basic signature:\n\n\npublic static void ImageUploaded(\n    [BlobTrigger(\nuserdata/{name}.{ext}\n)] Stream input,\n    string name,\n    string ext,\n    [Queue(\ndelete\n)] out string path)\n\n\n\n\nThe first parameter is always the trigger.  In this case, I am triggering when a blob upload is complete.  A\ntrigger can optionally be parameterized as it is in this case.  The trigger parameters come next.  In this case,\nI am extracting the filename and extension from the path of the uploaded file.  The final parameter is the\noutput binding.  For this trigger, I am going to write the path to be deleted to the delete queue so it can\nbe picked up by another function.\n\n\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\nusing System.Configuration;\nusing System.Diagnostics;\nusing System.Drawing;\nusing System.Drawing.Drawing2D;\nusing System.Drawing.Imaging;\nusing System.IO;\n\nnamespace ImageResizeWebJob\n{\n    public class Functions\n    {\n        static int requiredHeight = 600;\n        static int requiredWidth = 800;\n\n        public static void ImageUploaded(\n            [BlobTrigger(\nuserdata/{name}.{ext}\n)] Stream input,\n            string name,\n            string ext,\n            [Queue(\ndelete\n)] out string path)\n        {\n            if (!ext.ToLowerInvariant().Equals(\npng\n))\n            {\n                Debug.WriteLine($\nBlobTrigger: userdata/{name}.{ext} - not a PNG file (skipping)\n);\n                path = $\n{name}.{ext}\n;\n                return;\n            }\n\n            // Read the blob stream into an Image object\n            var image = Image.FromStream(input);\n\n            // Process the image object\n            if (image.Height \n requiredHeight || image.Width \n requiredWidth)\n            {\n                var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight);\n                var destImage = new Bitmap(requiredWidth, requiredHeight);\n\n                destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution);\n                using (var graphics = Graphics.FromImage(destImage))\n                {\n                    graphics.CompositingMode = CompositingMode.SourceCopy;\n                    graphics.CompositingQuality = CompositingQuality.Default;\n                    graphics.InterpolationMode = InterpolationMode.Bicubic;\n                    graphics.SmoothingMode = SmoothingMode.Default;\n                    graphics.PixelOffsetMode = PixelOffsetMode.Default;\n                    using (var wrapMode = new ImageAttributes())\n                    {\n                        wrapMode.SetWrapMode(WrapMode.TileFlipXY);\n                        graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode);\n                    }\n                }\n                // Replace the original image with the bitmap we created\n                image = destImage;\n            }\n\n            // Write the image out to the publicdata area\n            using (var stream = new MemoryStream())\n            {\n                image.Save(stream, ImageFormat.Png);\n                stream.Position = 0;\n                SaveFileToPublicBlob($\n{name}.{ext}\n, stream);\n            }\n\n            // Write the original path to the queue for deletion\n            path = $\n{name}.{ext}\n;\n        }\n\n        static void SaveFileToPublicBlob(string file, Stream input)\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[\nMS_AzureStorageAccountConnectionString\n].ConnectionString;\n            CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n            CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n\n            CloudBlobContainer publicdata = blobClient.GetContainerReference(\npublicdata\n);\n            CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(file);\n            blockBlob.UploadFromStream(input);\n        }\n    }\n}\n\n\n\n\nYou are always going to write custom code in the middle of a function.  In this case, it's my image resizing\ncode.  Once the code is run, I write out the blob and put the path on the deletion queue.\n\n\n\n\nTip\n\n\nTo learn more about image resizing in C#, this \nStack Overflow\n question has some great details.\n\n\n\n\nProcessing the queue is much easier than the image resizing:\n\n\n    public static void ProcessDeleteQueue([QueueTrigger(\ndelete\n)] string path)\n    {\n        var connectionString = ConfigurationManager.ConnectionStrings[\nMS_AzureStorageAccountConnectionString\n].ConnectionString;\n        CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n        CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n        CloudBlobContainer userdata = blobClient.GetContainerReference(\nuserdata\n);\n        CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(path);\n        blockBlob.DeleteIfExists();\n    }\n\n\n\n\nYou can connect this web job to your mobile backend project in the same way.  The \nWebJob run mode\n should be\nset to \nRun Continuously\n instead of the schedule setting we used prior.\n\n\n\n\nTip\n\n\nAzure App Service will turn off idle sites when they are not receiving requests.  If you have a WebJob that\nneeds to run irrespective of whether the App Service is receiving client requests, ensure the \nAlways On\n\nsetting is turned on.  This setting is in \nApplication settings\n.\n\n\n\n\nTo test the WebJob after publication:\n\n\n\n\nConnect an Azure Storage account to your mobile backend.  You can find instructions in the \nrecipes\n section.\n\n\nIn the mobile backend, select \nWebJobs\n, then your WebJob and click \nStart\n.\n\n\nIn Visual Studio, select the \nCloud Explorer\n.  If you cannot see the Cloud Explorer, you can start it\n   by selecting the \nView\n menu.\n\n\nExpand the \nStorage Accounts\n node \n your storage account \n \nBlob Containers\n \n \nuserdata\n.\n\n\nRight click \nuserdata\n and select \nOpen Blob Container Editor\n.\n\n\nClick the \nUpload Blob\n icon (it looks like a black up arrow).\n\n\nUpload a suitable file for testing the process.\n\n\n\n\nBenefits and Drawbacks of WebJobs\n\n\nWebJobs are great for long running async batch processing and maintenance tasks.  When considering if WebJobs\nare right for you:\n\n\n\n\nThey are included with your App Service Plan.\n\n\nThey scale with your App Service Plan.\n\n\nThey are resilient to errors.\n\n\nThey are easy to run in isolation and debug.\n\n\nThey are managed and deployed with your mobile backend project.\n\n\n\n\nHowever,\n\n\n\n\nThey may make your App Service use more resources than planned.\n\n\nThey can cause an automatic scale event on your App Service.", 
            "title": "WebJobs"
        }, 
        {
            "location": "/chapter4/webjobs/#a-database-clean-up-webjob", 
            "text": "As an example WebJob, let's implement the database clean-up process as a WebJob.  This will run on a regular\nbasis (say, once a day) and during processing, it will delete all records in our TodoItem table that are\ndeleted where the updatedAt field is older than 7 days.  The SQL command to run is this:  DELETE FROM\n    [dbo].[TodoItems]\nWHERE\n    [deleted] = 1 AND [updatedAt]   DATEADD(day, -7, SYSDATETIMEOFFSET())\nGO  A WebJob is always a separate project from your mobile backend.  To create a WebJob:   Right click the solution.  Choose  Add  -   New Item... .   Search for  WebJob .     Select  Azure WebJob  with a  Visual C#  language.   Enter a suitable name for the WebJob and click  Add .   Once the scaffolding has finished and the package restore is done, you will see a  Program.cs  file.  A WebJob\nis just a console application running with the WebJobs SDK.  By default, the WebJob will wait around for a\ntrigger.  We don't have a trigger since we are going to run this code each and every time the scheduler runs\nit.  As a result, our code in Program.cs is relatively simple:  using System;\nusing System.Configuration;\nusing System.Data.SqlClient;\nusing System.Diagnostics;\n\nnamespace CleanupDatabaseWebJob\n{\n    class Program\n    {\n        static void Main()\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[ MS_TableConnectionString ].ConnectionString;\n\n            using (SqlConnection sqlConnection = new SqlConnection(connectionString))\n            {\n                using (SqlCommand sqlCommand = sqlConnection.CreateCommand())\n                {\n                    Console.WriteLine( [CleanupDatabaseWebJob] Initiating SQL Connection );\n                    sqlConnection.Open();\n\n                    Console.WriteLine( [CleanupDatabaseWebJob] Executing SQL Statement );\n                    sqlCommand.CommandText =  DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt]   DATEADD(day, -7, SYSDATETIMEOFFSET()) ;\n                    var rowsAffected = sqlCommand.ExecuteNonQuery();\n                    Console.WriteLine($ [CleanupDatabaseWebJob] {rowsAffected} rows deleted. );\n\n                    sqlConnection.Close();\n                }\n            }\n        }\n    }\n}  Debug messages that you want captured in the logs must be output with  Console.WriteLine .  The  Debug \nchannel is not captured in the logs.  You must also define the connection string in the  App.config  file of the WebJob project.  The scaffolding\ncreates two connection strings, but they are not normally defined in Azure App Service.  The following\ndefines the  MS_TableConnectionString  that is used by the mobile backend:     connectionStrings \n     add name= MS_TableConnectionString  connectionString=  / \n     add name= AzureWebJobsDashboard  connectionString= / \n     add name= AzureWebJobsStorage  connectionString= / \n   /connectionStrings   This is also defined (in the same way) in the  Web.config  file for the mobile backend.  Note the two\nextra connection strings.  These are for running WebJobs, so you always need a connected storage account\nto run WebJobs.  WebJobs will create a number of blob containers in your storage account.  These containers\nall start with  azure-jobs  or  azure-webjobs .  You should not mess with these in any way as they are\nrequired for WebJobs functionality.   To set up the connection string, create a storage account. Once you have\ncreated the storage account:   Open the Storage account blade.  Click  Access keys  in the menu.  Right click the triple dot on the same line as  key1  and choose  View connection string .  Copy the connection string, then click  OK .  Close the Storage account blade.  Open the App Service blade for your mobile backend.  Click  Application settings  in the menu.  Scroll down to the  Connection Strings  section and add a new connection string:  Enter  AzureWebJobsDashboard  for the connection string name.  Paste the connection string you copied earlier into the value.  Select  Custom  for the type of connection string.    Repeat this process to create the  AzureWebJobsStorage  connection string.  Click  Save  at the top of the blade.   To deploy a WebJob, we need to link it to our mobile backend:   Right click the  Backend  project.   Select  Add  then  Existing Project as Azure WebJob .     Fill in the  Project name  and  WebJob name  (if you named your project differently).   In  WebJob run mode , select  Run on Demand .   This creates two files:   The  webjobs-list.json  in the Backend project.  The  webjob-publish-settings.json  in the WebJob project.   Visual Studio 2017 does not provide for running WebJobs on a scheduled basis.  You need to edit the  webjob-publish-settings.json  file.  For instance, to configure the WebJob as a scheduled job running once a day at 3am, use the following:  {\n   $schema :  http://schemastore.org/schemas/json/webjob-publish-settings.json ,\n   webJobName :  CleanupDatabaseWebJob ,\n   startTime :  2016-10-15T03:00:00-08:00 ,\n   endTime :  2016-10-16T04:00:00-08:00 ,\n   jobRecurrenceFrequency :  Day ,\n   interval : 1,\n   runMode :  Scheduled \n}  The limits for the ending time ensures that your WebJob doesn't run forever.  You may have a WebJob that runs\na complicated report each night to consolidate a lot of data into something that can be downloaded by your\nmobile client.  These reports can sometimes run for hours.  You should place reasonable limits on your WebJob\nto ensure that they don't affect the operation of your site.  Right click your  Backend  project and select  Publish...   Warn  WebJobs use the  App_Data  area.  Ensure your publish profile does not delete files in the  App_Data  area.   Since you have published before, the dialog will be on the  Preview  tab.  Click on the  Preview  button\nto generate a view of what would happen:   The WebJob has been bundled with your mobile backend for publication.  If you log into the portal, there are\nseveral things you can do.  Go to your mobile backend.  In the menu, select  WebJobs  to see your configured\nWebJob.  You can trigger a run of the WebJob independently of the schedule that you configured when you linked\nthe WebJob to the mobile backend.  You can also view the logs for the WebJob.   Info  You will see an additional resource group when you created a scheduler WebJob.  This holds the Azure\nScheduler resource that you will use.  Azure Scheduler has a free tier which includes up to 3,600\njob executions per month and 5 jobs.  Ensure you delete the scheduler if you are not using it.", 
            "title": "A Database Clean Up WebJob"
        }, 
        {
            "location": "/chapter4/webjobs/#an-image-resizing-webjob", 
            "text": "Scheduled WebJobs are great for report generation and maintenance tasks, but WebJobs can also be run continuously.\nIn a continuous mode, they wait for some trigger and then execute the code on that.  This is where the WebJobs\nSDK comes in.  Let's take an example.  In our  recipes  section, we have a method of uploading a photo to blob storage.\nHowever, we don't have any in-built methods of controlling what is uploaded.  The Azure Storage SDK will allow\nany file, so the user can upload a file that is not an image, or they can upload an image that is too big.\nWe can do something about this by running the following logic when a file is uploaded to the  userdata  area:   If the file is an image:  Resize the image to 800px x 600px.  Store the new file in the  publicdata  area.    Delete the file from the  userdata  area.   The monitoring is done lazily and depends on lots of factors (including how busy the storage account is).  It\nmay be several minutes before the service notices that a file is available.  To accomplish this, I am going to\ndo the processing of the file in one trigger function and the deletion in another, joining the two together via\nan Azure Storage queue.  My first step is to adjust the WebJob  App.config  to include the storage connection\nstring:     connectionStrings \n     add name= MS_AzureStorageAccountConnectionString , connectionString=  / \n     add name= AzureWebJobsDashboard  connectionString= / \n     add name= AzureWebJobsStorage  connectionString= / \n   /connectionStrings   My main program is in  Program.cs  as follows:  using Microsoft.Azure.WebJobs;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\nusing Microsoft.WindowsAzure.Storage.Queue;\nusing System.Configuration;\n\nnamespace ImageResizeWebJob\n{\n    class Program\n    {\n        static void Main()\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[ MS_AzureStorageAccountConnectionString ].ConnectionString;\n            CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n            CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n            CloudQueueClient queueClient = storageAccount.CreateCloudQueueClient();\n\n            // Ensure the userdata area exists\n            CloudBlobContainer userdata = blobClient.GetContainerReference( userdata );\n            userdata.CreateIfNotExists();\n\n            // Ensure the publicdata area exists\n            CloudBlobContainer publicdata = blobClient.GetContainerReference( publicdata );\n            publicdata.CreateIfNotExists();\n\n            // Ensure the delete queue exists\n            CloudQueue queue = queueClient.GetQueueReference( delete );\n            queue.CreateIfNotExists();\n\n            // Start running the WebJob\n            var host = new JobHost();\n            host.RunAndBlock();\n        }\n    }\n}  We must ensure that the areas we are monitoring to trigger activity exist before the WebJob starts processing.\nIf we don't, there is nothing to trigger.  The WebJob will not find the appropriate hooks for monitoring and\nthe WebJob will likely not run.   Tip  For more information on creating Azure Storage, see the  recipes  section of this chapter.   The actual trigger methods (called \"functions\") are located in the  Functions  class in the default namespace\nof the WebJob.   All trigger methods are static methods with the following basic signature:  public static void ImageUploaded(\n    [BlobTrigger( userdata/{name}.{ext} )] Stream input,\n    string name,\n    string ext,\n    [Queue( delete )] out string path)  The first parameter is always the trigger.  In this case, I am triggering when a blob upload is complete.  A\ntrigger can optionally be parameterized as it is in this case.  The trigger parameters come next.  In this case,\nI am extracting the filename and extension from the path of the uploaded file.  The final parameter is the\noutput binding.  For this trigger, I am going to write the path to be deleted to the delete queue so it can\nbe picked up by another function.  using Microsoft.Azure.WebJobs;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\nusing System.Configuration;\nusing System.Diagnostics;\nusing System.Drawing;\nusing System.Drawing.Drawing2D;\nusing System.Drawing.Imaging;\nusing System.IO;\n\nnamespace ImageResizeWebJob\n{\n    public class Functions\n    {\n        static int requiredHeight = 600;\n        static int requiredWidth = 800;\n\n        public static void ImageUploaded(\n            [BlobTrigger( userdata/{name}.{ext} )] Stream input,\n            string name,\n            string ext,\n            [Queue( delete )] out string path)\n        {\n            if (!ext.ToLowerInvariant().Equals( png ))\n            {\n                Debug.WriteLine($ BlobTrigger: userdata/{name}.{ext} - not a PNG file (skipping) );\n                path = $ {name}.{ext} ;\n                return;\n            }\n\n            // Read the blob stream into an Image object\n            var image = Image.FromStream(input);\n\n            // Process the image object\n            if (image.Height   requiredHeight || image.Width   requiredWidth)\n            {\n                var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight);\n                var destImage = new Bitmap(requiredWidth, requiredHeight);\n\n                destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution);\n                using (var graphics = Graphics.FromImage(destImage))\n                {\n                    graphics.CompositingMode = CompositingMode.SourceCopy;\n                    graphics.CompositingQuality = CompositingQuality.Default;\n                    graphics.InterpolationMode = InterpolationMode.Bicubic;\n                    graphics.SmoothingMode = SmoothingMode.Default;\n                    graphics.PixelOffsetMode = PixelOffsetMode.Default;\n                    using (var wrapMode = new ImageAttributes())\n                    {\n                        wrapMode.SetWrapMode(WrapMode.TileFlipXY);\n                        graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode);\n                    }\n                }\n                // Replace the original image with the bitmap we created\n                image = destImage;\n            }\n\n            // Write the image out to the publicdata area\n            using (var stream = new MemoryStream())\n            {\n                image.Save(stream, ImageFormat.Png);\n                stream.Position = 0;\n                SaveFileToPublicBlob($ {name}.{ext} , stream);\n            }\n\n            // Write the original path to the queue for deletion\n            path = $ {name}.{ext} ;\n        }\n\n        static void SaveFileToPublicBlob(string file, Stream input)\n        {\n            var connectionString = ConfigurationManager.ConnectionStrings[ MS_AzureStorageAccountConnectionString ].ConnectionString;\n            CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n            CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n\n            CloudBlobContainer publicdata = blobClient.GetContainerReference( publicdata );\n            CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(file);\n            blockBlob.UploadFromStream(input);\n        }\n    }\n}  You are always going to write custom code in the middle of a function.  In this case, it's my image resizing\ncode.  Once the code is run, I write out the blob and put the path on the deletion queue.   Tip  To learn more about image resizing in C#, this  Stack Overflow  question has some great details.   Processing the queue is much easier than the image resizing:      public static void ProcessDeleteQueue([QueueTrigger( delete )] string path)\n    {\n        var connectionString = ConfigurationManager.ConnectionStrings[ MS_AzureStorageAccountConnectionString ].ConnectionString;\n        CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n        CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n        CloudBlobContainer userdata = blobClient.GetContainerReference( userdata );\n        CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(path);\n        blockBlob.DeleteIfExists();\n    }  You can connect this web job to your mobile backend project in the same way.  The  WebJob run mode  should be\nset to  Run Continuously  instead of the schedule setting we used prior.   Tip  Azure App Service will turn off idle sites when they are not receiving requests.  If you have a WebJob that\nneeds to run irrespective of whether the App Service is receiving client requests, ensure the  Always On \nsetting is turned on.  This setting is in  Application settings .   To test the WebJob after publication:   Connect an Azure Storage account to your mobile backend.  You can find instructions in the  recipes  section.  In the mobile backend, select  WebJobs , then your WebJob and click  Start .  In Visual Studio, select the  Cloud Explorer .  If you cannot see the Cloud Explorer, you can start it\n   by selecting the  View  menu.  Expand the  Storage Accounts  node   your storage account    Blob Containers     userdata .  Right click  userdata  and select  Open Blob Container Editor .  Click the  Upload Blob  icon (it looks like a black up arrow).  Upload a suitable file for testing the process.", 
            "title": "An Image Resizing WebJob"
        }, 
        {
            "location": "/chapter4/webjobs/#benefits-and-drawbacks-of-webjobs", 
            "text": "WebJobs are great for long running async batch processing and maintenance tasks.  When considering if WebJobs\nare right for you:   They are included with your App Service Plan.  They scale with your App Service Plan.  They are resilient to errors.  They are easy to run in isolation and debug.  They are managed and deployed with your mobile backend project.   However,   They may make your App Service use more resources than planned.  They can cause an automatic scale event on your App Service.", 
            "title": "Benefits and Drawbacks of WebJobs"
        }, 
        {
            "location": "/chapter4/functions/", 
            "text": "WebJobs run in the context of your App Service Plan, which means they inherit the scaling capabilities of that plan, and may cause the site to scale unnecessarily.  What should you do if you don't want this to happen?\n\n\nEnter \nAzure Functions\n.  Azure Functions are a technology for running WebJobs in a dynamic compute context.  Dynamic Compute is a relative newcomer to the cloud computing stage and part of a new paradigm known as \"\nServerless\n\".\n\n\nLet's take a tour through the history of cloud.  In the beginning, cloud providers provided virtual machines, networking and storage, also known as \"\nInfrastructure as a Service\n\" or IaaS.  You built a cloud service in much the same way as you built an on-premise solution.  You cede control of the hardware to the cloud provider, but you are responsible for the maintenance of the platform.\n\n\n\"\nPlatform as a Service\n\" or PaaS is a step up from this.  With PaaS, you cede control of the operating system, security patching and platform maintenance to the cloud provider.  You are responsible for the code that is running your app. With PaaS, however, you are still somewhat responsible and aware of the underlying infrastructure.  You generally are making decisions on when to add a new virtual machine to the pool for scaling, for example.  The virtual machine is present.\n\n\n\"\nSoftware as a Service\n\" (or SaaS) is the opposite end of the cloud services to IaaS.  You are not responsible for anything in the platform.  You just use the software.\n\n\n\n\nA good analogy is how to get a meal.  IaaS is akin to going to the grocery store, picking all the ingredients you need, preparing the ingredients, cooking the meal, and serving the meal to you and your guest.  SaaS is akin to going out to a restaurant and telling the waiter what you want.  PaaS is similar to food box delivery services - they provide the ingredients and the recipe, but you do the cooking.\n\n\nIn both of these cases, you are trading off management convenience for management control.  IaaS has lots of control, but you have to do pretty much all the management yourself.   SaaS is run for you, but you have no control.  PaaS is in between these two extremes.\n\n\nBut what about when you want even more management convenience than PaaS can offer, but you still want to run your application?  Something inbetween PaaS and Saas, where scaling issues are taken care of for you.  This is where \nServerless\n technologies and dynamic compute come in.  With Serverless, you still manage your code.  However, they are infinitely scalable.  That comes at a cost in terms of flexibility.  Serverless does not mean \"without servers\". There are still servers involved.  You just don't need to manage them in any way.\n\n\nAzure Functions are an implementation of Serverless technology to allow WebJobs to be written that happen in dynamic compute.  You pay for the number of executions.  (Technically, pricing is more complex than this simplification, but you will notice that your price goes up as the number of executions goes up).  You can consider Azure Functions as \"WebJobs as a Service\".\n\n\nThis isn't the only Serverless technology on the Azure platform.  \nAzure Logic Apps\n is also a Serverless technology, covering Workflow.  Function execution is also not the only serverless technology.  You can find examples in authentication, message queuing, edge caching, search and others.\n\n\nIt's quite possible to write a mobile backend entirely in Azure Functions.  However, this is undesirable mostly because some processes (most notably SQL database access) require a relatively lengthy cold-start process.  Functions are short-lived processes and may change the server that they are running on frequently.  The mobile backend can provide efficiencies in this scenario by keeping a pool of connections open to the SQL database.  This efficiency is not possible in Azure Functions.\n\n\nBuilding an Azure Function\n\n\nThe first thing to note about Azure Functions is that they are a completely separate resource in the Azure Portal. This means that they are built separately, charged separately and scale independently to your mobile backend.  This is in contrast to WebJobs, where the WebJob shares infrastructure with the mobile backend and scales with the mobile backend.\n\n\nStart by logging in to the Azure Portal.\n\n\n\n\nClick on the \n+ NEW\n button, or the \n+ Add\n button on a resource group.\n\n\nSearch for \nFunction App\n, then click \nCreate\n.\n\n\nEnter a unique name for the Function App.  The Function App is still an App Service, so you cannot name\n   the Function App the same as your mobile backend.\n\n\nSet the \nHosting Plan\n to be \nConsumption Plan\n.\n\n\nPick your storage account that you created for the WebJobs demos (or skip to create a new one).\n\n\nClick on \nCreate\n.\n\n\n\n\n\n\n\n\nWarn\n\n\nDynamic Function Apps are not allowed in the same Region + Resource Group combination as non-Dynamic\napps.  That generally means you have to create another Resource Group to hold your Function Apps.\n\n\n\n\nFunctions have a \"Hosting Plan\" instead of an App Service Plan.  The choices are either \"Consumption Plan\", which uses dynamic compute, or \"App Service Plan\", which uses the virtual machines that host your App Services.  One could argue there isn't much difference between WebJobs and Functions running on an App Service Plan.\n\n\n\n\nOnce the deployment is complete, you will notice two new resources.  The lightning bolt type icon is the Function App and you will spend most of your time there. You might also have an additional storage account if you chose to create one (or just clicked create).   Just like WebJobs, Azure Functions needs a storage account to store runtime state and logs.\n\n\nNow that you have a Function App, you can start creating functions.  Click on your Function App to open the Functions Console.  Let's start by creating the same two WebJobs that we created in the last section, but using Azure Functions.\n\n\nDatabase Cleanup with Azure Functions\n\n\nOur first WebJob was a database cleanup process that was run on a schedule.  If this is your first Function, then click on \nCreate your own custom function\n.  If not, you can click on the \n+ New Function\n link in the side bar.  Your next task is to select a template:\n\n\n\n\nWe want a \nTimerTrigger\n for this example.  Note that we can write in multiple languages.  C#, F# and JavaScript are supported out of the box.  You can also bring other languages.  Other supported languages include Python, PowerShell, bash, and PHP.  Click on the \nTimerTrigger - CSharp\n template, which is near the end of the list.\n\n\nYou will need to name your function (I called mine \nDatabaseCleanup\n) and configure a schedule for the trigger. The schedule is in a simplified cron-style expression.  3am is written as \n0 0 3 * \n \n.  Once you have set the two fields, click on \nCreate\n.\n\n\n\n\nTip\n\n\nYou can create as many functions as you want inside of the Function App.  They will all run independently, so there is no reason to need more than one Function App per resource group.\n\n\n\n\nAt this point you have a fully functional Azure Function.  In the \nDevelop\n tab, you can click on \nRun\n to run your function.  The \nLogs\n panel will show you the logs for running the function.  If there is any output, you can see it in the \nOutput\n panel.  You can edit your code in-line.  There is just one method - the \nRun()\n method.  It looks quite like the WebJob.  The trigger comes first, the output binding second and the \nTraceWriter\n comes last for logging.  In a timer job, there is no output binding.\n\n\nReplace the code within the editor with the following:\n\n\n#r \nSystem.Data\n\n\nusing System;\nusing System.Configuration;\nusing System.Data.SqlClient;\n\npublic static void Run(TimerInfo myTimer, TraceWriter log)\n{\n    var connectionString = ConfigurationManager.ConnectionStrings[\nMS_TableConnectionString\n].ConnectionString;\n    log.Info($\nUsing Connection String {connectionString}\n);\n\n    using (var sqlConnection = new SqlConnection(connectionString))\n    {\n        using (var sqlCommand = sqlConnection.CreateCommand())\n        {\n            log.Info(\nInitiating SQL Connection\n);\n            sqlConnection.Open();\n\n            log.Info(\nExecuting SQL Statement\n);\n            sqlCommand.CommandText = \nDELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt] \n DATEADD(day, -7, SYSDATETIMEOFFSET())\n;\n            var rowsAffected = sqlCommand.ExecuteNonQuery();\n            log.Info($\n{rowsAffected} rows deleted.\n);\n\n            sqlConnection.Close();\n        }\n    }\n}\n\n\n\n\nNotice the \n#r\n directive.  Azure Functions comes with some built-in references.  They are listed in the \nC# Reference\n. System.Data is not one of those references, so you have to bring it in yourself.  The \n#r\n directive brings in the reference.\n\n\nOther than that, this looks remarkably like the WebJob that does the same thing.  This is normal.  In fact, it's ok to develop the functionality in WebJobs and then translate the WebJob to a function once you have it working. Aside from the \n#r\n, you will notice some other things:\n\n\n\n\nFunctions doesn't use \nConsole\n.  It provides a \nTraceWriter\n for logging instead.\n\n\nThe signature of the \nRun()\n method is different than WebJobs.\n\n\n\n\nOnce you save the file, the function is automatically compiled.  If there are any compilation errors, they will show up in the Logs panel.  If you click \nRun\n right now, you will see an error.  That's because the connection string is not defined.  In WebJobs, you defined this connection string in the \nApp.config\n file.  In Azure Functions, you just have to set the connection string up:\n\n\n\n\nClick \nFunction app settings\n in the lower left corner.\n\n\nClick \ngfo to App Service Settings\n.\n\n\nFind and click \nData Connections\n.\n\n\n\n\nNow connect your database to the function app in the same way that you did for the mobile backend.  This reinforces, for me anyway, that the Function App is an App Service.  It uses the same menu structure under the covers.  You can also set additional app settings, link storage, and so on in the same way as on App Services.\n\n\nWhen you click on \nRun\n in your function now, you will see the log output.\n\n\nImage Resize with Azure Functions\n\n\nLet's create another C# Function.  Before you start, link your storage account to the Function App using the Data Connections in the same way as you did your SQL database.  Then click on the \n+ New Function\n button and select the \nBlobTrigger - CSharp\n template.\n\n\n\n\nWarn\n\n\nYour storage account must be in the same region as your Function App.  In general, resources that talk to one another should be colocated in the same region.  However, this is a requirement for Azure Functions.\n\n\n\n\nName your function, enter \nuserdata\n within the Path field.  Click on \nnew\n next to the Storage account creation box.  This will prompt you for a valid storage account and create a connection string for you.\n\n\n\n\nClick \nCreate\n to create the blob.  This will create the following Code:\n\n\nusing System;\n\npublic static void Run(string myBlob, TraceWriter log)\n{\n    log.Info($\nC# Blob trigger function processed: {myBlob}\n);\n}\n\n\n\n\nI don't want to load each blob into a string, so this is definitely the wrong code.  However, you will note that any image I have stored within the \nuserdata\n area is displayed.  Let's take a look at the code I use for the image resizer:\n\n\n#r \nSystem.Drawing\n\n#r \nMicrosoft.WindowsAzure.Storage\n\n\nusing System;\nusing System.Drawing;\nusing System.Drawing.Drawing2D;\nusing System.Drawing.Imaging;\nusing System.IO;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\n\nstatic int requiredWidth = 800;\nstatic int requiredHeight = 600;\n\npublic static async Task Run(CloudBlockBlob inputBlob, ICollector\nstring\n outputQueueItem, TraceWriter log)\n{\n    log.Info($\nprocessing File {inputBlob.Name}\n);\n\n    var ext = Path.GetExtension(inputBlob.Name);\n    log.Info($\nExt = {ext}\n);\n    if (!ext.ToLowerInvariant().Equals(\n.png\n)) {\n        log.Info($\nPath {inputBlob.Name} is not a PNG file (skipping)\n);\n        outputQueueItem.Add(inputBlob.Name);\n        return;\n    }\n\n    var input = await inputBlob.OpenReadAsync();\n\n    // From WebJobs\n    var image = Image.FromStream(input);\n    if (image.Height \n requiredHeight || image.Width \n requiredWidth) {\n        log.Info($\nProcessing image {image.Height} x {image.Width}\n);\n        var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight);\n        var destImage = new Bitmap(requiredWidth, requiredHeight);\n        destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution);\n\n        using (var graphics = Graphics.FromImage(destImage))\n        {\n            graphics.CompositingMode = CompositingMode.SourceCopy;\n            graphics.CompositingQuality = CompositingQuality.Default;\n            graphics.InterpolationMode = InterpolationMode.Bicubic;\n            graphics.SmoothingMode = SmoothingMode.Default;\n            graphics.PixelOffsetMode = PixelOffsetMode.Default;\n            using (var wrapMode = new ImageAttributes()) {\n                wrapMode.SetWrapMode(WrapMode.TileFlipXY);\n                graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode);\n            }\n        }\n        image = destImage;\n    }\n\n    log.Info(\nWriting new Image to publicdata area\n);\n    using (var stream = new MemoryStream()) {\n        image.Save(stream, ImageFormat.Png);\n        stream.Position = 0;\n\n        var connectionString = Environment.GetEnvironmentVariable(\nzumobook_STORAGE\n);\n        CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n        CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n        CloudBlobContainer publicdata = blobClient.GetContainerReference(\npublicdata\n);\n        CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(inputBlob.Name);\n        blockBlob.UploadFromStream(stream);\n    }\n\n    outputQueueItem.Add(inputBlob.Name);\n}\n\n\n\n\nWhile this will work, we need to work on the integrations.  Save the function, then click on \nIntegrate\n.  This is where you can set triggers (the things that cause the function to run), inputs (what is fed into the function) and outputs (what is generated by the function).\n\n\nThe first step is to change the \nBlob parameter name\n to match what is included in the function call - in this case, my parameter is \ninputBlob\n.  I could have called the trigger (which also becomes an input) \nmyBlob\n and\nsaved myself the trouble.  I am against prefixing variables with \nmy\n, so I like to change this.\n\n\nMy original WebJob put the source file into a queue called \ndelete\n, which I've already pre-created.  I want to create an \nOutput\n to do this.  Click \n+ New Output\n and choose \nAzure Storage Queue\n.  Click on \nSelect\n.\n\n\n\n\nYou can fill in the details of the queue as follows:\n\n\n\n\nI'm selecting the same storage account as the rest of my application is using.  The Message parameter name must match the signature of the static method in my function.  Note that I'm using a \nICollector\n isntead of a string. Async methods cannot use \nout\n types (or \nref\n types, for that matter).  As a result, I need a type that allows me to add things to the queue.  The \nICollector\n does that for me.\n\n\n\n\nWarn\n\n\nDon't forget to disable the ImageResizer WebJob in your mobile backend before testing the function.  You can find selected WebJobs in the \nProperties\\webjobs-list.json\n file.\n\n\n\n\nYou can test this function in the same way that you tested the WebJob.  Use the Cloud Explorer in Visual Studio to upload a file into the userdata area of your Storage Account.  If things go well, you will see the Info logs in the Logs area of the Function App.  You will also see an entry in the \ndelete\n queue and a file of the same name in the \npublicdata\n area when you refresh them.\n\n\nHandling File Deletion\n\n\nYou can do a function to delete files using the \ndelete\n queue as well.  Create a new function with the \nQueueTrigger - CSharp\n template.  Replace the code with the following:\n\n\n#r \nMicrosoft.WindowsAzure.Storage\n\n\nusing System;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\n\npublic static void Run(string myQueueItem, TraceWriter log)\n{\n    log.Info($\nProcessing File for Deletion {myQueueItem}\n);\n    var connectionString = Environment.GetEnvironmentVariable(\nzumobook_STORAGE\n);\n    CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n    CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n    CloudBlobContainer userdata = blobClient.GetContainerReference(\nuserdata\n);\n    CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(myQueueItem);\n    blockBlob.DeleteIfExists();\n}\n\n\n\n\nWith this in place, when you upload an image to userdata, it will be processed and appear in the publicdata area.  The original file will be deleted.  You will be able to see the logs for all the functions in the Logs.\n\n\nBenefits and Drawbacks\n\n\nThere are a few things that stand out as drawbacks:\n\n\n\n\nA separate resource group is required, which makes managing all resources together difficult.\n\n\nTooling is limited right now and comprises of a command line (for Node.js development) and a plugin for Visual Studio 2015.  That means no Intellisense, which is a primary problem.\n\n\nIt uses a separate pricing model, so you are paying extra (at high volumes) for Functions.\n\n\n\n\nHowever, there are distinct advantages:\n\n\n\n\nYou don't have to worry about scaling - it happens automatically.\n\n\nThey are even simpler to debug and deploy than WebJobs (and that wasn't exactly tough).\n\n\nIntegrated Functions monitoring is better than WebJobs.\n\n\n\n\nIn all, I believe there is room in my toolbox for both WebJobs and Functions.  The fact that you can translate between them means that you don't have to pick - you can be very situational about the choice depending on need.", 
            "title": "Functions"
        }, 
        {
            "location": "/chapter4/functions/#building-an-azure-function", 
            "text": "The first thing to note about Azure Functions is that they are a completely separate resource in the Azure Portal. This means that they are built separately, charged separately and scale independently to your mobile backend.  This is in contrast to WebJobs, where the WebJob shares infrastructure with the mobile backend and scales with the mobile backend.  Start by logging in to the Azure Portal.   Click on the  + NEW  button, or the  + Add  button on a resource group.  Search for  Function App , then click  Create .  Enter a unique name for the Function App.  The Function App is still an App Service, so you cannot name\n   the Function App the same as your mobile backend.  Set the  Hosting Plan  to be  Consumption Plan .  Pick your storage account that you created for the WebJobs demos (or skip to create a new one).  Click on  Create .     Warn  Dynamic Function Apps are not allowed in the same Region + Resource Group combination as non-Dynamic\napps.  That generally means you have to create another Resource Group to hold your Function Apps.   Functions have a \"Hosting Plan\" instead of an App Service Plan.  The choices are either \"Consumption Plan\", which uses dynamic compute, or \"App Service Plan\", which uses the virtual machines that host your App Services.  One could argue there isn't much difference between WebJobs and Functions running on an App Service Plan.   Once the deployment is complete, you will notice two new resources.  The lightning bolt type icon is the Function App and you will spend most of your time there. You might also have an additional storage account if you chose to create one (or just clicked create).   Just like WebJobs, Azure Functions needs a storage account to store runtime state and logs.  Now that you have a Function App, you can start creating functions.  Click on your Function App to open the Functions Console.  Let's start by creating the same two WebJobs that we created in the last section, but using Azure Functions.", 
            "title": "Building an Azure Function"
        }, 
        {
            "location": "/chapter4/functions/#database-cleanup-with-azure-functions", 
            "text": "Our first WebJob was a database cleanup process that was run on a schedule.  If this is your first Function, then click on  Create your own custom function .  If not, you can click on the  + New Function  link in the side bar.  Your next task is to select a template:   We want a  TimerTrigger  for this example.  Note that we can write in multiple languages.  C#, F# and JavaScript are supported out of the box.  You can also bring other languages.  Other supported languages include Python, PowerShell, bash, and PHP.  Click on the  TimerTrigger - CSharp  template, which is near the end of the list.  You will need to name your function (I called mine  DatabaseCleanup ) and configure a schedule for the trigger. The schedule is in a simplified cron-style expression.  3am is written as  0 0 3 *    .  Once you have set the two fields, click on  Create .   Tip  You can create as many functions as you want inside of the Function App.  They will all run independently, so there is no reason to need more than one Function App per resource group.   At this point you have a fully functional Azure Function.  In the  Develop  tab, you can click on  Run  to run your function.  The  Logs  panel will show you the logs for running the function.  If there is any output, you can see it in the  Output  panel.  You can edit your code in-line.  There is just one method - the  Run()  method.  It looks quite like the WebJob.  The trigger comes first, the output binding second and the  TraceWriter  comes last for logging.  In a timer job, there is no output binding.  Replace the code within the editor with the following:  #r  System.Data \n\nusing System;\nusing System.Configuration;\nusing System.Data.SqlClient;\n\npublic static void Run(TimerInfo myTimer, TraceWriter log)\n{\n    var connectionString = ConfigurationManager.ConnectionStrings[ MS_TableConnectionString ].ConnectionString;\n    log.Info($ Using Connection String {connectionString} );\n\n    using (var sqlConnection = new SqlConnection(connectionString))\n    {\n        using (var sqlCommand = sqlConnection.CreateCommand())\n        {\n            log.Info( Initiating SQL Connection );\n            sqlConnection.Open();\n\n            log.Info( Executing SQL Statement );\n            sqlCommand.CommandText =  DELETE FROM [dbo].[TodoItems] WHERE [deleted] = 1 AND [updatedAt]   DATEADD(day, -7, SYSDATETIMEOFFSET()) ;\n            var rowsAffected = sqlCommand.ExecuteNonQuery();\n            log.Info($ {rowsAffected} rows deleted. );\n\n            sqlConnection.Close();\n        }\n    }\n}  Notice the  #r  directive.  Azure Functions comes with some built-in references.  They are listed in the  C# Reference . System.Data is not one of those references, so you have to bring it in yourself.  The  #r  directive brings in the reference.  Other than that, this looks remarkably like the WebJob that does the same thing.  This is normal.  In fact, it's ok to develop the functionality in WebJobs and then translate the WebJob to a function once you have it working. Aside from the  #r , you will notice some other things:   Functions doesn't use  Console .  It provides a  TraceWriter  for logging instead.  The signature of the  Run()  method is different than WebJobs.   Once you save the file, the function is automatically compiled.  If there are any compilation errors, they will show up in the Logs panel.  If you click  Run  right now, you will see an error.  That's because the connection string is not defined.  In WebJobs, you defined this connection string in the  App.config  file.  In Azure Functions, you just have to set the connection string up:   Click  Function app settings  in the lower left corner.  Click  gfo to App Service Settings .  Find and click  Data Connections .   Now connect your database to the function app in the same way that you did for the mobile backend.  This reinforces, for me anyway, that the Function App is an App Service.  It uses the same menu structure under the covers.  You can also set additional app settings, link storage, and so on in the same way as on App Services.  When you click on  Run  in your function now, you will see the log output.", 
            "title": "Database Cleanup with Azure Functions"
        }, 
        {
            "location": "/chapter4/functions/#image-resize-with-azure-functions", 
            "text": "Let's create another C# Function.  Before you start, link your storage account to the Function App using the Data Connections in the same way as you did your SQL database.  Then click on the  + New Function  button and select the  BlobTrigger - CSharp  template.   Warn  Your storage account must be in the same region as your Function App.  In general, resources that talk to one another should be colocated in the same region.  However, this is a requirement for Azure Functions.   Name your function, enter  userdata  within the Path field.  Click on  new  next to the Storage account creation box.  This will prompt you for a valid storage account and create a connection string for you.   Click  Create  to create the blob.  This will create the following Code:  using System;\n\npublic static void Run(string myBlob, TraceWriter log)\n{\n    log.Info($ C# Blob trigger function processed: {myBlob} );\n}  I don't want to load each blob into a string, so this is definitely the wrong code.  However, you will note that any image I have stored within the  userdata  area is displayed.  Let's take a look at the code I use for the image resizer:  #r  System.Drawing \n#r  Microsoft.WindowsAzure.Storage \n\nusing System;\nusing System.Drawing;\nusing System.Drawing.Drawing2D;\nusing System.Drawing.Imaging;\nusing System.IO;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\n\nstatic int requiredWidth = 800;\nstatic int requiredHeight = 600;\n\npublic static async Task Run(CloudBlockBlob inputBlob, ICollector string  outputQueueItem, TraceWriter log)\n{\n    log.Info($ processing File {inputBlob.Name} );\n\n    var ext = Path.GetExtension(inputBlob.Name);\n    log.Info($ Ext = {ext} );\n    if (!ext.ToLowerInvariant().Equals( .png )) {\n        log.Info($ Path {inputBlob.Name} is not a PNG file (skipping) );\n        outputQueueItem.Add(inputBlob.Name);\n        return;\n    }\n\n    var input = await inputBlob.OpenReadAsync();\n\n    // From WebJobs\n    var image = Image.FromStream(input);\n    if (image.Height   requiredHeight || image.Width   requiredWidth) {\n        log.Info($ Processing image {image.Height} x {image.Width} );\n        var destRect = new Rectangle(0, 0, requiredWidth, requiredHeight);\n        var destImage = new Bitmap(requiredWidth, requiredHeight);\n        destImage.SetResolution(image.HorizontalResolution, image.VerticalResolution);\n\n        using (var graphics = Graphics.FromImage(destImage))\n        {\n            graphics.CompositingMode = CompositingMode.SourceCopy;\n            graphics.CompositingQuality = CompositingQuality.Default;\n            graphics.InterpolationMode = InterpolationMode.Bicubic;\n            graphics.SmoothingMode = SmoothingMode.Default;\n            graphics.PixelOffsetMode = PixelOffsetMode.Default;\n            using (var wrapMode = new ImageAttributes()) {\n                wrapMode.SetWrapMode(WrapMode.TileFlipXY);\n                graphics.DrawImage(image, destRect, 0, 0, image.Width, image.Height, GraphicsUnit.Pixel, wrapMode);\n            }\n        }\n        image = destImage;\n    }\n\n    log.Info( Writing new Image to publicdata area );\n    using (var stream = new MemoryStream()) {\n        image.Save(stream, ImageFormat.Png);\n        stream.Position = 0;\n\n        var connectionString = Environment.GetEnvironmentVariable( zumobook_STORAGE );\n        CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n        CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n        CloudBlobContainer publicdata = blobClient.GetContainerReference( publicdata );\n        CloudBlockBlob blockBlob = publicdata.GetBlockBlobReference(inputBlob.Name);\n        blockBlob.UploadFromStream(stream);\n    }\n\n    outputQueueItem.Add(inputBlob.Name);\n}  While this will work, we need to work on the integrations.  Save the function, then click on  Integrate .  This is where you can set triggers (the things that cause the function to run), inputs (what is fed into the function) and outputs (what is generated by the function).  The first step is to change the  Blob parameter name  to match what is included in the function call - in this case, my parameter is  inputBlob .  I could have called the trigger (which also becomes an input)  myBlob  and\nsaved myself the trouble.  I am against prefixing variables with  my , so I like to change this.  My original WebJob put the source file into a queue called  delete , which I've already pre-created.  I want to create an  Output  to do this.  Click  + New Output  and choose  Azure Storage Queue .  Click on  Select .   You can fill in the details of the queue as follows:   I'm selecting the same storage account as the rest of my application is using.  The Message parameter name must match the signature of the static method in my function.  Note that I'm using a  ICollector  isntead of a string. Async methods cannot use  out  types (or  ref  types, for that matter).  As a result, I need a type that allows me to add things to the queue.  The  ICollector  does that for me.   Warn  Don't forget to disable the ImageResizer WebJob in your mobile backend before testing the function.  You can find selected WebJobs in the  Properties\\webjobs-list.json  file.   You can test this function in the same way that you tested the WebJob.  Use the Cloud Explorer in Visual Studio to upload a file into the userdata area of your Storage Account.  If things go well, you will see the Info logs in the Logs area of the Function App.  You will also see an entry in the  delete  queue and a file of the same name in the  publicdata  area when you refresh them.", 
            "title": "Image Resize with Azure Functions"
        }, 
        {
            "location": "/chapter4/functions/#handling-file-deletion", 
            "text": "You can do a function to delete files using the  delete  queue as well.  Create a new function with the  QueueTrigger - CSharp  template.  Replace the code with the following:  #r  Microsoft.WindowsAzure.Storage \n\nusing System;\nusing Microsoft.WindowsAzure.Storage;\nusing Microsoft.WindowsAzure.Storage.Blob;\n\npublic static void Run(string myQueueItem, TraceWriter log)\n{\n    log.Info($ Processing File for Deletion {myQueueItem} );\n    var connectionString = Environment.GetEnvironmentVariable( zumobook_STORAGE );\n    CloudStorageAccount storageAccount = CloudStorageAccount.Parse(connectionString);\n    CloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n    CloudBlobContainer userdata = blobClient.GetContainerReference( userdata );\n    CloudBlockBlob blockBlob = userdata.GetBlockBlobReference(myQueueItem);\n    blockBlob.DeleteIfExists();\n}  With this in place, when you upload an image to userdata, it will be processed and appear in the publicdata area.  The original file will be deleted.  You will be able to see the logs for all the functions in the Logs.", 
            "title": "Handling File Deletion"
        }, 
        {
            "location": "/chapter4/functions/#benefits-and-drawbacks", 
            "text": "There are a few things that stand out as drawbacks:   A separate resource group is required, which makes managing all resources together difficult.  Tooling is limited right now and comprises of a command line (for Node.js development) and a plugin for Visual Studio 2015.  That means no Intellisense, which is a primary problem.  It uses a separate pricing model, so you are paying extra (at high volumes) for Functions.   However, there are distinct advantages:   You don't have to worry about scaling - it happens automatically.  They are even simpler to debug and deploy than WebJobs (and that wasn't exactly tough).  Integrated Functions monitoring is better than WebJobs.   In all, I believe there is room in my toolbox for both WebJobs and Functions.  The fact that you can translate between them means that you don't have to pick - you can be very situational about the choice depending on need.", 
            "title": "Benefits and Drawbacks"
        }, 
        {
            "location": "/chapter4/recipes/", 
            "text": "Now that we have explored the syntax and methodologies of invoking server-side code, we can look at some very common use cases for custom code.\n\n\nStorage Related Operations\n\n\nWhen dealing with cloud concepts, there are multiple operating levels one can think about.  At the bottom layer is \nInfrastructure as a Service\n.  Most people think of this as the Virtual Machine layer, but it also incorporates basic networking and storage concepts.  As you move to higher level services, you gain a lot of efficiencies by adding software components, you lose a lot of the potential management headaches, but you also lose flexibility in what you can do to the platform.  At the top of the stack is \nSoftware as a Service\n.  You may be running a helpdesk, for example, but you are completely isolated from what operating system is being run, what web services are being run, APIs that can be accessed and language that is used.\n\n\nAzure Mobile Apps is an opinionated combination of a client and server SDK running on top of a standard ASP.NET based web service and is normally thought of as being a \nPlatform as a Service\n.  You get to choose what database to use, what tables to expose, and what programming language to use.  You don't get to determine when the operating system is patched or what patches are applied.  It's a middle of the road between SaaS and IaaS.\n\n\nThat isn't to say we can't dip down sometimes to deal with lower level cloud services, nor to access higher level SaaS APIs.  One of those times is when dealing with files.  Storage is conceptually easy - you have an amount of disk and you can store files on it.  However, the management of that storage is complicated.  Placing that storage at the service of a scalable web application is similarly complicated.  What we intend to do is develop a set of skills that make developing storage based mobile applications easy.\n\n\nBlobs, Table, Queues and files\n\n\nAt the top of my list of \"storage made complicated\" is the cloud storage concepts.  In the old days, we stored files on a file system and we didn't really have to worry about differing types of storage, redundancy and capabilities. Cloud storage tends to come in multiple flavors:\n\n\n\n\n\n\nThe base storage type is \nBlob Storage\n.  Put simply, you have containers (roughly analogous to directories) and blobs (roughly analogous to files).  It's the cheapest form of storage and is used for many things, including the underlying storage for virtual machine disks.  Blob storage has many advantages.  From a mobile perspective, developers will appreciate the upload/download restart capabilities within the SDK.\n\n\n\n\n\n\nWe've already introduced \nTable Storage\n in \nthe last chapter\n.  It is more analogous to a NoSQL store for storing key / attribute values.  It has a schemaless design, so you can store basic JSON objects.  However, it has limited query capabilities, as we discussed in the last chapter.  That makes it unsuited to large scale query-driven applications.\n\n\n\n\n\n\nYou may think you want \nFiles Storage\n.  This provides an SMB interface to the storage layer.  You would use Files Storage if you want to browse files from your PC or Mac as you can mount the file system directly from Azure Storage.\n\n\n\n\n\n\nFinally, \nQueue Storage\n provides cloud messaging between application components.  We'll get onto Azure Functions later on, during our look at Custom API.  Queue Storage will definitely be coming into play then.  Think of Queue Storage as the glue that ties work flow components together.\n\n\n\n\n\n\nThe real question is when should you use File Storage and when should you use Blob Storage.  For more applications, Blob Storage is going to save you money over File Storage, so it's pretty much always the better choice.  You should only be thinking of File Storage if you have other components of your system that need to access the data you upload that can only access that data via an SMB interface.\n\n\nIf you need to explore the data that you upload or download, you can use the \nAzure Storage Explorer\n as a standalone application or you can use the Cloud Explorer in \nVisual Studio\n.\n\n\nCreating and Linking a Storage Account\n\n\nBefore we can use storage, we need to set up a storage account and connect it to our environment.  This involves:\n\n\n\n\nCreate a Resource Group\n\n\nCreate an Azure App Service\n\n\nSet up authentication on the Azure App Service\n\n\nCreate a Storage Account\n\n\nLink the Storage Account to the Azure App Service.\n\n\n\n\nWe've already covered the first three items in previous chapters.  We've also created a storage account and linked it to the mobile backend during our look at the \nStorage Domain Manager\n.  To create a Storage Account:\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\nClick the big \n+ NEW\n button in the top left corner.\n\n\nClick \nData + Storage\n, then \nStorage account\n.\n\n\nFill in the form:\n\n\nThe name can only contain letters and numbers and must be unique.  A GUID without the dashes is a good choice.\n\n\nThe \nDeployment model\n should be set to \nResource manager\n.\n\n\nThe \nAccount kind\n should be set to \nGeneral purpose\n.\n\n\nThe \nPerformance\n should be set to \nStandard\n for this example.\n\n\nThe \nReplication\n should be set to \nLocally-redundant storage (LRS)\n.\n\n\nSet the \nResource group\n to your existing resource group.\n\n\nSet the \nLocation\n to the same location as your App Service.\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nJust like SQL Azure, Azure Storage has some great scalability and redundancy features if your backend takes advantage of them. For example, you have the option of \nPremium Storage\n - this provides all-SSD storage that has a large IOPS performance number.  You can also decide how redundant you want the storage.  Azure always keeps 3 copies of your data.  You can choose to increase the number of copies and decide whether the additional copies will be in the same datacenter, another datacenter in the same region or another region.  We have selected the slowest performance and least redundant options here to keep the cost down on your service.\n\n\n\n\nWarn\n\n\nThere is no \"free\" option for Azure Storage.  You pay by the kilobyte depending on the performance and redundancy selected.\n\n\n\n\nOnce the Azure Storage account is deployed, you can link the storage account to your App Service:\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick  \nData Connections\n under the \nMOBILE\n section in the settings menu.\n\n\nClick \n+ ADD\n\n\nIn the \nAdd data connection\n blade:\n\n\nSet the Type to \nStorage\n.\n\n\nClick the \nStorage\n link.\n\n\nIn the \nStorage Account\n selector, click the storage account you just created.\n\n\nClick the \nConnection string\n.\n\n\nIn the \nConnection string\n selector, make a note of the \nName\n field.\n\n\nClick \nOK\n.\n\n\nClick \nOK\n to close the \nAdd data connection\n blade.\n\n\n\n\n\n\n\n\nClick on the \nApplication Settings\n menu option, then scroll down to the \nConnection Strings\n section.  Note that the portal has created the connection string as an App Setting for you with the right value:\n\n\nDefaultEndpointsProtocol=https;AccountName=thebook;AccountKey=\nkey1\n\n\n\n\n\nBy default, the connection string is called \nMS_AzureStorageAccountConnectionString\n and we will use that throughout our examples.\n\n\nThe key is the access key for the storage.  When a storage account is created, two keys are also created.  The keys are used for secure access to the storage area.  You should never distribute the storage keys nor check them into source control.  If you feel they have been compromised, you should regenerate them.  There are two keys for this purpose.  The process of regeneration is:\n\n\n\n\nRegenerate KEY2\n\n\nPlace the regenerated KEY2 in the connection string and restart your App Service.\n\n\nRegenerate key1\n\n\nPlace the regenerated KEY1 in the connection string and restart your App Service.\n\n\n\n\nIn this way, your App Service will always be using KEY1 except during regeneration.  You can avoid the restart of your App Service by providing a management interface that sets the Account Key for the App Service.\n\n\n\n\nTip\n\n\nFor local development, there is the \nAzure Storage Emulator\n.  The connection string when using the Azure Storage Emulator is \nUseDevelopmentStorage=true\n.\n\n\n\n\nIt's normal to add the storage connection string to the \nWeb.config\n file with the following:\n\n\nconnectionStrings\n\n    \nadd name=\nMS_AzureStorageAccountConnectionString\n connectionString=\nUseDevelopmentStorage=true\n /\n\n\n/connectionStrings\n\n\n\n\n\nThis will be overwritten by the connection string in the App Service Application Settings.  Effectively, you will be using the Azure Storage Emulator during local development and Azure Storage when you deploy to Azure App Service.\n\n\nThe Shared Access Signature (SAS)\n\n\nThe storage account key is kind of like the root or Administrator password.  You should always protect it, never send it to a third party and regenerate it on a regular basis.  You avoid storing the storage account key in source code by linking the storage account to the App Service.  The key is stored in the connection string instead.  You should never ship an account key to your mobile account.\n\n\nThe Azure Storage SDK already has many of the features that you want in handling file upload and download.  Azure Storage is optimized for streaming, for example.  You can upload or download blobs in blocks, allowing you to restart the transfer and provide feedback to the user on progress, for example.   You will inevitably be drawn to having your mobile client interact with Azure Storage directly rather than having an intermediary web service for this reason.\n\n\nIf you want to interact with Azure Storage directly and you shouldn't give out the account key, how do you deal with the security of the service?  The answer is with a Shared Access Signature, or SAS.  The \nService SAS\n delegates access to just a single resource in one of the storage services (Blob, Table, Queue or File service).\n\n\n\n\nInfo\n\n\nThere is also an \nAccount SAS\n which delegates access to resources in more than one service.  You generally don't want this in application development.\n\n\n\n\nA service SAS is a URI that is used when accessing the resource.  It consists of the URI to the resource followed by a SAS token.  The SAS token is an cryptographically signed opaque token that the storage service decodes.  Minimally, it provides an expiry time and the permissions being granted to the SAS.\n\n\n\n\nWarn\n\n\nA SAS token \nALWAYS\n expires.  There is no way to produce a permanent SAS token.  If you think you need one, think again.  In mobile development, you \nNEVER\n want a non-expiring token.\n\n\n\n\nAccessing Azure Storage is always done with a specific \nversion of the REST API\n and that follows through to the SDK.  You should always request a SAS token for the appropriate API you are going to be using.   We'll cover the various methods of obtaining a SAS later in the chapter.\n\n\nUploading a File\n\n\nThe most normal tasks for dealing with files are the upload and download of files to blob storage.  There is a natural and consistent process to this which makes this recipe very repeatable.  First, deal with the things you need before you start:\n\n\n\n\nCreate an Azure Storage Account and link it to your Azure App Service.\n\n\nDecide how you want your files organized.\n\n\nCreate a WebAPI to generate a SAS token for your upload or download.\n\n\n\n\nBlob storage is organized in a typical directory structure.  Each directory is called a container, and each file is a blob.  In the examples for this section, I am going to store each uploaded file in a container based on the authenticated user.  My WebAPI will create the appropriate container and then return an appropriate SAS token.\n\n\nWe can set up our custom API as follows:\n\n\nnamespace Backend.Controllers\n{\n    [Authorize]\n    [MobileappController]\n    public class GetStorageTokenController : ApiController\n    {\n        private const string connString = \nCUSTOMCONNSTR_MS_AzureStorageAccountConnectionString\n;\n\n        public GetStorageTokenController()\n        {\n            ConnectionString = Environment.GetEnvironmentVariable(connString);\n            StorageAccount = CloudStorageAccount.Parse(ConnectionString);\n            BlobClient = StorageAccount.CreateCloudBlobClient();\n        }\n\n        public string ConnectionString { get; }\n\n        public CloudStorageAccount StorageAccount { get; }\n\n        public CloudBlobClient BlobClient { get; }\n    }\n}\n\n\n\n\nThe \nConnectionString\n property is the pointer to where the Azure Storage account is located and how to access it.  the \nStorageAccount\n is a reference to that Azure Storage account.  Finally, the \nBlobClient\n is an object used for accessing blob storage.  We can access any WebAPI methods in this class by using the endpoint \n/api/GetStorageToken\n within our mobile client or using Postman.\n\n\nAzure Storage doesn't have a true heirarchial container system.  It does have containers and directories to organize things though, so we are going to use that:\n\n\n    private const string containerName = \nuserdata\n;\n\n    [HttpGet]\n    public async Task\nStorageTokenViewModel\n GetAsync()\n    {\n        // The userId is the SID without the sid: prefix\n        var claimsPrincipal = User as ClaimsPrincipal;\n        var userId = claimsPrincipal\n            .FindFirst(ClaimTypes.NameIdentifier)\n            .Value.Substring(4);\n\n        // Errors creating the storage container result in a 500 Internal Server Error\n        var container = BlobClient.GetContainerReference(containerName);\n        await container.CreateIfNotExistsAsync();\n\n        // Get the user directory within the container\n        var directory = container.GetDirectoryReference(userId);\n        var blobName = Guid.NewGuid().ToString(\nN\n);\n        var blob = directory.GetBlockBlobReference(blobName);\n\n        // Create a policy for accessing the defined blob\n        var blobPolicy = new SharedAccessBlobPolicy\n        {\n            SharedAccessStartTime = DateTime.UtcNow.AddMinutes(-5),\n            SharedAccessExpiryTime = DateTime.UtcNow.AddMinutes(60),\n            Permissions = SharedAccessBlobPermissions.Read\n                        | SharedAccessBlobPermissions.Write\n                        | SharedAccessBlobPermissions.Create\n        };\n\n        return new StorageTokenViewModel\n        {\n            Name = blobName,\n            Uri = blob.Uri,\n            SasToken = blob.GetSharedAccessSignature(blobPolicy)\n        };\n    }\n\n\n\n\nThe main piece of work in this API is generating the policy that is then signed and returned to the user as the SAS Token.  The mobile device has permission to read, write and create the blob that we have defined for the next 60 minutes.  I've provided a policy that starts in the past in case there is a little amount of clock-skew between the mobile device and the backend.\n\n\n\n\nWarn\n\n\nContainer names must be a valid DNS name.  The most notable requirement here is between 3 and 64 lower-case letters.  Container names are case-sensitive.  Check \nthe documentation\n for full details on naming requirements.\n\n\n\n\nThe \nStorageTokenViewModel\n is used for serialization purposes:\n\n\npublic class StorageTokenViewModel\n{\n    public string Name { get; set; }\n    public Uri Uri { get; set; }\n    public string SasToken { get; set; }\n}\n\n\n\n\nWe can test this API using Postman.  First, generate an authentication token.  Then use Postman to\ndo a GET of the \n/api/GetStorageToken\n endpoint:\n\n\n\n\nThere are two pieces of information we need here.  Firstly, the \nuri\n property provides the URI that we are going to use to upload the file.  Secondly, the \nsasToken\n is appended to the \nuri\n when uploading to provide a link to the policy.  Note that the token start and expiry time are encoded and readable in the sasToken.\n\n\nIn real world applications, this is likely not the right method.  We might want to organize the files based on information that the mobile client provides us, for example.  We may also want to upload to a specific upload area and then download from another location, allowing processing of the files in between.  You may also want to append the uploaded file extension to the file before uploading.  There is no \"one size fits all\" token policy.  You must decide on the conditions under which you will allow upload and download capabilities and then provide the appropriate logic to generate the SAS token.\n\n\nThe Mobile Client\n\n\nOnce we have the logic to generate a SAS token, we can turn our attention to the mobile clients.  We need to do three things for uploading a file to the service:\n\n\n\n\nGet a reference to the file (as a Stream object).\n\n\nGenerate a SAS token using the custom API.\n\n\nUse the Azure Storage SDK to upload directly to the Azure Storage Account.\n\n\n\n\nYou should not upload to a custom API in your mobile backend.  This needlessly ties up your mobile backend, causing your mobile backend to be less efficient at scaling.  Your mobile backend will not have all the facilities that the Azure Storage endpoint has provided either.  Azure Storage provides upload and download restarts and progress bar capabilities.\n\n\nObtaining a reference to the file that you wish to upload is normally a per-platform API.  Obtaining a reference to a photo or video involves interacting with platform-specific APIs to provide access to camera and built-in photo storage capabilities on the phone. To support such a per-platform capability, we need to add an interface for the API to the \nAbstractions\\IPlatform.cs\n file:\n\n\nTask\nStream\n GetUploadFileAsync();\n\n\n\n\nThis API will interact with whatever photo sharing API is available on the device, open the requested file and return a standard \nStream\n object.  Loading a media file is made much simpler using the cross-platform \nXamarin Media plugin\n.  This plugin allows the user to take photos or video, or pick  the media file from a gallery.  It's available on NuGet, so add the \nXam.Plugin.Media\n plugin to each of the platform-specific projects.\n\n\n\n\nTip\n\n\nI still like separating out code that deals with the hardware of a mobile device into the platform-specific code.  You don't need to do such separation on this project.  I find that I inevitably have one thing or another that requires a platform-specific tweak, so starting with a platform-specific API is better.\n\n\n\n\nThe Xamarin Media plugin is used like this:\n\n\nawait CrossMedia.Current.Initialize();\n\nvar file = await CrossMedia.Current.PickPhotoAsync();\nvar stream = file.GetStream();\n\n\n\n\nThere are methods within the plugin to determine if a camera is available.  Different platforms require different permissions:\n\n\nAndroid\n\n\nAndroid requires the  \nWRITE_EXTERNAL_STORAGE\n, \nREAD_EXTERNAL_STORAGE\n and \nCAMERA\n permissions. If the mobile device is running Android M or later, the plugin will automatically prompt the user for runtime permissions.  You can set these permissions within Visual Studio:\n\n\n\n\nDouble-click the \nProperties\n node within the Android project.\n\n\nSelect \nAndroid Manifest\n.\n\n\nIn the \nRequired permissions\n list, check the box next to the required permissions by double-clicking the permission.\n\n\nSave the Properties (you may have to right-click on the TaskList.Droid tab and click on \nSave Selected Items\n).\n\n\n\n\niOS\n\n\nApple iOS requires the \nNSCameraUsageDescription\n and \nNSPhotoLibraryUsageDescription\n keys.  The string provided will be displayed to the user when they are prompted to provide permission.  You can set these keys within Visual Studio:\n\n\n\n\nRight-click on the \nInfo.plist\n file and select \nOpen with...\n\n\nChoose the \nXML (Text) Editor\n then click \nOK\n.\n\n\nWithin the \ndict\n node, add the following lines:\n\n\n\n\nkey\nNSCameraUsageDescription\n/key\n\n\nstring\nThis app needs access to the camera to take photos.\n/string\n\n\nkey\nNSPhotoLibraryUsageDescription\n/key\n\n\nstring\nThis app needs access to photos.\n/string\n\n\n\n\n\n\n\nSave and close the file.\n\n\n\n\nYou can choose whatever string you want to display to the user.  For more information on iOS 10 privacy permissions, review the \nXamarin Blog\n.\n\n\nUniversal Windows\n\n\nUniversal Windows may require the \nPictures Library\n capability:\n\n\n\n\nIn the \nTaskList.UWP (Universal Windows)\n project, open \nPackage.appxmanifest\n.\n\n\nSelect the \nCapabilities\n tab.\n\n\nCheck the box next to \nPictures Library\n.\n\n\nSave the manifest.\n\n\n\n\nImplementing the File Reader\n\n\nThe same code can be used in all three platform-specific projects, in the \n*Platform.cs\n file:\n\n\n    /// \nsummary\n\n    /// Picks a photo for uploading\n    /// \n/summary\n\n    /// \nreturns\nA Stream for the photo\n/returns\n\n    public async Task\nStream\n GetUploadFileAsync()\n    {\n        var mediaPlugin = CrossMedia.Current;\n        var mainPage = Xamarin.Forms.Application.Current.MainPage;\n\n        await mediaPlugin.Initialize();\n\n        if (mediaPlugin.IsPickPhotoSupported)\n        {\n            var mediaFile = await mediaPlugin.PickPhotoAsync();\n            return mediaFile.GetStream();\n        }\n        else\n        {\n            await mainPage.DisplayAlert(\nMedia Service Unavailable\n, \nCannot pick photo\n, \nOK\n);\n            return null;\n        }\n    }\n\n\n\n\nUploading a File\n\n\nWe can now put the individual pieces together to actually do an upload.  In this example, we are going to use the photo picker to pick a photo and then upload it, displaying a progress bar as it happens.  We start with the XAML code in \nPages\\TaskList.xaml\n.  We need a button in the toolbar to initiate the file upload:\n\n\n    \nContentPage.ToolbarItems\n\n        \nToolbarItem Name=\nRefresh\n\n                     Command=\n{Binding RefreshCommand}\n\n                     Icon=\nrefresh.png\n\n                     Order=\nPrimary\n\n                     Priority=\n0\n /\n\n        \nToolbarItem Name=\nAdd Task\n\n                     Command=\n{Binding AddNewItemCommand}\n\n                     Icon=\nadd.png\n\n                     Order=\nPrimary\n\n                     Priority=\n0\n /\n\n        \nToolbarItem Name=\nAdd File\n\n                     Command=\n{Binding AddNewFileCommand}\n\n                     Icon=\naddfile.png\n\n                     Order=\nPrimary\n\n                     Priority=\n0\n /\n\n    \n/ContentPage.ToolbarItems\n\n\n\n\n\nObtain a suitable \"Add File\" icon from the Internet and resize the image appropriately for the task.  You will need five images total:\n\n\n\n\nTaskList.Droid\\Resources\\drawable\\addfile.png should be 128x128 pixels\n\n\nTaskList.iOS\\Resources\\addfile.png should be 25x25 pixels\n\n\nTaskList.iOS\\Resources\\addfile@2x.png should be 50x50 pixels\n\n\nTaskList.iOS\\Resources\\addfile@3x.png should be 75x75 pixels\n\n\nTaskList.UWP\\addfile.png should be 128x128 pixels\n\n\n\n\nAll images should have a transparent background.\n\n\nThe storage token is retrieved from the backend via the cloud service.  Add the following to \nAbstractions\\ICloudService.cs\n:\n\n\n    // Custom APIs\n    Task\nStorageTokenViewModel\n GetSasTokenAsync();\n\n\n\n\nThis has a concrete implementation in \nServices\\AzureCloudService.cs\n:\n\n\n    public async Task\nStorageTokenViewModel\n GetSasTokenAsync()\n    {\n        var parameters = new Dictionary\nstring, string\n();\n        var storageToken = await Client.InvokeApiAsync\nStorageTokenViewModel\n(\nGetStorageToken\n, HttpMethod.Get, parameters);\n        return storageToken;\n    }\n\n\n\n\nThe \nStorageTokenViewModel\n is identical to the class in the \nGetStorageTokenController.cs\n controller in the Backend.  I've placed the class definition in the \nModels\n namespace for the client.  We could share this model between the backend and front end, but the case of sharing models is so rare I tend not to share the code.\n\n\nIn the \nTaskListViewModel.cs\n, we can define a command that is called when the Add File button is clicked:\n\n\n    /// \nsummary\n\n    /// Reference to the Platform Provider\n    /// \n/summary\n\n    public IPlatform PlatformProvider =\n DependencyService.Get\nIPlatform\n();\n\n    /// \nsummary\n\n    /// Bindable property for the AddNewFile Command\n    /// \n/summary\n\n    public ICommand AddNewFileCommand { get; }\n\n    /// \nsummary\n\n    /// User clicked on the Add New File button\n    /// \n/summary\n\n    private async Task AddNewFileAsync()\n    {\n        if (IsBusy)\n        {\n            return;\n        }\n        IsBusy = true;\n\n        try\n        {\n            // Get a stream for the file\n            var mediaStream = await PlatformProvider.GetUploadFileAsync();\n            if (mediaStream == null)\n            {\n                IsBusy = false;\n                return;\n            }\n\n            // Get the SAS token from the backend\n            var storageToken = await CloudService.GetSasTokenAsync();\n\n            // Use the SAS token to upload the file\n            var storageUri = new Uri($\n{storageToken.Uri}{storageToken.SasToken}\n);\n            var blobStorage = new CloudBlockBlob(storageUri);\n            await blobStorage.UploadFromStreamAsync(mediaStream);\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert(\nError Uploading File\n, ex.Message, \nOK\n);\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }\n\n\n\n\n\n\nWarn\n\n\nAzure Storage SDK support for PCL projects is only available in -preview editions.  When installing the SDK, ensure you check the \"Include prerelease\" box in the NuGet package manager.  The latest version with PCL (.NETPortable) support is v7.0.2-preview.\n\n\n\n\nYou can look at the uploaded files in the Azure portal:\n\n\n\n\nLog in to the \nAzure portal\n.\n\n\nClick \nAll resources\n, then your storage account.\n\n\nUnder \nSERVICES\n, click \nBlobs\n.\n\n\nClick your storage container (in this example, that's called \nuserdata\n)\n\n\n\n\n\n\nYou will see a folder for each user account.  The folder is named for the SID of the account - not the username.  It's a good idea to store the user SID with other data about the user in a table within your database.   This allows you to associate a real user with their SID since a user will never know what their SID is.\n\n\nImplementing a Progress bar\n\n\nIt's common to want to see the progress of the upload while it is happening.  For that, we need a progress bar.  First, let's add a hidden progress bar to our \nTaskList.xaml\n page:\n\n\n    \nActivityIndicator HorizontalOptions=\nFillAndExpand\n\n                        IsRunning=\n{Binding IsBusy}\n\n                        IsVisible=\n{Binding IsBusy}\n\n                        VerticalOptions=\nStart\n /\n\n    \nProgressBar x:Name=\nfileUploadProgress\n\n                    HeightRequest=\n3\n\n                    HorizontalOptions=\nFillAndExpand\n\n                    IsVisible=\n{Binding IsUploadingFile}\n\n                    Progress=\n{Binding FileProgress}\n /\n\n\n\n\n\nThis comes with two new bindable properties.  \nIsUploadingFile\n is a \nbool\n and \nFileProgress\n is a \nDouble\n.  \nFileProgress\n takes a value between 0 and 1 to indicate how far along the progress bar should be.  This code should be in the \nTaskListViewModel.cs\n file:\n\n\n    private bool isUploadingFile;\n    public bool IsUploadingFile\n    {\n        get { return isUploadingFile; }\n        set { SetProperty(ref isUploadingFile, value, \nIsUploadingFile\n); }\n    }\n\n    private Double fileProgress = 0.0;\n    public Double FileProgress\n    {\n        get { return fileProgress;  }\n        set { SetProperty(ref fileProgress, value, \nFileProgress\n); }\n    }\n\n\n\n\nFinally, we have to change the upload so that it happens a chunk at a time.  In the \nAddNewFileAsync()\n method, we can replace the upload code with this:\n\n\n    /// \nsummary\n\n    /// User clicked on the Add New File button\n    /// \n/summary\n\n    private async Task AddNewFileAsync()\n    {\n        if (IsBusy)\n        {\n            return;\n        }\n        IsBusy = true;\n\n        try\n        {\n            // Get a stream for the file\n            var mediaStream = await PlatformProvider.GetUploadFileAsync();\n            if (mediaStream == null)\n            {\n                IsBusy = false;\n                return;\n            }\n\n            // Get the SAS token from the backend\n            var storageToken = await CloudService.GetSasTokenAsync();\n\n            // Use the SAS token to get a reference to the blob storage\n            var storageUri = new Uri($\n{storageToken.Uri}{storageToken.SasToken}\n);\n            var blobStorage = new CloudBlockBlob(storageUri);\n\n            // Get the length of the stream\n            var mediaLength = mediaStream.Length;\n\n            // Initialize the blocks\n            int bytesInBlock = 1024;                // The number of bytes in a single block\n            var buffer = new byte[bytesInBlock];    // The buffer to hold the data during transfer\n            int totalBytesRead = 0;                 // The number of bytes read from the stream.\n            int bytesRead = 0;                      // The number of bytes read per block.\n            int blocksWritten = 0;                  // The # Blocks Written\n\n            IsUploadingFile = true;\n            FileProgress = 0.00;\n\n            // Loop through until we have processed the whole file\n            do\n            {\n                // Read a block from the media stream\n                bytesRead = mediaStream.Read(buffer, 0, bytesInBlock);\n\n                if (bytesRead \n 0)\n                {\n                    // Move the buffer into a memory stream\n                    using (var memoryStream = new MemoryStream(buffer, 0, bytesRead))\n                    {\n                        string blockId = GetBlockId(blocksWritten);\n                        await blobStorage.PutBlockAsync(blockId, memoryStream, null);\n                    }\n\n                    // Update the internal counters\n                    totalBytesRead += bytesRead;\n                    blocksWritten++;\n\n                    // Update the progress bar\n                    FileProgress = totalBytesRead / mediaLength;\n                }\n\n            } while (bytesRead \n 0);\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert(\nError Uploading File\n, ex.Message, \nOK\n);\n        }\n        finally\n        {\n            IsBusy = false;\n            IsUploadingFile = false;\n            FileProgress = 0.0;\n        }\n    }\n\n    /// \nsummary\n\n    /// Convert the Block ID to the string we need\n    /// \n/summary\n\n    /// \nparam name=\nblock\n/param\n\n    /// \nreturns\n/returns\n\n    private string GetBlockId(int block)\n    {\n        char[] tempID = new char[6];\n        string iStr = block.ToString();\n\n        for (int j = tempID.Length - 1; j \n (tempID.Length - iStr.Length - 1); j--)\n        {\n            tempID[j] = iStr[tempID.Length - j - 1];\n        }\n        byte[] blockIDBeforeEncoding = Encoding.UTF8.GetBytes(tempID);\n        return Convert.ToBase64String(blockIDBeforeEncoding);\n    }\n\n\n\n\nThe main work is done in the inner loop.  We split the media stream into 1024 byte blocks.  Each block is copied into a temporary buffer then transferred to cloud storage.  After each block is delivered, the \nFileProgress\n counter is updated which updates the progress bar.\n\n\nOne of the secrets for doing block-based streaming uploads is the \nGetBlockId()\n method.  This properly formats the block ID (based on a rather convoluted method that ends up being Base-64 encoded).  If you do not get this right, you will instead get a rather cryptic message about the query parameter for the HTTP request being wrong.\n\n\nDownloading a File\n\n\nYou can similarly download a file from blob storage.  The basics (such as the SAS token generator) are exactly the same as before.  To do the basic form:\n\n\n    // Get the SAS token from the backend\n    var storageToken = await CloudService.GetSasTokenAsync(filename);\n\n    // Use the SAS token to get a reference to the blob storage\n    var storageUri = new Uri($\n{storageToken.Uri}{storageToken.SasToken}\n);\n    var blobStorage = new CloudBlockBlob(storageUri);\n\n    // Get a stream for the blob file\n    var mediaStream = await blobStorage.OpenReadAsync();\n    // Do something with the mediaStream - like move it to storage\n    await PlatformProvider.StoreFileAsync(mediaStream);\n    // At the end, close the stream properly\n    mediaStream.Dispose();\n\n\n\n\nSimilarly, you can also produce a progress bar:\n\n\n    // Get the SAS token from the backend\n    var storageToken = await CloudService.GetSasTokenAsync(filename);\n\n    // Use the SAS token to get a reference to the blob storage\n    var storageUri = new Uri($\n{storageToken.Uri}{storageToken.SasToken}\n);\n    var blobStorage = new CloudBlockBlob(storageUri);\n    var mediaStream = await blobStorage.OpenReadAsync();\n\n    var mediaLength = mediaStream.Length;\n    byte[] buffer = new byte[1024];\n    var bytesRead = 0, totalBytesRead = 0;\n\n    // Do what you need to for opening your output file\n    do {\n        bytesRead = mediaStream.ReadAsync(buffer, 0, 1024);\n        if (bytesRead \n 0) {\n            // Do something with the buffer\n\n            totalBytesRead += bytesRead;\n            FileProgress = totalBytesRead / mediaLength;\n        }\n    } while (bytesRead \n 0);\n\n    // Potentially close your output file as well\n    mediaStream.Dispose();\n\n\n\n\nWhen downloading, you will need to update the \nGetStorageTokenController\n method to provide access to files.  One possibility is to provide read/write access to the entire container, allowing the mobile device to get a directory listing for browsing, for example.  You need to decide what permissions your mobile client needs, and provide a SAS token with those permissions.  Primarily, this is done by altering the policy that is generated when you retrieve the SAS token.\n\n\nTable Controllers and Webhooks\n\n\nI love writing asynchronous applications.  One of the four features I mentioned with Table Controllers is the\nWebhook.  In essence, if someone inserts, updates or deletes a record, you may want to do something asynchronously.\nFor example, you may want to do sentiment analysis on the record that was just uploaded, or perhaps execute some\ncustom code to simulate an offline custom API.\n\n\nA Webhook is a callback mechanism whereby the controller will do an HTTP POST when something happens.  It's an\nevent processing system over HTTP.  In this sample, we are going to generate a simple Webhook function that\nlogs the inserted record, then adjust our table controller to call the Webhook when an insert happens.\n\n\nLet's first of all create a Function that handles the request.  Create a Function App, and then create a new\nfunction based on the \nGeneric Webhook - CSharp\n template.  Call this function \nInsertTodoItemWebhook\n.  You\ncan call this whatever you want, but the URI of your Webhook is based on the name of your function.\n\n\nReplace the body of the function with the following:\n\n\n#r \nNewtonsoft.Json\n\n\nusing System;\nusing System.Net;\nusing Newtonsoft.Json;\n\npublic static async Task\nobject\n Run(HttpRequestMessage req, TraceWriter log)\n{\n    string jsonContent = await req.Content.ReadAsStringAsync();\n    dynamic data = JsonConvert.DeserializeObject(jsonContent);\n\n    log.Info($\nCreated New Todo ({data.Text}, {data.Complete})\n);\n\n    return req.CreateResponse(HttpStatusCode.OK);\n}\n\n\n\n\nNote the \nFunction Url\n at the top of the page.  You will need to copy and paste this later on.  You can\ntest the Function in isolation by putting the following in the \nRequest body\n panel:\n\n\n{\n    \nText\n: \ntest\n,\n    \nComplete\n: true\n}\n\n\n\n\nWhen you click the \nRun\n button, the log should show the Info line and the Output should show a \n200 OK\n\n\n\n\nYou can now turn your attention to the mobile backend.  I use a \nWebhook.cs\n helper:\n\n\nusing System;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading.Tasks;\n\nnamespace Backend.Helpers\n{\n    public static class Webhook\n    {\n        public static async Task\nHttpStatusCode\n SendAsync\nT\n(Uri uri, T data)\n        {\n            var httpClient = new HttpClient();\n            httpClient.BaseAddress = uri;\n            var response = await httpClient.PostAsJsonAsync\nT\n(\n, data);\n            return response.StatusCode;\n        }\n    }\n}\n\n\n\n\nThis allows me to call the Webhook in my \nTodoItemController.cs\n method like this:\n\n\n    public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n    {\n        TodoItem current = await InsertAsync(item);\n#pragma warning disable CS4014\n        Webhook.SendAsync\nTodoItem\n(new Uri(webhookUri), current);\n#pragma warning restore CS4014\n        return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n    }\n\n\n\n\nYou will see the Webhook is called when the value is inserted.  We don't await the \nSendAsync\nTodoItem\n()\n\ncall because we don't want the process to be held up while the webhook is running.  You will notice that\nthe response is sometimes returned to the user before the webhook is executed.", 
            "title": "Recipes"
        }, 
        {
            "location": "/chapter4/recipes/#storage-related-operations", 
            "text": "When dealing with cloud concepts, there are multiple operating levels one can think about.  At the bottom layer is  Infrastructure as a Service .  Most people think of this as the Virtual Machine layer, but it also incorporates basic networking and storage concepts.  As you move to higher level services, you gain a lot of efficiencies by adding software components, you lose a lot of the potential management headaches, but you also lose flexibility in what you can do to the platform.  At the top of the stack is  Software as a Service .  You may be running a helpdesk, for example, but you are completely isolated from what operating system is being run, what web services are being run, APIs that can be accessed and language that is used.  Azure Mobile Apps is an opinionated combination of a client and server SDK running on top of a standard ASP.NET based web service and is normally thought of as being a  Platform as a Service .  You get to choose what database to use, what tables to expose, and what programming language to use.  You don't get to determine when the operating system is patched or what patches are applied.  It's a middle of the road between SaaS and IaaS.  That isn't to say we can't dip down sometimes to deal with lower level cloud services, nor to access higher level SaaS APIs.  One of those times is when dealing with files.  Storage is conceptually easy - you have an amount of disk and you can store files on it.  However, the management of that storage is complicated.  Placing that storage at the service of a scalable web application is similarly complicated.  What we intend to do is develop a set of skills that make developing storage based mobile applications easy.", 
            "title": "Storage Related Operations"
        }, 
        {
            "location": "/chapter4/recipes/#blobs-table-queues-and-files", 
            "text": "At the top of my list of \"storage made complicated\" is the cloud storage concepts.  In the old days, we stored files on a file system and we didn't really have to worry about differing types of storage, redundancy and capabilities. Cloud storage tends to come in multiple flavors:    The base storage type is  Blob Storage .  Put simply, you have containers (roughly analogous to directories) and blobs (roughly analogous to files).  It's the cheapest form of storage and is used for many things, including the underlying storage for virtual machine disks.  Blob storage has many advantages.  From a mobile perspective, developers will appreciate the upload/download restart capabilities within the SDK.    We've already introduced  Table Storage  in  the last chapter .  It is more analogous to a NoSQL store for storing key / attribute values.  It has a schemaless design, so you can store basic JSON objects.  However, it has limited query capabilities, as we discussed in the last chapter.  That makes it unsuited to large scale query-driven applications.    You may think you want  Files Storage .  This provides an SMB interface to the storage layer.  You would use Files Storage if you want to browse files from your PC or Mac as you can mount the file system directly from Azure Storage.    Finally,  Queue Storage  provides cloud messaging between application components.  We'll get onto Azure Functions later on, during our look at Custom API.  Queue Storage will definitely be coming into play then.  Think of Queue Storage as the glue that ties work flow components together.    The real question is when should you use File Storage and when should you use Blob Storage.  For more applications, Blob Storage is going to save you money over File Storage, so it's pretty much always the better choice.  You should only be thinking of File Storage if you have other components of your system that need to access the data you upload that can only access that data via an SMB interface.  If you need to explore the data that you upload or download, you can use the  Azure Storage Explorer  as a standalone application or you can use the Cloud Explorer in  Visual Studio .", 
            "title": "Blobs, Table, Queues and files"
        }, 
        {
            "location": "/chapter4/recipes/#the-shared-access-signature-sas", 
            "text": "The storage account key is kind of like the root or Administrator password.  You should always protect it, never send it to a third party and regenerate it on a regular basis.  You avoid storing the storage account key in source code by linking the storage account to the App Service.  The key is stored in the connection string instead.  You should never ship an account key to your mobile account.  The Azure Storage SDK already has many of the features that you want in handling file upload and download.  Azure Storage is optimized for streaming, for example.  You can upload or download blobs in blocks, allowing you to restart the transfer and provide feedback to the user on progress, for example.   You will inevitably be drawn to having your mobile client interact with Azure Storage directly rather than having an intermediary web service for this reason.  If you want to interact with Azure Storage directly and you shouldn't give out the account key, how do you deal with the security of the service?  The answer is with a Shared Access Signature, or SAS.  The  Service SAS  delegates access to just a single resource in one of the storage services (Blob, Table, Queue or File service).   Info  There is also an  Account SAS  which delegates access to resources in more than one service.  You generally don't want this in application development.   A service SAS is a URI that is used when accessing the resource.  It consists of the URI to the resource followed by a SAS token.  The SAS token is an cryptographically signed opaque token that the storage service decodes.  Minimally, it provides an expiry time and the permissions being granted to the SAS.   Warn  A SAS token  ALWAYS  expires.  There is no way to produce a permanent SAS token.  If you think you need one, think again.  In mobile development, you  NEVER  want a non-expiring token.   Accessing Azure Storage is always done with a specific  version of the REST API  and that follows through to the SDK.  You should always request a SAS token for the appropriate API you are going to be using.   We'll cover the various methods of obtaining a SAS later in the chapter.", 
            "title": "The Shared Access Signature (SAS)"
        }, 
        {
            "location": "/chapter4/recipes/#uploading-a-file", 
            "text": "The most normal tasks for dealing with files are the upload and download of files to blob storage.  There is a natural and consistent process to this which makes this recipe very repeatable.  First, deal with the things you need before you start:   Create an Azure Storage Account and link it to your Azure App Service.  Decide how you want your files organized.  Create a WebAPI to generate a SAS token for your upload or download.   Blob storage is organized in a typical directory structure.  Each directory is called a container, and each file is a blob.  In the examples for this section, I am going to store each uploaded file in a container based on the authenticated user.  My WebAPI will create the appropriate container and then return an appropriate SAS token.  We can set up our custom API as follows:  namespace Backend.Controllers\n{\n    [Authorize]\n    [MobileappController]\n    public class GetStorageTokenController : ApiController\n    {\n        private const string connString =  CUSTOMCONNSTR_MS_AzureStorageAccountConnectionString ;\n\n        public GetStorageTokenController()\n        {\n            ConnectionString = Environment.GetEnvironmentVariable(connString);\n            StorageAccount = CloudStorageAccount.Parse(ConnectionString);\n            BlobClient = StorageAccount.CreateCloudBlobClient();\n        }\n\n        public string ConnectionString { get; }\n\n        public CloudStorageAccount StorageAccount { get; }\n\n        public CloudBlobClient BlobClient { get; }\n    }\n}  The  ConnectionString  property is the pointer to where the Azure Storage account is located and how to access it.  the  StorageAccount  is a reference to that Azure Storage account.  Finally, the  BlobClient  is an object used for accessing blob storage.  We can access any WebAPI methods in this class by using the endpoint  /api/GetStorageToken  within our mobile client or using Postman.  Azure Storage doesn't have a true heirarchial container system.  It does have containers and directories to organize things though, so we are going to use that:      private const string containerName =  userdata ;\n\n    [HttpGet]\n    public async Task StorageTokenViewModel  GetAsync()\n    {\n        // The userId is the SID without the sid: prefix\n        var claimsPrincipal = User as ClaimsPrincipal;\n        var userId = claimsPrincipal\n            .FindFirst(ClaimTypes.NameIdentifier)\n            .Value.Substring(4);\n\n        // Errors creating the storage container result in a 500 Internal Server Error\n        var container = BlobClient.GetContainerReference(containerName);\n        await container.CreateIfNotExistsAsync();\n\n        // Get the user directory within the container\n        var directory = container.GetDirectoryReference(userId);\n        var blobName = Guid.NewGuid().ToString( N );\n        var blob = directory.GetBlockBlobReference(blobName);\n\n        // Create a policy for accessing the defined blob\n        var blobPolicy = new SharedAccessBlobPolicy\n        {\n            SharedAccessStartTime = DateTime.UtcNow.AddMinutes(-5),\n            SharedAccessExpiryTime = DateTime.UtcNow.AddMinutes(60),\n            Permissions = SharedAccessBlobPermissions.Read\n                        | SharedAccessBlobPermissions.Write\n                        | SharedAccessBlobPermissions.Create\n        };\n\n        return new StorageTokenViewModel\n        {\n            Name = blobName,\n            Uri = blob.Uri,\n            SasToken = blob.GetSharedAccessSignature(blobPolicy)\n        };\n    }  The main piece of work in this API is generating the policy that is then signed and returned to the user as the SAS Token.  The mobile device has permission to read, write and create the blob that we have defined for the next 60 minutes.  I've provided a policy that starts in the past in case there is a little amount of clock-skew between the mobile device and the backend.   Warn  Container names must be a valid DNS name.  The most notable requirement here is between 3 and 64 lower-case letters.  Container names are case-sensitive.  Check  the documentation  for full details on naming requirements.   The  StorageTokenViewModel  is used for serialization purposes:  public class StorageTokenViewModel\n{\n    public string Name { get; set; }\n    public Uri Uri { get; set; }\n    public string SasToken { get; set; }\n}  We can test this API using Postman.  First, generate an authentication token.  Then use Postman to\ndo a GET of the  /api/GetStorageToken  endpoint:   There are two pieces of information we need here.  Firstly, the  uri  property provides the URI that we are going to use to upload the file.  Secondly, the  sasToken  is appended to the  uri  when uploading to provide a link to the policy.  Note that the token start and expiry time are encoded and readable in the sasToken.  In real world applications, this is likely not the right method.  We might want to organize the files based on information that the mobile client provides us, for example.  We may also want to upload to a specific upload area and then download from another location, allowing processing of the files in between.  You may also want to append the uploaded file extension to the file before uploading.  There is no \"one size fits all\" token policy.  You must decide on the conditions under which you will allow upload and download capabilities and then provide the appropriate logic to generate the SAS token.", 
            "title": "Uploading a File"
        }, 
        {
            "location": "/chapter4/recipes/#the-mobile-client", 
            "text": "Once we have the logic to generate a SAS token, we can turn our attention to the mobile clients.  We need to do three things for uploading a file to the service:   Get a reference to the file (as a Stream object).  Generate a SAS token using the custom API.  Use the Azure Storage SDK to upload directly to the Azure Storage Account.   You should not upload to a custom API in your mobile backend.  This needlessly ties up your mobile backend, causing your mobile backend to be less efficient at scaling.  Your mobile backend will not have all the facilities that the Azure Storage endpoint has provided either.  Azure Storage provides upload and download restarts and progress bar capabilities.  Obtaining a reference to the file that you wish to upload is normally a per-platform API.  Obtaining a reference to a photo or video involves interacting with platform-specific APIs to provide access to camera and built-in photo storage capabilities on the phone. To support such a per-platform capability, we need to add an interface for the API to the  Abstractions\\IPlatform.cs  file:  Task Stream  GetUploadFileAsync();  This API will interact with whatever photo sharing API is available on the device, open the requested file and return a standard  Stream  object.  Loading a media file is made much simpler using the cross-platform  Xamarin Media plugin .  This plugin allows the user to take photos or video, or pick  the media file from a gallery.  It's available on NuGet, so add the  Xam.Plugin.Media  plugin to each of the platform-specific projects.   Tip  I still like separating out code that deals with the hardware of a mobile device into the platform-specific code.  You don't need to do such separation on this project.  I find that I inevitably have one thing or another that requires a platform-specific tweak, so starting with a platform-specific API is better.   The Xamarin Media plugin is used like this:  await CrossMedia.Current.Initialize();\n\nvar file = await CrossMedia.Current.PickPhotoAsync();\nvar stream = file.GetStream();  There are methods within the plugin to determine if a camera is available.  Different platforms require different permissions:", 
            "title": "The Mobile Client"
        }, 
        {
            "location": "/chapter4/recipes/#android", 
            "text": "Android requires the   WRITE_EXTERNAL_STORAGE ,  READ_EXTERNAL_STORAGE  and  CAMERA  permissions. If the mobile device is running Android M or later, the plugin will automatically prompt the user for runtime permissions.  You can set these permissions within Visual Studio:   Double-click the  Properties  node within the Android project.  Select  Android Manifest .  In the  Required permissions  list, check the box next to the required permissions by double-clicking the permission.  Save the Properties (you may have to right-click on the TaskList.Droid tab and click on  Save Selected Items ).", 
            "title": "Android"
        }, 
        {
            "location": "/chapter4/recipes/#ios", 
            "text": "Apple iOS requires the  NSCameraUsageDescription  and  NSPhotoLibraryUsageDescription  keys.  The string provided will be displayed to the user when they are prompted to provide permission.  You can set these keys within Visual Studio:   Right-click on the  Info.plist  file and select  Open with...  Choose the  XML (Text) Editor  then click  OK .  Within the  dict  node, add the following lines:   key NSCameraUsageDescription /key  string This app needs access to the camera to take photos. /string  key NSPhotoLibraryUsageDescription /key  string This app needs access to photos. /string    Save and close the file.   You can choose whatever string you want to display to the user.  For more information on iOS 10 privacy permissions, review the  Xamarin Blog .", 
            "title": "iOS"
        }, 
        {
            "location": "/chapter4/recipes/#universal-windows", 
            "text": "Universal Windows may require the  Pictures Library  capability:   In the  TaskList.UWP (Universal Windows)  project, open  Package.appxmanifest .  Select the  Capabilities  tab.  Check the box next to  Pictures Library .  Save the manifest.", 
            "title": "Universal Windows"
        }, 
        {
            "location": "/chapter4/recipes/#implementing-the-file-reader", 
            "text": "The same code can be used in all three platform-specific projects, in the  *Platform.cs  file:      ///  summary \n    /// Picks a photo for uploading\n    ///  /summary \n    ///  returns A Stream for the photo /returns \n    public async Task Stream  GetUploadFileAsync()\n    {\n        var mediaPlugin = CrossMedia.Current;\n        var mainPage = Xamarin.Forms.Application.Current.MainPage;\n\n        await mediaPlugin.Initialize();\n\n        if (mediaPlugin.IsPickPhotoSupported)\n        {\n            var mediaFile = await mediaPlugin.PickPhotoAsync();\n            return mediaFile.GetStream();\n        }\n        else\n        {\n            await mainPage.DisplayAlert( Media Service Unavailable ,  Cannot pick photo ,  OK );\n            return null;\n        }\n    }", 
            "title": "Implementing the File Reader"
        }, 
        {
            "location": "/chapter4/recipes/#uploading-a-file_1", 
            "text": "We can now put the individual pieces together to actually do an upload.  In this example, we are going to use the photo picker to pick a photo and then upload it, displaying a progress bar as it happens.  We start with the XAML code in  Pages\\TaskList.xaml .  We need a button in the toolbar to initiate the file upload:       ContentPage.ToolbarItems \n         ToolbarItem Name= Refresh \n                     Command= {Binding RefreshCommand} \n                     Icon= refresh.png \n                     Order= Primary \n                     Priority= 0  / \n         ToolbarItem Name= Add Task \n                     Command= {Binding AddNewItemCommand} \n                     Icon= add.png \n                     Order= Primary \n                     Priority= 0  / \n         ToolbarItem Name= Add File \n                     Command= {Binding AddNewFileCommand} \n                     Icon= addfile.png \n                     Order= Primary \n                     Priority= 0  / \n     /ContentPage.ToolbarItems   Obtain a suitable \"Add File\" icon from the Internet and resize the image appropriately for the task.  You will need five images total:   TaskList.Droid\\Resources\\drawable\\addfile.png should be 128x128 pixels  TaskList.iOS\\Resources\\addfile.png should be 25x25 pixels  TaskList.iOS\\Resources\\addfile@2x.png should be 50x50 pixels  TaskList.iOS\\Resources\\addfile@3x.png should be 75x75 pixels  TaskList.UWP\\addfile.png should be 128x128 pixels   All images should have a transparent background.  The storage token is retrieved from the backend via the cloud service.  Add the following to  Abstractions\\ICloudService.cs :      // Custom APIs\n    Task StorageTokenViewModel  GetSasTokenAsync();  This has a concrete implementation in  Services\\AzureCloudService.cs :      public async Task StorageTokenViewModel  GetSasTokenAsync()\n    {\n        var parameters = new Dictionary string, string ();\n        var storageToken = await Client.InvokeApiAsync StorageTokenViewModel ( GetStorageToken , HttpMethod.Get, parameters);\n        return storageToken;\n    }  The  StorageTokenViewModel  is identical to the class in the  GetStorageTokenController.cs  controller in the Backend.  I've placed the class definition in the  Models  namespace for the client.  We could share this model between the backend and front end, but the case of sharing models is so rare I tend not to share the code.  In the  TaskListViewModel.cs , we can define a command that is called when the Add File button is clicked:      ///  summary \n    /// Reference to the Platform Provider\n    ///  /summary \n    public IPlatform PlatformProvider =  DependencyService.Get IPlatform ();\n\n    ///  summary \n    /// Bindable property for the AddNewFile Command\n    ///  /summary \n    public ICommand AddNewFileCommand { get; }\n\n    ///  summary \n    /// User clicked on the Add New File button\n    ///  /summary \n    private async Task AddNewFileAsync()\n    {\n        if (IsBusy)\n        {\n            return;\n        }\n        IsBusy = true;\n\n        try\n        {\n            // Get a stream for the file\n            var mediaStream = await PlatformProvider.GetUploadFileAsync();\n            if (mediaStream == null)\n            {\n                IsBusy = false;\n                return;\n            }\n\n            // Get the SAS token from the backend\n            var storageToken = await CloudService.GetSasTokenAsync();\n\n            // Use the SAS token to upload the file\n            var storageUri = new Uri($ {storageToken.Uri}{storageToken.SasToken} );\n            var blobStorage = new CloudBlockBlob(storageUri);\n            await blobStorage.UploadFromStreamAsync(mediaStream);\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert( Error Uploading File , ex.Message,  OK );\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }   Warn  Azure Storage SDK support for PCL projects is only available in -preview editions.  When installing the SDK, ensure you check the \"Include prerelease\" box in the NuGet package manager.  The latest version with PCL (.NETPortable) support is v7.0.2-preview.   You can look at the uploaded files in the Azure portal:   Log in to the  Azure portal .  Click  All resources , then your storage account.  Under  SERVICES , click  Blobs .  Click your storage container (in this example, that's called  userdata )    You will see a folder for each user account.  The folder is named for the SID of the account - not the username.  It's a good idea to store the user SID with other data about the user in a table within your database.   This allows you to associate a real user with their SID since a user will never know what their SID is.", 
            "title": "Uploading a File"
        }, 
        {
            "location": "/chapter4/recipes/#implementing-a-progress-bar", 
            "text": "It's common to want to see the progress of the upload while it is happening.  For that, we need a progress bar.  First, let's add a hidden progress bar to our  TaskList.xaml  page:       ActivityIndicator HorizontalOptions= FillAndExpand \n                        IsRunning= {Binding IsBusy} \n                        IsVisible= {Binding IsBusy} \n                        VerticalOptions= Start  / \n     ProgressBar x:Name= fileUploadProgress \n                    HeightRequest= 3 \n                    HorizontalOptions= FillAndExpand \n                    IsVisible= {Binding IsUploadingFile} \n                    Progress= {Binding FileProgress}  /   This comes with two new bindable properties.   IsUploadingFile  is a  bool  and  FileProgress  is a  Double .   FileProgress  takes a value between 0 and 1 to indicate how far along the progress bar should be.  This code should be in the  TaskListViewModel.cs  file:      private bool isUploadingFile;\n    public bool IsUploadingFile\n    {\n        get { return isUploadingFile; }\n        set { SetProperty(ref isUploadingFile, value,  IsUploadingFile ); }\n    }\n\n    private Double fileProgress = 0.0;\n    public Double FileProgress\n    {\n        get { return fileProgress;  }\n        set { SetProperty(ref fileProgress, value,  FileProgress ); }\n    }  Finally, we have to change the upload so that it happens a chunk at a time.  In the  AddNewFileAsync()  method, we can replace the upload code with this:      ///  summary \n    /// User clicked on the Add New File button\n    ///  /summary \n    private async Task AddNewFileAsync()\n    {\n        if (IsBusy)\n        {\n            return;\n        }\n        IsBusy = true;\n\n        try\n        {\n            // Get a stream for the file\n            var mediaStream = await PlatformProvider.GetUploadFileAsync();\n            if (mediaStream == null)\n            {\n                IsBusy = false;\n                return;\n            }\n\n            // Get the SAS token from the backend\n            var storageToken = await CloudService.GetSasTokenAsync();\n\n            // Use the SAS token to get a reference to the blob storage\n            var storageUri = new Uri($ {storageToken.Uri}{storageToken.SasToken} );\n            var blobStorage = new CloudBlockBlob(storageUri);\n\n            // Get the length of the stream\n            var mediaLength = mediaStream.Length;\n\n            // Initialize the blocks\n            int bytesInBlock = 1024;                // The number of bytes in a single block\n            var buffer = new byte[bytesInBlock];    // The buffer to hold the data during transfer\n            int totalBytesRead = 0;                 // The number of bytes read from the stream.\n            int bytesRead = 0;                      // The number of bytes read per block.\n            int blocksWritten = 0;                  // The # Blocks Written\n\n            IsUploadingFile = true;\n            FileProgress = 0.00;\n\n            // Loop through until we have processed the whole file\n            do\n            {\n                // Read a block from the media stream\n                bytesRead = mediaStream.Read(buffer, 0, bytesInBlock);\n\n                if (bytesRead   0)\n                {\n                    // Move the buffer into a memory stream\n                    using (var memoryStream = new MemoryStream(buffer, 0, bytesRead))\n                    {\n                        string blockId = GetBlockId(blocksWritten);\n                        await blobStorage.PutBlockAsync(blockId, memoryStream, null);\n                    }\n\n                    // Update the internal counters\n                    totalBytesRead += bytesRead;\n                    blocksWritten++;\n\n                    // Update the progress bar\n                    FileProgress = totalBytesRead / mediaLength;\n                }\n\n            } while (bytesRead   0);\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert( Error Uploading File , ex.Message,  OK );\n        }\n        finally\n        {\n            IsBusy = false;\n            IsUploadingFile = false;\n            FileProgress = 0.0;\n        }\n    }\n\n    ///  summary \n    /// Convert the Block ID to the string we need\n    ///  /summary \n    ///  param name= block /param \n    ///  returns /returns \n    private string GetBlockId(int block)\n    {\n        char[] tempID = new char[6];\n        string iStr = block.ToString();\n\n        for (int j = tempID.Length - 1; j   (tempID.Length - iStr.Length - 1); j--)\n        {\n            tempID[j] = iStr[tempID.Length - j - 1];\n        }\n        byte[] blockIDBeforeEncoding = Encoding.UTF8.GetBytes(tempID);\n        return Convert.ToBase64String(blockIDBeforeEncoding);\n    }  The main work is done in the inner loop.  We split the media stream into 1024 byte blocks.  Each block is copied into a temporary buffer then transferred to cloud storage.  After each block is delivered, the  FileProgress  counter is updated which updates the progress bar.  One of the secrets for doing block-based streaming uploads is the  GetBlockId()  method.  This properly formats the block ID (based on a rather convoluted method that ends up being Base-64 encoded).  If you do not get this right, you will instead get a rather cryptic message about the query parameter for the HTTP request being wrong.", 
            "title": "Implementing a Progress bar"
        }, 
        {
            "location": "/chapter4/recipes/#downloading-a-file", 
            "text": "You can similarly download a file from blob storage.  The basics (such as the SAS token generator) are exactly the same as before.  To do the basic form:      // Get the SAS token from the backend\n    var storageToken = await CloudService.GetSasTokenAsync(filename);\n\n    // Use the SAS token to get a reference to the blob storage\n    var storageUri = new Uri($ {storageToken.Uri}{storageToken.SasToken} );\n    var blobStorage = new CloudBlockBlob(storageUri);\n\n    // Get a stream for the blob file\n    var mediaStream = await blobStorage.OpenReadAsync();\n    // Do something with the mediaStream - like move it to storage\n    await PlatformProvider.StoreFileAsync(mediaStream);\n    // At the end, close the stream properly\n    mediaStream.Dispose();  Similarly, you can also produce a progress bar:      // Get the SAS token from the backend\n    var storageToken = await CloudService.GetSasTokenAsync(filename);\n\n    // Use the SAS token to get a reference to the blob storage\n    var storageUri = new Uri($ {storageToken.Uri}{storageToken.SasToken} );\n    var blobStorage = new CloudBlockBlob(storageUri);\n    var mediaStream = await blobStorage.OpenReadAsync();\n\n    var mediaLength = mediaStream.Length;\n    byte[] buffer = new byte[1024];\n    var bytesRead = 0, totalBytesRead = 0;\n\n    // Do what you need to for opening your output file\n    do {\n        bytesRead = mediaStream.ReadAsync(buffer, 0, 1024);\n        if (bytesRead   0) {\n            // Do something with the buffer\n\n            totalBytesRead += bytesRead;\n            FileProgress = totalBytesRead / mediaLength;\n        }\n    } while (bytesRead   0);\n\n    // Potentially close your output file as well\n    mediaStream.Dispose();  When downloading, you will need to update the  GetStorageTokenController  method to provide access to files.  One possibility is to provide read/write access to the entire container, allowing the mobile device to get a directory listing for browsing, for example.  You need to decide what permissions your mobile client needs, and provide a SAS token with those permissions.  Primarily, this is done by altering the policy that is generated when you retrieve the SAS token.", 
            "title": "Downloading a File"
        }, 
        {
            "location": "/chapter4/recipes/#table-controllers-and-webhooks", 
            "text": "I love writing asynchronous applications.  One of the four features I mentioned with Table Controllers is the\nWebhook.  In essence, if someone inserts, updates or deletes a record, you may want to do something asynchronously.\nFor example, you may want to do sentiment analysis on the record that was just uploaded, or perhaps execute some\ncustom code to simulate an offline custom API.  A Webhook is a callback mechanism whereby the controller will do an HTTP POST when something happens.  It's an\nevent processing system over HTTP.  In this sample, we are going to generate a simple Webhook function that\nlogs the inserted record, then adjust our table controller to call the Webhook when an insert happens.  Let's first of all create a Function that handles the request.  Create a Function App, and then create a new\nfunction based on the  Generic Webhook - CSharp  template.  Call this function  InsertTodoItemWebhook .  You\ncan call this whatever you want, but the URI of your Webhook is based on the name of your function.  Replace the body of the function with the following:  #r  Newtonsoft.Json \n\nusing System;\nusing System.Net;\nusing Newtonsoft.Json;\n\npublic static async Task object  Run(HttpRequestMessage req, TraceWriter log)\n{\n    string jsonContent = await req.Content.ReadAsStringAsync();\n    dynamic data = JsonConvert.DeserializeObject(jsonContent);\n\n    log.Info($ Created New Todo ({data.Text}, {data.Complete}) );\n\n    return req.CreateResponse(HttpStatusCode.OK);\n}  Note the  Function Url  at the top of the page.  You will need to copy and paste this later on.  You can\ntest the Function in isolation by putting the following in the  Request body  panel:  {\n     Text :  test ,\n     Complete : true\n}  When you click the  Run  button, the log should show the Info line and the Output should show a  200 OK   You can now turn your attention to the mobile backend.  I use a  Webhook.cs  helper:  using System;\nusing System.Net;\nusing System.Net.Http;\nusing System.Threading.Tasks;\n\nnamespace Backend.Helpers\n{\n    public static class Webhook\n    {\n        public static async Task HttpStatusCode  SendAsync T (Uri uri, T data)\n        {\n            var httpClient = new HttpClient();\n            httpClient.BaseAddress = uri;\n            var response = await httpClient.PostAsJsonAsync T ( , data);\n            return response.StatusCode;\n        }\n    }\n}  This allows me to call the Webhook in my  TodoItemController.cs  method like this:      public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n    {\n        TodoItem current = await InsertAsync(item);\n#pragma warning disable CS4014\n        Webhook.SendAsync TodoItem (new Uri(webhookUri), current);\n#pragma warning restore CS4014\n        return CreatedAtRoute( Tables , new { id = current.Id }, current);\n    }  You will see the Webhook is called when the value is inserted.  We don't await the  SendAsync TodoItem () \ncall because we don't want the process to be held up while the webhook is running.  You will notice that\nthe response is sometimes returned to the user before the webhook is executed.", 
            "title": "Table Controllers and Webhooks"
        }, 
        {
            "location": "/chapter5/concepts/", 
            "text": "Thus far, we've looked at options for communicating with the backend while the client is running.  When the user changes apps, the client is suspended or placed on a low priority background thread.  No user interaction is possible during this time.\n\n\nMost developers have a need to communicate interesting things to the user and this can only happen if the app is running.  Fortunately, the major mobile platform providers have implemented some form of push notifications which are delivered to the app when it isn't running.\n\n\nPush notifications are messages that are sent to your mobile client whether the app is running or not.  The mobile device will wake up your app to deliver the message.  You have probably seen many examples of push notifications in your daily mobile phone usage.  There are several uses, but they fall into two broad areas.  Marketing messages are sent to inform the user of the app of something.  Perhaps it's a new version, or a specific promotion for your favorite store.  Silent notifications are sent to inform the app that something important has happened.  For example, you may want to send a message when a data element has been updated in a table.  Silent notifications are generally not to be seen by the user.\n\n\nDelivery of these messages comes with some significant penalties.  You cannot guarantee the delivery of a message.  The user of the device decides whether to accept messages or not.  You cannot guarantee a delivery time, even though most messages are delivered within a couple of minutes.  Finally, there is no built in acknowledgement of the message.  You have to do something extra in code to ensure the delivery happens.\n\n\nHow Push Notifications Works\n\n\nEach platform provider provides their own push notification service.  For example, iOS uses \nApple Push Notification Service (APNS)\n.  Google uses \nFirebase Communications Manager (FCM)\n.  This used to be called Google Communications Manager or GCM.  It's the same service; just rebranded.  Newer versions of Windows (including Universal Windows) use \nWindows Notification Service (WNS)\n whereas older versions of Windows Phone used \nMicrosoft Platform Notification\nService (MPNS)\n.  There are other push notification services, for example, for FireOS (run by Amazon) and China (run by Baidu).\n\n\nIn all cases, the process is the same:\n\n\n\n\nThe mobile device initiates the process, registering with the Platform Notification Service (PNS).  It will receive a \nRegistration ID\n in return. The registration ID is specific to an app running on a specific device. Once you have the registration ID, you will pass that registration ID to your backend.  The backend will use the registration ID when communicating with the PNS to send your app messages.\n\n\nThis is where complexity rears its ugly head.  Without an intervening service, the backend will need to do the following:\n\n\n\n\nStore the registration ID and PNS in a database for later reference.\n\n\nLookup the list of registration IDs on a per-PNS basis and send a provided message in batches.\n\n\nHandle retry, incremental back-off, throttling and tracking for each message.\n\n\nDeal with registration failure and maintenance of the database.\n\n\n\n\nThis is just the start of the functionality.  In general, marketeers will want tracking of the messages (such as how many were opened or acted on, what demographics were the opened messages, etc.) and they will want to push to only a subset of users, targetted by opt-in lists or other demographic information.\n\n\nIntroducing Notification Hubs\n\n\nDeveloping a system for pushing notifications to devices is a significant undertaking.  I would not recommend anyone undertake such a service for their app.  Fortunately, there are a number of services that can do this for you.  Azure's entry into this space is \nAzure Notification Hubs\n.  Notification Hubs (or NH as we will call it) handles all the registration and bulk sending logic to allow you to send a single message to multiple recipients without having to\nworry about what platform they are on.  In addition, NH has support for tagging individual device registrations with information about the user, groups, or opt-in lists.\n\n\nAzure Mobile Apps has direct support for Notification Hubs within the client SDK and Azure App Service has a registration service built right in for NH, allowing you to easily integrate your mobile app with the facilities that NH provides.\n\n\n\n\nTip\n\n\nYou do not have to run Azure Mobile Apps or Azure App Service to use Notification Hubs.  You do need to have a registration service somewhere.  However, Notification Hubs is a standalone service.  Do not use Notification Hubs as the registration service - you will need to distribute the key with your service, and that opens up security concerns.\n\n\n\n\nNotification Hubs has two features that are important in mobile push scenarios - tags and templates.  while you will see these two features a lot in this chapter, we will also use other features of Notification Hubs, such as Scheduled Push, Geofenced Push and Analytics.\n\n\nTags\n\n\nWhen a device registers itself with Notification Hubs (via the registration endpoint), you can specify a number of tags that are associated with the device.  The tag allows you to segment the devices and push a message to only a portion of the devices.  Technically, a tag is a string.  The string can be up to 120 characters long, but has a restricted character set (alphanumeric plus a few special characters).\n\n\nYou can use tags to allow the user to register interest in a topic, or register on their behalf based on just about anything you want.  If you want to push to a department or users in a specific location, you can automatically register for those tags within the registration service.  You can also use tags to do \"user tagging\" - allowing you to push to a user ID or email address instead of a device ID.\n\n\nWhen sending a push notification, you can broadcast a message to everyone, but it's generally better to send to a tag.  You can also combine tags with boolean operations.  For example, you might want to push a Marketing message to all sales people in Washington with \n(state:Washington \n dept:Sales)\n.  Tag expressions like this are limited in the number of tags allowed.\n\n\nTemplates\n\n\nThere are multiple plaform notification systems and each one wants a message sent to them in a specific format.  APNS and FCM require a JSON payload (each of which is different), while WNS requires an XML payload.  Effectively, this makes the backend of your app responsible for a part of the presentation layer, which is something that has been avoided thus far.  In addition, you might want to localize the message for your audience and potentially use string replacement to customize the message for the recipient.\n\n\nTemplates provide a way to send cross-platform notifications and customize the message for each recipient.  You can use locale files to insert locale-specific messages into the template.\n\n\nConfiguring Notification Hubs\n\n\nOur first step is to configure our backend.  Thus far, we have implemented an Azure App Service with a SQL Azure database, and that is our starting point again.  To those resources, we will add the Notification Hub, which starts just like the addition of any other resource:\n\n\n\n\nLog into the \nAzure portal\n.\n\n\nClick on the \n+ NEW\n button in the top right corner (or the \n+ ADD\n button at the top of your resource group).\n\n\nSelect or search for \nNotification Hub\n.\n\n\nClick on \nCreate\n.\n\n\n\n\n\n\n\n\nEnter the information required.  You will need to create both a notification hub and a namespace.  I generally add the \n-ns\n designation to a namespace.\n\n\nClick on \nCreate\n.\n\n\n\n\nNotification Hubs has three tiers which give you increasing numbers of pushes, plus additional features.  The \nFree\n tier provides just push services.  The \nBasic\n tier gives you the more pushes plus the opportunity to buy additional pushes. Telemetry, scheduled push and multi-tenancy are only provided in the \nStandard\n tier, which should be your choice for production workloads.\n\n\nWhen considering architecture, you should use one Notification Hub per mobile backend.  Notification Hub namespaces are used for deployment grouping.  For example, you might want to give a different namespace to each tenant in a multi-tenant environment.\n\n\n\n\nApp Service Push is Global to the App Service\n\n\nIf you run your App Service with multiple slots (see \nChapter 9\n for details on slots), then note that the configuration of App Service Push is global to all slots within your App Service.\n\n\n\n\nOnce your notification hub has been created, you will see both the hub and the namespace listed in your resource group.  Note that we have not actually linked the notification hub to any platform notification services yet.  We will do that later.\n\n\nConfiguring Push Registration\n\n\nPush registration is generally handled by the mobile backend, and there is a feature of the App Service for this purpose.  Although it is possible, resist trying to get mobile clients to register with the notification hub directly.\n\n\n\n\nLog into the \nAzure portal\n.\n\n\nSelect your mobile backend.\n\n\nClick on \nPush\n (under the \nSETTINGS\n menu).\n\n\nClick on \nConnect\n.\n\n\nSelect the notification hub you created earlier.\n\n\nWait for the notification hub to be connected (it takes approximately 10 seconds).\n\n\n\n\n\n\nYou can now decide which tags are valid for this application.  The mobile client will request a list of tags.  The push registration service will use this information to register the appropriate tags with the notification hub.\n\n\n\n\nInfo\n\n\nThe process is called push registration, but the entity that is created in notification hubs is called an \nInstallation\n.\n\n\n\n\nThere are two types of tags.  Client requested tags may be requested by the mobile client during the registration process.  Automatically added tags are added by the mobile backend.  Let's take an example. My mobile backend is connected to Azure Active Directory.   I've configured my backend as follows:\n\n\n\n\nHere, I have three client requested tags and an automatically generated tag that is only added when authenticated.  Let's suppose that the mobile client requested \n[ topic:World topic:Sports ]\n and the mobile client was authenticated as username \nuser@foo.com\n.  With this configuration, the user would be registered for the following tags:\n\n\n\n\ntopic:Sports\n\n\nauto:user@foo.com\n\n\n\n\nThe topic:Politics and topic:News tags would not be added because the user did not request them.  The topic:World tag would not be added because it is not in the whitelist of allowed client requested tags.\n\n\n\n\nInfo\n\n\nThere is no ability to request any tag (a wild-card) because it is a large security hole.  With a wild-card tag, you could request push notifications for a user or group to which you were not allowed.\n\n\n\n\nThe \n$(provider.claim)\n format is used in automatically added tags to add claims from the authenticated userinto the notification hub installation.  You can use any claim that is returned by the \n/.auth/me\n endpoint.  The standard identifiers are:\n\n\n\n\n$(\nprovider\n.emailaddress)\n\n\n$(\nprovider\n.identityprovider)\n\n\n$(\nprovider\n.name)\n\n\n\n\nReplace \nprovider\n with the provider name (facebook, google, microsoftaccount, twitter or aad).  The list of claims that are available is different for each provider and additional claims may be available.  It is possible to configure Azure AD to return groups, for example.  Check the output of the  \n/.auth/me\n endpoint to determine which claims are available.\n\n\n\n\nInfo\n\n\nThe automatic claim tags are only included when authenticated with a supported authentication provider.  If you are using Custom Authentication, you must also use a custom WebAPI controller to do push registration if you wants claims based on your authentication.\n\n\n\n\nA claim name can resolve to multiple claim types.  For example, \n$(aad.name)\n resolves to the following claims:\n\n\n\n\nhttp://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\n\n\nhttp://schemas.microsoft.com/ws/2008/06/identity/claims/name\n\n\nname\n\n\n\n\nIn this case, three tags may be created - one for each unique name.\n\n\n\n\nWarn\n\n\nApp Service Push provides a default tag called _UserId.  This is currently badly formed based on the MD5 of the SID provided in the authentication token.  My recommendation is to not rely on or use the default tags that are provided.  Set up your own tags.\n\n\n\n\nOnce you have configure the tags, click on \nSave\n to save your work.\n\n\nRegistering Your Mobile Client\n\n\nAfter you have gained the requisite platform-specific registration ID, you need to pass this to the mobile backend.  The registration endpoint listens on \n/push/installations/{guid}\n where the GUID is a unique ID for the app on a specific mobile device.  The Azure Mobile Apps client generates this for you.  You must \nHTTP PUT\n a JSON \nInstallation\n object to this URL.  A simple (empty) \nInstallation\n object looks like this:\n\n\n{\n    \ninstallationId\n: \n{guid}\n,\n    \nplatform\n: \ngcm\n,\n    \npushChannel\n: \n{registrationid}\n\n}\n\n\n\n\nThe minimal settings are the installationId, platform, and pushChannel.  The pushChannel field needs to be set to the registration ID of the platform.  The platform is set to the appropriate value for the platform you are using - gcm, apns or wns.   You can also add tags and template into this installation object.  Check out the \nrecipes section\n for details on requesting tags and templates.\n\n\nThere is a simple class for implementing an installation, which I place in \nAbstractions\\DeviceInstallation.cs\n within the shared project:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json;\n\nnamespace TaskList.Abstractions\n{\n    public class DeviceInstallation\n    {\n        public DeviceInstallation()\n        {\n            Tags = new List\nstring\n();\n            Templates = new Dictionary\nstring, PushTemplate\n();\n        }\n\n        [JsonProperty(PropertyName = \ninstallationId\n)]\n        public string InstallationId { get; set; }\n\n        [JsonProperty(PropertyName = \nplatform\n)]\n        public string Platform { get; set; }\n\n        [JsonProperty(PropertyName = \npushChannel\n)]\n        public string PushChannel { get; set; }\n\n        [JsonProperty(PropertyName = \ntags\n)]\n        public List\nstring\n Tags { get; set; }\n\n        [JsonProperty(PropertyName = \ntemplates\n)]\n        public Dictionary\nstring, PushTemplate\n Templates { get; set; }\n    }\n\n    public class PushTemplate\n    {\n        public PushTemplate()\n        {\n            Tags = new List\nstring\n();\n        }\n\n        [JsonProperty(PropertyName = \nbody\n)]\n        public string Body { get; set; }\n\n        [JsonProperty(PropertyName = \ntags\n)]\n        public List\nstring\n Tags { get; set; }\n    }\n}\n\n\n\n\nThe \nInstallation\n object is also available in the Notification Hubs SDK, but I find bringing in the entire Notification Hubs SDK just for this is a little overkill.  You can create a suitable installation and register with the \nInvokeApiAsync\nT,U\n()\n method:\n\n\n    public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (GcmClient.IsRegistered(RootView))\n        {\n            try\n            {\n                var registrationId = GcmClient.GetRegistrationId(RootView);\n                //var push = client.GetPush();\n                //await push.RegisterAsync(registrationId);\n\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform = \ngcm\n,\n                    PushChannel = registrationId\n                };\n\n                // Set up tags to request\n                installation.Tags.Add(\ntopic:Sports\n);\n\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @\n{\ndata\n:{\nmessage\n:\n$(messageParam)\n}}\n\n                };\n                installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync\nDeviceInstallation, DeviceInstallation\n(\n                    $\n/push/installations/{client.InstallationId}\n,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary\nstring, string\n());\n            }\n            catch (Exception ex)\n            {\n                Log.Error(\nDroidPlatformProvider\n, $\nCould not register with NH: {ex.Message}\n);\n            }\n        }\n        else\n        {\n            Log.Error(\nDroidPlatformProvider\n, $\nNot registered with GCM\n);\n        }\n    }\n\n\n\n\n\n\nWhere is that endpoint?\n\n\nThe \n/push/installations\n endpoint is part of App Service Push - a feature of the Azure App Service resource.  This exists on your \n.azurewebsites.net\n domain.  It is not part of Notification Hubs.\n\n\n\n\nThis is normally placed within the platform-specific provider since the details on how to get the registration ID and platform are different.  This version is for the Android edition as an example.  Using \nInvokeApiAsync()\n instead of relying on the in-built Azure Mobile Apps push registration methods gives you more control over the process of registration.\n\n\nNext Steps\n\n\nEach push notification system is different and requires different configuration in both the mobile client and backend registration system.  You can jump directly to a specific platform:\n\n\n\n\nAndroid\n.\n\n\niOS\n.\n\n\nWindows\n.", 
            "title": "Concepts"
        }, 
        {
            "location": "/chapter5/concepts/#how-push-notifications-works", 
            "text": "Each platform provider provides their own push notification service.  For example, iOS uses  Apple Push Notification Service (APNS) .  Google uses  Firebase Communications Manager (FCM) .  This used to be called Google Communications Manager or GCM.  It's the same service; just rebranded.  Newer versions of Windows (including Universal Windows) use  Windows Notification Service (WNS)  whereas older versions of Windows Phone used  Microsoft Platform Notification\nService (MPNS) .  There are other push notification services, for example, for FireOS (run by Amazon) and China (run by Baidu).  In all cases, the process is the same:   The mobile device initiates the process, registering with the Platform Notification Service (PNS).  It will receive a  Registration ID  in return. The registration ID is specific to an app running on a specific device. Once you have the registration ID, you will pass that registration ID to your backend.  The backend will use the registration ID when communicating with the PNS to send your app messages.  This is where complexity rears its ugly head.  Without an intervening service, the backend will need to do the following:   Store the registration ID and PNS in a database for later reference.  Lookup the list of registration IDs on a per-PNS basis and send a provided message in batches.  Handle retry, incremental back-off, throttling and tracking for each message.  Deal with registration failure and maintenance of the database.   This is just the start of the functionality.  In general, marketeers will want tracking of the messages (such as how many were opened or acted on, what demographics were the opened messages, etc.) and they will want to push to only a subset of users, targetted by opt-in lists or other demographic information.", 
            "title": "How Push Notifications Works"
        }, 
        {
            "location": "/chapter5/concepts/#introducing-notification-hubs", 
            "text": "Developing a system for pushing notifications to devices is a significant undertaking.  I would not recommend anyone undertake such a service for their app.  Fortunately, there are a number of services that can do this for you.  Azure's entry into this space is  Azure Notification Hubs .  Notification Hubs (or NH as we will call it) handles all the registration and bulk sending logic to allow you to send a single message to multiple recipients without having to\nworry about what platform they are on.  In addition, NH has support for tagging individual device registrations with information about the user, groups, or opt-in lists.  Azure Mobile Apps has direct support for Notification Hubs within the client SDK and Azure App Service has a registration service built right in for NH, allowing you to easily integrate your mobile app with the facilities that NH provides.   Tip  You do not have to run Azure Mobile Apps or Azure App Service to use Notification Hubs.  You do need to have a registration service somewhere.  However, Notification Hubs is a standalone service.  Do not use Notification Hubs as the registration service - you will need to distribute the key with your service, and that opens up security concerns.   Notification Hubs has two features that are important in mobile push scenarios - tags and templates.  while you will see these two features a lot in this chapter, we will also use other features of Notification Hubs, such as Scheduled Push, Geofenced Push and Analytics.", 
            "title": "Introducing Notification Hubs"
        }, 
        {
            "location": "/chapter5/concepts/#tags", 
            "text": "When a device registers itself with Notification Hubs (via the registration endpoint), you can specify a number of tags that are associated with the device.  The tag allows you to segment the devices and push a message to only a portion of the devices.  Technically, a tag is a string.  The string can be up to 120 characters long, but has a restricted character set (alphanumeric plus a few special characters).  You can use tags to allow the user to register interest in a topic, or register on their behalf based on just about anything you want.  If you want to push to a department or users in a specific location, you can automatically register for those tags within the registration service.  You can also use tags to do \"user tagging\" - allowing you to push to a user ID or email address instead of a device ID.  When sending a push notification, you can broadcast a message to everyone, but it's generally better to send to a tag.  You can also combine tags with boolean operations.  For example, you might want to push a Marketing message to all sales people in Washington with  (state:Washington   dept:Sales) .  Tag expressions like this are limited in the number of tags allowed.", 
            "title": "Tags"
        }, 
        {
            "location": "/chapter5/concepts/#templates", 
            "text": "There are multiple plaform notification systems and each one wants a message sent to them in a specific format.  APNS and FCM require a JSON payload (each of which is different), while WNS requires an XML payload.  Effectively, this makes the backend of your app responsible for a part of the presentation layer, which is something that has been avoided thus far.  In addition, you might want to localize the message for your audience and potentially use string replacement to customize the message for the recipient.  Templates provide a way to send cross-platform notifications and customize the message for each recipient.  You can use locale files to insert locale-specific messages into the template.", 
            "title": "Templates"
        }, 
        {
            "location": "/chapter5/concepts/#configuring-notification-hubs", 
            "text": "Our first step is to configure our backend.  Thus far, we have implemented an Azure App Service with a SQL Azure database, and that is our starting point again.  To those resources, we will add the Notification Hub, which starts just like the addition of any other resource:   Log into the  Azure portal .  Click on the  + NEW  button in the top right corner (or the  + ADD  button at the top of your resource group).  Select or search for  Notification Hub .  Click on  Create .     Enter the information required.  You will need to create both a notification hub and a namespace.  I generally add the  -ns  designation to a namespace.  Click on  Create .   Notification Hubs has three tiers which give you increasing numbers of pushes, plus additional features.  The  Free  tier provides just push services.  The  Basic  tier gives you the more pushes plus the opportunity to buy additional pushes. Telemetry, scheduled push and multi-tenancy are only provided in the  Standard  tier, which should be your choice for production workloads.  When considering architecture, you should use one Notification Hub per mobile backend.  Notification Hub namespaces are used for deployment grouping.  For example, you might want to give a different namespace to each tenant in a multi-tenant environment.   App Service Push is Global to the App Service  If you run your App Service with multiple slots (see  Chapter 9  for details on slots), then note that the configuration of App Service Push is global to all slots within your App Service.   Once your notification hub has been created, you will see both the hub and the namespace listed in your resource group.  Note that we have not actually linked the notification hub to any platform notification services yet.  We will do that later.", 
            "title": "Configuring Notification Hubs"
        }, 
        {
            "location": "/chapter5/concepts/#configuring-push-registration", 
            "text": "Push registration is generally handled by the mobile backend, and there is a feature of the App Service for this purpose.  Although it is possible, resist trying to get mobile clients to register with the notification hub directly.   Log into the  Azure portal .  Select your mobile backend.  Click on  Push  (under the  SETTINGS  menu).  Click on  Connect .  Select the notification hub you created earlier.  Wait for the notification hub to be connected (it takes approximately 10 seconds).    You can now decide which tags are valid for this application.  The mobile client will request a list of tags.  The push registration service will use this information to register the appropriate tags with the notification hub.   Info  The process is called push registration, but the entity that is created in notification hubs is called an  Installation .   There are two types of tags.  Client requested tags may be requested by the mobile client during the registration process.  Automatically added tags are added by the mobile backend.  Let's take an example. My mobile backend is connected to Azure Active Directory.   I've configured my backend as follows:   Here, I have three client requested tags and an automatically generated tag that is only added when authenticated.  Let's suppose that the mobile client requested  [ topic:World topic:Sports ]  and the mobile client was authenticated as username  user@foo.com .  With this configuration, the user would be registered for the following tags:   topic:Sports  auto:user@foo.com   The topic:Politics and topic:News tags would not be added because the user did not request them.  The topic:World tag would not be added because it is not in the whitelist of allowed client requested tags.   Info  There is no ability to request any tag (a wild-card) because it is a large security hole.  With a wild-card tag, you could request push notifications for a user or group to which you were not allowed.   The  $(provider.claim)  format is used in automatically added tags to add claims from the authenticated userinto the notification hub installation.  You can use any claim that is returned by the  /.auth/me  endpoint.  The standard identifiers are:   $( provider .emailaddress)  $( provider .identityprovider)  $( provider .name)   Replace  provider  with the provider name (facebook, google, microsoftaccount, twitter or aad).  The list of claims that are available is different for each provider and additional claims may be available.  It is possible to configure Azure AD to return groups, for example.  Check the output of the   /.auth/me  endpoint to determine which claims are available.   Info  The automatic claim tags are only included when authenticated with a supported authentication provider.  If you are using Custom Authentication, you must also use a custom WebAPI controller to do push registration if you wants claims based on your authentication.   A claim name can resolve to multiple claim types.  For example,  $(aad.name)  resolves to the following claims:   http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name  http://schemas.microsoft.com/ws/2008/06/identity/claims/name  name   In this case, three tags may be created - one for each unique name.   Warn  App Service Push provides a default tag called _UserId.  This is currently badly formed based on the MD5 of the SID provided in the authentication token.  My recommendation is to not rely on or use the default tags that are provided.  Set up your own tags.   Once you have configure the tags, click on  Save  to save your work.", 
            "title": "Configuring Push Registration"
        }, 
        {
            "location": "/chapter5/concepts/#registering-your-mobile-client", 
            "text": "After you have gained the requisite platform-specific registration ID, you need to pass this to the mobile backend.  The registration endpoint listens on  /push/installations/{guid}  where the GUID is a unique ID for the app on a specific mobile device.  The Azure Mobile Apps client generates this for you.  You must  HTTP PUT  a JSON  Installation  object to this URL.  A simple (empty)  Installation  object looks like this:  {\n     installationId :  {guid} ,\n     platform :  gcm ,\n     pushChannel :  {registrationid} \n}  The minimal settings are the installationId, platform, and pushChannel.  The pushChannel field needs to be set to the registration ID of the platform.  The platform is set to the appropriate value for the platform you are using - gcm, apns or wns.   You can also add tags and template into this installation object.  Check out the  recipes section  for details on requesting tags and templates.  There is a simple class for implementing an installation, which I place in  Abstractions\\DeviceInstallation.cs  within the shared project:  using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json;\n\nnamespace TaskList.Abstractions\n{\n    public class DeviceInstallation\n    {\n        public DeviceInstallation()\n        {\n            Tags = new List string ();\n            Templates = new Dictionary string, PushTemplate ();\n        }\n\n        [JsonProperty(PropertyName =  installationId )]\n        public string InstallationId { get; set; }\n\n        [JsonProperty(PropertyName =  platform )]\n        public string Platform { get; set; }\n\n        [JsonProperty(PropertyName =  pushChannel )]\n        public string PushChannel { get; set; }\n\n        [JsonProperty(PropertyName =  tags )]\n        public List string  Tags { get; set; }\n\n        [JsonProperty(PropertyName =  templates )]\n        public Dictionary string, PushTemplate  Templates { get; set; }\n    }\n\n    public class PushTemplate\n    {\n        public PushTemplate()\n        {\n            Tags = new List string ();\n        }\n\n        [JsonProperty(PropertyName =  body )]\n        public string Body { get; set; }\n\n        [JsonProperty(PropertyName =  tags )]\n        public List string  Tags { get; set; }\n    }\n}  The  Installation  object is also available in the Notification Hubs SDK, but I find bringing in the entire Notification Hubs SDK just for this is a little overkill.  You can create a suitable installation and register with the  InvokeApiAsync T,U ()  method:      public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (GcmClient.IsRegistered(RootView))\n        {\n            try\n            {\n                var registrationId = GcmClient.GetRegistrationId(RootView);\n                //var push = client.GetPush();\n                //await push.RegisterAsync(registrationId);\n\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform =  gcm ,\n                    PushChannel = registrationId\n                };\n\n                // Set up tags to request\n                installation.Tags.Add( topic:Sports );\n\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @ { data :{ message : $(messageParam) }} \n                };\n                installation.Templates.Add( genericTemplate , genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync DeviceInstallation, DeviceInstallation (\n                    $ /push/installations/{client.InstallationId} ,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary string, string ());\n            }\n            catch (Exception ex)\n            {\n                Log.Error( DroidPlatformProvider , $ Could not register with NH: {ex.Message} );\n            }\n        }\n        else\n        {\n            Log.Error( DroidPlatformProvider , $ Not registered with GCM );\n        }\n    }   Where is that endpoint?  The  /push/installations  endpoint is part of App Service Push - a feature of the Azure App Service resource.  This exists on your  .azurewebsites.net  domain.  It is not part of Notification Hubs.   This is normally placed within the platform-specific provider since the details on how to get the registration ID and platform are different.  This version is for the Android edition as an example.  Using  InvokeApiAsync()  instead of relying on the in-built Azure Mobile Apps push registration methods gives you more control over the process of registration.", 
            "title": "Registering Your Mobile Client"
        }, 
        {
            "location": "/chapter5/concepts/#next-steps", 
            "text": "Each push notification system is different and requires different configuration in both the mobile client and backend registration system.  You can jump directly to a specific platform:   Android .  iOS .  Windows .", 
            "title": "Next Steps"
        }, 
        {
            "location": "/chapter5/android/", 
            "text": "Push notifications for Android devices are handled by Firebase Cloud Messaging - a service run by Google.  This service used to be called \nGoogle Cloud Messaging\n and nothing has really changed since they rebranded the service.  Google wanted to bundle all their mobile offerings under one roof.  Firebase Cloud Messaging can still be used independently of the rest of Firebase.\n\n\nPreparing for development\n\n\n\n\nWarn\n\n\nPush notifications are one of those areas where it really pays to have a real device instead of an emulator.  It's frustrating to bump into so many issues with emulation, but it's almost inevitable.  If you are having problems, use a real device.\n\n\n\n\nBefore continuing, you will need an Android Emulator with the Google Play SDKs installed, or a real Android device.  You cannot use a vanilla emulator without the Google Play SDKs.  To set up an emulator with the appropriate SDKs.\n\n\n\n\nVisual Studio 2017\n\n\nVisual Studio 2017 provides four emulators and all of them include the Google APIs already, so you can skip this section if you are using Visual Studio 2017.  You still need to create an appropriate emulator if you are using Visual Studio 2015 or earlier.\n\n\n\n\nIn Visual Studio:\n\n\n\n\nClick \nTools\n -\n \nAndroid\n -\n \nAndroid SDK Manager\n.\n\n\nExpand \nAndroid 6.0 (API 23)\n.\n\n\nSelect \nGoogle APIs Intel x86 Atom System Image\n.\n\n\nExpand the \nExtras\n.\n\n\nSelect \nGoogle Play Services\n.\n\n\nClick on \nInstall\n.\n\n\n\n\nWait for the installation to complete, then close the Android SDK Manager.\n\n\n\n\nDisable Hyper-V\n\n\nThe Android Virtual Device from Google is incompatible with Hyper-V.  To disable Hyper-V, open a PowerShell prompt as an Administrator, then run the command  \nbcdedit /set hypervisorlaunchtype off\n and reboot.  You may also have to install \nIntel HAXM\n, which is available for download through the Android SDK Manager but may need to be installed separately, depending on how you installed the Android SDK.\n\n\n\n\n\n\nClick \nTools\n -\n \nAndroid\n -\n \nAndroid Emulator Manager\n.\n\n\nClick \nCreate...\n\n\nFill in the form.   You must specify:\n\n\nTarget: \nAndroid 6.0 - API Level 23\n\n\nCPU/ABI: \nGoogle APIs Intel Atom (x86)\n\n\nMemory Options: RAM: \n768\n\nOther than these settings, there is a lot of flexibility.  Consult the \nAndroid documentation\n.\n\n\n\n\n\n\nOnce ready, click on \nOK\n to create the device.\n\n\nClick \nOK\n to confirm the creation.\n\n\n\n\nTest that the emulator device works:\n\n\n\n\nClick the device you just created.\n\n\nClick \nStart...\n.\n\n\nClick \nLaunch...\n.\n\n\n\n\n\n\nFixing Permission Denied\n\n\nYou may get an error for \n...\\/system.img: Permission denied\n.  To fix this, open up a File Explorer and go to \nC:\\Program Files (x86)\\Android\n.  Right-click on \nandroid-sdk\n and select \nProperties\n.  Click the \nSecurity\n tab, then \nEdit\n.  Highlight Users in the Group or user names box, select Allow Full control in the Permissions for Users box, then click \nOK\n.  Once the Security settings have been applied, you can try to start the emulator again.\n\n\n\n\nIf the device starts and looks like a regular Android device, then you've completed the task.  You must have a working emulator or real device before continuing, so don't continue until you've got something working.\n\n\nRegistering your app with FCM\n\n\nTo start, you need a Firebase Developer Account.  Go to the \nFirebase Developer Console\n and sign in with a Google account.  If you have never been a Google developer before, the site will ask you to agree to their legal terms so your account can be converted to a developer account.\n\n\nOnce done, create a Firebase application.  If you previously created a Google project for authentication, you can import the Google project instead.  The effect is the same - you need a Firebase project at the end.\n\n\nClick on \nAdd Firebase to your Android app\n.  You aren't actually adding Firebase - just the push capabilities.\n\n\n\n\nThe next screen is confusing - it's talking about Android native development and we are developing in Xamarin Forms.  We need to enter a namespace, so use the namespace of your application.  The actual value does not matter as we are not using the majority of the Firebase SDK:\n\n\n\n\nClick on \nADD APP\n, then on \nCONTINUE\n, and finally \nFINISH\n.  The process will download a file: \ngoogle-services.json\n, which is used by Android Studio in native applications.  The instructions along the way are also for Android Studio.\n\n\nOnce done, click on the cog next to your project name and select \nPROJECT SETTINGS\n.\n\n\n\n\nClick on the \nCLOUD MESSAGING\n tab:\n\n\n\n\nThis gives you a server key and a sender ID.  You need the \"Legacy Server Key\" if you have two keys listed.  We will need these later.\n\n\nLinking Notification Hubs to FCM\n\n\nNow that you have the server key and sender ID, you can enter that information into Notification Hubs to enable it to push to your Android clients.\n\n\n\n\nLog in to the \nAzure portal\n.\n\n\nFind your App Service:\n\n\nUse \nAll Resources\n, then enter the name in the \nFilter items...\n box.\n\n\nUse \nResource Groups\n, find the resource group, then click on the items.\nWhich ever way you choose, enter the App Service.\n\n\n\n\n\n\nSelect \nPush\n from the menu (under \nSETTINGS\n).\n\n\nClick \nConfigure push notification services\n.\n\n\nClick \nGoogle (GCM)\n.\n\n\nEnter the server key in the \nAPI Key\n box.\n\n\nClick on \nSave\n.\n\n\n\n\nWe can now turn our attention to the mobile client.\n\n\nRegistering for Push Notifications\n\n\nRegistering for push notification is always a per-platform piece, so it has to go into the platform specific code.  We've seen what this means in terms of code before.  First, we create a new method definition in the \nIPlatformProvider.cs\n interface and the \nICloudService.cs\n interface, then we update the \nAzureCloudService.cs\n to call the platform-specific code.  Finally, we need an platform-specific implementation.\n\n\nFirst, the \nIPlatformProvider.cs\n - I'm going to add a new method: \nRegisterForPushNotifications()\n that will do all the work for me:\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\n\nnamespace TaskList.Abstractions\n{\n    public interface IPlatformProvider\n    {\n        MobileServiceUser RetrieveTokenFromSecureStore();\n\n        void StoreTokenInSecureStore(MobileServiceUser user);\n\n        void RemoveTokenFromSecureStore();\n\n        Task\nMobileServiceUser\n LoginAsync(MobileServiceClient client);\n\n        Task RegisterForPushNotifications(MobileServiceClient client);\n    }\n}\n\n\n\n\nThe first four methods are from our authentication work.  The final method is our new method.  There is a similar method in the \nICloudService.cs\n interface:\n\n\nusing System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n\n        Task\nMobileServiceUser\n LoginAsync();\n\n        Task LogoutAsync();\n\n        Task\nAppServiceIdentity\n GetIdentityAsync();\n\n        Task RegisterForPushNotifications();\n    }\n}\n\n\n\n\nWe also have a concrete implementation of this in \nAzureCloudService.cs\n that simply calls the platform specific version:\n\n\n    public async Task RegisterForPushNotifications()\n    {\n        var platformProvider = DependencyService.Get\nIPlatformProvider\n();\n        await platformProvider.RegisterForPushNotifications(client);\n    }\n\n\n\n\nThese are required irrespective of whether you are implementing iOS, Android, UWP or any combination of those platforms.  Let's now work with the Android platform-specific code.  Before we look at the platform specific code, we are going to need a library that implements the GCM/FCM library.\n\n\n\n\nAlternative Libraries\n\n\nWe are going to use a fairly old and venerable GCM Client.  There is an official Xamarin client for \nGoogle Play Services\n for example.  Feel free to experiment with other libraries.  The process for registration tends to be very similar between SDKs.\n\n\n\n\n\n\nRight-click on \nComponents\n in the \nTaskList.Droid\n project.\n\n\nSelect \nGet More Components...\n.\n\n\nEnter \nGoogle Cloud Messaging Client\n in the search box.\n\n\nSelect the \nGoogle Cloud Messaging Client\n.\n\n\nClick \nAdd to App\n.\n\n\n\n\nNow that you have the library installed, you can configure registration with FCM as follows:\n\n\n    public void Init(Context context)\n    {\n        RootView = context;\n        AccountStore = AccountStore.Create(context);\n\n        try\n        {\n            // Check to see if this client has the right permissions\n            GcmClient.CheckDevice(RootView);\n            GcmClient.CheckManifest(RootView);\n\n            // Register for push\n            GcmClient.Register(RootView, GcmHandler.SenderId);\n            Debug.WriteLine($\nGcmClient: Registered for push with FCM: {GcmClient.GetRegistrationId(RootView)}\n);\n        }\n        catch (Exception ex)\n        {\n            Debug.WriteLine($\nGcmClient: Cannot register for push: {ex.Message}\n);\n        }\n    }\n\n\n\n\nMost of this \nInit()\n method existed before.  The \nGcmClient\n calls are new.  The first two calls ensure that the mobile device is capable of handling registrations and that the application is properly configured.  The \nGcmClient.Register()\n call registers with FCM.  I'm dumping the registration ID we get back to the debug channel.\n\n\nWe also need to define a handler for the FCM calls.  FCM will send an out-of-band notification to the Android OS, which will figure out which application to wake up, and then call the defined handler.  Mine is in \nServices\\GcmHandler.cs\n:\n\n\nusing Android.App;\nusing Android.Content;\nusing Android.Util;\nusing Gcm.Client;\n\n[assembly: Permission(Name = \n@PACKAGE_NAME@.permission.C2D_MESSAGE\n)]\n[assembly: UsesPermission(Name = \n@PACKAGE_NAME@.permission.C2D_MESSAGE\n)]\n[assembly: UsesPermission(Name = \ncom.google.android.c2dm.permission.RECEIVE\n)]\n[assembly: UsesPermission(Name = \nandroid.permission.INTERNET\n)]\n[assembly: UsesPermission(Name = \nandroid.permission.WAKE_LOCK\n)]\nnamespace TaskList.Droid.Services\n{\n    [BroadcastReceiver(Permission = Constants.PERMISSION_GCM_INTENTS)]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_MESSAGE }, Categories = new string[] { \n@PACKAGE_NAME@\n })]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_REGISTRATION_CALLBACK }, Categories = new string[] { \n@PACKAGE_NAME@\n })]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_LIBRARY_RETRY }, Categories = new string[] { \n@PACKAGE_NAME@\n })]\n    public class GcmHandler : GcmBroadcastReceiverBase\nGcmService\n\n    {\n        // Replace with your Sender ID from the Firebase Console\n        public static string[] SenderId = new string[] { \n493509995880\n };\n\n    }\n\n    [Service]\n    public class GcmService : GcmServiceBase\n    {\n        public static string RegistrationID { get; private set; }\n\n        public GcmService() : base(GcmHandler.SenderId)\n        {\n\n        }\n\n        protected override void OnMessage(Context context, Intent intent)\n        {\n            Log.Info(\nGcmService\n, $\nMessage {intent.ToString()}\n);\n        }\n\n        protected override void OnError(Context context, string errorId)\n        {\n            Log.Error(\nGcmService\n, $\nERROR: {errorId}\n);\n        }\n\n        protected override void OnRegistered(Context context, string registrationId)\n        {\n            Log.Info(\nGcmService\n, $\nRegistered: {registrationId}\n);\n            GcmService.RegistrationID = registrationId;\n        }\n\n        protected override void OnUnRegistered(Context context, string registrationId)\n        {\n            Log.Info(\nGcmService\n, $\nUnregistered device from FCM\n);\n            GcmService.RegistrationID = null;\n        }\n    }\n}\n\n\n\n\n\n\nReplace the SenderId\n\n\nYou must replace the SenderId with your sender ID that you copied from the Firebase Console.\n\n\n\n\nLet's start at the top.  In order to use push notifications from Firebase, we need to tell our application to ask for that permission.  We need the OS to wake us up (WAKE_LOCK), access the Internet (INTERNET), and handle push notifications (C2D_MESSAGE and RECEIVE).\n\n\nWe use the GcmHandler to register a receiver.  This is done via the intents that are provided by the GcmClient package.  The class that will receive the events is the \nGcmService\n class.  There are four methods that must be defined there - registration, un-registration, messages and errors.  Right now, I'm just setting up some debug messages so I can see what is going on.\n\n\nWe can test this right now.  Place a breakpoint on each \nLog\n method, then run the app.\n\n\n\n\nThe application could not be started\n\n\nIf you get the error \"The application could not be started.  Ensure that the application has been installed to the target device and has a launchable activity (MainLauncher=true)\".  This is because the Android system can't handle package names in upper case.  Right-click the \nTaskList.Droid\n project and select \nProperties\n then \nAndroid Manifest\n.  Change the \nPackage name\n field to something lower case.\n\n\n\n\nOnce the application launches, the \nGcmService.OnRegistered()\n method is hit immediately.  You will be able to see the registration ID.  Click on \nContinue\n.  The device is now registered with FCM.  Note the registration ID as you will need it in the next step.\n\n\nSend an ad-hoc message as follows:\n\n\n\n\nGo to the \nFirebase Developer Console\n.\n\n\nClick your project.\n\n\nClick \nNotifications\n in the left hand nav.\n\n\nClick \nSend your first message\n.\n\n\nEnter something in the \nMessage text\n box.\n\n\nSelect \nSingle device\n under Target.\n\n\nPaste the registration ID in the \nGCM registration token\n box.\n\n\n\n\nClick \nSend Message\n.\n\n\n\n\n\n\n\n\nClick \nSend\n.\n\n\n\n\n\n\nAt this point the breakpoint in \nGcmService.OnMessage()\n will be hit.  You can examine the push by looking at the \nintent\n variable.  Click on \nStop\n to stop the application.\n\n\nMoving onto registration with Notification Hubs, we need to pass the registration ID we received from FCM to our mobile backend.  The App Service will ensure our device is registered properly.  We need to do this at the appropriate time, and that depends on a lot of factors.  There is no problem with registering multiple times if our requirements change.  In this app, I might choose to register at the beginning of the app, once the user has authenticated and if I had a settings page, when the settings changed.  This activity is done in the \nServices\\DroidPlatformProvider.cs\n file:\n\n\n    public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (GcmClient.IsRegistered(RootView))\n        {\n            try\n            {\n                var registrationId = GcmClient.GetRegistrationId(RootView);\n                //var push = client.GetPush();\n                //await push.RegisterAsync(registrationId);\n\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform = \ngcm\n,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add(\ntopic:Sports\n);\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @\n{\ndata\n:{\nmessage\n:\n$(message)\n}}\n\n                };\n                installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync\nDeviceInstallation, DeviceInstallation\n(\n                    $\n/push/installations/{client.InstallationId}\n,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary\nstring, string\n());\n            }\n            catch (Exception ex)\n            {\n                Log.Error(\nDroidPlatformProvider\n, $\nCould not register with NH: {ex.Message}\n);\n            }\n        }\n        else\n        {\n            Log.Error(\nDroidPlatformProvider\n, $\nNot registered with FCM\n);\n        }\n    }\n\n\n\n\n\n\nRegistering without Tags\n\n\nYou can also register without tags using the commented-out single line of code \npush.RegisterAsync(registrationId);\n\n\n\n\nYou should call \nRegisterForPushNotifications()\n whenever you feel that the definition of the push endpoint should change.  In my application, I added the registration after the \nLoginAsync()\n method in the \nViewModels\\EntryPageViewModel.cs\n file:\n\n\n    async Task ExecuteLoginCommand()\n    {\n        if (IsBusy)\n            return;\n        IsBusy = true;\n\n        try\n        {\n            var cloudService = ServiceLocator.Instance.Resolve\nICloudService\n();\n            await cloudService.LoginAsync();\n            await cloudService.RegisterForPushNotifications();\n            Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert(\nLogin Failed\n, ex.Message, \nOK\n);\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }\n\n\n\n\nRun the application again, with the same breakpoint in the \nGcmService.OnMessage()\n method.  You can remove the other breakpoints at this point.  Log into the application this time.  Let's explore some of the debugging tools for Notification Hubs.  Within Visual Studio, you can use \nView\n -\n \nServer Explorer\n to open the server explorer.  Expand the \nAzure\n node then the \nNotification Hubs\n node:\n\n\n\n\nDouble-click on your Notification Hub to open the developer console.  This provides two functions.  Firstly, we can click on the \nDevice Registrations\n tab:\n\n\n\n\nWe can see the registration of our test emulator device.  Note that our request for the \ntopic:Sports\n tag has also been honored.  If we did not configure that tag within the Push blade in the portal, that would not have been added to our registration.\n\n\nWe can also send to a specific device using the test send facility.  Click over to the \nTest Send\n facility.  Since we only have one device, we can use broadcast. Each installation will also be given a tag: \n$InstallationId:{guid}\n, where {guid} is the installation ID.  Select \nGoogle (GCM)\n -\n \nDefault\n to send a message to FCM.  The body will be filled in for you.\n\n\nSince we have already set a breakpoint at the \nOnMessage()\n method in \nGcmService.cs\n, our app is running and we have entered the app and logged in, click \nSend\n.  The breakpoint should be triggered within a reasonable amount of time.  I'd like to say that it will be near instantaneous, but push notifications may take some time depending on what is happening within the push notification system at the time.  The push should not take more than a couple of minutes to arrive and will likely arrive much quicker.\n\n\n\n\nWe now have the full registration lifecycle working and we can do a test send to hit the right piece of code.\n\n\nProcessing a Push Notification\n\n\nProcessing of push notifications is done within your mobile app, so you can process the push notifications however you want.  For example, you may want to silently pull a specific record from the server and insert it into your SQLite offline cache when a push arrives, or you may want to pop up a message that opens the mobile app.\n\n\nIn this example, we are going to show the message that comes in the \nmessage\n field of the data block.  There are more examples in the \nrecipes section\n.  The \nOnMessage()\n method in \nGcmService.cs\n is triggered on a push.  A simple notification looks like this:\n\n\n    protected override void OnMessage(Context context, Intent intent)\n    {\n        Log.Info(\nGcmService\n, $\nMessage {intent.ToString()}\n);\n\n        var message = intent.Extras.GetString(\nmessage\n);\n\n        var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager;\n        var uiIntent = new Intent(context, typeof(MainActivity));\n        NotificationCompat.Builder builder = new NotificationCompat.Builder(context);\n\n        var notification = builder.SetContentIntent(PendingIntent.GetActivity(context, 0, uiIntent, 0))\n            .SetSmallIcon(Android.Resource.Drawable.SymDefAppIcon)\n            .SetTicker(\nTaskList\n)\n            .SetContentTitle(\nTaskList\n)\n            .SetContentText(message)\n            .SetSound(RingtoneManager.GetDefaultUri(RingtoneType.Notification))\n            .SetAutoCancel(true)\n            .Build();\n\n        notificationManager.Notify(1, notification);\n    }\n\n\n\n\nThe major thing to note here is how we get the contents of the message.  The data block from the Notification Hub is received by the Intent into the Extras property.  If you have other properties in that block, you can retrieve them the same way.  The message field is standard, but you can pass other things.  An example would be to pass the table name and ID of an inserted record.", 
            "title": "Android Push"
        }, 
        {
            "location": "/chapter5/android/#preparing-for-development", 
            "text": "Warn  Push notifications are one of those areas where it really pays to have a real device instead of an emulator.  It's frustrating to bump into so many issues with emulation, but it's almost inevitable.  If you are having problems, use a real device.   Before continuing, you will need an Android Emulator with the Google Play SDKs installed, or a real Android device.  You cannot use a vanilla emulator without the Google Play SDKs.  To set up an emulator with the appropriate SDKs.   Visual Studio 2017  Visual Studio 2017 provides four emulators and all of them include the Google APIs already, so you can skip this section if you are using Visual Studio 2017.  You still need to create an appropriate emulator if you are using Visual Studio 2015 or earlier.   In Visual Studio:   Click  Tools  -   Android  -   Android SDK Manager .  Expand  Android 6.0 (API 23) .  Select  Google APIs Intel x86 Atom System Image .  Expand the  Extras .  Select  Google Play Services .  Click on  Install .   Wait for the installation to complete, then close the Android SDK Manager.   Disable Hyper-V  The Android Virtual Device from Google is incompatible with Hyper-V.  To disable Hyper-V, open a PowerShell prompt as an Administrator, then run the command   bcdedit /set hypervisorlaunchtype off  and reboot.  You may also have to install  Intel HAXM , which is available for download through the Android SDK Manager but may need to be installed separately, depending on how you installed the Android SDK.    Click  Tools  -   Android  -   Android Emulator Manager .  Click  Create...  Fill in the form.   You must specify:  Target:  Android 6.0 - API Level 23  CPU/ABI:  Google APIs Intel Atom (x86)  Memory Options: RAM:  768 \nOther than these settings, there is a lot of flexibility.  Consult the  Android documentation .    Once ready, click on  OK  to create the device.  Click  OK  to confirm the creation.   Test that the emulator device works:   Click the device you just created.  Click  Start... .  Click  Launch... .    Fixing Permission Denied  You may get an error for  ...\\/system.img: Permission denied .  To fix this, open up a File Explorer and go to  C:\\Program Files (x86)\\Android .  Right-click on  android-sdk  and select  Properties .  Click the  Security  tab, then  Edit .  Highlight Users in the Group or user names box, select Allow Full control in the Permissions for Users box, then click  OK .  Once the Security settings have been applied, you can try to start the emulator again.   If the device starts and looks like a regular Android device, then you've completed the task.  You must have a working emulator or real device before continuing, so don't continue until you've got something working.", 
            "title": "Preparing for development"
        }, 
        {
            "location": "/chapter5/android/#registering-your-app-with-fcm", 
            "text": "To start, you need a Firebase Developer Account.  Go to the  Firebase Developer Console  and sign in with a Google account.  If you have never been a Google developer before, the site will ask you to agree to their legal terms so your account can be converted to a developer account.  Once done, create a Firebase application.  If you previously created a Google project for authentication, you can import the Google project instead.  The effect is the same - you need a Firebase project at the end.  Click on  Add Firebase to your Android app .  You aren't actually adding Firebase - just the push capabilities.   The next screen is confusing - it's talking about Android native development and we are developing in Xamarin Forms.  We need to enter a namespace, so use the namespace of your application.  The actual value does not matter as we are not using the majority of the Firebase SDK:   Click on  ADD APP , then on  CONTINUE , and finally  FINISH .  The process will download a file:  google-services.json , which is used by Android Studio in native applications.  The instructions along the way are also for Android Studio.  Once done, click on the cog next to your project name and select  PROJECT SETTINGS .   Click on the  CLOUD MESSAGING  tab:   This gives you a server key and a sender ID.  You need the \"Legacy Server Key\" if you have two keys listed.  We will need these later.", 
            "title": "Registering your app with FCM"
        }, 
        {
            "location": "/chapter5/android/#linking-notification-hubs-to-fcm", 
            "text": "Now that you have the server key and sender ID, you can enter that information into Notification Hubs to enable it to push to your Android clients.   Log in to the  Azure portal .  Find your App Service:  Use  All Resources , then enter the name in the  Filter items...  box.  Use  Resource Groups , find the resource group, then click on the items.\nWhich ever way you choose, enter the App Service.    Select  Push  from the menu (under  SETTINGS ).  Click  Configure push notification services .  Click  Google (GCM) .  Enter the server key in the  API Key  box.  Click on  Save .   We can now turn our attention to the mobile client.", 
            "title": "Linking Notification Hubs to FCM"
        }, 
        {
            "location": "/chapter5/android/#registering-for-push-notifications", 
            "text": "Registering for push notification is always a per-platform piece, so it has to go into the platform specific code.  We've seen what this means in terms of code before.  First, we create a new method definition in the  IPlatformProvider.cs  interface and the  ICloudService.cs  interface, then we update the  AzureCloudService.cs  to call the platform-specific code.  Finally, we need an platform-specific implementation.  First, the  IPlatformProvider.cs  - I'm going to add a new method:  RegisterForPushNotifications()  that will do all the work for me:  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\n\nnamespace TaskList.Abstractions\n{\n    public interface IPlatformProvider\n    {\n        MobileServiceUser RetrieveTokenFromSecureStore();\n\n        void StoreTokenInSecureStore(MobileServiceUser user);\n\n        void RemoveTokenFromSecureStore();\n\n        Task MobileServiceUser  LoginAsync(MobileServiceClient client);\n\n        Task RegisterForPushNotifications(MobileServiceClient client);\n    }\n}  The first four methods are from our authentication work.  The final method is our new method.  There is a similar method in the  ICloudService.cs  interface:  using System.Threading.Tasks;\nusing Microsoft.WindowsAzure.MobileServices;\nusing TaskList.Models;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n\n        Task MobileServiceUser  LoginAsync();\n\n        Task LogoutAsync();\n\n        Task AppServiceIdentity  GetIdentityAsync();\n\n        Task RegisterForPushNotifications();\n    }\n}  We also have a concrete implementation of this in  AzureCloudService.cs  that simply calls the platform specific version:      public async Task RegisterForPushNotifications()\n    {\n        var platformProvider = DependencyService.Get IPlatformProvider ();\n        await platformProvider.RegisterForPushNotifications(client);\n    }  These are required irrespective of whether you are implementing iOS, Android, UWP or any combination of those platforms.  Let's now work with the Android platform-specific code.  Before we look at the platform specific code, we are going to need a library that implements the GCM/FCM library.   Alternative Libraries  We are going to use a fairly old and venerable GCM Client.  There is an official Xamarin client for  Google Play Services  for example.  Feel free to experiment with other libraries.  The process for registration tends to be very similar between SDKs.    Right-click on  Components  in the  TaskList.Droid  project.  Select  Get More Components... .  Enter  Google Cloud Messaging Client  in the search box.  Select the  Google Cloud Messaging Client .  Click  Add to App .   Now that you have the library installed, you can configure registration with FCM as follows:      public void Init(Context context)\n    {\n        RootView = context;\n        AccountStore = AccountStore.Create(context);\n\n        try\n        {\n            // Check to see if this client has the right permissions\n            GcmClient.CheckDevice(RootView);\n            GcmClient.CheckManifest(RootView);\n\n            // Register for push\n            GcmClient.Register(RootView, GcmHandler.SenderId);\n            Debug.WriteLine($ GcmClient: Registered for push with FCM: {GcmClient.GetRegistrationId(RootView)} );\n        }\n        catch (Exception ex)\n        {\n            Debug.WriteLine($ GcmClient: Cannot register for push: {ex.Message} );\n        }\n    }  Most of this  Init()  method existed before.  The  GcmClient  calls are new.  The first two calls ensure that the mobile device is capable of handling registrations and that the application is properly configured.  The  GcmClient.Register()  call registers with FCM.  I'm dumping the registration ID we get back to the debug channel.  We also need to define a handler for the FCM calls.  FCM will send an out-of-band notification to the Android OS, which will figure out which application to wake up, and then call the defined handler.  Mine is in  Services\\GcmHandler.cs :  using Android.App;\nusing Android.Content;\nusing Android.Util;\nusing Gcm.Client;\n\n[assembly: Permission(Name =  @PACKAGE_NAME@.permission.C2D_MESSAGE )]\n[assembly: UsesPermission(Name =  @PACKAGE_NAME@.permission.C2D_MESSAGE )]\n[assembly: UsesPermission(Name =  com.google.android.c2dm.permission.RECEIVE )]\n[assembly: UsesPermission(Name =  android.permission.INTERNET )]\n[assembly: UsesPermission(Name =  android.permission.WAKE_LOCK )]\nnamespace TaskList.Droid.Services\n{\n    [BroadcastReceiver(Permission = Constants.PERMISSION_GCM_INTENTS)]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_MESSAGE }, Categories = new string[] {  @PACKAGE_NAME@  })]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_REGISTRATION_CALLBACK }, Categories = new string[] {  @PACKAGE_NAME@  })]\n    [IntentFilter(new string[] { Constants.INTENT_FROM_GCM_LIBRARY_RETRY }, Categories = new string[] {  @PACKAGE_NAME@  })]\n    public class GcmHandler : GcmBroadcastReceiverBase GcmService \n    {\n        // Replace with your Sender ID from the Firebase Console\n        public static string[] SenderId = new string[] {  493509995880  };\n\n    }\n\n    [Service]\n    public class GcmService : GcmServiceBase\n    {\n        public static string RegistrationID { get; private set; }\n\n        public GcmService() : base(GcmHandler.SenderId)\n        {\n\n        }\n\n        protected override void OnMessage(Context context, Intent intent)\n        {\n            Log.Info( GcmService , $ Message {intent.ToString()} );\n        }\n\n        protected override void OnError(Context context, string errorId)\n        {\n            Log.Error( GcmService , $ ERROR: {errorId} );\n        }\n\n        protected override void OnRegistered(Context context, string registrationId)\n        {\n            Log.Info( GcmService , $ Registered: {registrationId} );\n            GcmService.RegistrationID = registrationId;\n        }\n\n        protected override void OnUnRegistered(Context context, string registrationId)\n        {\n            Log.Info( GcmService , $ Unregistered device from FCM );\n            GcmService.RegistrationID = null;\n        }\n    }\n}   Replace the SenderId  You must replace the SenderId with your sender ID that you copied from the Firebase Console.   Let's start at the top.  In order to use push notifications from Firebase, we need to tell our application to ask for that permission.  We need the OS to wake us up (WAKE_LOCK), access the Internet (INTERNET), and handle push notifications (C2D_MESSAGE and RECEIVE).  We use the GcmHandler to register a receiver.  This is done via the intents that are provided by the GcmClient package.  The class that will receive the events is the  GcmService  class.  There are four methods that must be defined there - registration, un-registration, messages and errors.  Right now, I'm just setting up some debug messages so I can see what is going on.  We can test this right now.  Place a breakpoint on each  Log  method, then run the app.   The application could not be started  If you get the error \"The application could not be started.  Ensure that the application has been installed to the target device and has a launchable activity (MainLauncher=true)\".  This is because the Android system can't handle package names in upper case.  Right-click the  TaskList.Droid  project and select  Properties  then  Android Manifest .  Change the  Package name  field to something lower case.   Once the application launches, the  GcmService.OnRegistered()  method is hit immediately.  You will be able to see the registration ID.  Click on  Continue .  The device is now registered with FCM.  Note the registration ID as you will need it in the next step.  Send an ad-hoc message as follows:   Go to the  Firebase Developer Console .  Click your project.  Click  Notifications  in the left hand nav.  Click  Send your first message .  Enter something in the  Message text  box.  Select  Single device  under Target.  Paste the registration ID in the  GCM registration token  box.   Click  Send Message .     Click  Send .    At this point the breakpoint in  GcmService.OnMessage()  will be hit.  You can examine the push by looking at the  intent  variable.  Click on  Stop  to stop the application.  Moving onto registration with Notification Hubs, we need to pass the registration ID we received from FCM to our mobile backend.  The App Service will ensure our device is registered properly.  We need to do this at the appropriate time, and that depends on a lot of factors.  There is no problem with registering multiple times if our requirements change.  In this app, I might choose to register at the beginning of the app, once the user has authenticated and if I had a settings page, when the settings changed.  This activity is done in the  Services\\DroidPlatformProvider.cs  file:      public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (GcmClient.IsRegistered(RootView))\n        {\n            try\n            {\n                var registrationId = GcmClient.GetRegistrationId(RootView);\n                //var push = client.GetPush();\n                //await push.RegisterAsync(registrationId);\n\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform =  gcm ,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add( topic:Sports );\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @ { data :{ message : $(message) }} \n                };\n                installation.Templates.Add( genericTemplate , genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync DeviceInstallation, DeviceInstallation (\n                    $ /push/installations/{client.InstallationId} ,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary string, string ());\n            }\n            catch (Exception ex)\n            {\n                Log.Error( DroidPlatformProvider , $ Could not register with NH: {ex.Message} );\n            }\n        }\n        else\n        {\n            Log.Error( DroidPlatformProvider , $ Not registered with FCM );\n        }\n    }   Registering without Tags  You can also register without tags using the commented-out single line of code  push.RegisterAsync(registrationId);   You should call  RegisterForPushNotifications()  whenever you feel that the definition of the push endpoint should change.  In my application, I added the registration after the  LoginAsync()  method in the  ViewModels\\EntryPageViewModel.cs  file:      async Task ExecuteLoginCommand()\n    {\n        if (IsBusy)\n            return;\n        IsBusy = true;\n\n        try\n        {\n            var cloudService = ServiceLocator.Instance.Resolve ICloudService ();\n            await cloudService.LoginAsync();\n            await cloudService.RegisterForPushNotifications();\n            Application.Current.MainPage = new NavigationPage(new Pages.TaskList());\n        }\n        catch (Exception ex)\n        {\n            await Application.Current.MainPage.DisplayAlert( Login Failed , ex.Message,  OK );\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }  Run the application again, with the same breakpoint in the  GcmService.OnMessage()  method.  You can remove the other breakpoints at this point.  Log into the application this time.  Let's explore some of the debugging tools for Notification Hubs.  Within Visual Studio, you can use  View  -   Server Explorer  to open the server explorer.  Expand the  Azure  node then the  Notification Hubs  node:   Double-click on your Notification Hub to open the developer console.  This provides two functions.  Firstly, we can click on the  Device Registrations  tab:   We can see the registration of our test emulator device.  Note that our request for the  topic:Sports  tag has also been honored.  If we did not configure that tag within the Push blade in the portal, that would not have been added to our registration.  We can also send to a specific device using the test send facility.  Click over to the  Test Send  facility.  Since we only have one device, we can use broadcast. Each installation will also be given a tag:  $InstallationId:{guid} , where {guid} is the installation ID.  Select  Google (GCM)  -   Default  to send a message to FCM.  The body will be filled in for you.  Since we have already set a breakpoint at the  OnMessage()  method in  GcmService.cs , our app is running and we have entered the app and logged in, click  Send .  The breakpoint should be triggered within a reasonable amount of time.  I'd like to say that it will be near instantaneous, but push notifications may take some time depending on what is happening within the push notification system at the time.  The push should not take more than a couple of minutes to arrive and will likely arrive much quicker.   We now have the full registration lifecycle working and we can do a test send to hit the right piece of code.", 
            "title": "Registering for Push Notifications"
        }, 
        {
            "location": "/chapter5/android/#processing-a-push-notification", 
            "text": "Processing of push notifications is done within your mobile app, so you can process the push notifications however you want.  For example, you may want to silently pull a specific record from the server and insert it into your SQLite offline cache when a push arrives, or you may want to pop up a message that opens the mobile app.  In this example, we are going to show the message that comes in the  message  field of the data block.  There are more examples in the  recipes section .  The  OnMessage()  method in  GcmService.cs  is triggered on a push.  A simple notification looks like this:      protected override void OnMessage(Context context, Intent intent)\n    {\n        Log.Info( GcmService , $ Message {intent.ToString()} );\n\n        var message = intent.Extras.GetString( message );\n\n        var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager;\n        var uiIntent = new Intent(context, typeof(MainActivity));\n        NotificationCompat.Builder builder = new NotificationCompat.Builder(context);\n\n        var notification = builder.SetContentIntent(PendingIntent.GetActivity(context, 0, uiIntent, 0))\n            .SetSmallIcon(Android.Resource.Drawable.SymDefAppIcon)\n            .SetTicker( TaskList )\n            .SetContentTitle( TaskList )\n            .SetContentText(message)\n            .SetSound(RingtoneManager.GetDefaultUri(RingtoneType.Notification))\n            .SetAutoCancel(true)\n            .Build();\n\n        notificationManager.Notify(1, notification);\n    }  The major thing to note here is how we get the contents of the message.  The data block from the Notification Hub is received by the Intent into the Extras property.  If you have other properties in that block, you can retrieve them the same way.  The message field is standard, but you can pass other things.  An example would be to pass the table name and ID of an inserted record.", 
            "title": "Processing a Push Notification"
        }, 
        {
            "location": "/chapter5/ios/", 
            "text": "Push notifications for Apple devices is handled by \nApple Push Notification Service\n or APNS.  APNS is certificate based, rather than secret based as is the case with FCM.  You will find that there are two certificates - a test certificate that is used for test devices, and a production certificate that is used for production devices.  You can use a common certificate for both (a so-called Universal Certificate).  However, you must ensure that you use the appropriate endpoints - test or production.\n\n\nIt is imperitive that you use a Mac for this configuration.  You will be using Apple native tools to generate certificates and the process of configuring the APNS gateway is made easier by using XCode tools.  You can do certain things on a PC (like editing the plist files appropriately), but you will end up spending a significant amount of time on the Mac.  As a result, I'm going to do this entire section on a Mac.\n\n\nIf you have not done so already, read through the \nAndroid Push\n section to get all the code for the shared project - it won't be repeated in this section.\n\n\nRegistering with APNS\n\n\nRegistering with APNS is a multi-step process:\n\n\n\n\nRegister an App ID for your app, and select Push Notifications as a capability.\n\n\nCreate an appropriate certificate for the push channel (either a Development or Distribution certificate).\n\n\nConfigure Notification Hubs to use APNS.\n\n\nConfigure your application to support Push Notifications.\n\n\nAdd code for handling push notifications to your app.\n\n\n\n\nLet's cover each one in turn:\n\n\nRegister an App ID for your app\n\n\nOnce you get to adding push notifications to your application, you are going to need that full developers license from Apple.  You need to work with real devices and that means you need code signing certificates on your mac.  If you have not spent the cash for the Apple Developers program, then you will probably find you need to at this point.\n\n\nRegistering an App ID is handled on the \nApple Developer Portal\n.  Apple does a good job of \ndocumenting the process\n, so these instructions are duplicative of the instructions that Apple provides.\n\n\n\n\nGo to the \nApple Developer Portal\n and log in with your Apple developer ID.\n\n\nIn the left-hand menu, click \nCertificates, IDs \n Profiles\n.\n\n\nIn the left-hand menu, \nIdentifiers\n, click \nApp IDs\n.\n\n\nClick the \n+\n button in the top right corner.\n\n\n\n\nFill in the form:\n\n\n\n\nThe App ID Description is not used and can be set to anything (subject to validation rules)\n\n\nChoose an \nExplicit App ID\n for this app.\n\n\n\n\nEnter the App ID suffix according to the rules.  I used \ncom.shellmonger.tasklist\n.\n\n\n\n\n\n\n\n\nSelect \nPush Notifications\n in the \nApp Services\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nContinue\n when the form is complete.\n\n\n\n\nMake a note of the \nIdentifier\n in the next screen, then click \nRegister\n.\n\n\nClick \nDone\n.\n\n\n\n\nNote that the Push Notifications capability will be listed as \nConfigurable\n until you create a certificate that is used for push notifications.  Once that happens, the capability will be listed as \nEnabled\n.\n\n\nCreate a certificate for the push channel\n\n\nI mentioned earlier that APNS is certificate based.  That means that you need to generate an SSL certificate to fully configure push notifications:\n\n\n\n\nStaying in \nCertificates, Identifiers \n Profiles\n, click \nAll\n under the \nCertificates\n heading in the left hand menu.\n\n\nClick on the \n+\n button in the top right corner.\n\n\n\n\nSelect the \nApple Push Notification service SSL (Sandbox \n Production)\n\n\n\n\n\n\n\n\nClick \nContinue\n.\n\n\n\n\nSelect the App ID you just created from the list, then click \nContinue\n.\n\n\n\n\nFollow the on-screen instructions for creating a Certificate Signing Request (CSR).\n\n\n\n\n\n\n\n\nOnce you have generated the CSR, click \nContinue\n in the browser.\n\n\n\n\nSelect the CSR you just generated using the \nChoose File\n button, then click \nContinue\n.\n\n\nClick \nDownload\n to download the resulting certificate.\n\n\nClick \nDone\n when the download is complete.\n\n\nFind your downloaded certificate and double-click on it to import it into Keychain Access\n\n\n\n\nYour certificate will also appear in the \nCertificates\n \n \nAll\n list within the Apple Developer console.\n\n\nConfigure Notification Hubs\n\n\nNotification Hubs requires you to upload the certificate as a .p12 (PKCS#12) file.  To generate this file:\n\n\n\n\nOpen Keychain Access.\n\n\nSelect \nMy Certificates\n from the left hand menu\n\n\nLook for the certificate you just generated, and expand it to show the private key.\n\n\nRight-click the private key and select \nExport...\n.\n\n\nSelect \nPersonal Information Exchange (.p12)\n as the type and give it a name and location.\n\n\nClick \nSave\n.\n\n\nEnter a password (twice) to protect the certificate.\n\n\nClick \nOK\n.\n\n\n\n\nUpload the certificate to Azure:\n\n\n\n\nOpen and log into the \nAzure portal\n.\n\n\nSelect \nNotification Hubs\n, then the notification hub that is connected to your mobile backend.\n\n\nClick \nPush Notification Services\n, then select \nApple (APNS)\n.\n\n\nClick \n+ Upload Certificate\n.\n\n\n\n\nFill in the form:\n\n\n\n\nSelect the .p12 file you just created.\n\n\nEnter the password that you entered to secure the .p12 file.\n\n\nSelect \nSandbox\n (probably) or \nProduction\n as appropriate.\n\n\n\n\n\n\n\n\n\n\nClick \nOK\n.\n\n\n\n\n\n\nIt's important to figure out whether you are operating in the \nSandbox\n (Development) or \nProduction\n mode.  During development, it's likely that your device will be registered on the Apple Developer console and you will be operating in the sandbox.  Any device not listed with the developer console is considered \"production\".  You must update the certificate to a production certificate and specify the production mode when you release your app.\n\n\nApple APNS provides two endpoints for pushing notifications.  If you use the wrong one, then APNS will return an error code.  This will cause Notification Hubs to delete the registration and your push will fail.\n\n\nConfigure your application\n\n\nBefore we start with code, you will want a \nProvisioning Profile\n.  This small file is key to being able to use push notifications on your device.  You \nMUST\n have a real device at this point.  The easiest way for this to happen is to plug the iPhone or iPad that you want to use into your development system.  Once your device is recognized by iTunes, close iTunes down and start XCode.\n\n\nFirst, locate the Device ID for your iDevice.  This can be found by opening \nWindow\n -\n \nDevices\n.  Click on your iDevice in the left hand bar and copy the Identifier field.  There are several other ways of finding the device ID.  Refer to the \nApple documentation\n for the other ways.\n\n\nOnce you have the Device ID, you can register the device as a development device.  Sign into the \nApple Developer Portal\n, then:\n\n\n\n\nUnder \nDevices\n, click \nAll\n.\n\n\nClick the \n+\n button in the upper-right corner.\n\n\nSelect \nRegister Device\n.\n\n\nEnter a device name and the device ID you found earlier.\n\n\nClick \nContinue\n.\n\n\nClick \nRegister\n.\n\n\nClick \nDone\n.\n\n\n\n\nNow, create a Provisioning Profile:\n\n\n\n\nUnder \nProvisioning Profiles\n, click \nAll\n.\n\n\nClick the \n+\n button in the upper-right corner.\n\n\nSelect \niOS App Development\n, then click \nContinue\n.\n\n\nSelect the App ID you created earlier from the dropdown, then click \nContinue\n.\n\n\nSelect the certificates you want to include, then click \nContinue\n.  If you are unsure, include them all.\n\n\nSelect the device(s) you want to use, then click \nContinue\n.\n\n\nEnter a Profile name, then click \nContinue\n.\n\n\nYou can (and should) download your provisioning profile to your local machine.\n\n\nClick \nDone\n.\n\n\n\n\nFor more information on creating a Provisioning Profile, see the \nApple documentation\n.\n\n\n\n\nDownload your Provisioning Profile to XCode\n\n\nYou can also download your provisioning profile within XCode for later use.  Visual Studio for Mac will be able to more easily detect it.  Open XCode, then open \nXCode\n \n \nPreferences\n.  Click \nAccounts\n, then your account.  Click your Agent entry in the right hand panel, then click \nView Details\n.  Finally, click \nDownload All Profiles\n.  Once the download is complete, you can close the windows and return to Visual Studio.\n\n\n\n\nNext, configure the iOS project for push notifications.  Start by loading your project in Visual Studio for Mac.\n\n\n\n\nExpand the \nTaskList.iOS\n project and open the \nInfo.plist\n file.\n\n\n\n\nIn the \nIdentity\n section, fill in the \nBundle Identifier\n.  It must match the App ID Suffix that you set earlier.\n\n\n\n\n\n\n\n\nScroll down until you see \nBackground Modes\n.  Check the \nEnable Background Modes\n checkbox.\n\n\n\n\nAdding an Account\n\n\nVisual Studio for Mac uses fastlane for account authentication.  You will be walked through the process of adding an account the first time, and prompted to select an account thereafter.  Note that fastlane does not work when your Apple ID has 2-factor authentication enabled.  \nTurn 2FA off\n before you try to add an account.\n\n\n\n\n\n\n\n\nCheck the \nRemote notifications\n checkbox.\n\n\n\n\n\n\n\n\nSave and close the \nInfo.plist\n file.\n\n\n\n\nRight-click on the \nTaskList.iOS\n project, then select \nOptions\n.\n\n\nClick \niOS Bundle Signing\n in the left hand menu.\n\n\nEnsure the Platform is set to \niPhone\n and not \niPhoneSimulator\n.\n\n\nSelect your Signing Identity and Provisioning Profile.\n\n\nClick \nOK\n.\n\n\n\n\n\n\nProvisioning Profiles are frustrating\n\n\nIf you find yourself going round and round in circles on getting the signing certificate and provisioning profile right, you are not alone.  This is possibly one of the most frustrating pieces of iOS development.  See this \nXamarin Forums post\n for a good list of details.\n\n\n\n\nCode the push handler\n\n\nThe push handler is coded in the \nAppDelegate.cs\n file.  Unlike other platforms (like Android), you don't have to write code to define the push handler.  It's always in the same place.  Add the following code to the \nAppDelegate.cs\n file:\n\n\n    public static NSData PushDeviceToken { get; private set; } = null;\n\n    public override bool FinishedLaunching(UIApplication app, NSDictionary options)\n    {\n        Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n        global::Xamarin.Forms.Forms.Init();\n        LoadApplication(new App());\n\n        if (UIDevice.CurrentDevice.CheckSystemVersion(8, 0))\n        {\n            var pushSettings = UIUserNotificationSettings.GetSettingsForTypes(\n                UIUserNotificationType.Alert | UIUserNotificationType.Badge | UIUserNotificationType.Sound,\n                new NSSet());\n            UIApplication.SharedApplication.RegisterUserNotificationSettings(pushSettings);\n            UIApplication.SharedApplication.RegisterForRemoteNotifications();\n        }\n\n        return base.FinishedLaunching(app, options);\n    }\n\n    /// \nsummary\n\n    /// Called when the push notification system is registered\n    /// \n/summary\n\n    /// \nparam name=\napplication\nApplication.\n/param\n\n    /// \nparam name=\ndeviceToken\nDevice token.\n/param\n\n    public override void RegisteredForRemoteNotifications(UIApplication application, NSData deviceToken)\n    {\n        AppDelegate.PushDeviceToken = deviceToken;\n    }\n\n    public override void DidReceiveRemoteNotification(UIApplication application,\n        NSDictionary userInfo, Action\nUIBackgroundFetchResult\n completionHandler)\n    {\n        NSDictionary aps = userInfo.ObjectForKey(new NSString(\naps\n)) as NSDictionary;\n\n        // The aps is a dictionary with the template values in it\n        // You can adjust this section to do whatever you need to with the push notification\n\n        string alert = string.Empty;\n        if (aps.ContainsKey(new NSString(\nalert\n)))\n            alert = (aps[new NSString(\nalert\n)] as NSString).ToString();\n\n        //show alert\n        if (!string.IsNullOrEmpty(alert))\n        {\n            UIAlertView avAlert = new UIAlertView(\nNotification\n, alert, null, \nOK\n, null);\n            avAlert.Show();\n        }\n    }\n\n\n\n\nThe \nNSDictionary\n, \nNSData\n, and \nNSString\n classes are part of the iOS programming model and do exactly what you would expect them to do.  The \nUIAlertView\n class provides a standard alert.  We need to add a little bit of code to the \nFinishedLaunching()\n method to send the registration request to APNS.  When the response is received, the \nRegisteredForRemoteNotifications()\n method is called.  Finally, the \nDidReceiveRemoteNotification()\n method is called whenever a remote push notification is received.\n\n\n\n\nCall common code for push notifications\n\n\nOne of the great things about Xamarin Forms is that it is cross-platform.  However, that all breaks down when you move to push notifications.  One of the things you can do is to use the push handler to generate a model and then pass that model to a method in your PCL project.  This allows you to express the differences clearly and yet still do the majority of the logic in a cross-platform manner.\n\n\n\n\nRegistering with Azure Mobile Apps\n\n\nAs with Android, I recommend using a \nHttpClient\n for registering with Notification Hubs via the Azure Mobile Apps Push handler.  Here is the code that does basically the same thing as the Android version from the \nServices\\iOSPlatformProvider.cs\n file:\n\n\n    public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (AppDelegate.PushDeviceToken != null)\n        {\n            try\n            {\n                var registrationId = AppDelegate.PushDeviceToken.Description\n                    .Trim('\n', '\n').Replace(\n \n, string.Empty).ToUpperInvariant();\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform = \napns\n,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add(\ntopic:Sports\n);\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @\n{\naps\n:{\nalert\n:\n$(messageParam)\n}}\n\n                };\n                installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync\nDeviceInstallation, DeviceInstallation\n(\n                    $\n/push/installations/{client.InstallationId}\n,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary\nstring, string\n());\n            }\n            catch (Exception ex)\n            {\n                System.Diagnostics.Debug.Fail($\n[iOSPlatformProvider]: Could not register with NH: {ex.Message}\n);\n            }\n        }\n    }\n\n\n\n\nIn this case, we don't have a service class to deal with - the iOS AppDelegate does all the work for us.  The registration Id is stored in the AppDelegate once registered, but needs to be decoded (which is relatively simple).  Similar to the Android version, we make the template we are using match what we are expecting within our push handler.\n\n\n\n\nReceiving Notifications in the background\n\n\nIf you want your app to be notified when a notification is received when your app is in the background, you need to set the Background Fetch capability and your payload should include the key \ncontent-available\n with a value of 1 (true).  You can add this to the Body of the template in the above sample.  iOS will wake up the app and you will have 30 seconds to fetch any information you might need to update.  Check \nthe documentation\n for more details.\n\n\n\n\nTesting Notifications\n\n\nOur final step is to test the whole process.  As with Android, there are two tests we need to perform.  The first is to ensure that a registration happens when we expect it to.  In the case of our app, that happens immediately after the authentication.  There is no Notifications Hub registration monitor in Visual Studio for Mac, so we have to get that information an alternate way, by querying the hub registration endpoint.  I've written [a script] for this purpose.  To install:\n\n\n\n\nInstall \nNodeJS\n.\n\n\nGo to the \ntools\n directory on the books GitHub repository.\n\n\nRun \nnpm install\n.\n\n\n\n\nTo use, you will need the endpoint for your notification hub namespace.\n\n\n\n\nLog onto the \nAzure portal\n.\n\n\nOpen your Notification Hub namespace.\n\n\nClick \nAccess Policies\n.\n\n\nCopy the connection string of the \nRootManagedSharedAccessKey\n (which is probably the only policy you have).\n\n\n\n\nYou can now use the program using:\n\n\nnode get_nh_registrations.js -c '\nyour connection string\n' -h \nyour hub name\n\n\n\n\n\nYou will need to put the connection string in quotes generally.  For example:\n\n\nnode .\\get_nh_registrations.js -c 'Endpoint=sb://zumobook-ns.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=\n****c9VoZHtxSGliSIhH5EEuar1B/jsrgTQTHOTA=' -h zumobook-hub\n\n\n\n\nThe output will look something like the following:\n\n\nType:         APNS (Template)\nId:           219869525209729025-4738338868778066550-1\nDevice Token: 681F6BB012C62A61AA2185A676B23907A5FEFE9268283DD226B08B5F0336A552\nTag:          topic:Sports\nTag:          _UserId:a9650e1c4d3268ec912f4d9ca6d1d933\nTag:          photoadrian@outlook.com\nTag:          $InstallationId:{d7323fe4-64bb-4d99-a6cc-e7690032350f}\nExpires:      9999-12-31T23:59:59.9999999Z\n\n\n\n\nNote that this script does not deal with \"continuation tokens\", so it can only return the first page of information.  This is generally suitable for testing purposes.\n\n\nWe can also send a test message for push notifications.  This can be done via the Azure Portal.\n\n\n\n\nLog onto the \nAzure portal\n.\n\n\nFind the Hub resource for your connected Notification Hub and open it.\n\n\nClick \nTest Send\n.\n\n\nSelect \nApple\n as the Platform, then click on \nSend\n.\n\n\n\n\n\n\nYour device should also receive the push notification and display an alert.  You can also do a test send to an explicit tag.  This can narrow the test send to just one device if necessary.  To send to a specific device, you need to know the installation ID of the registration.\n\n\nCommon Problems\n\n\nAs you might expect, there is plenty to go wrong here.  The majority of the issues come down to the fact that there are two endpoints on APNS - a Sandbox (or Developer) endpoint and a Production endpoint.  If you are using the wrong endpoint, the notification hub will receive an error.  If the notification hub receives an error from the APNS endpoint, it will remove the registration causing the error.  This manifests itself in two ways.  Firstly, your device will not receive the push notification.  Secondly, the registration will be removed from the list of valid registrations, causing you to think that the device has not been registered.\n\n\nThis has not been made easy by the fact that Apple has combined the certificates needed to push into a single certificate for both Sandbox and Production use cases.  To correct this issue, ensure the notifiction hub is set up with the appropriate endpoint - Sandbox or Production.\n\n\nNext you can move onto \nWindows Push\n or skip to the \nRecipes Section\n.", 
            "title": "iOS Push"
        }, 
        {
            "location": "/chapter5/ios/#registering-with-apns", 
            "text": "Registering with APNS is a multi-step process:   Register an App ID for your app, and select Push Notifications as a capability.  Create an appropriate certificate for the push channel (either a Development or Distribution certificate).  Configure Notification Hubs to use APNS.  Configure your application to support Push Notifications.  Add code for handling push notifications to your app.   Let's cover each one in turn:", 
            "title": "Registering with APNS"
        }, 
        {
            "location": "/chapter5/ios/#register-an-app-id-for-your-app", 
            "text": "Once you get to adding push notifications to your application, you are going to need that full developers license from Apple.  You need to work with real devices and that means you need code signing certificates on your mac.  If you have not spent the cash for the Apple Developers program, then you will probably find you need to at this point.  Registering an App ID is handled on the  Apple Developer Portal .  Apple does a good job of  documenting the process , so these instructions are duplicative of the instructions that Apple provides.   Go to the  Apple Developer Portal  and log in with your Apple developer ID.  In the left-hand menu, click  Certificates, IDs   Profiles .  In the left-hand menu,  Identifiers , click  App IDs .  Click the  +  button in the top right corner.   Fill in the form:   The App ID Description is not used and can be set to anything (subject to validation rules)  Choose an  Explicit App ID  for this app.   Enter the App ID suffix according to the rules.  I used  com.shellmonger.tasklist .     Select  Push Notifications  in the  App Services  section.       Click  Continue  when the form is complete.   Make a note of the  Identifier  in the next screen, then click  Register .  Click  Done .   Note that the Push Notifications capability will be listed as  Configurable  until you create a certificate that is used for push notifications.  Once that happens, the capability will be listed as  Enabled .", 
            "title": "Register an App ID for your app"
        }, 
        {
            "location": "/chapter5/ios/#create-a-certificate-for-the-push-channel", 
            "text": "I mentioned earlier that APNS is certificate based.  That means that you need to generate an SSL certificate to fully configure push notifications:   Staying in  Certificates, Identifiers   Profiles , click  All  under the  Certificates  heading in the left hand menu.  Click on the  +  button in the top right corner.   Select the  Apple Push Notification service SSL (Sandbox   Production)     Click  Continue .   Select the App ID you just created from the list, then click  Continue .   Follow the on-screen instructions for creating a Certificate Signing Request (CSR).     Once you have generated the CSR, click  Continue  in the browser.   Select the CSR you just generated using the  Choose File  button, then click  Continue .  Click  Download  to download the resulting certificate.  Click  Done  when the download is complete.  Find your downloaded certificate and double-click on it to import it into Keychain Access   Your certificate will also appear in the  Certificates     All  list within the Apple Developer console.", 
            "title": "Create a certificate for the push channel"
        }, 
        {
            "location": "/chapter5/ios/#configure-notification-hubs", 
            "text": "Notification Hubs requires you to upload the certificate as a .p12 (PKCS#12) file.  To generate this file:   Open Keychain Access.  Select  My Certificates  from the left hand menu  Look for the certificate you just generated, and expand it to show the private key.  Right-click the private key and select  Export... .  Select  Personal Information Exchange (.p12)  as the type and give it a name and location.  Click  Save .  Enter a password (twice) to protect the certificate.  Click  OK .   Upload the certificate to Azure:   Open and log into the  Azure portal .  Select  Notification Hubs , then the notification hub that is connected to your mobile backend.  Click  Push Notification Services , then select  Apple (APNS) .  Click  + Upload Certificate .   Fill in the form:   Select the .p12 file you just created.  Enter the password that you entered to secure the .p12 file.  Select  Sandbox  (probably) or  Production  as appropriate.      Click  OK .    It's important to figure out whether you are operating in the  Sandbox  (Development) or  Production  mode.  During development, it's likely that your device will be registered on the Apple Developer console and you will be operating in the sandbox.  Any device not listed with the developer console is considered \"production\".  You must update the certificate to a production certificate and specify the production mode when you release your app.  Apple APNS provides two endpoints for pushing notifications.  If you use the wrong one, then APNS will return an error code.  This will cause Notification Hubs to delete the registration and your push will fail.", 
            "title": "Configure Notification Hubs"
        }, 
        {
            "location": "/chapter5/ios/#configure-your-application", 
            "text": "Before we start with code, you will want a  Provisioning Profile .  This small file is key to being able to use push notifications on your device.  You  MUST  have a real device at this point.  The easiest way for this to happen is to plug the iPhone or iPad that you want to use into your development system.  Once your device is recognized by iTunes, close iTunes down and start XCode.  First, locate the Device ID for your iDevice.  This can be found by opening  Window  -   Devices .  Click on your iDevice in the left hand bar and copy the Identifier field.  There are several other ways of finding the device ID.  Refer to the  Apple documentation  for the other ways.  Once you have the Device ID, you can register the device as a development device.  Sign into the  Apple Developer Portal , then:   Under  Devices , click  All .  Click the  +  button in the upper-right corner.  Select  Register Device .  Enter a device name and the device ID you found earlier.  Click  Continue .  Click  Register .  Click  Done .   Now, create a Provisioning Profile:   Under  Provisioning Profiles , click  All .  Click the  +  button in the upper-right corner.  Select  iOS App Development , then click  Continue .  Select the App ID you created earlier from the dropdown, then click  Continue .  Select the certificates you want to include, then click  Continue .  If you are unsure, include them all.  Select the device(s) you want to use, then click  Continue .  Enter a Profile name, then click  Continue .  You can (and should) download your provisioning profile to your local machine.  Click  Done .   For more information on creating a Provisioning Profile, see the  Apple documentation .   Download your Provisioning Profile to XCode  You can also download your provisioning profile within XCode for later use.  Visual Studio for Mac will be able to more easily detect it.  Open XCode, then open  XCode     Preferences .  Click  Accounts , then your account.  Click your Agent entry in the right hand panel, then click  View Details .  Finally, click  Download All Profiles .  Once the download is complete, you can close the windows and return to Visual Studio.   Next, configure the iOS project for push notifications.  Start by loading your project in Visual Studio for Mac.   Expand the  TaskList.iOS  project and open the  Info.plist  file.   In the  Identity  section, fill in the  Bundle Identifier .  It must match the App ID Suffix that you set earlier.     Scroll down until you see  Background Modes .  Check the  Enable Background Modes  checkbox.   Adding an Account  Visual Studio for Mac uses fastlane for account authentication.  You will be walked through the process of adding an account the first time, and prompted to select an account thereafter.  Note that fastlane does not work when your Apple ID has 2-factor authentication enabled.   Turn 2FA off  before you try to add an account.     Check the  Remote notifications  checkbox.     Save and close the  Info.plist  file.   Right-click on the  TaskList.iOS  project, then select  Options .  Click  iOS Bundle Signing  in the left hand menu.  Ensure the Platform is set to  iPhone  and not  iPhoneSimulator .  Select your Signing Identity and Provisioning Profile.  Click  OK .    Provisioning Profiles are frustrating  If you find yourself going round and round in circles on getting the signing certificate and provisioning profile right, you are not alone.  This is possibly one of the most frustrating pieces of iOS development.  See this  Xamarin Forums post  for a good list of details.", 
            "title": "Configure your application"
        }, 
        {
            "location": "/chapter5/ios/#code-the-push-handler", 
            "text": "The push handler is coded in the  AppDelegate.cs  file.  Unlike other platforms (like Android), you don't have to write code to define the push handler.  It's always in the same place.  Add the following code to the  AppDelegate.cs  file:      public static NSData PushDeviceToken { get; private set; } = null;\n\n    public override bool FinishedLaunching(UIApplication app, NSDictionary options)\n    {\n        Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n        global::Xamarin.Forms.Forms.Init();\n        LoadApplication(new App());\n\n        if (UIDevice.CurrentDevice.CheckSystemVersion(8, 0))\n        {\n            var pushSettings = UIUserNotificationSettings.GetSettingsForTypes(\n                UIUserNotificationType.Alert | UIUserNotificationType.Badge | UIUserNotificationType.Sound,\n                new NSSet());\n            UIApplication.SharedApplication.RegisterUserNotificationSettings(pushSettings);\n            UIApplication.SharedApplication.RegisterForRemoteNotifications();\n        }\n\n        return base.FinishedLaunching(app, options);\n    }\n\n    ///  summary \n    /// Called when the push notification system is registered\n    ///  /summary \n    ///  param name= application Application. /param \n    ///  param name= deviceToken Device token. /param \n    public override void RegisteredForRemoteNotifications(UIApplication application, NSData deviceToken)\n    {\n        AppDelegate.PushDeviceToken = deviceToken;\n    }\n\n    public override void DidReceiveRemoteNotification(UIApplication application,\n        NSDictionary userInfo, Action UIBackgroundFetchResult  completionHandler)\n    {\n        NSDictionary aps = userInfo.ObjectForKey(new NSString( aps )) as NSDictionary;\n\n        // The aps is a dictionary with the template values in it\n        // You can adjust this section to do whatever you need to with the push notification\n\n        string alert = string.Empty;\n        if (aps.ContainsKey(new NSString( alert )))\n            alert = (aps[new NSString( alert )] as NSString).ToString();\n\n        //show alert\n        if (!string.IsNullOrEmpty(alert))\n        {\n            UIAlertView avAlert = new UIAlertView( Notification , alert, null,  OK , null);\n            avAlert.Show();\n        }\n    }  The  NSDictionary ,  NSData , and  NSString  classes are part of the iOS programming model and do exactly what you would expect them to do.  The  UIAlertView  class provides a standard alert.  We need to add a little bit of code to the  FinishedLaunching()  method to send the registration request to APNS.  When the response is received, the  RegisteredForRemoteNotifications()  method is called.  Finally, the  DidReceiveRemoteNotification()  method is called whenever a remote push notification is received.   Call common code for push notifications  One of the great things about Xamarin Forms is that it is cross-platform.  However, that all breaks down when you move to push notifications.  One of the things you can do is to use the push handler to generate a model and then pass that model to a method in your PCL project.  This allows you to express the differences clearly and yet still do the majority of the logic in a cross-platform manner.", 
            "title": "Code the push handler"
        }, 
        {
            "location": "/chapter5/ios/#registering-with-azure-mobile-apps", 
            "text": "As with Android, I recommend using a  HttpClient  for registering with Notification Hubs via the Azure Mobile Apps Push handler.  Here is the code that does basically the same thing as the Android version from the  Services\\iOSPlatformProvider.cs  file:      public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (AppDelegate.PushDeviceToken != null)\n        {\n            try\n            {\n                var registrationId = AppDelegate.PushDeviceToken.Description\n                    .Trim(' ', ' ').Replace(   , string.Empty).ToUpperInvariant();\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform =  apns ,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add( topic:Sports );\n                // Set up templates to request\n                PushTemplate genericTemplate = new PushTemplate\n                {\n                    Body = @ { aps :{ alert : $(messageParam) }} \n                };\n                installation.Templates.Add( genericTemplate , genericTemplate);\n\n                // Register with NH\n                var response = await client.InvokeApiAsync DeviceInstallation, DeviceInstallation (\n                    $ /push/installations/{client.InstallationId} ,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary string, string ());\n            }\n            catch (Exception ex)\n            {\n                System.Diagnostics.Debug.Fail($ [iOSPlatformProvider]: Could not register with NH: {ex.Message} );\n            }\n        }\n    }  In this case, we don't have a service class to deal with - the iOS AppDelegate does all the work for us.  The registration Id is stored in the AppDelegate once registered, but needs to be decoded (which is relatively simple).  Similar to the Android version, we make the template we are using match what we are expecting within our push handler.   Receiving Notifications in the background  If you want your app to be notified when a notification is received when your app is in the background, you need to set the Background Fetch capability and your payload should include the key  content-available  with a value of 1 (true).  You can add this to the Body of the template in the above sample.  iOS will wake up the app and you will have 30 seconds to fetch any information you might need to update.  Check  the documentation  for more details.", 
            "title": "Registering with Azure Mobile Apps"
        }, 
        {
            "location": "/chapter5/ios/#testing-notifications", 
            "text": "Our final step is to test the whole process.  As with Android, there are two tests we need to perform.  The first is to ensure that a registration happens when we expect it to.  In the case of our app, that happens immediately after the authentication.  There is no Notifications Hub registration monitor in Visual Studio for Mac, so we have to get that information an alternate way, by querying the hub registration endpoint.  I've written [a script] for this purpose.  To install:   Install  NodeJS .  Go to the  tools  directory on the books GitHub repository.  Run  npm install .   To use, you will need the endpoint for your notification hub namespace.   Log onto the  Azure portal .  Open your Notification Hub namespace.  Click  Access Policies .  Copy the connection string of the  RootManagedSharedAccessKey  (which is probably the only policy you have).   You can now use the program using:  node get_nh_registrations.js -c ' your connection string ' -h  your hub name   You will need to put the connection string in quotes generally.  For example:  node .\\get_nh_registrations.js -c 'Endpoint=sb://zumobook-ns.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=\n****c9VoZHtxSGliSIhH5EEuar1B/jsrgTQTHOTA=' -h zumobook-hub  The output will look something like the following:  Type:         APNS (Template)\nId:           219869525209729025-4738338868778066550-1\nDevice Token: 681F6BB012C62A61AA2185A676B23907A5FEFE9268283DD226B08B5F0336A552\nTag:          topic:Sports\nTag:          _UserId:a9650e1c4d3268ec912f4d9ca6d1d933\nTag:          photoadrian@outlook.com\nTag:          $InstallationId:{d7323fe4-64bb-4d99-a6cc-e7690032350f}\nExpires:      9999-12-31T23:59:59.9999999Z  Note that this script does not deal with \"continuation tokens\", so it can only return the first page of information.  This is generally suitable for testing purposes.  We can also send a test message for push notifications.  This can be done via the Azure Portal.   Log onto the  Azure portal .  Find the Hub resource for your connected Notification Hub and open it.  Click  Test Send .  Select  Apple  as the Platform, then click on  Send .    Your device should also receive the push notification and display an alert.  You can also do a test send to an explicit tag.  This can narrow the test send to just one device if necessary.  To send to a specific device, you need to know the installation ID of the registration.", 
            "title": "Testing Notifications"
        }, 
        {
            "location": "/chapter5/ios/#common-problems", 
            "text": "As you might expect, there is plenty to go wrong here.  The majority of the issues come down to the fact that there are two endpoints on APNS - a Sandbox (or Developer) endpoint and a Production endpoint.  If you are using the wrong endpoint, the notification hub will receive an error.  If the notification hub receives an error from the APNS endpoint, it will remove the registration causing the error.  This manifests itself in two ways.  Firstly, your device will not receive the push notification.  Secondly, the registration will be removed from the list of valid registrations, causing you to think that the device has not been registered.  This has not been made easy by the fact that Apple has combined the certificates needed to push into a single certificate for both Sandbox and Production use cases.  To correct this issue, ensure the notifiction hub is set up with the appropriate endpoint - Sandbox or Production.  Next you can move onto  Windows Push  or skip to the  Recipes Section .", 
            "title": "Common Problems"
        }, 
        {
            "location": "/chapter5/windows/", 
            "text": "Push Notifications for Windows are handled by one of two services. Use the \nMicrosoft Push Notification Service\n (or MPNS) for Windows Phone devices (not including Windows Phone 10).  Use \nWindows Notification Service\n (or WNS) for Windows 10, including Phone, Universal Windows applications and Windows store applications.  Inevitably, this will mean you are more interested in WNS than MPNS.\n\n\nRegistering with WNS\n\n\nThere are a few steps for registering with WNS:\n\n\n\n\nApply for a \nWindows Developer Account\n.\n\n\nRegister your application with the Windows Store.\n\n\nConfigure Notification Hubs.\n\n\nFinally, add code to your project to register and handle push notifications.\n\n\n\n\nIf you are pushing to WNS, the application doesn't have to be a mobile app - it can be a Universal Windows app running on your laptop or desktop development computer.  This makes dealing with push notifications much easier than you would expect.\n\n\nApply for a Windows Developer Account\n\n\nThe first thing you need is a \nWindows Developer Account\n.  This is a paid account, but the account is a one-time cost.  If you have an MSDN subscription, the account is free.  Just go to the \nMSDN Website\n, log in and check your MSDN Subscription.  Under \nMy Account\n there is a section entitled \nWindows and Windows Phone developer accounts\n.  Click on the \nGet Code\n link to get your registration code.  When you are prompted for payment, there is a place to put the code for a free\nregistration.\n\n\n\n\nWarn\n\n\nUse the same account as the account you use to sign in to Visual Studio.\n\n\n\n\nRegister your application with the Windows Store\n\n\nTo register your application with the Windows Store:\n\n\n\n\nOpen your solution in Visual Studio\n\n\nRight-click the \nTaskList.UWP\n project, select \nStore\n \n \nAssociate App with the Store\n.\n\n\n\n\nClick \nNext\n.\n\n\n\n\n\n\n\n\nEnter a unique name for your app, then click \nReserve\n.\n\n\n\n\n\n\nOnce the app list is updated, highlight the new app name, then click \nNext\n.\n\n\n\n\n\n\n\n\nClick \nAssociate\n.\n\n\n\n\n\n\nThis will create a \nPackage.StoreAssociation.xml\n file.  This file can easily be recreated by re-associating the app with the store.  You should not check this file into source code control.  If you use the \n.gitignore generator\n, then it is automatically added to your .gitignore file.\n\n\nAdd Push Notifications to your App registration\n\n\nUsing a web browser, navigate to the \nApp Overview\n page on the Windows Developer Center.  You will need to log in with the same account used for your Windows Developer Account.  Click the application you just associated.\n\n\n\n\nClick the \nGet started\n link under \nPush Notifications\n.\n\n\n\n\nClick the \nLive Services site\n in the middle of the WNS section.\n\n\n\n\nLeave this page open, but make a note of the two values I have highlighted.  You will need these values when you register your WNS connection with the notification hub.  You should never reveal these values to anyone as they will enable anyone to impersonate your applications to the WNS push notification system, allowing them to push to your users.\n\n\nConfigure Notification Hubs\n\n\nThe Notification Hubs configuration follows a similar pattern to the other push notification systems:\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\n\n\nFind your notification hub, then select \nPush notification services\n.\n\n\n\n\n\n\n\n\nEnter the Package SID from the Live Services site for your app in the appropriate box.\n\n\n\n\nEnter the application secret from the Live Services site in the Security Key box.\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\n\n\nThe notification hub will be updated with your security credentials.\n\n\n\n\nTip\n\n\nIf you think your application secret has been compromised (or you publish it in a book), go back to the Live Services site and click \nGenerate new password\n.  Then click \nActivate\n next to your new application secret. Finally, click \nSave\n at the bottom of the page. Copy the new secret into the \nSecurity Key\n field of your notification hub configuration for WNS and click \nSave\n there to store the new security key.\n\n\n\n\nRegister for Push Notifications in your App\n\n\nEdit the \nApp.xaml.cs\n file in your \nTaskList.UWP\n project.  Add the following code to the \nOnLaunched()\n method:\n\n\n    protected async override void OnLaunched(LaunchActivatedEventArgs e)\n    {\n        UWPPlatformProvider.Channel = await PushNotificationChannelManager\n            .CreatePushNotificationChannelForApplicationAsync();\n\n        // Rest of the OnLaunched method goes here\n    }\n\n\n\n\nWe have also made the \nOnLaunched()\n method asynchronous to accomodate the push notification channel manager.  The \nPushNotificationChannelManager\n class is in \nWindows.Networking.PushNotifications\n.  We are going to store the channel in the \nUWPPlatformProvider\n:\n\n\n    public static PushNotificationChannel Channel { get; set; } = null;\n\n\n\n\nThis will make the channel globally available to our push registration code.\n\n\nRegistering with Azure Mobile Apps\n\n\nThe registration code is also in the \nUWPPlatformProvider\n since it is part of the \nIPlatformProvider\n interface we have been using:\n\n\n    public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (UWPPlatformProvider.Channel != null)\n        {\n            try\n            {\n                var registrationId = UWPPlatformProvider.Channel.Uri.ToString();\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform = \nwns\n,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add(\ntopic:Sports\n);\n                // Set up templates to request\n                var genericTemplate = new WindowsPushTemplate\n                {\n                    Body = @\ntoast\nvisual\nbinding template=\ngenericTemplate\ntext id=\n1\n$(message)\n/text\n/binding\n/visual\n/toast\n\n                };\n                genericTemplate.Headers.Add(\nX-WNS-Type\n, \nwns/toast\n);\n\n                installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n                // Register with NH\n                var recordedInstallation = await client.InvokeApiAsync\nDeviceInstallation, DeviceInstallation\n(\n                    $\n/push/installations/{client.InstallationId}\n,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary\nstring, string\n());\n                System.Diagnostics.Debug.WriteLine(\nCompleted NH Push Installation\n);\n            }\n            catch (Exception ex)\n            {\n                System.Diagnostics.Debug.Fail($\n[UWPPlatformProvider]: Could not register with NH: {ex.Message}\n);\n            }\n        }\n    }\n\n\n\n\nThe template used by WNS is slightly different.  It is based on the regular \nPushTemplate\n used by iOS and Android, but it has an extra \"Headers\" field.  You must specify the \nX-WNS-Type\n, which can be one of the following:\n\n\n\n\nwns/toast\n\n\nwns/tile\n\n\nwns/badge\n\n\nwns/raw\n\n\n\n\nEach of these has their own properties and require a specific set of properties to be specified in the XML body.  As a result of this configuration, you don't need to code anything to receive push messages - UWP already knows how to decode and display them.  The \nWindowsPushTemplate\n is added to the \nAbstractions\\DeviceInstallation.cs\n file:\n\n\n    public class WindowsPushTemplate : PushTemplate\n    {\n        public WindowsPushTemplate() : base()\n        {\n            Headers = new Dictionary\nstring, string\n();\n        }\n\n        [JsonProperty(PropertyName = \nheaders\n)]\n        public Dictionary\nstring, string\n Headers { get; set; }\n    }\n\n\n\n\nTesting Notifications\n\n\nYou can send an appropriately formed test message to WNS within Visual Studio:\n\n\n\n\nOpen the \nServer Explorer\n.\n\n\nExpand \nAzure\n \n \nNotification Hubs\n.\n\n\nDouble-click the notification hub.\n\n\nSelect \nWindows (WNS)\n \n \nToast\n as the type.\n\n\nChange the body if required.\n\n\nClick \nSend\n.\n\n\n\n\nThe message will appear in the notification area as a \"New Notification\".", 
            "title": "Windows Push"
        }, 
        {
            "location": "/chapter5/windows/#registering-with-wns", 
            "text": "There are a few steps for registering with WNS:   Apply for a  Windows Developer Account .  Register your application with the Windows Store.  Configure Notification Hubs.  Finally, add code to your project to register and handle push notifications.   If you are pushing to WNS, the application doesn't have to be a mobile app - it can be a Universal Windows app running on your laptop or desktop development computer.  This makes dealing with push notifications much easier than you would expect.", 
            "title": "Registering with WNS"
        }, 
        {
            "location": "/chapter5/windows/#apply-for-a-windows-developer-account", 
            "text": "The first thing you need is a  Windows Developer Account .  This is a paid account, but the account is a one-time cost.  If you have an MSDN subscription, the account is free.  Just go to the  MSDN Website , log in and check your MSDN Subscription.  Under  My Account  there is a section entitled  Windows and Windows Phone developer accounts .  Click on the  Get Code  link to get your registration code.  When you are prompted for payment, there is a place to put the code for a free\nregistration.   Warn  Use the same account as the account you use to sign in to Visual Studio.", 
            "title": "Apply for a Windows Developer Account"
        }, 
        {
            "location": "/chapter5/windows/#register-your-application-with-the-windows-store", 
            "text": "To register your application with the Windows Store:   Open your solution in Visual Studio  Right-click the  TaskList.UWP  project, select  Store     Associate App with the Store .   Click  Next .     Enter a unique name for your app, then click  Reserve .    Once the app list is updated, highlight the new app name, then click  Next .     Click  Associate .    This will create a  Package.StoreAssociation.xml  file.  This file can easily be recreated by re-associating the app with the store.  You should not check this file into source code control.  If you use the  .gitignore generator , then it is automatically added to your .gitignore file.", 
            "title": "Register your application with the Windows Store"
        }, 
        {
            "location": "/chapter5/windows/#add-push-notifications-to-your-app-registration", 
            "text": "Using a web browser, navigate to the  App Overview  page on the Windows Developer Center.  You will need to log in with the same account used for your Windows Developer Account.  Click the application you just associated.   Click the  Get started  link under  Push Notifications .   Click the  Live Services site  in the middle of the WNS section.   Leave this page open, but make a note of the two values I have highlighted.  You will need these values when you register your WNS connection with the notification hub.  You should never reveal these values to anyone as they will enable anyone to impersonate your applications to the WNS push notification system, allowing them to push to your users.", 
            "title": "Add Push Notifications to your App registration"
        }, 
        {
            "location": "/chapter5/windows/#configure-notification-hubs", 
            "text": "The Notification Hubs configuration follows a similar pattern to the other push notification systems:   Log on to the  Azure portal .   Find your notification hub, then select  Push notification services .     Enter the Package SID from the Live Services site for your app in the appropriate box.   Enter the application secret from the Live Services site in the Security Key box.   Click  Save .     The notification hub will be updated with your security credentials.   Tip  If you think your application secret has been compromised (or you publish it in a book), go back to the Live Services site and click  Generate new password .  Then click  Activate  next to your new application secret. Finally, click  Save  at the bottom of the page. Copy the new secret into the  Security Key  field of your notification hub configuration for WNS and click  Save  there to store the new security key.", 
            "title": "Configure Notification Hubs"
        }, 
        {
            "location": "/chapter5/windows/#register-for-push-notifications-in-your-app", 
            "text": "Edit the  App.xaml.cs  file in your  TaskList.UWP  project.  Add the following code to the  OnLaunched()  method:      protected async override void OnLaunched(LaunchActivatedEventArgs e)\n    {\n        UWPPlatformProvider.Channel = await PushNotificationChannelManager\n            .CreatePushNotificationChannelForApplicationAsync();\n\n        // Rest of the OnLaunched method goes here\n    }  We have also made the  OnLaunched()  method asynchronous to accomodate the push notification channel manager.  The  PushNotificationChannelManager  class is in  Windows.Networking.PushNotifications .  We are going to store the channel in the  UWPPlatformProvider :      public static PushNotificationChannel Channel { get; set; } = null;  This will make the channel globally available to our push registration code.", 
            "title": "Register for Push Notifications in your App"
        }, 
        {
            "location": "/chapter5/windows/#registering-with-azure-mobile-apps", 
            "text": "The registration code is also in the  UWPPlatformProvider  since it is part of the  IPlatformProvider  interface we have been using:      public async Task RegisterForPushNotifications(MobileServiceClient client)\n    {\n        if (UWPPlatformProvider.Channel != null)\n        {\n            try\n            {\n                var registrationId = UWPPlatformProvider.Channel.Uri.ToString();\n                var installation = new DeviceInstallation\n                {\n                    InstallationId = client.InstallationId,\n                    Platform =  wns ,\n                    PushChannel = registrationId\n                };\n                // Set up tags to request\n                installation.Tags.Add( topic:Sports );\n                // Set up templates to request\n                var genericTemplate = new WindowsPushTemplate\n                {\n                    Body = @ toast visual binding template= genericTemplate text id= 1 $(message) /text /binding /visual /toast \n                };\n                genericTemplate.Headers.Add( X-WNS-Type ,  wns/toast );\n\n                installation.Templates.Add( genericTemplate , genericTemplate);\n                // Register with NH\n                var recordedInstallation = await client.InvokeApiAsync DeviceInstallation, DeviceInstallation (\n                    $ /push/installations/{client.InstallationId} ,\n                    installation,\n                    HttpMethod.Put,\n                    new Dictionary string, string ());\n                System.Diagnostics.Debug.WriteLine( Completed NH Push Installation );\n            }\n            catch (Exception ex)\n            {\n                System.Diagnostics.Debug.Fail($ [UWPPlatformProvider]: Could not register with NH: {ex.Message} );\n            }\n        }\n    }  The template used by WNS is slightly different.  It is based on the regular  PushTemplate  used by iOS and Android, but it has an extra \"Headers\" field.  You must specify the  X-WNS-Type , which can be one of the following:   wns/toast  wns/tile  wns/badge  wns/raw   Each of these has their own properties and require a specific set of properties to be specified in the XML body.  As a result of this configuration, you don't need to code anything to receive push messages - UWP already knows how to decode and display them.  The  WindowsPushTemplate  is added to the  Abstractions\\DeviceInstallation.cs  file:      public class WindowsPushTemplate : PushTemplate\n    {\n        public WindowsPushTemplate() : base()\n        {\n            Headers = new Dictionary string, string ();\n        }\n\n        [JsonProperty(PropertyName =  headers )]\n        public Dictionary string, string  Headers { get; set; }\n    }", 
            "title": "Registering with Azure Mobile Apps"
        }, 
        {
            "location": "/chapter5/windows/#testing-notifications", 
            "text": "You can send an appropriately formed test message to WNS within Visual Studio:   Open the  Server Explorer .  Expand  Azure     Notification Hubs .  Double-click the notification hub.  Select  Windows (WNS)     Toast  as the type.  Change the body if required.  Click  Send .   The message will appear in the notification area as a \"New Notification\".", 
            "title": "Testing Notifications"
        }, 
        {
            "location": "/chapter5/recipes/", 
            "text": "This section is dedicated to exploring various common recipes for push notifications and how we can achieve those recipes through cross-platform code.\n\n\nMarketing Push\n\n\nThe most common requirement for push notifications is to alert users of a special offer or other marketing information.  The general idea is that the marketing person will create a \"campaign\" that includes a push notification.  When the user receives the push notification, they will accept it.  If a user accepts the push notification, the mobile app will deep-link into a specific page and store the fact that the user viewed the page within the database.\n\n\nTo implement this sort of functionality within a cross-platform application, we need to implement \nTemplates\n.  We gave demonstrations of the implementation of the templates while we were discussing the various platform implementations.  However, we didn't actually use them.  A template is provides by the mobile client when registering.  Let's take a look at a typical template as implemented by each platform:\n\n\nAndroid\n:\n\n\n{\n    \ndata\n: {\n        \nmessage\n: \n$(message)\n,\n        \npicture\n: \n$(picture)\n\n    }\n}\n\n\n\n\niOS\n:\n\n\n{\n    \naps\n: {\n        \nalert\n: \n$(message)\n,\n        \npicture\n: \n$(picture)\n\n    }\n}\n\n\n\n\nWindows\n:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n?\n\n\ntoast launch=\nzumobook\n\n  \nvisual\n\n    \nbinding template=\nToastGeneric\n\n      \ntext\n$(message)\n/text\n\n    \n/binding\n\n  \n/visual\n\n  \nactions\n\n    \naction content=\nOpen\n arguments=\n$(picture)\n /\n\n    \naction content=\nCancel\n arguments=\ncancel\n /\n\n  \n/actions\n\n\n/toast\n\n\n\n\n\n\n\nToast, Tile and Badge Schemas\n\n\nIf you want to understand the format of the XML that we are using in the Windows section, it's laid out in the \nMSDN documentation\n.\n\n\n\n\nEach of these formats can be specified in the appropriate registration call:\n\n\n    // Android Version\n    var genericTemplate = new PushTemplate\n    {\n        Body = @\n{\ndata\n:{\nmessage\n:\n$(message)\n,\npicture\n:\n$(picture)\n}}\n\n    };\n    installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n    // iOS Version\n    var genericTemplate = new PushTemplate\n    {\n        Body = @\n{\naps\n:{\nalert\n:\n$(message)\n,\npicture\n:\n$(picture)\n}}\n\n    };\n    installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n    // Windows Version\n    var genericTemplate = new WindowsPushTemplate\n    {\n        Body = @\n?xml version=\n1.0\n encoding=\nutf-8\n?\n\n\ntoast launch=\nzumobook\n\n  \nvisual\n\n    \nbinding template=\nToastGeneric\n\n      \ntext\n$(message)\n/text\n\n    \n/binding\n\n  \n/visual\n\n  \nactions\n\n    \naction content=\nOpen\n arguments=\n$(picture)\n /\n\n    \naction content=\nCancel\n arguments=\ncancel\n /\n\n  \n/actions\n\n\n/toast\n\n    };\n    genericTemplate.Headers.Add(\nX-WNS-Type\n, \nwns/toast\n);\n    installation.Templates.Add(\ngenericTemplate\n, genericTemplate);\n\n\n\n\nTo push, we can use the same Test Send facility in the Azure Portal.  In the Test Send screen, set the \nPlatforms\n field to be \nCustom Template\n, and the payload to be a JSON document with the two fields:\n\n\n{\n    \nmessage\n: \nTest Message\n,\n    \npicture\n: \nhttp://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\n\n}\n\n\n\n\nIf you have done all the changes thus far, you will receive the same notification as before.  The difference is that you are pushing a message once and receiving that same message across all the Android, iOS and Windows systems at the same time.  You no longer have to know what sort of device your users are holding - the message will get to them.\n\n\nWe can take this a step further, however, by deep-linking.  Deep-linking is a technique often used in push notification systems whereby we present the user a dialog that asks them to open the notification.  If the notification is opened, they are taken directly to a new view with the appropriate content provided.\n\n\nDeep Linking with Android\n\n\nLet's start our investigation with the Android code-base.  Our push notification is received by the \nOnMessage()\n method within the \nGcmService\n class in the \nGcmHandler.cs\n file.  We can easily extract the two fields we need to execute our deep-link:\n\n\nprotected override void OnMessage(Context context, Intent intent)\n{\n    Log.Info(\nGcmService\n, $\nMessage {intent.ToString()}\n);\n    var message = intent.Extras.GetString(\nmessage\n) ?? \nUnknown Message\n;\n    var picture = intent.Extras.GetString(\npicture\n);\n    CreateNotification(\nTaskList\n, message, picture);\n}\n\n\n\n\nWe can continue by implementing a special format of the notification message we used earlier to send a notification:\n\n\nprivate void CreateNotification(string title, string msg, string parameter = null)\n{\n    var startupIntent = new Intent(this, typeof(MainActivity));\n    startupIntent.PutExtra(\nparam\n, parameter);\n\n    var stackBuilder = TaskStackBuilder.Create(this);\n    stackBuilder.AddParentStack(Java.Lang.Class.FromType(typeof(MainActivity)));\n    stackBuilder.AddNextIntent(startupIntent);\n\n    var pendingIntent = stackBuilder.GetPendingIntent(0, PendingIntentFlags.OneShot);\n    var notification = new Notification.Builder(this)\n        .SetContentIntent(pendingIntent)\n        .SetContentTitle(title)\n        .SetContentText(msg)\n        .SetSmallIcon(Resource.Drawable.icon)\n        .SetAutoCancel(true)\n        .Build();\n    var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager;\n    notificationManager.Notify(0, notification);\n}\n\n\n\n\nThe additional piece is the \nstartupIntent\n.  When the user clicks on open, the mobile app is called with the \nstartupIntent\n included in the context.  We update the \nOnCreate()\n method with \nMainActivity.cs\n to read this intent:\n\n\n[Activity(Label = \nTaskList.Droid\n, Icon = \n@drawable/icon\n, MainLauncher = true, ConfigurationChanges = ConfigChanges.ScreenSize | ConfigChanges.Orientation)]\npublic class MainActivity : global::Xamarin.Forms.Platform.Android.FormsApplicationActivity\n{\n    protected override void OnCreate(Bundle bundle)\n    {\n        base.OnCreate(bundle);\n\n        Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n        global::Xamarin.Forms.Forms.Init(this, bundle);\n\n        ((DroidPlatformProvider)DependencyService.Get\nIPlatformProvider\n()).Init(this);\n\n        string param = this.Intent.GetStringExtra(\nparam\n);\n        LoadApplication(new App(loadParameter: param));\n    }\n}\n\n\n\n\nThe param string is null on the first start (or when the intent is not present).  This get's passed to our \nApp()\n constructor (in the shared project):\n\n\npublic App(string loadParameter = null)\n{\n    ServiceLocator.Instance.Add\nICloudService, AzureCloudService\n();\n\n    if (loadParameter == null)\n    {\n        MainPage = new NavigationPage(new Pages.EntryPage());\n    }\n    else\n    {\n        MainPage = new NavigationPage(new Pages.PictureView(loadParameter));\n    }\n}\n\n\n\n\nIf the \nApp()\n constructor is passed a non-null parameter, then we deep-link to a new page instead of going to the entry page.  Now all we need to do is create a XAML page as follows that loads a picture.  The \nPages.PictureView.xaml\n is small enough since its only function is to display a picture:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage\n    x:Class=\nTaskList.Pages.PictureView\n\n    xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n    xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n    \nImage x:Name=\nbackground\n Source=\n{Binding PictureSource, Mode=OneWay}\n /\n\n\n/ContentPage\n\n\n\n\n\nThe code behind file looks similar to the \nTaskDetail\n page:\n\n\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TaskList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class PictureView : ContentPage\n    {\n        public PictureView(string picture)\n        {\n            InitializeComponent();\n            BindingContext = new ViewModels.PictureViewModel(picture);\n        }\n    }\n}\n\n\n\n\nFinally, the view model should be familiar at this point:\n\n\nusing TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class PictureViewModel : BaseViewModel\n    {\n        public PictureViewModel(string picture = null)\n        {\n            if (picture != null)\n            {\n                PictureSource = picture;\n            }\n            Title = \nA Picture for you\n;\n        }\n\n        public string PictureSource { get; }\n    }\n}\n\n\n\n\nIf I were to continue, I would add some controls that allow me to go back to the task list (if I am logged in) or the entry page (if I am not logged in).\n\n\n\n\nKeep the Push Small\n\n\nYou should keep the push payload as small as possible.  There are limits and they vary by platform (but are in the range of 4-5Kb).  Note that I don't include the full URL of the picture, for example, nor do I include the picture as binary data.  This allows me to adjust to an appropriate image URLwithin the client.  This keeps the number of bytes in the push small, but also allows me to adjust the image for the platform, if necessary.\n\n\n\n\nDeep Linking with iOS\n\n\nDeep linking with iOS follows a similar pattern to Android.  The notification is received by \nDidReceiveRemoteNotification()\n method in the \nAppDelegate.cs\n, which we can then process to load the appropriate page from the background.\n\n\nFirst, update the \nDidReceiveRemoteNotification()\n method to call a new method we will define in a moment.  This allows us to call the notification processor from multiple places:\n\n\n/// \nsummary\n\n/// Handler for Push Notifications\n/// \n/summary\n\npublic override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action\nUIBackgroundFetchResult\n completionHandler)\n{\n    ProcessNotification(userInfo, false);\n}\n\n\n\n\nThis method is also defined in the \nAppDelegate.cs\n class:\n\n\nprivate void ProcessNotification(NSDictionary options, bool fromFinishedLoading)\n{\n    if (!(options != null \n options.ContainsKey(new NSString(\naps\n))))\n    {\n        // Short circuit - nothing to do\n        return;\n    }\n\n    NSDictionary aps = options.ObjectForKey(new NSString(\naps\n)) as NSDictionary;\n\n    // Obtain the alert and picture elements if they are there\n    var alertString = GetStringFromOptions(aps, \nalert\n);\n    var pictureString = GetStringFromOptions(aps, \npicture\n);\n\n    if (!fromFinishedLoading)\n    {\n        // Manually show an alert\n        if (!string.IsNullOrEmpty(alertString))\n        {\n            UIAlertView alertView = new UIAlertView(\n                \nTaskList\n,\n                alertString,\n                null,\n                NSBundle.MainBundle.LocalizedString(\nCancel\n, \nCancel\n),\n                NSBundle.MainBundle.LocalizedString(\nOK\n, \nOK\n)\n            );\n            alertView.Clicked += (sender, args) =\n\n            {\n                if (args.ButtonIndex != alertView.CancelButtonIndex)\n                {\n                    if (!string.IsNullOrEmpty(pictureString))\n                    {\n                        App.Current.MainPage = new NavigationPage(new Pages.PictureView(pictureString));\n                    }\n                }\n            };\n            alertView.Show();\n        }\n    }\n}\n\nprivate string GetStringFromOptions(NSDictionary options, string key)\n{\n    string v = string.Empty;\n    if (options.ContainsKey(new NSString(key)))\n    {\n        v = (options[new NSString(key)] as NSString).ToString();\n    }\n    return v;\n}\n\n\n\n\nThis method checks to see if there is something to do.  If there is, it generates the alert as before.  This time, however, if the user clicks on OK, then it sets the current page to the same \nPictureView\n view that was used by the Android application.  The \nGetStringFromOptions()\n method is a convenience method for extracting strings from the push notification payload.\n\n\nSend the following push notification to receive the picture:\n\n\n{\n    \naps\n:{\n        \nalert\n:\nNotification Hub test notification\n,\n        \npicture\n:\nhttp://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg\n\n    }\n}\n\n\n\n\nYou should test this in the following cases:\n\n\n\n\nThe app is running and in the foreground.\n\n\nThe app is running, but in the background.\n\n\nThe app is not running at all.\n\n\n\n\nDeep Linking with UWP\n\n\nUniversal Windows is perhaps the most complete story for notifications out there.  Firstly, let's construct our notification.  On the \nTest Send\n blade within your notification hub in the Azure portal, choose \nWindows\n as the platform and cut and paste the following into the Payload:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n?\n\n\ntoast launch=\nzumobook\n\n  \nvisual\n\n    \nbinding template=\nToastGeneric\n\n      \ntext\nThis is a simple toast notification example\n/text\n\n    \n/binding\n\n  \n/visual\n\n  \nactions\n\n    \naction content=\nOpen\n arguments=\nhttp://static.boredpanda.com/blog/wp-content/uploads/2016/08/cute-kittens-7-57b30aa10707a__605.jpg\n /\n\n    \naction content=\nCancel\n arguments=\ncancel\n /\n\n  \n/actions\n\n\n/toast\n\n\n\n\n\nThis payload provides a textual response with two buttons - an open button and a cancel button.  The most important part of this, however, is the \nlaunch=\"zumobook\"\n.  If the user clicks on Open, the application it is associated with is launched via the \nOnActivated()\n method, and the toast information is passed into that method.  This method is located in the \nApp.xaml.cs\n file of the TaskList.UWP project:\n\n\nprotected override void OnActivated(IActivatedEventArgs args)\n{\n    if (args.Kind == ActivationKind.ToastNotification)\n    {\n        var toastArgs = args as ToastNotificationActivatedEventArgs;\n        Xamarin.Forms.Application.Current.MainPage = new Xamarin.Forms.NavigationPage(\n            new Pages.PictureView(toastArgs.Argument));\n    }\n}\n\n\n\n\nThe only real problem here is that there is a conflict within this file between the standard \nFrame\n object and the Xamarin Forms version of the \nFrame\n object.  If you use \nusing Xamarin.Forms;\n in this file, you have to fully qualify conflicting classes.  It's just as easy to fully-qualify the specific Xamarin Forms classes when they are needed, as I did above.\n\n\nPush to Sync\n\n\nSometimes, you want to alert the user that there is something new for that user.  When the user is alerted, acceptance of the push notification indicates that the user wants to go to the app and synchronize the database before viewing the data.\n\n\nPush to Sync is very similar to the Marketing Push, but there are some caveats.  In general, the synchronization process should happen within 30 seconds.  That's not very long in the mobile world.  So, what do you do?\n\n\nFirstly, let's look at the code for the server-side.  We need to generate an asynchronous push whenever a record is updated.  We will pass the ID of the updated record with the push.  Here is an example table controller:\n\n\nusing System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Microsoft.Azure.Mobile.Server;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Microsoft.Azure.NotificationHubs;\nusing System.Collections.Generic;\nusing System;\n\nnamespace Backend.Controllers\n{\n    [Authorize]\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems()\n        {\n            return Query();\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id)\n        {\n            return Lookup(id);\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch)\n        {\n            var item = await UpdateAsync(id, patch);\n            await PushToSyncAsync(\ntodoitem\n, item.Id);\n            return item;\n        }\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            await PushToSyncAsync(\ntodoitem\n, item.Id);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task DeleteTodoItem(string id)\n        {\n            await PushToSyncAsync(\ntodoitem\n, id);\n            await DeleteAsync(id);\n        }\n\n        private async Task PushToSyncAsync(string table, string id)\n        {\n            var appSettings = this.Configuration.GetMobileAppSettingsProvider().GetMobileAppSettings();\n            var nhName = appSettings.NotificationHubName;\n            var nhConnection = appSettings.Connections[MobileAppSettingsKeys.NotificationHubConnectionString].ConnectionString;\n\n            // Create a new Notification Hub client\n            var hub = NotificationHubClient.CreateClientFromConnectionString(nhConnection, nhName);\n\n            // Create a template message\n            var templateParams = new Dictionary\nstring, string\n();\n            templateParams[\nop\n] = \nsync\n;\n            templateParams[\ntable\n] = table;\n            templateParams[\nid\n] = id;\n\n            // Send the template message\n            try\n            {\n                var result = await hub.SendTemplateNotificationAsync(templateParams);\n                Configuration.Services.GetTraceWriter().Info(result.State.ToString());\n            }\n            catch (Exception ex)\n            {\n                Configuration.Services.GetTraceWriter().Error(ex.Message, null, \nPushToSync Error\n);\n            }\n        }\n    }\n}\n\n\n\n\nThe important code here is the \nPushToSyncAsync()\n method.  This does the actual push to your clients.  In this version, any client that has registered a template with the \n$(op)\n, \n$(table)\n and \n$(id)\n variables will get the push notification.  The Notifications Hub SDK \nSendTemplateNotificationAsync()\n method can also send to a list of devices and a tag expression via various overloads.\n\n\nWe have to also register a new template.  Here are the two versions:\n\n\n// Android version\nvar pushToSyncTemplate = new PushTemplate\n{\n    Body = @\n{\ndata\n:{\nop\n:\n$(op)\n,\ntable\n:\n$(table)\n,\nid\n:\n$(id)\n}}\n\n};\ninstallation.Templates.Add(\npushToSync\n, pushToSyncTemplate);\n\n// iOS version\nPushTemplate pushToSyncTemplate = new PushTemplate\n{\n    Body = @\n{\naps\n:{\nop\n:\n$(op)\n,\ntable\n:\n$(table)\n,\nid\n:\n$(id)\n},\ncontent-available\n:1}\n\n}\ninstallation.Templates.Add(\npushToSync\n, pushToSyncTemplate);\n\n\n\n\n\n\nWhat about Universal Windows\n\n\nYou can do some remarkable things with Universal Windows, but you have to resort to raw pushes.  At that \npoint, you can decide what to put in the payload.  When running, these are handled the same way as the \nmarketing push.  For more information, see \nthe WNS documentation\n.\n\n\n\n\nThe Push to Sync message needs to be handled by the TaskList view.  The easiest mechanism of communicating with the view is to use the \nMessagingCenter\n.  The TaskList view already has the appropriate code to refresh the list when it receives a message:\n\n\n    // Execute the refresh command\n    RefreshCommand.Execute(null);\n    MessagingCenter.Subscribe\nTaskDetailViewModel\n(this, \nItemsChanged\n, async (sender) =\n\n    {\n        await ExecuteRefreshCommand();\n    });\n\n\n\n\nWe can add an appropriate push-to-sync version like this:\n\n\n    MessageCenter.Subscribe\nPushToSync\n(this, \nItemsChanged\n, async (sender) =\n \n    {\n        await ExecuteRefreshCommand();\n    });\n\n\n\n\nThis is the same code, but listens for a notification from a different class.  That class is defined in the shared project \nModels\n folder:\n\n\nnamespace TaskList.Models\n{\n    public class PushToSync\n    {\n        public string Table { get; set; }\n        public string Id { get; set; }\n    }\n}\n\n\n\n\nWhen the MessagingCenter is sent a message for push-to-sync, it will execute the refresh command, thus refreshing the data.  All that remains is to actually send that message in response to the notification.  For Android, this is done in the \nServices\\GcmHandler.cs\n file in the \nOnMessage()\n method:\n\n\n    protected override void OnMessage(Context context, Intent intent)\n    {\n        Log.Info(\nGcmService\n, $\nMessage {intent.ToString()}\n);\n        var op = intent.Extras.GetString(\nop\n);\n        if (op != null)\n        {\n            var syncMessage = new PushToSync()\n            {\n                Table = intent.Extras.GetString(\ntable\n),\n                Id = intent.Extras.GetString(\nid\n)\n            };\n            MessagingCenter.Send\nPushToSync\n(syncMessage, \nItemsChanged\n);\n        }\n        else\n        {\n            var message = intent.Extras.GetString(\nmessage\n) ?? \nUnknown Message\n;\n            var picture = intent.Extras.GetString(\npicture\n);\n            CreateNotification(\nTaskList\n, message, picture);\n        }\n    }\n\n\n\n\nFor iOS, the send happens in the \nProcessNotification()\n method of the \nAppDelegate.cs\n class:\n\n\n    private void ProcessNotification(NSDictionary options, bool fromFinishedLoading)\n    {\n        if (!(options != null \n options.ContainsKey(new NSString(\naps\n))))\n            return;\n\n        NSDictionary aps = options.ObjectForKey(new NSString(\naps\n)) as NSDictionary;\n        if (!fromFinishedLoading)\n        {\n            var alertString = GetStringFromOptions(aps, \nalert\n);\n            if (!string.IsNullOrEmpty(alertString))\n            {\n                // Create the alert (removed for brevity)\n            }\n\n            var opString = GetStringFromOptions(aps, \nop\n);\n            if (!string.IsNullOrEmpty(opString) \n opString.Equals(\nsync\n))\n            {\n                var syncMessage = new PushToSync()\n                {\n                    Table = GetStringFromOptions(aps, \ntable\n),\n                    Id = GetStringFromOptions(aps, \nid\n)\n                };\n                MessagingCenter.Send\nPushToSync\n(syncMessage, \nItemsChanged\n);\n            }\n        }\n    }\n\n\n\n\nWhen a client inserts or updates a record into the database on the server, \nPushToSync()\n is called.  That emits a push notification in the proper form defined within the mobile app.  When the mobile app receives that push notification, it sends an \"ItemsChanged\" event to the messaging center.  The TaskList view subscribes to those events and performs a sync in response to that event.\n\n\nThere are several things we could do to this code, including:\n\n\n\n\nPush to the UserId that owns the record only - this will reduce the number of pushes that happen.\n\n\nOnly pull the specific record on the specific table that is needed.  This is available in the sender object.", 
            "title": "Push Recipes"
        }, 
        {
            "location": "/chapter5/recipes/#marketing-push", 
            "text": "The most common requirement for push notifications is to alert users of a special offer or other marketing information.  The general idea is that the marketing person will create a \"campaign\" that includes a push notification.  When the user receives the push notification, they will accept it.  If a user accepts the push notification, the mobile app will deep-link into a specific page and store the fact that the user viewed the page within the database.  To implement this sort of functionality within a cross-platform application, we need to implement  Templates .  We gave demonstrations of the implementation of the templates while we were discussing the various platform implementations.  However, we didn't actually use them.  A template is provides by the mobile client when registering.  Let's take a look at a typical template as implemented by each platform:  Android :  {\n     data : {\n         message :  $(message) ,\n         picture :  $(picture) \n    }\n}  iOS :  {\n     aps : {\n         alert :  $(message) ,\n         picture :  $(picture) \n    }\n}  Windows :  ?xml version= 1.0  encoding= utf-8 ?  toast launch= zumobook \n   visual \n     binding template= ToastGeneric \n       text $(message) /text \n     /binding \n   /visual \n   actions \n     action content= Open  arguments= $(picture)  / \n     action content= Cancel  arguments= cancel  / \n   /actions  /toast    Toast, Tile and Badge Schemas  If you want to understand the format of the XML that we are using in the Windows section, it's laid out in the  MSDN documentation .   Each of these formats can be specified in the appropriate registration call:      // Android Version\n    var genericTemplate = new PushTemplate\n    {\n        Body = @ { data :{ message : $(message) , picture : $(picture) }} \n    };\n    installation.Templates.Add( genericTemplate , genericTemplate);\n\n    // iOS Version\n    var genericTemplate = new PushTemplate\n    {\n        Body = @ { aps :{ alert : $(message) , picture : $(picture) }} \n    };\n    installation.Templates.Add( genericTemplate , genericTemplate);\n\n    // Windows Version\n    var genericTemplate = new WindowsPushTemplate\n    {\n        Body = @ ?xml version= 1.0  encoding= utf-8 ?  toast launch= zumobook \n   visual \n     binding template= ToastGeneric \n       text $(message) /text \n     /binding \n   /visual \n   actions \n     action content= Open  arguments= $(picture)  / \n     action content= Cancel  arguments= cancel  / \n   /actions  /toast \n    };\n    genericTemplate.Headers.Add( X-WNS-Type ,  wns/toast );\n    installation.Templates.Add( genericTemplate , genericTemplate);  To push, we can use the same Test Send facility in the Azure Portal.  In the Test Send screen, set the  Platforms  field to be  Custom Template , and the payload to be a JSON document with the two fields:  {\n     message :  Test Message ,\n     picture :  http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg \n}  If you have done all the changes thus far, you will receive the same notification as before.  The difference is that you are pushing a message once and receiving that same message across all the Android, iOS and Windows systems at the same time.  You no longer have to know what sort of device your users are holding - the message will get to them.  We can take this a step further, however, by deep-linking.  Deep-linking is a technique often used in push notification systems whereby we present the user a dialog that asks them to open the notification.  If the notification is opened, they are taken directly to a new view with the appropriate content provided.", 
            "title": "Marketing Push"
        }, 
        {
            "location": "/chapter5/recipes/#deep-linking-with-android", 
            "text": "Let's start our investigation with the Android code-base.  Our push notification is received by the  OnMessage()  method within the  GcmService  class in the  GcmHandler.cs  file.  We can easily extract the two fields we need to execute our deep-link:  protected override void OnMessage(Context context, Intent intent)\n{\n    Log.Info( GcmService , $ Message {intent.ToString()} );\n    var message = intent.Extras.GetString( message ) ??  Unknown Message ;\n    var picture = intent.Extras.GetString( picture );\n    CreateNotification( TaskList , message, picture);\n}  We can continue by implementing a special format of the notification message we used earlier to send a notification:  private void CreateNotification(string title, string msg, string parameter = null)\n{\n    var startupIntent = new Intent(this, typeof(MainActivity));\n    startupIntent.PutExtra( param , parameter);\n\n    var stackBuilder = TaskStackBuilder.Create(this);\n    stackBuilder.AddParentStack(Java.Lang.Class.FromType(typeof(MainActivity)));\n    stackBuilder.AddNextIntent(startupIntent);\n\n    var pendingIntent = stackBuilder.GetPendingIntent(0, PendingIntentFlags.OneShot);\n    var notification = new Notification.Builder(this)\n        .SetContentIntent(pendingIntent)\n        .SetContentTitle(title)\n        .SetContentText(msg)\n        .SetSmallIcon(Resource.Drawable.icon)\n        .SetAutoCancel(true)\n        .Build();\n    var notificationManager = GetSystemService(Context.NotificationService) as NotificationManager;\n    notificationManager.Notify(0, notification);\n}  The additional piece is the  startupIntent .  When the user clicks on open, the mobile app is called with the  startupIntent  included in the context.  We update the  OnCreate()  method with  MainActivity.cs  to read this intent:  [Activity(Label =  TaskList.Droid , Icon =  @drawable/icon , MainLauncher = true, ConfigurationChanges = ConfigChanges.ScreenSize | ConfigChanges.Orientation)]\npublic class MainActivity : global::Xamarin.Forms.Platform.Android.FormsApplicationActivity\n{\n    protected override void OnCreate(Bundle bundle)\n    {\n        base.OnCreate(bundle);\n\n        Microsoft.WindowsAzure.MobileServices.CurrentPlatform.Init();\n\n        global::Xamarin.Forms.Forms.Init(this, bundle);\n\n        ((DroidPlatformProvider)DependencyService.Get IPlatformProvider ()).Init(this);\n\n        string param = this.Intent.GetStringExtra( param );\n        LoadApplication(new App(loadParameter: param));\n    }\n}  The param string is null on the first start (or when the intent is not present).  This get's passed to our  App()  constructor (in the shared project):  public App(string loadParameter = null)\n{\n    ServiceLocator.Instance.Add ICloudService, AzureCloudService ();\n\n    if (loadParameter == null)\n    {\n        MainPage = new NavigationPage(new Pages.EntryPage());\n    }\n    else\n    {\n        MainPage = new NavigationPage(new Pages.PictureView(loadParameter));\n    }\n}  If the  App()  constructor is passed a non-null parameter, then we deep-link to a new page instead of going to the entry page.  Now all we need to do is create a XAML page as follows that loads a picture.  The  Pages.PictureView.xaml  is small enough since its only function is to display a picture:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage\n    x:Class= TaskList.Pages.PictureView \n    xmlns= http://xamarin.com/schemas/2014/forms \n    xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n     Image x:Name= background  Source= {Binding PictureSource, Mode=OneWay}  /  /ContentPage   The code behind file looks similar to the  TaskDetail  page:  using Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace TaskList.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class PictureView : ContentPage\n    {\n        public PictureView(string picture)\n        {\n            InitializeComponent();\n            BindingContext = new ViewModels.PictureViewModel(picture);\n        }\n    }\n}  Finally, the view model should be familiar at this point:  using TaskList.Abstractions;\nusing Xamarin.Forms;\n\nnamespace TaskList.ViewModels\n{\n    public class PictureViewModel : BaseViewModel\n    {\n        public PictureViewModel(string picture = null)\n        {\n            if (picture != null)\n            {\n                PictureSource = picture;\n            }\n            Title =  A Picture for you ;\n        }\n\n        public string PictureSource { get; }\n    }\n}  If I were to continue, I would add some controls that allow me to go back to the task list (if I am logged in) or the entry page (if I am not logged in).   Keep the Push Small  You should keep the push payload as small as possible.  There are limits and they vary by platform (but are in the range of 4-5Kb).  Note that I don't include the full URL of the picture, for example, nor do I include the picture as binary data.  This allows me to adjust to an appropriate image URLwithin the client.  This keeps the number of bytes in the push small, but also allows me to adjust the image for the platform, if necessary.", 
            "title": "Deep Linking with Android"
        }, 
        {
            "location": "/chapter5/recipes/#deep-linking-with-ios", 
            "text": "Deep linking with iOS follows a similar pattern to Android.  The notification is received by  DidReceiveRemoteNotification()  method in the  AppDelegate.cs , which we can then process to load the appropriate page from the background.  First, update the  DidReceiveRemoteNotification()  method to call a new method we will define in a moment.  This allows us to call the notification processor from multiple places:  ///  summary \n/// Handler for Push Notifications\n///  /summary \npublic override void DidReceiveRemoteNotification(UIApplication application, NSDictionary userInfo, Action UIBackgroundFetchResult  completionHandler)\n{\n    ProcessNotification(userInfo, false);\n}  This method is also defined in the  AppDelegate.cs  class:  private void ProcessNotification(NSDictionary options, bool fromFinishedLoading)\n{\n    if (!(options != null   options.ContainsKey(new NSString( aps ))))\n    {\n        // Short circuit - nothing to do\n        return;\n    }\n\n    NSDictionary aps = options.ObjectForKey(new NSString( aps )) as NSDictionary;\n\n    // Obtain the alert and picture elements if they are there\n    var alertString = GetStringFromOptions(aps,  alert );\n    var pictureString = GetStringFromOptions(aps,  picture );\n\n    if (!fromFinishedLoading)\n    {\n        // Manually show an alert\n        if (!string.IsNullOrEmpty(alertString))\n        {\n            UIAlertView alertView = new UIAlertView(\n                 TaskList ,\n                alertString,\n                null,\n                NSBundle.MainBundle.LocalizedString( Cancel ,  Cancel ),\n                NSBundle.MainBundle.LocalizedString( OK ,  OK )\n            );\n            alertView.Clicked += (sender, args) = \n            {\n                if (args.ButtonIndex != alertView.CancelButtonIndex)\n                {\n                    if (!string.IsNullOrEmpty(pictureString))\n                    {\n                        App.Current.MainPage = new NavigationPage(new Pages.PictureView(pictureString));\n                    }\n                }\n            };\n            alertView.Show();\n        }\n    }\n}\n\nprivate string GetStringFromOptions(NSDictionary options, string key)\n{\n    string v = string.Empty;\n    if (options.ContainsKey(new NSString(key)))\n    {\n        v = (options[new NSString(key)] as NSString).ToString();\n    }\n    return v;\n}  This method checks to see if there is something to do.  If there is, it generates the alert as before.  This time, however, if the user clicks on OK, then it sets the current page to the same  PictureView  view that was used by the Android application.  The  GetStringFromOptions()  method is a convenience method for extracting strings from the push notification payload.  Send the following push notification to receive the picture:  {\n     aps :{\n         alert : Notification Hub test notification ,\n         picture : http://r.ddmcdn.com/w_606/s_f/o_1/cx_0/cy_15/cw_606/ch_404/APL/uploads/2014/06/01-kitten-cuteness-1.jpg \n    }\n}  You should test this in the following cases:   The app is running and in the foreground.  The app is running, but in the background.  The app is not running at all.", 
            "title": "Deep Linking with iOS"
        }, 
        {
            "location": "/chapter5/recipes/#deep-linking-with-uwp", 
            "text": "Universal Windows is perhaps the most complete story for notifications out there.  Firstly, let's construct our notification.  On the  Test Send  blade within your notification hub in the Azure portal, choose  Windows  as the platform and cut and paste the following into the Payload:  ?xml version= 1.0  encoding= utf-8 ?  toast launch= zumobook \n   visual \n     binding template= ToastGeneric \n       text This is a simple toast notification example /text \n     /binding \n   /visual \n   actions \n     action content= Open  arguments= http://static.boredpanda.com/blog/wp-content/uploads/2016/08/cute-kittens-7-57b30aa10707a__605.jpg  / \n     action content= Cancel  arguments= cancel  / \n   /actions  /toast   This payload provides a textual response with two buttons - an open button and a cancel button.  The most important part of this, however, is the  launch=\"zumobook\" .  If the user clicks on Open, the application it is associated with is launched via the  OnActivated()  method, and the toast information is passed into that method.  This method is located in the  App.xaml.cs  file of the TaskList.UWP project:  protected override void OnActivated(IActivatedEventArgs args)\n{\n    if (args.Kind == ActivationKind.ToastNotification)\n    {\n        var toastArgs = args as ToastNotificationActivatedEventArgs;\n        Xamarin.Forms.Application.Current.MainPage = new Xamarin.Forms.NavigationPage(\n            new Pages.PictureView(toastArgs.Argument));\n    }\n}  The only real problem here is that there is a conflict within this file between the standard  Frame  object and the Xamarin Forms version of the  Frame  object.  If you use  using Xamarin.Forms;  in this file, you have to fully qualify conflicting classes.  It's just as easy to fully-qualify the specific Xamarin Forms classes when they are needed, as I did above.", 
            "title": "Deep Linking with UWP"
        }, 
        {
            "location": "/chapter5/recipes/#push-to-sync", 
            "text": "Sometimes, you want to alert the user that there is something new for that user.  When the user is alerted, acceptance of the push notification indicates that the user wants to go to the app and synchronize the database before viewing the data.  Push to Sync is very similar to the Marketing Push, but there are some caveats.  In general, the synchronization process should happen within 30 seconds.  That's not very long in the mobile world.  So, what do you do?  Firstly, let's look at the code for the server-side.  We need to generate an asynchronous push whenever a record is updated.  We will pass the ID of the updated record with the push.  Here is an example table controller:  using System.Linq;\nusing System.Threading.Tasks;\nusing System.Web.Http;\nusing System.Web.Http.Controllers;\nusing System.Web.Http.OData;\nusing Microsoft.Azure.Mobile.Server;\nusing Backend.DataObjects;\nusing Backend.Models;\nusing Microsoft.Azure.Mobile.Server.Config;\nusing Microsoft.Azure.NotificationHubs;\nusing System.Collections.Generic;\nusing System;\n\nnamespace Backend.Controllers\n{\n    [Authorize]\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request, enableSoftDelete: true);\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems()\n        {\n            return Query();\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id)\n        {\n            return Lookup(id);\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch)\n        {\n            var item = await UpdateAsync(id, patch);\n            await PushToSyncAsync( todoitem , item.Id);\n            return item;\n        }\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            TodoItem current = await InsertAsync(item);\n            await PushToSyncAsync( todoitem , item.Id);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public async Task DeleteTodoItem(string id)\n        {\n            await PushToSyncAsync( todoitem , id);\n            await DeleteAsync(id);\n        }\n\n        private async Task PushToSyncAsync(string table, string id)\n        {\n            var appSettings = this.Configuration.GetMobileAppSettingsProvider().GetMobileAppSettings();\n            var nhName = appSettings.NotificationHubName;\n            var nhConnection = appSettings.Connections[MobileAppSettingsKeys.NotificationHubConnectionString].ConnectionString;\n\n            // Create a new Notification Hub client\n            var hub = NotificationHubClient.CreateClientFromConnectionString(nhConnection, nhName);\n\n            // Create a template message\n            var templateParams = new Dictionary string, string ();\n            templateParams[ op ] =  sync ;\n            templateParams[ table ] = table;\n            templateParams[ id ] = id;\n\n            // Send the template message\n            try\n            {\n                var result = await hub.SendTemplateNotificationAsync(templateParams);\n                Configuration.Services.GetTraceWriter().Info(result.State.ToString());\n            }\n            catch (Exception ex)\n            {\n                Configuration.Services.GetTraceWriter().Error(ex.Message, null,  PushToSync Error );\n            }\n        }\n    }\n}  The important code here is the  PushToSyncAsync()  method.  This does the actual push to your clients.  In this version, any client that has registered a template with the  $(op) ,  $(table)  and  $(id)  variables will get the push notification.  The Notifications Hub SDK  SendTemplateNotificationAsync()  method can also send to a list of devices and a tag expression via various overloads.  We have to also register a new template.  Here are the two versions:  // Android version\nvar pushToSyncTemplate = new PushTemplate\n{\n    Body = @ { data :{ op : $(op) , table : $(table) , id : $(id) }} \n};\ninstallation.Templates.Add( pushToSync , pushToSyncTemplate);\n\n// iOS version\nPushTemplate pushToSyncTemplate = new PushTemplate\n{\n    Body = @ { aps :{ op : $(op) , table : $(table) , id : $(id) }, content-available :1} \n}\ninstallation.Templates.Add( pushToSync , pushToSyncTemplate);   What about Universal Windows  You can do some remarkable things with Universal Windows, but you have to resort to raw pushes.  At that \npoint, you can decide what to put in the payload.  When running, these are handled the same way as the \nmarketing push.  For more information, see  the WNS documentation .   The Push to Sync message needs to be handled by the TaskList view.  The easiest mechanism of communicating with the view is to use the  MessagingCenter .  The TaskList view already has the appropriate code to refresh the list when it receives a message:      // Execute the refresh command\n    RefreshCommand.Execute(null);\n    MessagingCenter.Subscribe TaskDetailViewModel (this,  ItemsChanged , async (sender) = \n    {\n        await ExecuteRefreshCommand();\n    });  We can add an appropriate push-to-sync version like this:      MessageCenter.Subscribe PushToSync (this,  ItemsChanged , async (sender) =  \n    {\n        await ExecuteRefreshCommand();\n    });  This is the same code, but listens for a notification from a different class.  That class is defined in the shared project  Models  folder:  namespace TaskList.Models\n{\n    public class PushToSync\n    {\n        public string Table { get; set; }\n        public string Id { get; set; }\n    }\n}  When the MessagingCenter is sent a message for push-to-sync, it will execute the refresh command, thus refreshing the data.  All that remains is to actually send that message in response to the notification.  For Android, this is done in the  Services\\GcmHandler.cs  file in the  OnMessage()  method:      protected override void OnMessage(Context context, Intent intent)\n    {\n        Log.Info( GcmService , $ Message {intent.ToString()} );\n        var op = intent.Extras.GetString( op );\n        if (op != null)\n        {\n            var syncMessage = new PushToSync()\n            {\n                Table = intent.Extras.GetString( table ),\n                Id = intent.Extras.GetString( id )\n            };\n            MessagingCenter.Send PushToSync (syncMessage,  ItemsChanged );\n        }\n        else\n        {\n            var message = intent.Extras.GetString( message ) ??  Unknown Message ;\n            var picture = intent.Extras.GetString( picture );\n            CreateNotification( TaskList , message, picture);\n        }\n    }  For iOS, the send happens in the  ProcessNotification()  method of the  AppDelegate.cs  class:      private void ProcessNotification(NSDictionary options, bool fromFinishedLoading)\n    {\n        if (!(options != null   options.ContainsKey(new NSString( aps ))))\n            return;\n\n        NSDictionary aps = options.ObjectForKey(new NSString( aps )) as NSDictionary;\n        if (!fromFinishedLoading)\n        {\n            var alertString = GetStringFromOptions(aps,  alert );\n            if (!string.IsNullOrEmpty(alertString))\n            {\n                // Create the alert (removed for brevity)\n            }\n\n            var opString = GetStringFromOptions(aps,  op );\n            if (!string.IsNullOrEmpty(opString)   opString.Equals( sync ))\n            {\n                var syncMessage = new PushToSync()\n                {\n                    Table = GetStringFromOptions(aps,  table ),\n                    Id = GetStringFromOptions(aps,  id )\n                };\n                MessagingCenter.Send PushToSync (syncMessage,  ItemsChanged );\n            }\n        }\n    }  When a client inserts or updates a record into the database on the server,  PushToSync()  is called.  That emits a push notification in the proper form defined within the mobile app.  When the mobile app receives that push notification, it sends an \"ItemsChanged\" event to the messaging center.  The TaskList view subscribes to those events and performs a sync in response to that event.  There are several things we could do to this code, including:   Push to the UserId that owns the record only - this will reduce the number of pushes that happen.  Only pull the specific record on the specific table that is needed.  This is available in the sender object.", 
            "title": "Push to Sync"
        }, 
        {
            "location": "/chapter6/mvc/", 
            "text": "At some point, you will likely want to pair your mobile application with a web interface.  This may be because you have a simplified mobile app whereas you may have a more fully featured app within the web.  For example,  I see this design featured prominently in fitness apps.  The mobile app is a news feed and recording device, whereas the web interface contains all the fitness analytics.  You may also have some sort of administrative interface that provides an alternate view of the data.\n\n\nWhatever the reason you decide to support web and mobile together, you will need to convert your Azure Mobile App backend to a fully-fledged ASP.NET MVC application.  Fortunately, the process of merging Azure Mobile Apps with an existing ASP.NET MVC application is simple.  Doing the reverse (merging MVC into Azure Mobile Apps) is considerably more complex.\n\n\nStart by creating a new ASP.NET application with File -\n New Project.  Select the \nASP.NET Web Application (.NET Framework)\n project template.  Then select th \nMVC\n template.  Change the Authentication to \nNo Authentication\n.\n\n\n\n\nClick \nOK\n to create the project.  Run your project to ensure it is working correctly.\n\n\n\n\nWhy is merging MVC into Azure Mobile Apps so hard?\n\n\nASP.NET requires a large number of NuGet packages to implement MVC.  These are provided for you when you start from the appropriate template, but you will need to add them yourself when you start from the Azure Mobile Apps template.\n\n\n\n\nNow that you have an MVC project, let's add Azure Mobile Apps to it.  Start by adding the following two NuGet packages to your project:\n\n\n\n\nMicrosoft.Azure.Mobile.Server.Quickstart\n\n\nMicrosoft.Owin.Host.SystemWeb\n\n\n\n\nThe \nMicrosoft.Azure.Mobile.Server.Quickstart\n NuGet package contains dependencies for all the other Azure Mobile Apps SDK requirements.  If you want the big long list instead, add the following:\n\n\n\n\nAutoMapper\n\n\nEntityFramework\n\n\nMicrosoft.AspNet.WebApi.Client\n\n\nMicrosoft.AspNet.WebApi.Core\n\n\nMicrosoft.AspNet.WebApi.Owin\n\n\nMicrosoft.Azure.Mobile.Server\n\n\nMicrosoft.Azure.Mobile.Server.Authentication\n\n\nMicrosoft.Azure.Mobile.Server.Notifications\n\n\nMicrosoft.Azure.NotificationHubs\n\n\nMicrosoft.Data.Edm\n\n\nMicrosoft.Owin\n\n\nMicrosoft.Owin.Security\n\n\nMicrosoft.WindowsAzure.ConfigurationManager\n\n\nOwin\n\n\nSystem.IdentityModel.Tokens.Jwt (v4.0.x - do not install v5.x)\n\n\nSystem.Spatial\n\n\n\n\n\n\nCustom Authentication\n\n\nIf you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides.  There is no assistance provided with the platform.  You will also need to add the \nMicrosoft.Azure.Mobile.Server.Login\n package to your project.\n\n\n\n\nUsing the Quickstart package is a serious time saver over having to type in 16 package names.  The SystemWeb package enables the use of the Owin Startup.cs class.  This is used to bootstrap the Azure Mobile Apps configuration.\n\n\n\n\nUpgrading to .NET 4.6\n\n\nIf you want to run your ASP.NET service under .NET Framework 4.6, you can upgrade just about everything.  However, the \nSystem.IdentityModel.Tokens.Jwt\n package should not be upgraded - leave it on the latest v4.x release.  Do not upgrade \nAutoMapper\n beyond v3.3.1 if you are using the \nMappedEntityDomainManager\n class.\n\n\n\n\nYou can then copy the the following files from your original Azure Mobile Apps server project to the new project.\n\n\n\n\nApp_Start\\Startup.MobileApp.cs\n\n\nControllers\\*.cs\n\n\nDataObjects\\*.cs\n\n\nModels\\MobileServiceContext.cs\n\n\n\n\nFinally, adjust or create the \nStartup.cs\n file as follows:\n\n\nusing Microsoft.Owin;\nusing Owin;\n\n[assembly: OwinStartup(typeof(Backend.Startup))]\nnamespace Backend\n{\n    public partial class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            ConfigureMobileApp(app);\n        }\n    }\n}\n\n\n\n\nNote the addition of the \nConfigureMobileApp()\n call.  If you are starting from the suggested template, this file does not exist and you will need to create it.\n\n\nFinally, you must update the \nWeb.config\n file.  Firstly, in the \nconfigSections\n tag, add the following to support Entity Framework:\n\n\n    \nsection name=\nentityFramework\n type=\nSystem.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\n requirePermission=\nfalse\n /\n\n\n\n\n\nAdd the following \nconnectionStrings\n section:\n\n\n  \nconnectionStrings\n\n    \nadd name=\nMS_TableConnectionString\n connectionString=\nData Source=(localdb)\\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\\aspnet-Backend.mdf;Initial Catalog=aspnet-Backend-20160720081828;Integrated Security=True;MultipleActiveResultSets=True\n providerName=\nSystem.Data.SqlClient\n /\n\n  \n/connectionStrings\n\n\n\n\n\nAdd the following entries to the \nappSettings\n section:\n\n\n  \nappSettings\n\n    \nadd key=\nwebpages:Enabled\n value=\nfalse\n /\n\n    \nadd key=\nPreserveLoginUrl\n value=\ntrue\n /\n\n    \nadd key=\nMS_SigningKey\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nEMA_RuntimeUrl\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nMS_NotificationHubName\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nSigningKey\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nValidAudience\n value=\nhttps://chapter6.azurewebsites.net/\n /\n\n    \nadd key=\nValidIssuer\n value=\nhttps://chapter6.azurewebsites.net/\n /\n\n  \n/appSettings\n\n\n\n\n\nThe first appSetting key should already be present.  These changes can be copied from the \nWeb.config\n file from your Azure Mobile Apps project.\n\n\nSharing the Database\n\n\nUnderneath the covers, Azure Mobile Apps uses \nEntityFramework\n to access the database.  It requires certain adjustments to the models, as we discussed in \nChapter 3\n.  However, you can still use the same Entity Framework context to access the database.  There are some caveats that must be followed, however:\n\n\n\n\nInserts must set the fields that are not managed by the database (such as \nId\n).\n\n\nDeletes must set the \nDeleted\n column if using Soft Delete, instead of directly deleting records.\n\n\n\n\nBefore you get started, you have to enable EF code-first migrations.  If you don't, you will get an error about a duplicate clustered index in your application for each table based on EntityData.  To enable migrations:\n\n\n\n\nOpen the Package Manager Console in Visual Studio\n\n\nRun \nEnable-Migrations\n\n\n\n\nAdjust the created \nMigrations\\Configuration.cs\n constructor as follows:\n\n\ncsharp\n    public Configuration()\n    {\n        AutomaticMigrationsEnabled = false;\n        SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator());\n    }\n\n\n\n\n\n\nIn your \nApp_Start\\Startup.MobileApp.cs\n, comment out or remove your  \nDatabase.SetInitializer()\n call, and replace with:\n\n\ncsharp\nvar migrator = new DbMigrator(new Migrations.Configuration());\nmigrator.Update();\n\n\nYou can also remove the MobileServiceInitializer class in the same file, if you wish.\n\n\n\n\n\n\nRun \nAdd-Migration initial\n in the Package Manager Console.\n\n\n\n\n\n\nThis is an abbreviated set of instructions from our work in \nChapter 3\n.\n\n\nAs an example, let's create a default view for handling our TodoItem controller.  In MVC, you need a Model, View and Controller class.  The Model will be handled by our existing \nDataObjects\\TodoItem.cs\n class.  The Controller and View will be new classes.  Let's take a look at the replacement \nHomeController.cs\n class first:\n\n\nusing System.Linq;\nusing System.Web.Mvc;\nusing Backend.Models;\n\nnamespace Backend.Controllers\n{\n    public class HomeController : Controller\n    {\n        private MobileServiceContext context;\n\n        public HomeController()\n        {\n            context = new MobileServiceContext();\n        }\n\n        public ActionResult Index()\n        {\n            var list = context.TodoItems.ToList();\n            return View(list);\n        }\n\n        [HttpPost]\n        [ValidateAntiForgeryToken]\n        public async Task\nActionResult\n Create([Bind(Include = \nText\n)]TodoItem item)\n        {\n            try\n            {\n                if (ModelState.IsValid)\n                {\n                    item.Id = Guid.NewGuid().ToString(\nN\n);\n                    context.TodoItems.Add(item);\n                    await context.SaveChangesAsync();\n                }\n            }\n            catch (DataException)\n            {\n                ModelState.AddModelError(\n, \nUnable to save changes.\n);\n            }\n            return RedirectToAction(\nIndex\n);\n        }\n    }\n}\n\n\n\n\nI am using the existing MobileServiceContext as the Entity Framework context.  The list of tasks is taken directly from the \nDbSet\n that was established for the mobile backend table controller.  I also have a method for creating a new todo item within the controller.  If you look for a tutorial on implementing CRUD in ASP.NET MVC, it's likely you will see code similar to this.\n\n\n\n\nInfo\n\n\nI've removed some additional views from this backend (About and Contact) plus the links in the layout partial view.  This is just to make the code cleaner.  You can leave them in if you desire.\n\n\n\n\nThe view in \nViews\\Home\\Index.cshtml\n is similarly changed:\n\n\n@{\n    ViewBag.Title = \nHome Page\n;\n}\n@model IEnumerable\nBackend.DataObjects.TodoItem\n\n\n\ndiv class=\nrow\n style=\nmargin-top: 8px;\n\n    \ndiv class=\ncol-md-1\n/div\n\n    \ndiv class=\ncol-md-10\n\n        @using (Html.BeginForm(\nCreate\n, \nHome\n, FormMethod.Post))\n        {\n            @Html.AntiForgeryToken()\n            \ninput type=\ntext\n name=\nText\n placeholder=\nEnter TodoItem Text...\n/\n\n            \ninput type=\nsubmit\n value=\nAdd Todo Item\n/\n\n        }\n    \n/div\n\n    \ndiv class=\ncol-md-1\n/div\n\n\n/div\n\n\ndiv class=\nrow\n style=\nmargin-top: 8px;\n\n    \ndiv class=\ncol-md-1\n/div\n\n    \ndiv class=\ncol-md-10\n\n        \ndiv class=\ntable-responsive\n\n            \ntable class=\ntable table-striped table-bordered table-hover table-condensed\n\n                \nthead\n\n                    \ntr\n\n                        \nth\n#\n/th\n\n                        \nth\nText\n/th\n\n                        \nth\nComplete\n/th\n\n                    \n/tr\n\n                \n/thead\n\n                \ntbody\n\n                    @foreach (var item in Model)\n                    {\n                        \ntr\n\n                            \ntd\n@Html.DisplayFor(modelItem =\n item.Id)\n/td\n\n                            \ntd\n@Html.DisplayFor(modelItem =\n item.Text)\n/td\n\n                            \ntd\n@Html.DisplayFor(modelItem =\n item.Complete)\n/td\n\n                        \n/tr\n\n                    }\n                \n/tbody\n\n            \n/table\n\n        \n/div\n\n    \n/div\n\n    \ndiv class=\ncol-md-1\n/div\n\n\n/div\n\n\n\n\n\nThe HTML classes are from \nBootstrap\n - a common CSS framework.  The first row div encapsulates the form for adding a new todo item, and the second row div encapsulates the list.  If you enter some text into the box and click the submit button, it will be added to the database.\n\n\nSharing Authentication\n\n\n\n\nCustom Authentication\n\n\nThis section only pertains to using the standard authentication techniques provided with Azure App Service.  It does not pertain to custom authentication.  If you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides.  There is no assistance provided with the platform.\n\n\n\n\nAs one would suspect, \nsetting up App Service Authentication\n, then adding an \n[Authorize]\n attribute to the \nHomeController\n is a good starting point for making our application authenticated.  However, it isn't enough.  If you just do this, you will got something like the following:\n\n\n\n\nIn order to properly capture the login flow, we have to redirect a missing authentication to the appropriate authentication endpoint.  In ASP.NET, this functionality is configured within the \nWeb.config\n file.  Locate the \nsystem.web\n section and add the following:\n\n\n  \nsystem.web\n\n    \ncompilation debug=\ntrue\n targetFramework=\n4.5.2\n/\n\n    \nhttpRuntime targetFramework=\n4.5.2\n/\n\n    \nauthentication mode=\nForms\n\n      \nforms loginUrl=\n/.auth/login/aad\n timeout=\n2880\n/\n\n    \n/authentication\n\n  \n/system.web\n\n\n\n\n\nThe \ncompilation\n and \nhttpRuntime\n values should already be present.  The \nauthentication\n section is new.  Once you deploy this code to Azure App Service, you will be redirected to your authentication provider.  Once the authentication process is complete, you will receive a page indicating successful authentication, with a link back to the website.\n\n\n\n\nUse a private browsing window for testing\n\n\nOne of the major problems with using Azure AD for authentication as a developer is that the Azure Portal authentication uses the same providers.  This can cause problems in your app.  Always use a private browsing window and/or a different browser for testing your code.\n\n\n\n\nUsing Anti-Forgery Tokens\n\n\nOne of the gotchas for using ASP.NET MVC with Azure App Service Authentication is that the Anti-Forgery Token no longer works as advertised.  If you try to use the anti-forgery token on POST operations (and the associated \n[ValidateAntiForgeryToken]\n within your controller), you will receive the following exception:\n\n\n\n\nA claim of type 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier' or 'http://schemas.microsoft.com/accesscontrolservice/2010/07/claims/identityprovider' was not present on the provided ClaimsIdentity. To enable anti-forgery token support with claims-based authentication, please verify that the configured claims provider is providing both of these claims on the ClaimsIdentity instances it generates. If the configured claims provider instead uses a different claim type as a unique identifier, it can be configured by setting the static property AntiForgeryConfig.UniqueClaimTypeIdentifier.\n\n\n\n\nUnfortunately, the logic here is wrong.  Check the \nsource code\n and you will see that both listed claims must be present to not throw the exception.  The Azure App Service Authentication claims do not include the \nidentityprovider\n claim.  With the advent of .NET Core, I do not expect this bug to be fixed.  The workaround is to explicitly specify the identifier to use somewhere in your application startup:\n\n\nAntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier;\n\n\n\n\nI place this in the MVC specific \nApp_Start\\RouteConfig.cs\n file:\n\n\nnamespace Backend\n{\n    public class RouteConfig\n    {\n        public static void RegisterRoutes(RouteCollection routes)\n        {\n            routes.IgnoreRoute(\n{resource}.axd/{*pathInfo}\n);\n\n            routes.MapRoute(\n                name: \nDefault\n,\n                url: \n{controller}/{action}/{id}\n,\n                defaults: new { controller = \nHome\n, action = \nIndex\n, id = UrlParameter.Optional }\n            );\n\n            AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier;\n        }\n    }\n}\n\n\n\n\nThis will force the use of the (singular) claim rather than requiring both claims to be present, thus allowing you to use the anti-forgery token.", 
            "title": "MVC Applications"
        }, 
        {
            "location": "/chapter6/mvc/#sharing-the-database", 
            "text": "Underneath the covers, Azure Mobile Apps uses  EntityFramework  to access the database.  It requires certain adjustments to the models, as we discussed in  Chapter 3 .  However, you can still use the same Entity Framework context to access the database.  There are some caveats that must be followed, however:   Inserts must set the fields that are not managed by the database (such as  Id ).  Deletes must set the  Deleted  column if using Soft Delete, instead of directly deleting records.   Before you get started, you have to enable EF code-first migrations.  If you don't, you will get an error about a duplicate clustered index in your application for each table based on EntityData.  To enable migrations:   Open the Package Manager Console in Visual Studio  Run  Enable-Migrations   Adjust the created  Migrations\\Configuration.cs  constructor as follows:  csharp\n    public Configuration()\n    {\n        AutomaticMigrationsEnabled = false;\n        SetSqlGenerator(\"System.Data.SqlClient\", new EntityTableSqlGenerator());\n    }    In your  App_Start\\Startup.MobileApp.cs , comment out or remove your   Database.SetInitializer()  call, and replace with:  csharp\nvar migrator = new DbMigrator(new Migrations.Configuration());\nmigrator.Update();  You can also remove the MobileServiceInitializer class in the same file, if you wish.    Run  Add-Migration initial  in the Package Manager Console.    This is an abbreviated set of instructions from our work in  Chapter 3 .  As an example, let's create a default view for handling our TodoItem controller.  In MVC, you need a Model, View and Controller class.  The Model will be handled by our existing  DataObjects\\TodoItem.cs  class.  The Controller and View will be new classes.  Let's take a look at the replacement  HomeController.cs  class first:  using System.Linq;\nusing System.Web.Mvc;\nusing Backend.Models;\n\nnamespace Backend.Controllers\n{\n    public class HomeController : Controller\n    {\n        private MobileServiceContext context;\n\n        public HomeController()\n        {\n            context = new MobileServiceContext();\n        }\n\n        public ActionResult Index()\n        {\n            var list = context.TodoItems.ToList();\n            return View(list);\n        }\n\n        [HttpPost]\n        [ValidateAntiForgeryToken]\n        public async Task ActionResult  Create([Bind(Include =  Text )]TodoItem item)\n        {\n            try\n            {\n                if (ModelState.IsValid)\n                {\n                    item.Id = Guid.NewGuid().ToString( N );\n                    context.TodoItems.Add(item);\n                    await context.SaveChangesAsync();\n                }\n            }\n            catch (DataException)\n            {\n                ModelState.AddModelError( ,  Unable to save changes. );\n            }\n            return RedirectToAction( Index );\n        }\n    }\n}  I am using the existing MobileServiceContext as the Entity Framework context.  The list of tasks is taken directly from the  DbSet  that was established for the mobile backend table controller.  I also have a method for creating a new todo item within the controller.  If you look for a tutorial on implementing CRUD in ASP.NET MVC, it's likely you will see code similar to this.   Info  I've removed some additional views from this backend (About and Contact) plus the links in the layout partial view.  This is just to make the code cleaner.  You can leave them in if you desire.   The view in  Views\\Home\\Index.cshtml  is similarly changed:  @{\n    ViewBag.Title =  Home Page ;\n}\n@model IEnumerable Backend.DataObjects.TodoItem  div class= row  style= margin-top: 8px; \n     div class= col-md-1 /div \n     div class= col-md-10 \n        @using (Html.BeginForm( Create ,  Home , FormMethod.Post))\n        {\n            @Html.AntiForgeryToken()\n             input type= text  name= Text  placeholder= Enter TodoItem Text... / \n             input type= submit  value= Add Todo Item / \n        }\n     /div \n     div class= col-md-1 /div  /div  div class= row  style= margin-top: 8px; \n     div class= col-md-1 /div \n     div class= col-md-10 \n         div class= table-responsive \n             table class= table table-striped table-bordered table-hover table-condensed \n                 thead \n                     tr \n                         th # /th \n                         th Text /th \n                         th Complete /th \n                     /tr \n                 /thead \n                 tbody \n                    @foreach (var item in Model)\n                    {\n                         tr \n                             td @Html.DisplayFor(modelItem =  item.Id) /td \n                             td @Html.DisplayFor(modelItem =  item.Text) /td \n                             td @Html.DisplayFor(modelItem =  item.Complete) /td \n                         /tr \n                    }\n                 /tbody \n             /table \n         /div \n     /div \n     div class= col-md-1 /div  /div   The HTML classes are from  Bootstrap  - a common CSS framework.  The first row div encapsulates the form for adding a new todo item, and the second row div encapsulates the list.  If you enter some text into the box and click the submit button, it will be added to the database.", 
            "title": "Sharing the Database"
        }, 
        {
            "location": "/chapter6/mvc/#sharing-authentication", 
            "text": "Custom Authentication  This section only pertains to using the standard authentication techniques provided with Azure App Service.  It does not pertain to custom authentication.  If you are using custom authentication, then you need to produce the entire login flow for both web and mobile sides.  There is no assistance provided with the platform.   As one would suspect,  setting up App Service Authentication , then adding an  [Authorize]  attribute to the  HomeController  is a good starting point for making our application authenticated.  However, it isn't enough.  If you just do this, you will got something like the following:   In order to properly capture the login flow, we have to redirect a missing authentication to the appropriate authentication endpoint.  In ASP.NET, this functionality is configured within the  Web.config  file.  Locate the  system.web  section and add the following:     system.web \n     compilation debug= true  targetFramework= 4.5.2 / \n     httpRuntime targetFramework= 4.5.2 / \n     authentication mode= Forms \n       forms loginUrl= /.auth/login/aad  timeout= 2880 / \n     /authentication \n   /system.web   The  compilation  and  httpRuntime  values should already be present.  The  authentication  section is new.  Once you deploy this code to Azure App Service, you will be redirected to your authentication provider.  Once the authentication process is complete, you will receive a page indicating successful authentication, with a link back to the website.   Use a private browsing window for testing  One of the major problems with using Azure AD for authentication as a developer is that the Azure Portal authentication uses the same providers.  This can cause problems in your app.  Always use a private browsing window and/or a different browser for testing your code.", 
            "title": "Sharing Authentication"
        }, 
        {
            "location": "/chapter6/mvc/#using-anti-forgery-tokens", 
            "text": "One of the gotchas for using ASP.NET MVC with Azure App Service Authentication is that the Anti-Forgery Token no longer works as advertised.  If you try to use the anti-forgery token on POST operations (and the associated  [ValidateAntiForgeryToken]  within your controller), you will receive the following exception:   A claim of type 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier' or 'http://schemas.microsoft.com/accesscontrolservice/2010/07/claims/identityprovider' was not present on the provided ClaimsIdentity. To enable anti-forgery token support with claims-based authentication, please verify that the configured claims provider is providing both of these claims on the ClaimsIdentity instances it generates. If the configured claims provider instead uses a different claim type as a unique identifier, it can be configured by setting the static property AntiForgeryConfig.UniqueClaimTypeIdentifier.   Unfortunately, the logic here is wrong.  Check the  source code  and you will see that both listed claims must be present to not throw the exception.  The Azure App Service Authentication claims do not include the  identityprovider  claim.  With the advent of .NET Core, I do not expect this bug to be fixed.  The workaround is to explicitly specify the identifier to use somewhere in your application startup:  AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier;  I place this in the MVC specific  App_Start\\RouteConfig.cs  file:  namespace Backend\n{\n    public class RouteConfig\n    {\n        public static void RegisterRoutes(RouteCollection routes)\n        {\n            routes.IgnoreRoute( {resource}.axd/{*pathInfo} );\n\n            routes.MapRoute(\n                name:  Default ,\n                url:  {controller}/{action}/{id} ,\n                defaults: new { controller =  Home , action =  Index , id = UrlParameter.Optional }\n            );\n\n            AntiForgeryConfig.UniqueClaimTypeIdentifier = ClaimTypes.NameIdentifier;\n        }\n    }\n}  This will force the use of the (singular) claim rather than requiring both claims to be present, thus allowing you to use the anti-forgery token.", 
            "title": "Using Anti-Forgery Tokens"
        }, 
        {
            "location": "/chapter6/jquery/", 
            "text": "One of the major changes that has happened within web applications in the past few years is the single page application, or SPA, coupled with the rise of JavaScript frameworks.  No-one can directly support all the JavaScript frameworks out there, but this section will cover a couple of the main ones - \njQuery\n, \nReact\n and \nAngular\n.  Azure Mobile Apps has a \nJavaScript SDK\n that can be used for accessing table controllers and identity services within your mobile backend.\n\n\nThe JavaScript SDK and jQuery\n\n\njQuery has a long history in web development at this point.  It is easy to pick up and functionally great for small applications.  The important thing to note here is that you cannot instantiate the Azure Mobile Apps client SDK until all the scripts are loaded.  Fortunately, JavaScript (and jQuery) provide events when this happens.\n\n\nI tend to add SPA applications to ASP.NET MVC apps by making the main HTML page a View.  First, add a controller called \"Controllers\\SPAController\" with the following contents:\n\n\nusing System.Web.Mvc;\n\nnamespace Backend.Controllers\n{\n    public class SPAController : Controller\n    {\n        public ActionResult JQuery()\n        {\n            return View();\n        }\n    }\n}\n\n\n\n\nAlso, create a directory \nViews\\SPA\n and add a \nJQuery.cshtml\n file:\n\n\n@{\n    Layout = null;\n}\n\n\n!DOCTYPE html\n\n\nhtml\n\n\nhead\n\n    \nmeta charset=\nutf-8\n\n    \nmeta name=\nviewport\n content=\nwidth=device-width\n\n    \ntitle\nJQuery\n/title\n\n    \nlink rel=\nstylesheet\n href=\n~/Content/spa/jquery/application.css\n/\n\n\n/head\n\n\nbody\n\n    \ndiv id=\nwrapper\n\n        \narticle\n\n            \nheader\n\n                \nh2\nAzure\n/h2\n\n                \nh1\nMobile Apps\n/h1\n\n                \nform id=\nadd-item\n\n                    \nbutton type=\nsubmit\n id=\nrefresh\nRefresh\n/button\n\n                    \nbutton type=\nsubmit\nAdd\n/button\n\n                    \ndiv\n\n                        \ninput type=\ntext\n id=\nnew-item-text\n placeholder=\nEnter new task\n/\n\n                    \n/div\n\n                \n/form\n\n            \n/header\n\n            \nul id=\ntodo-items\n/ul\n\n            \np id=\nsummary\nInitializing...\n/p\n\n        \n/article\n\n        \nfooter\n\n            \nul id=\nerrorlog\n/ul\n\n        \n/footer\n\n    \n/div\n\n    \nscript src=\nhttps://code.jquery.com/jquery-2.2.1.min.js\n/script\n\n    \nscript src=\nhttps://zumo.blob.core.windows.net/sdk/azure-mobile-apps-client.min.js\n/script\n\n    \nscript src=\n~/Content/spa/jquery/application.js\n/script\n\n\n/body\n\n\n/html\n\n\n\n\n\nThis will be accessed via the \n/SPA/JQuery\n path.  Unlike other (mostly component based) applications, jQuery uses mark-up extensively, so we are really setting up the basics of the application.  You can find the CSS on the \nGitHub repository\n as its contents are not germane to the discussion here.\n\n\nThe main item to note in the scripts section is the path of the Azure Mobile Apps client.  This is loaded from the ZUMO CDN.  You can also download the JavaScript file (both minified and non-minified versions exist), and include it in your \nScripts\n directory if you prefer a local copy.\n\n\nThe \n~/Content/spa/jquery/application.js\n file contains our JavaScript code.  It starts with a standard \nIIFE\n (immediately invoked function expression) - a common method of wrapping code such that variables don't leak into the global namespace:\n\n\n(function () {\n    \nuse strict\n;\n\n    $(onBrowserReady);\n\n    /* Rest of the application */\n})();\n\n\n\n\nWe cannot create a client definition until the \"DOMContentLoaded\" event has fired, which ensures that all libraries (including those slower libraries loaded from a CDN) have been loaded and executed. This, in turn, ensures that the \nWindowsAzure.MobileServiceClient\n object is available.  The \nonBrowserReady()\n function will be called when this happens:\n\n\n    var client, table;\n\n    /**\n     * Event handler, called when the browser has loaded all scripts\n     * @event\n     */\n    function onBrowserReady() {\n        // Create a connection reference to our Azure Mobile Apps backend\n        client = new WindowsAzure.MobileServiceClient(location.origin);\n\n        // Create a table reference\n        table = client.getTable('todoitem');\n\n        // Refresh the todoItems\n        refreshDisplay();\n\n        // Wire up the UI event handler for the add item\n        $('#add-item').submit(addItemHandler);\n        $('#refresh').on('click', refreshDisplay);\n    }\n\n\n\n\nOnce the browser is ready for action, we create a \nMobileServiceClient\n, get a reference to the table, then wire up the rest of the UI that we need to handle.  You may notice that the API that the JavaScript SDK for Azure Mobile Apps exposes is very similar to the .NET SDK.  This is intentional across all the Azure Mobile Apps SDK.  If you know the API surface of one version, it's likely you can take that knowledge to the other SDKs.  You only have to learn the new programming language.\n\n\nReading Table Data\n\n\nWe can see this more clearly when working with the \nrefreshDisplay()\n function:\n\n\n    /**\n     * Refresh the items within a page\n     */\n    function refreshDisplay() {\n        updateSummaryMessage('Loading data from Azure');\n\n        // Execute a query for uncompleted items and process\n        table\n            .where({ complete: false })     // Set up the query\n            .read()                         // Send query and read results\n            .then(createTodoItemList, handleError);\n    }\n\n    /**\n     * Create the DOM for a single todo item\n     * @param {Object} item the Todo item\n     * @param {string} item.id the ID of the item\n     * @param {bool} item.complete true if the item is complete\n     * @param {string} item.text the text value\n     * @returns {jQuery} jQuery DOM object\n     */\n    function createTodoItem(item) {\n        return $('\nli\n')\n            .attr('data-todoitem-id', item.id)\n            .append($('\nbutton class=\nitem-delete\nDelete\n/button\n'))\n            .append($('\ninput type=\ncheckbox\n class=\nitem-complete\n')\n                .prop('checked', item.complete))\n            .append($('\ndiv\n')\n                .append($('\ninput class=\nitem-text\n').val(item.text)));\n    }\n\n    /**\n     * Create a list of Todo Items\n     * @param {TodoItem[]} items an array of items\n     * @returns {void}\n     */\n    function createTodoItemList(items) {\n        // Cycle through each item received from Azure and add items to the item list\n        var listItems = $.map(items, createTodoItem);\n        $('#todo-items').empty().append(listItems).toggle(listItems.length \n 0);\n        updateSummaryMessage('\nstrong\n' + items.length + '\n/strong\n item(s)');\n\n        // Wire up event handlers\n        $('.item-delete').on('click', deleteItemHandler);\n        $('.item-text').on('change', updateItemTextHandler);\n        $('.item-complete').on('change', updateItemCompleteHandler);\n    }\n\n\n\n\nIn the \nrefreshDisplay()\n function, we can see the parallel with the LINQ language within the .NET world.  JavaScript does not have a LINQ library, so Azure Mobile Apps provides a simplified version of LINQ (called \nQueryJS\n) for use with Azure Mobile Apps.  Once you have set up the appropriate query, calling \n.read()\n will actually execute the HTTP GET to obtain the results.  The other two functions fill in the list of tasks with the appropriate HTML trimmings.\n\n\n\n\nThere is no concept of \"offline-sync\" in the Azure Mobile Apps JavaScript SDK as there is not an equivalent of the SQLite database available.\n\n\n\n\nYou can perform paging with \n.skip(n)\n and \n.take(n)\n, include the total number of records that would be returned without paging with \n.includeTotalCount()\n, order the returned results and filter by function instead of a specific value.  For example, let's say you had an orders table and you wanted to produce a paged list of results where the state was OPEN (a constant) that belonged to a specific user, ordered by their completion date?:\n\n\nfunction filter (userId, state) {\n    return this.owner === userId \n this.state == state;\n}\n\nfunction processResults (results) {\n    totalCount = results.totalCount;\n    // Handle results here\n}\n\ntable\n    .where(filter, filteredUser, constants.OPEN)\n    .orderBy('completionDate')\n    .skip(startRecord)\n    .take(pageSize)\n    .includeTotalCount()\n    .read()\n    .then(processResults, handleError);\n\n\n\n\nYou can get just the total count using \n.take(0).includeTotalCount()\n.\n\n\nQueryJS is extremely effective at (and optimized for) selecting the exact data you need to do live displays without extra data being transferred.\n\n\nModifying Data\n\n\nIn a similar way to the \n.InsertAsync()\n, \n.UpdateAsync()\n and \n.DeleteAsync()\n methods in the .NET SDK, there are methods for inserting, modifying and deleting data:\n\n\n    /**\n     * Given a sub-element of an LI, find the TodoItem ID associated with the list member\n     *\n     * @param {DOMElement} el the form element\n     * @returns {string} the ID of the TodoItem\n     */\n    function getTodoItemId(el) {\n        return $(el).closest('li').attr('data-todoitem-id');\n    }\n\n    /**\n     * Event handler for when the user enters some text and clicks on Add\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function addItemHandler(event) {\n        var textbox = $('#new-item-text'),\n            itemText = textbox.val();\n\n        updateSummaryMessage('Adding New Item');\n        if (itemText !== '') {\n            table.insert({\n                text: itemText,\n                complete: false\n            }).then(refreshDisplay, handleError);\n        }\n\n        textbox.val('').focus();\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user clicks on Delete next to a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function deleteItemHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget);\n\n        updateSummaryMessage('Deleting Item in Azure');\n        table\n            .del({ id: itemId })   // Async send the deletion to backend\n            .then(refreshDisplay, handleError); // Update the UI\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user updates the text of a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function updateItemTextHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget),\n            newText = $(event.currentTarget).val();\n\n        updateSummaryMessage('Updating Item in Azure');\n        table\n            .update({ id: itemId, text: newText })  // Async send the update to backend\n            .then(refreshDisplay, handleError); // Update the UI\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user updates the completed checkbox of a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function updateItemCompleteHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget),\n            isComplete = $(event.currentTarget).prop('checked');\n\n        updateSummaryMessage('Updating Item in Azure');\n        table\n            .update({ id: itemId, complete: isComplete })  // Async send the update to backend\n            .then(refreshDisplay, handleError);        // Update the UI\n    }\n\n\n\n\nWe insert data by passing the object to insert to \ntable.insert()\n, modify with \ntable.update()\n and delete with \ntable.del()\n.  These functions all operate using promises.  Once the promise returns, we call the \nrefreshDisplay()\n method to refresh the data.  In the case of the \n.insert()\n and \n.update()\n functions, the promise is called with the updated item (containing the extra fields that the server adds), allowing the code to update a cache if necessary and avoiding a round-trip for the search.", 
            "title": "jQuery Applications"
        }, 
        {
            "location": "/chapter6/jquery/#the-javascript-sdk-and-jquery", 
            "text": "jQuery has a long history in web development at this point.  It is easy to pick up and functionally great for small applications.  The important thing to note here is that you cannot instantiate the Azure Mobile Apps client SDK until all the scripts are loaded.  Fortunately, JavaScript (and jQuery) provide events when this happens.  I tend to add SPA applications to ASP.NET MVC apps by making the main HTML page a View.  First, add a controller called \"Controllers\\SPAController\" with the following contents:  using System.Web.Mvc;\n\nnamespace Backend.Controllers\n{\n    public class SPAController : Controller\n    {\n        public ActionResult JQuery()\n        {\n            return View();\n        }\n    }\n}  Also, create a directory  Views\\SPA  and add a  JQuery.cshtml  file:  @{\n    Layout = null;\n} !DOCTYPE html  html  head \n     meta charset= utf-8 \n     meta name= viewport  content= width=device-width \n     title JQuery /title \n     link rel= stylesheet  href= ~/Content/spa/jquery/application.css /  /head  body \n     div id= wrapper \n         article \n             header \n                 h2 Azure /h2 \n                 h1 Mobile Apps /h1 \n                 form id= add-item \n                     button type= submit  id= refresh Refresh /button \n                     button type= submit Add /button \n                     div \n                         input type= text  id= new-item-text  placeholder= Enter new task / \n                     /div \n                 /form \n             /header \n             ul id= todo-items /ul \n             p id= summary Initializing... /p \n         /article \n         footer \n             ul id= errorlog /ul \n         /footer \n     /div \n     script src= https://code.jquery.com/jquery-2.2.1.min.js /script \n     script src= https://zumo.blob.core.windows.net/sdk/azure-mobile-apps-client.min.js /script \n     script src= ~/Content/spa/jquery/application.js /script  /body  /html   This will be accessed via the  /SPA/JQuery  path.  Unlike other (mostly component based) applications, jQuery uses mark-up extensively, so we are really setting up the basics of the application.  You can find the CSS on the  GitHub repository  as its contents are not germane to the discussion here.  The main item to note in the scripts section is the path of the Azure Mobile Apps client.  This is loaded from the ZUMO CDN.  You can also download the JavaScript file (both minified and non-minified versions exist), and include it in your  Scripts  directory if you prefer a local copy.  The  ~/Content/spa/jquery/application.js  file contains our JavaScript code.  It starts with a standard  IIFE  (immediately invoked function expression) - a common method of wrapping code such that variables don't leak into the global namespace:  (function () {\n     use strict ;\n\n    $(onBrowserReady);\n\n    /* Rest of the application */\n})();  We cannot create a client definition until the \"DOMContentLoaded\" event has fired, which ensures that all libraries (including those slower libraries loaded from a CDN) have been loaded and executed. This, in turn, ensures that the  WindowsAzure.MobileServiceClient  object is available.  The  onBrowserReady()  function will be called when this happens:      var client, table;\n\n    /**\n     * Event handler, called when the browser has loaded all scripts\n     * @event\n     */\n    function onBrowserReady() {\n        // Create a connection reference to our Azure Mobile Apps backend\n        client = new WindowsAzure.MobileServiceClient(location.origin);\n\n        // Create a table reference\n        table = client.getTable('todoitem');\n\n        // Refresh the todoItems\n        refreshDisplay();\n\n        // Wire up the UI event handler for the add item\n        $('#add-item').submit(addItemHandler);\n        $('#refresh').on('click', refreshDisplay);\n    }  Once the browser is ready for action, we create a  MobileServiceClient , get a reference to the table, then wire up the rest of the UI that we need to handle.  You may notice that the API that the JavaScript SDK for Azure Mobile Apps exposes is very similar to the .NET SDK.  This is intentional across all the Azure Mobile Apps SDK.  If you know the API surface of one version, it's likely you can take that knowledge to the other SDKs.  You only have to learn the new programming language.", 
            "title": "The JavaScript SDK and jQuery"
        }, 
        {
            "location": "/chapter6/jquery/#reading-table-data", 
            "text": "We can see this more clearly when working with the  refreshDisplay()  function:      /**\n     * Refresh the items within a page\n     */\n    function refreshDisplay() {\n        updateSummaryMessage('Loading data from Azure');\n\n        // Execute a query for uncompleted items and process\n        table\n            .where({ complete: false })     // Set up the query\n            .read()                         // Send query and read results\n            .then(createTodoItemList, handleError);\n    }\n\n    /**\n     * Create the DOM for a single todo item\n     * @param {Object} item the Todo item\n     * @param {string} item.id the ID of the item\n     * @param {bool} item.complete true if the item is complete\n     * @param {string} item.text the text value\n     * @returns {jQuery} jQuery DOM object\n     */\n    function createTodoItem(item) {\n        return $(' li ')\n            .attr('data-todoitem-id', item.id)\n            .append($(' button class= item-delete Delete /button '))\n            .append($(' input type= checkbox  class= item-complete ')\n                .prop('checked', item.complete))\n            .append($(' div ')\n                .append($(' input class= item-text ').val(item.text)));\n    }\n\n    /**\n     * Create a list of Todo Items\n     * @param {TodoItem[]} items an array of items\n     * @returns {void}\n     */\n    function createTodoItemList(items) {\n        // Cycle through each item received from Azure and add items to the item list\n        var listItems = $.map(items, createTodoItem);\n        $('#todo-items').empty().append(listItems).toggle(listItems.length   0);\n        updateSummaryMessage(' strong ' + items.length + ' /strong  item(s)');\n\n        // Wire up event handlers\n        $('.item-delete').on('click', deleteItemHandler);\n        $('.item-text').on('change', updateItemTextHandler);\n        $('.item-complete').on('change', updateItemCompleteHandler);\n    }  In the  refreshDisplay()  function, we can see the parallel with the LINQ language within the .NET world.  JavaScript does not have a LINQ library, so Azure Mobile Apps provides a simplified version of LINQ (called  QueryJS ) for use with Azure Mobile Apps.  Once you have set up the appropriate query, calling  .read()  will actually execute the HTTP GET to obtain the results.  The other two functions fill in the list of tasks with the appropriate HTML trimmings.   There is no concept of \"offline-sync\" in the Azure Mobile Apps JavaScript SDK as there is not an equivalent of the SQLite database available.   You can perform paging with  .skip(n)  and  .take(n) , include the total number of records that would be returned without paging with  .includeTotalCount() , order the returned results and filter by function instead of a specific value.  For example, let's say you had an orders table and you wanted to produce a paged list of results where the state was OPEN (a constant) that belonged to a specific user, ordered by their completion date?:  function filter (userId, state) {\n    return this.owner === userId   this.state == state;\n}\n\nfunction processResults (results) {\n    totalCount = results.totalCount;\n    // Handle results here\n}\n\ntable\n    .where(filter, filteredUser, constants.OPEN)\n    .orderBy('completionDate')\n    .skip(startRecord)\n    .take(pageSize)\n    .includeTotalCount()\n    .read()\n    .then(processResults, handleError);  You can get just the total count using  .take(0).includeTotalCount() .  QueryJS is extremely effective at (and optimized for) selecting the exact data you need to do live displays without extra data being transferred.", 
            "title": "Reading Table Data"
        }, 
        {
            "location": "/chapter6/jquery/#modifying-data", 
            "text": "In a similar way to the  .InsertAsync() ,  .UpdateAsync()  and  .DeleteAsync()  methods in the .NET SDK, there are methods for inserting, modifying and deleting data:      /**\n     * Given a sub-element of an LI, find the TodoItem ID associated with the list member\n     *\n     * @param {DOMElement} el the form element\n     * @returns {string} the ID of the TodoItem\n     */\n    function getTodoItemId(el) {\n        return $(el).closest('li').attr('data-todoitem-id');\n    }\n\n    /**\n     * Event handler for when the user enters some text and clicks on Add\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function addItemHandler(event) {\n        var textbox = $('#new-item-text'),\n            itemText = textbox.val();\n\n        updateSummaryMessage('Adding New Item');\n        if (itemText !== '') {\n            table.insert({\n                text: itemText,\n                complete: false\n            }).then(refreshDisplay, handleError);\n        }\n\n        textbox.val('').focus();\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user clicks on Delete next to a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function deleteItemHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget);\n\n        updateSummaryMessage('Deleting Item in Azure');\n        table\n            .del({ id: itemId })   // Async send the deletion to backend\n            .then(refreshDisplay, handleError); // Update the UI\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user updates the text of a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function updateItemTextHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget),\n            newText = $(event.currentTarget).val();\n\n        updateSummaryMessage('Updating Item in Azure');\n        table\n            .update({ id: itemId, text: newText })  // Async send the update to backend\n            .then(refreshDisplay, handleError); // Update the UI\n        event.preventDefault();\n    }\n\n    /**\n     * Event handler for when the user updates the completed checkbox of a todo item\n     * @param {Event} event the event that caused the request\n     * @returns {void}\n     */\n    function updateItemCompleteHandler(event) {\n        var itemId = getTodoItemId(event.currentTarget),\n            isComplete = $(event.currentTarget).prop('checked');\n\n        updateSummaryMessage('Updating Item in Azure');\n        table\n            .update({ id: itemId, complete: isComplete })  // Async send the update to backend\n            .then(refreshDisplay, handleError);        // Update the UI\n    }  We insert data by passing the object to insert to  table.insert() , modify with  table.update()  and delete with  table.del() .  These functions all operate using promises.  Once the promise returns, we call the  refreshDisplay()  method to refresh the data.  In the case of the  .insert()  and  .update()  functions, the promise is called with the updated item (containing the extra fields that the server adds), allowing the code to update a cache if necessary and avoiding a round-trip for the search.", 
            "title": "Modifying Data"
        }, 
        {
            "location": "/chapter6/angular/", 
            "text": "Angular Applications\n\n\nFor much of the last couple of years, \nAngular\n has been the JavaScript framework of choice For front-end developers.  Developed by Google, it is fully-featured, albeit complex, but with a solid community of developers willing to help, and the support for learning through \ntutorials\n, \nvideos\n, and \nblogs\n.  The most recent iteration, Angular 2, is taking off as a great framework as well.\n\n\nLearning a new framework is time consuming, but the outcomes can be remarkable.  In this section, I'm going to adjust a single class in the \nAngular version\n of \nToDoMVC\n so that it works with the Azure Mobile Apps JavaScript SDK.  You can find the full source code in the \nChapter6\n project on the books\nGitHub page.\n\n\nAngular in ASP.NET MVC\n\n\nBefore we get started, let's get the default ToDoMVC application running in our ASP.NET MVC framework.\n\n\nAdd a Controller Method and a View\n\n\nEdit the \nControllers\\SPAController.cs\n and add the following method:\n\n\npublic ActionResult Angular()\n{\n    return View();\n}\n\n\n\n\nAlso, add the following in \nViews\\SPA\\Angular.cshtml\n:\n\n\n@{\n    Layout = null;\n}\n\n\n!doctype html\n\n\nhtml lang=\nen\n data-framework=\nangularjs\n\n\nhead\n\n    \nmeta charset=\nutf-8\n\n    \ntitle\nAngularJS \u2022 TodoMVC\n/title\n\n    \nlink rel=\nstylesheet\n href=\n~/Content/spa/todomvc/base.css\n\n    \nlink rel=\nstylesheet\n href=\n~/Content/spa/todomvc/index.css\n\n    \nstyle\n\n        [ng-cloak] {\n            display: none;\n        }\n    \n/style\n\n\n/head\n\n\nbody ng-app=\ntodomvc\n\n    \nng-view /\n\n\n    \nscript type=\ntext/ng-template\n id=\ntodomvc-index.html\n\n        \nsection id=\ntodoapp\n\n            \nheader id=\nheader\n\n                \nh1\ntodos\n/h1\n\n                \nform id=\ntodo-form\n ng-submit=\naddTodo()\n\n                    \ninput id=\nnew-todo\n placeholder=\nWhat needs to be done?\n ng-model=\nnewTodo\n ng-disabled=\nsaving\n autofocus\n\n                \n/form\n\n            \n/header\n\n            \nsection id=\nmain\n ng-show=\ntodos.length\n ng-cloak\n\n                \ninput id=\ntoggle-all\n type=\ncheckbox\n ng-model=\nallChecked\n ng-click=\nmarkAll(allChecked)\n\n                \nlabel for=\ntoggle-all\nMark all as complete\n/label\n\n                \nul id=\ntodo-list\n\n                    \nli ng-repeat=\ntodo in todos | filter:statusFilter track by $index\n ng-class=\n{completed: todo.completed, editing: todo == editedTodo}\n\n                        \ndiv class=\nview\n\n                            \ninput class=\ntoggle\n type=\ncheckbox\n ng-model=\ntodo.completed\n ng-change=\ntoggleCompleted(todo)\n\n                            \nlabel ng-dblclick=\neditTodo(todo)\n{{todo.title}}\n/label\n\n                            \nbutton class=\ndestroy\n ng-click=\nremoveTodo(todo)\n/button\n\n                        \n/div\n\n                        \nform ng-submit=\nsaveEdits(todo, 'submit')\n\n                            \ninput class=\nedit\n ng-trim=\nfalse\n ng-model=\ntodo.title\n todo-escape=\nrevertEdits(todo)\n ng-blur=\nsaveEdits(todo, 'blur')\n todo-focus=\ntodo == editedTodo\n\n                        \n/form\n\n                    \n/li\n\n                \n/ul\n\n            \n/section\n\n            \nfooter id=\nfooter\n ng-show=\ntodos.length\n ng-cloak\n\n                \nspan id=\ntodo-count\n\n                    \nstrong\n{{remainingCount}}\n/strong\n\n                    \nng-pluralize count=\nremainingCount\n when=\n{ one: 'item left', other: 'items left' }\n/ng-pluralize\n\n                \n/span\n\n                \nul id=\nfilters\n\n                    \nli\n\n                        \na ng-class=\n{selected: status == ''} \n href=\n#/\nAll\n/a\n\n                    \n/li\n\n                    \nli\n\n                        \na ng-class=\n{selected: status == 'active'}\n href=\n#/active\nActive\n/a\n\n                    \n/li\n\n                    \nli\n\n                        \na ng-class=\n{selected: status == 'completed'}\n href=\n#/completed\nCompleted\n/a\n\n                    \n/li\n\n                \n/ul\n\n                \nbutton id=\nclear-completed\n ng-click=\nclearCompletedTodos()\n ng-show=\ncompletedCount\nClear completed\n/button\n\n            \n/footer\n\n        \n/section\n\n        \nfooter id=\ninfo\n\n            \np\nDouble-click to edit a todo\n/p\n\n            \np\n\n                Credits:\n                \na href=\nhttp://twitter.com/cburgdorf\nChristoph Burgdorf\n/a\n,\n                \na href=\nhttp://ericbidelman.com\nEric Bidelman\n/a\n,\n                \na href=\nhttp://jacobmumm.com\nJacob Mumm\n/a\n and\n                \na href=\nhttp://blog.igorminar.com\nIgor Minar\n/a\n\n            \n/p\n\n            \np\nAdjusted for Azure Mobile Apps by \na href=\nhttps://github.com/adrianhall\nAdrian Hall\n/a\n.\n/p\n\n            \np\nPart of \na href=\nhttp://todomvc.com\nTodoMVC\n/a\n/p\n\n        \n/footer\n\n    \n/script\n\n    \nscript src=\n~/Content/spa/todomvc/base.js\n/script\n\n    \nscript src=\nhttps://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular.min.js\n/script\n\n    \nscript src=\nhttps://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-route.min.js\n/script\n\n    \nscript src=\nhttps://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-resource.min.js\n/script\n\n    \nscript src=\n~/Content/spa/angular/app.js\n/script\n\n    \nscript src=\n~/Content/spa/angular/controllers/todoCtrl.js\n/script\n\n    \nscript src=\n~/Content/spa/angular/services/todoStorage.js\n/script\n\n    \nscript src=\n~/Content/spa/angular/directives/todoFocus.js\n/script\n\n    \nscript src=\n~/Content/spa/angular/directives/todoEscape.js\n/script\n\n\n/body\n\n\n/html\n\n\n\n\n\nI haven't done much to these except adjust the script links to resolve to the JavaScript CDN.  In addition, there are some basic CSS/JS libraries that all the ToDoMVC applications use.  I've copied those from the ToDoMVC site into my project in \n~/Content/spa/todomvc\n.\n\n\nCopy the ToDoMVC application into Content\n\n\nI've created a directory \n~/Content/spa/angular\n with a direct copy of the \nAngularJS\n application.  Everything under the \njs\n directory has been copied, preserving the directory structure. At this point, you can publish your application and you will see the original ToDoMVC application prior to making it work with Azure Mobile Apps.\n\n\nCloud Connectivity\n\n\nThe logic for the storage of the data behind the task list all happens in \nservices\\todoStorage.js\n, and our changes are all limited to that file.  In this case, we will have a local cache of the data.  This local cache is read at the beginning of the application.  When the user wants to make a change to the data, we modify the data locally and remotely at the same time.\n\n\nWe have a small complexity - the model used by ToDoMVC does not match the model on the backend.  As a result, we need to do conversions between the two models when we perform backend operations.  This is surprisingly common, especially when using backend databases that you do not control.\n\n\nLet's start with the basics.  Here is the recipe for the promise-based Angular factory, with our Azure Mobile Apps initializer embedded:\n\n\n/*global angular */\n\n/**\n * Services that persists and retrieves todos from localStorage or a backend API\n * if available.\n *\n * They both follow the same API, returning promises for all changes to the\n * model.\n */\nangular.module('todomvc')\n    .factory('todoStorage', function ($q) {\n        var store = {\n            todos: [],\n            client: null,\n            table: null,\n\n            // Additional methods Here\n            get: function() {\n\n            },\n\n            delete: function(todo) {\n\n            },\n\n            insert: function(todo) {\n\n            },\n\n            put: function(todo, index) {\n\n            },\n\n            clearCompleted: function() {\n\n            }\n        };\n\n        var deferred = $q.defer();\n\n        store.client = new WindowsAzure.MobileServiceClient(location.origin);\n        store.table = store.client.getTable('todoitem');\n\n        deferred.resolve(store);\n        return deferred.promise;\n    });\n\n\n\n\nSince our application is doing all the filtering client-side, we are going to cache the table data in the store.todos variable.  I've created the API that the ToDoMVC application expects, but with empty contents.  Each method is expected to return a promise that resolves to the new list of todo items.\n\n\nGetting the data is easier than the jQuery version as we don't have to deal with filtering:\n\n\n    get: function () {\n        var deferred = $q.defer();\n\n        store.table.read().then(function (items) {\n            // Convert the items into todos for this application\n            var todoList = items.map(function (item) {\n                return {\n                    id: item.id,\n                    completed: item.complete,\n                    title: item.text\n                };\n            });\n\n            angular.copy(todoList, store.todos);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n\n\n\nAngular comes with a A+/Promise library that is referenced in a similar way to either the regular Promise API or like the jQuery deferred API.  Here, I am creating a promise, then doing the work, resolving the promise when the work is complete.  The \nArray.map()\n method in JavaScript is great for doing the work of converting one model to a new shape.\n\n\nDeleting, Updating and Inserting are all very similar to the jQuery version.  Since we are maintaing a cache, we don't resolve the promise we need until the server comes back with the new data:\n\n\n    delete: function (todo) {\n        var deferred = $q.defer();\n\n        store.table.del({ id: todo.id }).done(function () {\n            store.todos.splice(store.todos.indexOf(todo), 1);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n    insert: function (todo) {\n        var deferred = $q.defer();\n\n        store.table.insert({ text: todo.title }).then(function (newItem) {\n            todo.id = newItem.id;\n            todo.title = newItem.text;\n            todo.completed = newItem.complete;\n\n            store.todos.push(todo);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n    put: function (todo, index) {\n        var deferred = $q.defer();\n\n        store.table.update({ id: todo.id, text: todo.title, complete: todo.completed })\n            .then(function (item) {\n                todo.title = item.text;\n                todo.completed = item.complete;\n                store.todos[index] = todo;\n                deferred.resolve(store.todos);\n            });\n\n        return deferred.promise;\n    }\n\n\n\n\nThe major reason for not updating the cache directly is that we need the ID of the new record. That ID is created on the server for us.  We could, as an improvement, include the \nuuid\n package and generate a GUID on the client, storing that instead.\n\n\nFinally, there is a method for clearing (aka deleting) the completed records.  This is difficult primarily because the server only handles one record at a time.  The Angular promise library has an API for that called \n.all()\n.  This method is given an array of promises and waits for all of them to be resolved.  We can use this as follows:\n\n\n    clearCompleted: function () {\n        var deferred = $q.defer();\n\n        var promises = [];\n\n        var completeTodos = store.todos.filter(function (todo) { return todo.completed; });\n        completeTodo.forEach(function (todo) {\n            promises.push(store.table.del({ id: todo.id }));\n        });\n\n        $q.all(promises).then(function () {\n            var incompleteTodos = store.todos.filter(function (todo) {\n                return !todo.completed;\n            });\n\n            angular.copy(incompleteTodos, store.todos);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n\n\n\nWe spend our initial time creating a promise for each record to be deleted.  That promise resolves when the record is deleted.  Once all the records have been deleted, we filter the cache similarly.\n\n\nAngular Gotchas\n\n\nThe main problem I see over and over is that the \nWindowsAzure.MobileServiceClient\n class is not available when it is used.  By default, Angular waits for the DOMContentLoaded event, which signals that all the scripts have been loaded.  Inevitably, when I look at the failing code, the call to initialize the MobileServiceClient is called outside of a factory.\n\n\nIf you place the \nnew WindowsAzure.MobileServiceClient()\n call inside of a service or factory, then you fix the two major problems.  Firstly, the MobileServiceClient class will be available when called.  Secondly, you instantiate a singleton copy of the MobileServiceClient, which is exactly what is required by the SDK.\n\n\nAuthentication\n\n\nThe Azure Mobile Apps JavaScript SDK includes a call \nclient.login('provider')\n for server-flow authentication and a similar functionality for client-flow authentication.  If you have configured your authentication service in the Azure Portal properly, then calling the \n.login()\n method will pop up a small window to complete the normal authentication flow.  The token is then stored inside the MobileServiceClient object.\n\n\nWhen using authentication this way, it is vital that you have a singleton model for your MobileServiceClient. In this case, I would break down the backend connectivity into three or more distinct services - one for the client connection, one for authenticating users, and one for each table controller you wish to expose.", 
            "title": "Angular Applications"
        }, 
        {
            "location": "/chapter6/angular/#angular-applications", 
            "text": "For much of the last couple of years,  Angular  has been the JavaScript framework of choice For front-end developers.  Developed by Google, it is fully-featured, albeit complex, but with a solid community of developers willing to help, and the support for learning through  tutorials ,  videos , and  blogs .  The most recent iteration, Angular 2, is taking off as a great framework as well.  Learning a new framework is time consuming, but the outcomes can be remarkable.  In this section, I'm going to adjust a single class in the  Angular version  of  ToDoMVC  so that it works with the Azure Mobile Apps JavaScript SDK.  You can find the full source code in the  Chapter6  project on the books\nGitHub page.", 
            "title": "Angular Applications"
        }, 
        {
            "location": "/chapter6/angular/#angular-in-aspnet-mvc", 
            "text": "Before we get started, let's get the default ToDoMVC application running in our ASP.NET MVC framework.", 
            "title": "Angular in ASP.NET MVC"
        }, 
        {
            "location": "/chapter6/angular/#add-a-controller-method-and-a-view", 
            "text": "Edit the  Controllers\\SPAController.cs  and add the following method:  public ActionResult Angular()\n{\n    return View();\n}  Also, add the following in  Views\\SPA\\Angular.cshtml :  @{\n    Layout = null;\n} !doctype html  html lang= en  data-framework= angularjs  head \n     meta charset= utf-8 \n     title AngularJS \u2022 TodoMVC /title \n     link rel= stylesheet  href= ~/Content/spa/todomvc/base.css \n     link rel= stylesheet  href= ~/Content/spa/todomvc/index.css \n     style \n        [ng-cloak] {\n            display: none;\n        }\n     /style  /head  body ng-app= todomvc \n     ng-view / \n\n     script type= text/ng-template  id= todomvc-index.html \n         section id= todoapp \n             header id= header \n                 h1 todos /h1 \n                 form id= todo-form  ng-submit= addTodo() \n                     input id= new-todo  placeholder= What needs to be done?  ng-model= newTodo  ng-disabled= saving  autofocus \n                 /form \n             /header \n             section id= main  ng-show= todos.length  ng-cloak \n                 input id= toggle-all  type= checkbox  ng-model= allChecked  ng-click= markAll(allChecked) \n                 label for= toggle-all Mark all as complete /label \n                 ul id= todo-list \n                     li ng-repeat= todo in todos | filter:statusFilter track by $index  ng-class= {completed: todo.completed, editing: todo == editedTodo} \n                         div class= view \n                             input class= toggle  type= checkbox  ng-model= todo.completed  ng-change= toggleCompleted(todo) \n                             label ng-dblclick= editTodo(todo) {{todo.title}} /label \n                             button class= destroy  ng-click= removeTodo(todo) /button \n                         /div \n                         form ng-submit= saveEdits(todo, 'submit') \n                             input class= edit  ng-trim= false  ng-model= todo.title  todo-escape= revertEdits(todo)  ng-blur= saveEdits(todo, 'blur')  todo-focus= todo == editedTodo \n                         /form \n                     /li \n                 /ul \n             /section \n             footer id= footer  ng-show= todos.length  ng-cloak \n                 span id= todo-count \n                     strong {{remainingCount}} /strong \n                     ng-pluralize count= remainingCount  when= { one: 'item left', other: 'items left' } /ng-pluralize \n                 /span \n                 ul id= filters \n                     li \n                         a ng-class= {selected: status == ''}   href= #/ All /a \n                     /li \n                     li \n                         a ng-class= {selected: status == 'active'}  href= #/active Active /a \n                     /li \n                     li \n                         a ng-class= {selected: status == 'completed'}  href= #/completed Completed /a \n                     /li \n                 /ul \n                 button id= clear-completed  ng-click= clearCompletedTodos()  ng-show= completedCount Clear completed /button \n             /footer \n         /section \n         footer id= info \n             p Double-click to edit a todo /p \n             p \n                Credits:\n                 a href= http://twitter.com/cburgdorf Christoph Burgdorf /a ,\n                 a href= http://ericbidelman.com Eric Bidelman /a ,\n                 a href= http://jacobmumm.com Jacob Mumm /a  and\n                 a href= http://blog.igorminar.com Igor Minar /a \n             /p \n             p Adjusted for Azure Mobile Apps by  a href= https://github.com/adrianhall Adrian Hall /a . /p \n             p Part of  a href= http://todomvc.com TodoMVC /a /p \n         /footer \n     /script \n     script src= ~/Content/spa/todomvc/base.js /script \n     script src= https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular.min.js /script \n     script src= https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-route.min.js /script \n     script src= https://cdnjs.cloudflare.com/ajax/libs/angular.js/1.4.14/angular-resource.min.js /script \n     script src= ~/Content/spa/angular/app.js /script \n     script src= ~/Content/spa/angular/controllers/todoCtrl.js /script \n     script src= ~/Content/spa/angular/services/todoStorage.js /script \n     script src= ~/Content/spa/angular/directives/todoFocus.js /script \n     script src= ~/Content/spa/angular/directives/todoEscape.js /script  /body  /html   I haven't done much to these except adjust the script links to resolve to the JavaScript CDN.  In addition, there are some basic CSS/JS libraries that all the ToDoMVC applications use.  I've copied those from the ToDoMVC site into my project in  ~/Content/spa/todomvc .", 
            "title": "Add a Controller Method and a View"
        }, 
        {
            "location": "/chapter6/angular/#copy-the-todomvc-application-into-content", 
            "text": "I've created a directory  ~/Content/spa/angular  with a direct copy of the  AngularJS  application.  Everything under the  js  directory has been copied, preserving the directory structure. At this point, you can publish your application and you will see the original ToDoMVC application prior to making it work with Azure Mobile Apps.", 
            "title": "Copy the ToDoMVC application into Content"
        }, 
        {
            "location": "/chapter6/angular/#cloud-connectivity", 
            "text": "The logic for the storage of the data behind the task list all happens in  services\\todoStorage.js , and our changes are all limited to that file.  In this case, we will have a local cache of the data.  This local cache is read at the beginning of the application.  When the user wants to make a change to the data, we modify the data locally and remotely at the same time.  We have a small complexity - the model used by ToDoMVC does not match the model on the backend.  As a result, we need to do conversions between the two models when we perform backend operations.  This is surprisingly common, especially when using backend databases that you do not control.  Let's start with the basics.  Here is the recipe for the promise-based Angular factory, with our Azure Mobile Apps initializer embedded:  /*global angular */\n\n/**\n * Services that persists and retrieves todos from localStorage or a backend API\n * if available.\n *\n * They both follow the same API, returning promises for all changes to the\n * model.\n */\nangular.module('todomvc')\n    .factory('todoStorage', function ($q) {\n        var store = {\n            todos: [],\n            client: null,\n            table: null,\n\n            // Additional methods Here\n            get: function() {\n\n            },\n\n            delete: function(todo) {\n\n            },\n\n            insert: function(todo) {\n\n            },\n\n            put: function(todo, index) {\n\n            },\n\n            clearCompleted: function() {\n\n            }\n        };\n\n        var deferred = $q.defer();\n\n        store.client = new WindowsAzure.MobileServiceClient(location.origin);\n        store.table = store.client.getTable('todoitem');\n\n        deferred.resolve(store);\n        return deferred.promise;\n    });  Since our application is doing all the filtering client-side, we are going to cache the table data in the store.todos variable.  I've created the API that the ToDoMVC application expects, but with empty contents.  Each method is expected to return a promise that resolves to the new list of todo items.  Getting the data is easier than the jQuery version as we don't have to deal with filtering:      get: function () {\n        var deferred = $q.defer();\n\n        store.table.read().then(function (items) {\n            // Convert the items into todos for this application\n            var todoList = items.map(function (item) {\n                return {\n                    id: item.id,\n                    completed: item.complete,\n                    title: item.text\n                };\n            });\n\n            angular.copy(todoList, store.todos);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },  Angular comes with a A+/Promise library that is referenced in a similar way to either the regular Promise API or like the jQuery deferred API.  Here, I am creating a promise, then doing the work, resolving the promise when the work is complete.  The  Array.map()  method in JavaScript is great for doing the work of converting one model to a new shape.  Deleting, Updating and Inserting are all very similar to the jQuery version.  Since we are maintaing a cache, we don't resolve the promise we need until the server comes back with the new data:      delete: function (todo) {\n        var deferred = $q.defer();\n\n        store.table.del({ id: todo.id }).done(function () {\n            store.todos.splice(store.todos.indexOf(todo), 1);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n    insert: function (todo) {\n        var deferred = $q.defer();\n\n        store.table.insert({ text: todo.title }).then(function (newItem) {\n            todo.id = newItem.id;\n            todo.title = newItem.text;\n            todo.completed = newItem.complete;\n\n            store.todos.push(todo);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },\n\n    put: function (todo, index) {\n        var deferred = $q.defer();\n\n        store.table.update({ id: todo.id, text: todo.title, complete: todo.completed })\n            .then(function (item) {\n                todo.title = item.text;\n                todo.completed = item.complete;\n                store.todos[index] = todo;\n                deferred.resolve(store.todos);\n            });\n\n        return deferred.promise;\n    }  The major reason for not updating the cache directly is that we need the ID of the new record. That ID is created on the server for us.  We could, as an improvement, include the  uuid  package and generate a GUID on the client, storing that instead.  Finally, there is a method for clearing (aka deleting) the completed records.  This is difficult primarily because the server only handles one record at a time.  The Angular promise library has an API for that called  .all() .  This method is given an array of promises and waits for all of them to be resolved.  We can use this as follows:      clearCompleted: function () {\n        var deferred = $q.defer();\n\n        var promises = [];\n\n        var completeTodos = store.todos.filter(function (todo) { return todo.completed; });\n        completeTodo.forEach(function (todo) {\n            promises.push(store.table.del({ id: todo.id }));\n        });\n\n        $q.all(promises).then(function () {\n            var incompleteTodos = store.todos.filter(function (todo) {\n                return !todo.completed;\n            });\n\n            angular.copy(incompleteTodos, store.todos);\n            deferred.resolve(store.todos);\n        });\n\n        return deferred.promise;\n    },  We spend our initial time creating a promise for each record to be deleted.  That promise resolves when the record is deleted.  Once all the records have been deleted, we filter the cache similarly.", 
            "title": "Cloud Connectivity"
        }, 
        {
            "location": "/chapter6/angular/#angular-gotchas", 
            "text": "The main problem I see over and over is that the  WindowsAzure.MobileServiceClient  class is not available when it is used.  By default, Angular waits for the DOMContentLoaded event, which signals that all the scripts have been loaded.  Inevitably, when I look at the failing code, the call to initialize the MobileServiceClient is called outside of a factory.  If you place the  new WindowsAzure.MobileServiceClient()  call inside of a service or factory, then you fix the two major problems.  Firstly, the MobileServiceClient class will be available when called.  Secondly, you instantiate a singleton copy of the MobileServiceClient, which is exactly what is required by the SDK.", 
            "title": "Angular Gotchas"
        }, 
        {
            "location": "/chapter6/angular/#authentication", 
            "text": "The Azure Mobile Apps JavaScript SDK includes a call  client.login('provider')  for server-flow authentication and a similar functionality for client-flow authentication.  If you have configured your authentication service in the Azure Portal properly, then calling the  .login()  method will pop up a small window to complete the normal authentication flow.  The token is then stored inside the MobileServiceClient object.  When using authentication this way, it is vital that you have a singleton model for your MobileServiceClient. In this case, I would break down the backend connectivity into three or more distinct services - one for the client connection, one for authenticating users, and one for each table controller you wish to expose.", 
            "title": "Authentication"
        }, 
        {
            "location": "/chapter6/react/", 
            "text": "React Applications\n\n\nThere have been two major shifts in the way that JavaScript applications are built in the past few years.  The first is in the user of transpilers for compiling another variant of JavaScript into something that a browser can use.  There are languages that add whole new feature sets to the language, like \nCoffeeScript\n, \nTypeScript\n and \nDart\n, plus others that extend JavaScript to include unsupported features of the JavaScript language early, like \nBabelJS\n which provides access to the latest specifications for JavaScript on the current set of browsers.  The process of converting one of these languages into more normal JavaScript is called transpiling.  The resultant code is rarely readable.\n\n\nThe other major shift is in the use of bundlers.  In prior iterations of the JavaScript language, there was no module system.  Developers included a number of \nscript\n tags to load the code.  As JavaScript became more of a development platform, the size of the code grew and the need for a module system also grew.  There are three in general use - \nRequireJS\n, \nCommonJS\n and \nES2015 modules\n.  In general, developers will use a bundler like \nBrowserify\n or \nWebpack\n to bundle their application source together so that it loads faster and they can take care of alternate module specifications like CommonJS and ES2015 modules.  (RequireJS doesn't require bundling).\n\n\nThis brings us to \nReact\n applications.  React is a newcomer to the JavaScript framework scene.  It provides a component model for your application.  It is generally paired with a \nFlux\n state manager like \nRedux\n.  Thus, most React applications can more properly be called React/Redux applications.  While React can be used with just straight JavaScript and script tags, it is more normal to use ES2015 syntax and a bundler like Webpack to distribute the application.\n\n\nSetting Up\n\n\nIn this example, we are going to walk through the set up of an application that uses Webpack and the BabelJS transpiler to build our web application into our MVC framework.\n\n\nMVC Controller \n View\n\n\nAs with the jQuery and Angular examples, we need a controller (within the \nControllers\\SPAController.cs\n file):\n\n\n    public ActionResult React()\n    {\n        return View();\n    }\n\n\n\n\nWe also need a suitable View.  This is similar in construction to the Angular view, but it requires some different elements, as is normal when you change a JavaScript framework.  This is \nViews\\SPA\\React.cshtml\n:\n\n\n@{\n    Layout = null;\n}\n\n\n!DOCTYPE html\n\n\nhtml\n\n\nhead\n\n    \nmeta charset=\nutf-8\n\n    \nmeta name=\nviewport\n content=\nwidth=device-width, initial-scale=1\n\n    \ntitle\nAzure Mobile Apps Quickstart\n/title\n\n    \nlink rel=\nstylesheet\n href=\n~/Content/spa/react/app.css\n /\n\n\n/head\n\n\nbody\n\n    \ndiv id=\napp\n/div\n\n\n    \nscript src=\n~/Content/spa/react/vendor.bundle.js\n/script\n\n    \nscript src=\n~/Content/spa/react/app.js\n/script\n\n\n/body\n\n\n/html\n\n\n\n\n\nThis view is much simpler than the Angular view because none of the markup for the actual application is contained within the view.  All markup is done by the React components in our code.  There are three files referenced in this file that do not yet exist.  The JavaScript files will be created by the Webpack bundler, and the \napp.css\n file is the \nregular CSS file\n from the normal Quickstart application.\n\n\nSetting up a Webpack project\n\n\nThere is some work to do in order to set up a reasonable Webpack-enabled project.  Let's first of all set up a basic React application in a new directory called \nReactClient\n.  Here is the \nReactClient\\app.jsx\n file that will serve as our entry-point into the application:\n\n\nimport React from 'react';\nimport ReactDOM from 'react-dom';\n\nReactDOM.render(\n    \nh1\nIn a React Application\n/h1\n\n, document.getElementById('app')\n);\n\n\n\n\nOnce we have the build correctly done, the application will insert the \nh1\nIn a React Application\n/h1\n inside the \ndiv id=\"app\"\n/div\n from the main page.  This is the most basic of React applications, but we can already see some things we may not have seen before.  The \nimport\n statements bring in ES2015 modules for use.  Also, you can embed pseudo-HTML into a React application rendering method by using \nJSX\n.  When the application is transpiled with BabelJS, this pseudo-HTML get compiled into normal JavaScript.\n\n\nThere are three other files we want to produce that are build related.  The first is the \n.babelrc\n file.  This contains instructions to the BabelJS transpiler that tells it what specifications need to be transpiled:\n\n\n{\n    \npresets\n: [ \nes2015\n, \nreact\n ],\n    \nplugins\n: [\n        \ntransform-class-properties\n,\n        \ntransform-object-rest-spread\n\n    ]\n}\n\n\n\n\nOur application will be written in ES2015 syntax with React extensions (those are presets) and I write with a couple of specifications that may or may not make it into the next JavaScript specification, but are useful - \nClass Properties\n and \nObject Rest Spread\n.  Although these are considered \"experimental\", they make your life much easier as a developer.\n\n\nThe second file is \nwebpack.config.js\n.  This is a JavaScript file that tells the Webpack system how to build the application.  Here is a suitable file for this application:\n\n\n'use strict';\n/* global __dirname */\n\nvar path = require('path'),\n    webpack = require('webpack');\n\nvar configuration = {\n    devtool: 'source-map',\n    entry: {\n        app: [\n            path.join(__dirname, 'ReactClient/app.jsx')\n        ],\n        vendor: [\n            'react',\n            'react-dom'\n        ]\n    },\n    module: {\n        loaders: [\n            { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ }\n        ]\n    },\n    output: {\n        path: path.join(__dirname, 'Content/spa/react'),\n        publicPath: '/',\n        filename: 'app.js'\n    },\n    plugins: [\n        new webpack.optimize.CommonsChunkPlugin('vendor', 'vendor.bundle.js'),\n        new webpack.optimize.DedupePlugin()\n    ],\n    resolve: {\n        modulesDirectories: [ 'node_modules' ],\n        extensions: [ '', '.js', '.jsx' ]\n    },\n    target: 'web'\n};\n\nmodule.exports = configuration;\n\n\n\n\nThere are three areas of the configuration object that are interesting.  The \nentry\n block contains the entry-point for the application, but it also contains a vendor array.  This array is the list of modules that will be placed into the \nvendor.bundle.js\n file.  When we add a new external module, we need to add it to the list so that it is included in the bundle.  The \nmodule\n section contains instructions on how to transpile the source files.  The single entry tells Webpack to run all JavaScript and JSX files through the BabelJS transpiler.  Finally, the \noutput\n section tells Webpack where to store the output files.\n\n\nOur final file is \npackage.json\n.  This is a fairly standard file for loading modules from the NPM repository.  You can create one using \nnpm init --yes\n, then use \nnpm install modulename --save\n to install the individual modules.  Alternatively, you can create the \npackage.json\n file and then run \nnpm install\n to install the modules that are named in the file.\n\n\n{\n  \nversion\n: \n0.0.1\n,\n  \nname\n: \nreact-quickstart\n,\n  \nprivate\n: true,\n  \nmain\n: \nReactClient/app.jsx\n,\n  \nscripts\n: {\n    \nbuild\n: \nwebpack -p\n\n  },\n  \ndevDependencies\n: {},\n  \ndependencies\n: {\n    \nbabel-core\n: \n^6.21.0\n,\n    \nbabel-loader\n: \n^6.2.10\n,\n    \nbabel-plugin-transform-class-properties\n: \n^6.19.0\n,\n    \nbabel-plugin-transform-object-rest-spread\n: \n^6.20.2\n,\n    \nbabel-preset-es2015\n: \n^6.18.0\n,\n    \nbabel-preset-react\n: \n^6.16.0\n,\n    \nreact\n: \n^15.4.1\n,\n    \nreact-dom\n: \n^15.4.1\n,\n    \nwebpack\n: \n^1.14.0\n\n  }\n}\n\n\n\n\nRun \nnpm install\n in the \nBackend\n directory once you have saved this file.  It will download all the modules needed.\n\n\n\n\nVisual Studio Extensions\n\n\nThere are a number of Visual Studio extensions that can assist with this work.  Check out the \nAppendix\n for more details on these.  You should definitely install the \nNPM Task Runner\n extension before continuing though.\n\n\n\n\nBoth \nwebpack.config.js\n and \npackage.json\n should be at the top of the directory tree, in the \nBackend\n folder.  The \n.babeljs\n file should be in the \nReactClient\n directory.\n\n\nYou should exclude any files that are not going to be distributed from the publication process.  This includes all three build files and the contents of the ReactClient directory.  Right-click on each object and select \nProperties...\n.  Set the \nBuild Action\n to \nNone\n and the \nCopy to Output Directory\n to \nDo not copy\n.\n\n\n\n\nAdd node_modules to .gitignore\n\n\nYou should ensure you add the \nnode\\_modules\n directory to your .gitignore file or it will be checked into source code. The \nnode\\_modules\n is normally very large and you can easily regenerate it by running \nnpm install\n.\n\n\n\n\nBuilding the Webpack files automatically\n\n\nIf you have installed the \nNPM Task Runner\n, you can build the entire JavaScript application within Visual Studio.  Open the Task Runner Explorer (\nView\n -\n \nOther Windows\n -\n \nTask Runner Explorer\n).  There will be a \npackage.json\n section with \nDefaults\n and \nCustom\n sections within.  In the \nDefaults\n section, right-click on \ninstall\n and select \nRun\n.  This will install all the npm packages you are missing.\n\n\nThe next step is to do the first build.  Right-click on \nbuild\n in the \nCustom\n section and select \nRun\n.  This will generate the \nvendor.bundle.js\n and \napp.js\n files (plus their map equivalents).\n\n\n\n\nMap Files\n\n\nSince there is no direct correlation between what is running in the browser and what your source code looks like when using transpilers and bundlers, these tools produce map files.  This is a (generally much larger) file that tells the browser what line represents a piece of code in the browser.  They are not distributed (nor required) in a production build.\n\n\n\n\nAs a one-time activity, we need to add the generated files to the project.  In the \nSolution Explorer\n, click on the \nShow All Files\n button (which is to the left of the spanner icon).  Expand \nContent\n -\n \nspa\n -\n \nreact\n and you will see four files which have dotted icons:\n\n\n\n\nRight-click on each file and select \nInclude in Project\n.  Ensure their \nBuild Action\n* is set to \nContent** in the Properties pane.\n\n\nAssuming all went well, you can now set up the build to happen automatically.  Right-click on \nbuild\n in the \nCustom\n section, then select \nBindings\n \n \nBefore Build\n.  This will generate the files on each build (before the build occurs).  If you publish the project as this point, you will be able to browse to \n/SPA/React\n and see the following:\n\n\n\n\nIntegrating Azure Mobile Apps\n\n\nThere are three parts to adding Azure Mobile Apps to a Redux: the store, the actions and the reducer.  The store is a central part of a redux application and you don't need to do anything special.  Here is the \nReactClient/redux/store.js\n code:\n\n\nimport { createStore, combineReducers, applyMiddleware } from 'redux';\nimport createLogger from 'redux-logger';\nimport thunkMiddleware from 'redux-thunk';\nimport promiseMiddleware from 'redux-promise';\n\nimport * as reducers from './reducers';\nimport * as todoActions from './actions/todo';\n\nconst appReducers = combineReducers({ ...reducers });\n\nconst reduxStore = applyMiddleware(\n    thunkMiddleware,\n    promiseMiddleware,\n    createLogger()\n)(createStore);\n\nexport const store = reduxStore(appReducers);\n\n// Dispatch a refresh action\nstore.dispatch(todoActions.refreshTodoItems());\n\n\n\n\nThe only real addition is that we dispatch an action (which initiates a change in the store) when we start up.  This action is for refreshing the content from the mobile backend.  Our reducer converts the actions into changes in the store.  This is the \nReactClient/reducers/todo.js\n:\n\n\nimport constants from '../constants';\n\nconst initialState = {\n    todoItems: [],\n    network: 0,\n    error: \n\n};\n\nexport default function todoReducers(state = initialState, action) {\n    switch (action.type) {\n        case constants.todo.addItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.concat(action.item)\n            });\n\n        case constants.todo.removeItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.filter(item =\n item.id !== action.id)\n            });\n\n        case constants.todo.updateItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.map(item =\n { return (item.id === action.item.id) ? action.item : item; })\n            });\n\n        case constants.todo.replaceItems:\n            return Object.assign({}, state, {\n                todoItems: action.items\n            });\n\n        case constants.todo.network:\n            return Object.assign({}, state, {\n                network: state.network + action.counter\n            });\n\n        case constants.todo.error:\n            return Object.assign({}, state, {\n                error: action.error\n            });\n\n        default:\n            return state;\n    }\n};\n\n\n\n\nThere is no network activity here because all the store changes are being done against a cached copy of the data.  You should not have to go to the cloud to fetch the data, except when refreshing the cache.  All the actual network activity happens inside the \nReactClient/actions/todo.js\n:\n\n\nimport * as WindowsAzure from 'azure-mobile-apps-client';\nimport constants from '../constants';\n\nconst ZUMOAPPURL = location.origin;\n\nconst zumoClient = new WindowsAzure.MobileServiceClient(ZUMOAPPURL);\nconst todoTable = zumoClient.getTable('todoitem');\n\nfunction networkProcess(counter) {\n    return {\n        type: constants.todo.network,\n        counter: counter\n    };\n}\n\nfunction zumoError(error) {\n    return {\n        type: constants.todo.error,\n        error: error\n    };\n}\n\nexport function addTodoItem(item) {\n    return (dispatch) =\n {\n        dispatch({\n            type: constants.todo.addItem,\n            item: item\n        });\n\n        const success = (item) =\n {\n            dispatch({\n                type: constants.todo.updateItem,\n                item: item\n            });\n        };\n        const failure = (error) =\n { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.insert(item).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n}\n\nexport function removeTodoItem(id) {\n    return (dispatch) =\n {\n        dispatch({\n            type: constants.todo.removeItem,\n            id: id\n        });\n\n        const success = () =\n {};\n        const failure = (error) =\n { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.del({ id: id }).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n}\n\nexport function updateTodoItem(item) {\n    return (dispatch) =\n {\n        dispatch({\n            type: constants.todo.updateItem,\n            item: item\n        });\n\n        const success = (item) =\n {\n            dispatch({\n                type: constants.todo.updateItem,\n                item: item\n            });\n        };\n        const failure = (error) =\n { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.update(item).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n\n}\n\nexport function refreshTodoItems() {\n    return (dispatch) =\n {\n        const success = (data) =\n {\n            dispatch({\n                type: constants.todo.replaceItems,\n                items: data\n            });\n        };\n        const failure = (error) =\n { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.read().then(success, failure);\n        dispatch(networkProcess(-1));\n    }\n}\n\n\n\n\nEach action that is called by the client code kicks off a call into the standard Azure Mobile Apps JavaScript SDK to do the relevant change in the mobile backend.  When that returns, another action is dispatched to make the change in the store via the reducer.  In addition, we keep track of how many network requests are happening and we also handle errors via the \nzumoError()\n action.\n\n\nI have not included all the code in this chapter as much of it is fairly standard React/Redux, but it is available on \nthe GitHub repository\n for the book.  Remember that you will have to update the \npackage.json\n file and run \nnpm install\n to install the additional libraries.\n\n\nUpdating Webpack for Azure Mobile Apps JavaScript SDK\n\n\nThe Azure Mobile Apps JavaScript SDK requires an additional loader for Webpack to handle it.  Install the \njson-loader\n module using \nnpm install --save json-loader\n, then add the following to the \nloaders\n section of the \nwebpack.config.js\n:\n\n\n    module: {\n        loaders: [\n            { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ },\n            { test: /\\.json$/, loader: 'json' }\n        ]\n    },\n\n\n\n\nOnce the additional line is added, Webpack will handle the \nazure-mobile-apps-client\n without problems.\n\n\nThere is a lot more to be said about the capabilities of React, Redux, BabelJS and Webpack that is beyond the scope of this book.  I hope you will take the opportunity to try the vibrant JavaScript community out as you build single page applications.", 
            "title": "React Applications"
        }, 
        {
            "location": "/chapter6/react/#react-applications", 
            "text": "There have been two major shifts in the way that JavaScript applications are built in the past few years.  The first is in the user of transpilers for compiling another variant of JavaScript into something that a browser can use.  There are languages that add whole new feature sets to the language, like  CoffeeScript ,  TypeScript  and  Dart , plus others that extend JavaScript to include unsupported features of the JavaScript language early, like  BabelJS  which provides access to the latest specifications for JavaScript on the current set of browsers.  The process of converting one of these languages into more normal JavaScript is called transpiling.  The resultant code is rarely readable.  The other major shift is in the use of bundlers.  In prior iterations of the JavaScript language, there was no module system.  Developers included a number of  script  tags to load the code.  As JavaScript became more of a development platform, the size of the code grew and the need for a module system also grew.  There are three in general use -  RequireJS ,  CommonJS  and  ES2015 modules .  In general, developers will use a bundler like  Browserify  or  Webpack  to bundle their application source together so that it loads faster and they can take care of alternate module specifications like CommonJS and ES2015 modules.  (RequireJS doesn't require bundling).  This brings us to  React  applications.  React is a newcomer to the JavaScript framework scene.  It provides a component model for your application.  It is generally paired with a  Flux  state manager like  Redux .  Thus, most React applications can more properly be called React/Redux applications.  While React can be used with just straight JavaScript and script tags, it is more normal to use ES2015 syntax and a bundler like Webpack to distribute the application.", 
            "title": "React Applications"
        }, 
        {
            "location": "/chapter6/react/#setting-up", 
            "text": "In this example, we are going to walk through the set up of an application that uses Webpack and the BabelJS transpiler to build our web application into our MVC framework.", 
            "title": "Setting Up"
        }, 
        {
            "location": "/chapter6/react/#mvc-controller-view", 
            "text": "As with the jQuery and Angular examples, we need a controller (within the  Controllers\\SPAController.cs  file):      public ActionResult React()\n    {\n        return View();\n    }  We also need a suitable View.  This is similar in construction to the Angular view, but it requires some different elements, as is normal when you change a JavaScript framework.  This is  Views\\SPA\\React.cshtml :  @{\n    Layout = null;\n} !DOCTYPE html  html  head \n     meta charset= utf-8 \n     meta name= viewport  content= width=device-width, initial-scale=1 \n     title Azure Mobile Apps Quickstart /title \n     link rel= stylesheet  href= ~/Content/spa/react/app.css  /  /head  body \n     div id= app /div \n\n     script src= ~/Content/spa/react/vendor.bundle.js /script \n     script src= ~/Content/spa/react/app.js /script  /body  /html   This view is much simpler than the Angular view because none of the markup for the actual application is contained within the view.  All markup is done by the React components in our code.  There are three files referenced in this file that do not yet exist.  The JavaScript files will be created by the Webpack bundler, and the  app.css  file is the  regular CSS file  from the normal Quickstart application.", 
            "title": "MVC Controller &amp; View"
        }, 
        {
            "location": "/chapter6/react/#setting-up-a-webpack-project", 
            "text": "There is some work to do in order to set up a reasonable Webpack-enabled project.  Let's first of all set up a basic React application in a new directory called  ReactClient .  Here is the  ReactClient\\app.jsx  file that will serve as our entry-point into the application:  import React from 'react';\nimport ReactDOM from 'react-dom';\n\nReactDOM.render(\n     h1 In a React Application /h1 \n, document.getElementById('app')\n);  Once we have the build correctly done, the application will insert the  h1 In a React Application /h1  inside the  div id=\"app\" /div  from the main page.  This is the most basic of React applications, but we can already see some things we may not have seen before.  The  import  statements bring in ES2015 modules for use.  Also, you can embed pseudo-HTML into a React application rendering method by using  JSX .  When the application is transpiled with BabelJS, this pseudo-HTML get compiled into normal JavaScript.  There are three other files we want to produce that are build related.  The first is the  .babelrc  file.  This contains instructions to the BabelJS transpiler that tells it what specifications need to be transpiled:  {\n     presets : [  es2015 ,  react  ],\n     plugins : [\n         transform-class-properties ,\n         transform-object-rest-spread \n    ]\n}  Our application will be written in ES2015 syntax with React extensions (those are presets) and I write with a couple of specifications that may or may not make it into the next JavaScript specification, but are useful -  Class Properties  and  Object Rest Spread .  Although these are considered \"experimental\", they make your life much easier as a developer.  The second file is  webpack.config.js .  This is a JavaScript file that tells the Webpack system how to build the application.  Here is a suitable file for this application:  'use strict';\n/* global __dirname */\n\nvar path = require('path'),\n    webpack = require('webpack');\n\nvar configuration = {\n    devtool: 'source-map',\n    entry: {\n        app: [\n            path.join(__dirname, 'ReactClient/app.jsx')\n        ],\n        vendor: [\n            'react',\n            'react-dom'\n        ]\n    },\n    module: {\n        loaders: [\n            { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ }\n        ]\n    },\n    output: {\n        path: path.join(__dirname, 'Content/spa/react'),\n        publicPath: '/',\n        filename: 'app.js'\n    },\n    plugins: [\n        new webpack.optimize.CommonsChunkPlugin('vendor', 'vendor.bundle.js'),\n        new webpack.optimize.DedupePlugin()\n    ],\n    resolve: {\n        modulesDirectories: [ 'node_modules' ],\n        extensions: [ '', '.js', '.jsx' ]\n    },\n    target: 'web'\n};\n\nmodule.exports = configuration;  There are three areas of the configuration object that are interesting.  The  entry  block contains the entry-point for the application, but it also contains a vendor array.  This array is the list of modules that will be placed into the  vendor.bundle.js  file.  When we add a new external module, we need to add it to the list so that it is included in the bundle.  The  module  section contains instructions on how to transpile the source files.  The single entry tells Webpack to run all JavaScript and JSX files through the BabelJS transpiler.  Finally, the  output  section tells Webpack where to store the output files.  Our final file is  package.json .  This is a fairly standard file for loading modules from the NPM repository.  You can create one using  npm init --yes , then use  npm install modulename --save  to install the individual modules.  Alternatively, you can create the  package.json  file and then run  npm install  to install the modules that are named in the file.  {\n   version :  0.0.1 ,\n   name :  react-quickstart ,\n   private : true,\n   main :  ReactClient/app.jsx ,\n   scripts : {\n     build :  webpack -p \n  },\n   devDependencies : {},\n   dependencies : {\n     babel-core :  ^6.21.0 ,\n     babel-loader :  ^6.2.10 ,\n     babel-plugin-transform-class-properties :  ^6.19.0 ,\n     babel-plugin-transform-object-rest-spread :  ^6.20.2 ,\n     babel-preset-es2015 :  ^6.18.0 ,\n     babel-preset-react :  ^6.16.0 ,\n     react :  ^15.4.1 ,\n     react-dom :  ^15.4.1 ,\n     webpack :  ^1.14.0 \n  }\n}  Run  npm install  in the  Backend  directory once you have saved this file.  It will download all the modules needed.   Visual Studio Extensions  There are a number of Visual Studio extensions that can assist with this work.  Check out the  Appendix  for more details on these.  You should definitely install the  NPM Task Runner  extension before continuing though.   Both  webpack.config.js  and  package.json  should be at the top of the directory tree, in the  Backend  folder.  The  .babeljs  file should be in the  ReactClient  directory.  You should exclude any files that are not going to be distributed from the publication process.  This includes all three build files and the contents of the ReactClient directory.  Right-click on each object and select  Properties... .  Set the  Build Action  to  None  and the  Copy to Output Directory  to  Do not copy .   Add node_modules to .gitignore  You should ensure you add the  node\\_modules  directory to your .gitignore file or it will be checked into source code. The  node\\_modules  is normally very large and you can easily regenerate it by running  npm install .", 
            "title": "Setting up a Webpack project"
        }, 
        {
            "location": "/chapter6/react/#building-the-webpack-files-automatically", 
            "text": "If you have installed the  NPM Task Runner , you can build the entire JavaScript application within Visual Studio.  Open the Task Runner Explorer ( View  -   Other Windows  -   Task Runner Explorer ).  There will be a  package.json  section with  Defaults  and  Custom  sections within.  In the  Defaults  section, right-click on  install  and select  Run .  This will install all the npm packages you are missing.  The next step is to do the first build.  Right-click on  build  in the  Custom  section and select  Run .  This will generate the  vendor.bundle.js  and  app.js  files (plus their map equivalents).   Map Files  Since there is no direct correlation between what is running in the browser and what your source code looks like when using transpilers and bundlers, these tools produce map files.  This is a (generally much larger) file that tells the browser what line represents a piece of code in the browser.  They are not distributed (nor required) in a production build.   As a one-time activity, we need to add the generated files to the project.  In the  Solution Explorer , click on the  Show All Files  button (which is to the left of the spanner icon).  Expand  Content  -   spa  -   react  and you will see four files which have dotted icons:   Right-click on each file and select  Include in Project .  Ensure their  Build Action * is set to  Content** in the Properties pane.  Assuming all went well, you can now set up the build to happen automatically.  Right-click on  build  in the  Custom  section, then select  Bindings     Before Build .  This will generate the files on each build (before the build occurs).  If you publish the project as this point, you will be able to browse to  /SPA/React  and see the following:", 
            "title": "Building the Webpack files automatically"
        }, 
        {
            "location": "/chapter6/react/#integrating-azure-mobile-apps", 
            "text": "There are three parts to adding Azure Mobile Apps to a Redux: the store, the actions and the reducer.  The store is a central part of a redux application and you don't need to do anything special.  Here is the  ReactClient/redux/store.js  code:  import { createStore, combineReducers, applyMiddleware } from 'redux';\nimport createLogger from 'redux-logger';\nimport thunkMiddleware from 'redux-thunk';\nimport promiseMiddleware from 'redux-promise';\n\nimport * as reducers from './reducers';\nimport * as todoActions from './actions/todo';\n\nconst appReducers = combineReducers({ ...reducers });\n\nconst reduxStore = applyMiddleware(\n    thunkMiddleware,\n    promiseMiddleware,\n    createLogger()\n)(createStore);\n\nexport const store = reduxStore(appReducers);\n\n// Dispatch a refresh action\nstore.dispatch(todoActions.refreshTodoItems());  The only real addition is that we dispatch an action (which initiates a change in the store) when we start up.  This action is for refreshing the content from the mobile backend.  Our reducer converts the actions into changes in the store.  This is the  ReactClient/reducers/todo.js :  import constants from '../constants';\n\nconst initialState = {\n    todoItems: [],\n    network: 0,\n    error:  \n};\n\nexport default function todoReducers(state = initialState, action) {\n    switch (action.type) {\n        case constants.todo.addItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.concat(action.item)\n            });\n\n        case constants.todo.removeItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.filter(item =  item.id !== action.id)\n            });\n\n        case constants.todo.updateItem:\n            return Object.assign({}, state, {\n                todoItems: state.todoItems.map(item =  { return (item.id === action.item.id) ? action.item : item; })\n            });\n\n        case constants.todo.replaceItems:\n            return Object.assign({}, state, {\n                todoItems: action.items\n            });\n\n        case constants.todo.network:\n            return Object.assign({}, state, {\n                network: state.network + action.counter\n            });\n\n        case constants.todo.error:\n            return Object.assign({}, state, {\n                error: action.error\n            });\n\n        default:\n            return state;\n    }\n};  There is no network activity here because all the store changes are being done against a cached copy of the data.  You should not have to go to the cloud to fetch the data, except when refreshing the cache.  All the actual network activity happens inside the  ReactClient/actions/todo.js :  import * as WindowsAzure from 'azure-mobile-apps-client';\nimport constants from '../constants';\n\nconst ZUMOAPPURL = location.origin;\n\nconst zumoClient = new WindowsAzure.MobileServiceClient(ZUMOAPPURL);\nconst todoTable = zumoClient.getTable('todoitem');\n\nfunction networkProcess(counter) {\n    return {\n        type: constants.todo.network,\n        counter: counter\n    };\n}\n\nfunction zumoError(error) {\n    return {\n        type: constants.todo.error,\n        error: error\n    };\n}\n\nexport function addTodoItem(item) {\n    return (dispatch) =  {\n        dispatch({\n            type: constants.todo.addItem,\n            item: item\n        });\n\n        const success = (item) =  {\n            dispatch({\n                type: constants.todo.updateItem,\n                item: item\n            });\n        };\n        const failure = (error) =  { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.insert(item).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n}\n\nexport function removeTodoItem(id) {\n    return (dispatch) =  {\n        dispatch({\n            type: constants.todo.removeItem,\n            id: id\n        });\n\n        const success = () =  {};\n        const failure = (error) =  { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.del({ id: id }).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n}\n\nexport function updateTodoItem(item) {\n    return (dispatch) =  {\n        dispatch({\n            type: constants.todo.updateItem,\n            item: item\n        });\n\n        const success = (item) =  {\n            dispatch({\n                type: constants.todo.updateItem,\n                item: item\n            });\n        };\n        const failure = (error) =  { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.update(item).done(success, failure);\n        dispatch(networkProcess(-1));\n    };\n\n}\n\nexport function refreshTodoItems() {\n    return (dispatch) =  {\n        const success = (data) =  {\n            dispatch({\n                type: constants.todo.replaceItems,\n                items: data\n            });\n        };\n        const failure = (error) =  { dispatch(zumoError(error)); };\n\n        dispatch(networkProcess(1));\n        todoTable.read().then(success, failure);\n        dispatch(networkProcess(-1));\n    }\n}  Each action that is called by the client code kicks off a call into the standard Azure Mobile Apps JavaScript SDK to do the relevant change in the mobile backend.  When that returns, another action is dispatched to make the change in the store via the reducer.  In addition, we keep track of how many network requests are happening and we also handle errors via the  zumoError()  action.  I have not included all the code in this chapter as much of it is fairly standard React/Redux, but it is available on  the GitHub repository  for the book.  Remember that you will have to update the  package.json  file and run  npm install  to install the additional libraries.", 
            "title": "Integrating Azure Mobile Apps"
        }, 
        {
            "location": "/chapter6/react/#updating-webpack-for-azure-mobile-apps-javascript-sdk", 
            "text": "The Azure Mobile Apps JavaScript SDK requires an additional loader for Webpack to handle it.  Install the  json-loader  module using  npm install --save json-loader , then add the following to the  loaders  section of the  webpack.config.js :      module: {\n        loaders: [\n            { test: /\\.jsx?$/, loader: 'babel', exclude: /node_modules/ },\n            { test: /\\.json$/, loader: 'json' }\n        ]\n    },  Once the additional line is added, Webpack will handle the  azure-mobile-apps-client  without problems.  There is a lot more to be said about the capabilities of React, Redux, BabelJS and Webpack that is beyond the scope of this book.  I hope you will take the opportunity to try the vibrant JavaScript community out as you build single page applications.", 
            "title": "Updating Webpack for Azure Mobile Apps JavaScript SDK"
        }, 
        {
            "location": "/chapter7/services/", 
            "text": "When developing mobile applications, certain cloud services - data access, authentication and push notifications - almost always make their way into the requirements for the mobile backend.  However, there are a number of other Azure services that can also make an appearance.\n\n\nAzure has a number of \"platform as a service\" type services.  We've built Azure Mobile Apps on top of some of them - App Service and Notification Hubs.  PaaS, as it is known, is a microservice where you don't have to deal with the underlying operating system or scaling issues.  While we won't cover all possible Azure services, it's worth mentioning a few of the more useful ones here.  In addition, there are some things you can't get from Azure.  We'll take a look at those later on.\n\n\nWe will be covering \nSearch\n and \nVideo\n services within this chapter in more depth.\n\n\nAzure Services\n\n\nThe following services are available with your Azure subscription.  The all incur some kind of cost, although search has a free tier.\n\n\nCognitive Services\n\n\nMachine Learning has been a hot topic in development circles recently.  It's highly complex and very specific to the application being handled.  Fortunately, some applications are relatively easily generalized.  For example, if I want to speak to my app (for example, like Siri or Cortana), then I can use \nCognitive Services\n to process the speech.  There are SDKs for speech processing, textual language analytics and image processing.  The image processing SDKs are particularly useful for mobile photo applications.\n\n\nContent Delivery Networks and Traffic Manager\n\n\nI hope your application is successful.  If you enjoy worldwide success or your mobile backend just can't go down, you will want to augment the single region solution with multiple region availability.  Perhaps you just want an automated failover to another region within the country, or perhaps you want to support other continents with closer capabilities.  Whatever the reason, you will want to take a look at \nTraffic Manager\n, which routes a HTTP request to the closest service endpoint.\n\n\nIn addition, you may want to make static assets available closer to the user.  You can't do anything about dynamic content.  However, static assets like images, videos or long-lived documents can be published to a \nContent Delivery Network\n or CDN.  The Azure CDN uses multiple providers (Akamai and Verizon) so that you can choose the footprint and costs that you need.\n\n\nDocumentDB\n\n\nWe have concentrated on integrating SQL services with offline-sync capabilities in this book.  However, there is a whole different paradigm for storing data on the server, mostly known as NoSQL stores.  These use JSON blobs to store their data.  Their main advantage is that you don't have to think about the schema of the data.    The Azure entry into this market is called \nDocumentDB\n.  It's highly scalable, geo-replicated, and allows you to define business logic in JavaScript to be run on the server (as stored procedures and triggers).\n\n\nNoSQL databases tend to be prevalent in gaming, social data and IoT applications.  SQL tends to be more prevalent in web and enterprise applications.  Both types of data store have their place in the development world.  It really depends on what sort of work load you are considering with your app.  For assistance with the choice, check out the \nNoSQL vs. SQL\n article.\n\n\nMedia services\n\n\nOne of the major pieces of functionality that I see in a lot of mobile applications is video.  It may be an advertisement, a training video, or a major video platform like Netflix or Hulu.  Video is a complex subject and can take up \na book\n just by itself.  Fortunately, \nAzure Media Services\n simplifies the process by providing a scalable platform for video encoding and streaming to your mobile clients.  It also integrates media analytics, the ability to use cognitive services, and content protection in the platform.  We are going to go \nmore in-depth\n in Media Services later on.\n\n\nSearch\n\n\nIf you have some sort of catalog (for example, a shopping cart or video library), then it's likely you will want to search within the catalog or library.  This is where \nAzure Search\n comes in.  It is a platform service that implements an easily consumed API for searching your catalog.  This goes beyond basic SQL queries and gets into natural language processing, fuzzy search, proximity search, term boosting and regular expressions.  It supports 56 languages and provides feedback to your user in terms of search suggestions, highlighting and faceted navigation.  We are going to do a more \nin-depth study\n of Azure Search later in the chapter.\n\n\nService Fabric\n\n\nNot all mobile applications are straight-forward view-based applications.  Some are games, for example, or highly scalable interactive platforms.  When you need custom microservices on the backend, then \nAzure Service Fabric\n will provide an architectural framework for developing the most complex, low-latency, data-intensive scenarios.  In these cases, a simple CRUD model is never going to be enough architecturally for the job of powering your backend.  However, such complexity comes at a cost.  In this case, the cost is the developer time necessry to invest in a highly scalable custom backend for your data flows.  If you think that you need more than basic CRUD style APIs because of latency concerns, then Service Fabric is the appropriate service.\n\n\nNon-Azure Services\n\n\nPlease note that I do not receive compensation and do not endorse any of the companies or products listed in this section.  Please do your research and ensure that the product will assist you.\n\n\nCustomer Feedback\n\n\nYou can get some information from your customers without asking.  Crash analytics and page-use analytics can be driven by code within your application and submitted in real-time or batched to an application analytics package like \nMobile Center\n or \nApp Insights\n.  You may, however, need to ask your customers to leave feedback.  It could be as simple as a button click (like the Visual Studio happy face / sad face), or it may be more extensive, providing support capabilities, star ratings, and free-form text.  You can use these insights to drive UX improvements or features of your mobile app.\n\n\nSome companies that provide mobile-based feedback systems include \nOpinionLab\n and \nApptentive\n.\n\n\nIn App Purchases and Mobile Advertising\n\n\nOne of the big topics that mobile application developers face is how to monetize (i.e. get paid) their app.  Mobile app buyers do not like paying a lot for an app, so supplementary income comes from in-app purchases and mobile advertising.  In-app purchases (or In-app billing, as Android folks call it) are driven via an SDK provided by the mobile platform - \niOS\n or \nAndroid\n.  You will need to integrate these SDKs into the platform-specific project, and Xamarin has some tools to assist for \niOS\n and \nAndroid\n.\n\n\nMobile Advertising is another good area for investigation.  There are a number of considerations when choosing a mobile advertising partner (which is the partner who will feed your app advertisements to display and pay you for their display).  The good news for most mobile app developers is that you can use the social authentication provider as a hint as to which mobile ad provider is going to be appropriate:\n\n\n\n\nFacebook uses \nFacebook App Ads\n\n\nGoogle uses \nAdMob\n\n\n\n\nThere are also other mobile ad providers, like \nOGMobi\n, \nSmaato\n and \nAdIquity\n, should you not be using one of these social auth providers.  It is not uncommon to see top-20 lists of mobile ad networks, so do some research - in particular, pay attention to the \"fill rate\" for customers in your area and the performance of the network (which is generally referred to as eCPM or effective cost per mille - one thousand impressions).  Your mobile ad performance will be less if the advertisements being offered by the network are not appropriate for your customers.\n\n\nIn general, you will need to integrate the ad network SDK into your application and add view elements to your pages where the mobile ads will be displayed.\n\n\nReal-time Communications\n\n\nYou might want to get an instant alert of a pending change to a table, or perhaps produce the next great chat app.  Real-time Communications keeps a TCP or UDP channel open to the mobile client for communications, allowing you to send instant alerts to the connected mobile clients.  This enables quite a few collaborative use cases. There\nare quite a few \"standards\" here:\n\n\n\n\nWebRTC\n\n\nWebSockets\n\n\nSignalR\n\n\nsocket.io\n\n\n\n\nWebRTC uses UDP underneath, so it's better for lossy communication with low latency - for example, audio or video.  WebSockets (and the frameworks that depend on it, like socket.io and SignalR) use TCP.  This means higher latency, but lossless communication.  Since we are developing an ASP.NET mobile backend and using C# for the mobile application, SignalR would be more appropriate to use. You can find \na full example of integrating SignalR with Xamarin Forms on GitHub\n.", 
            "title": "Useful Azure Services"
        }, 
        {
            "location": "/chapter7/services/#azure-services", 
            "text": "The following services are available with your Azure subscription.  The all incur some kind of cost, although search has a free tier.", 
            "title": "Azure Services"
        }, 
        {
            "location": "/chapter7/services/#cognitive-services", 
            "text": "Machine Learning has been a hot topic in development circles recently.  It's highly complex and very specific to the application being handled.  Fortunately, some applications are relatively easily generalized.  For example, if I want to speak to my app (for example, like Siri or Cortana), then I can use  Cognitive Services  to process the speech.  There are SDKs for speech processing, textual language analytics and image processing.  The image processing SDKs are particularly useful for mobile photo applications.", 
            "title": "Cognitive Services"
        }, 
        {
            "location": "/chapter7/services/#content-delivery-networks-and-traffic-manager", 
            "text": "I hope your application is successful.  If you enjoy worldwide success or your mobile backend just can't go down, you will want to augment the single region solution with multiple region availability.  Perhaps you just want an automated failover to another region within the country, or perhaps you want to support other continents with closer capabilities.  Whatever the reason, you will want to take a look at  Traffic Manager , which routes a HTTP request to the closest service endpoint.  In addition, you may want to make static assets available closer to the user.  You can't do anything about dynamic content.  However, static assets like images, videos or long-lived documents can be published to a  Content Delivery Network  or CDN.  The Azure CDN uses multiple providers (Akamai and Verizon) so that you can choose the footprint and costs that you need.", 
            "title": "Content Delivery Networks and Traffic Manager"
        }, 
        {
            "location": "/chapter7/services/#documentdb", 
            "text": "We have concentrated on integrating SQL services with offline-sync capabilities in this book.  However, there is a whole different paradigm for storing data on the server, mostly known as NoSQL stores.  These use JSON blobs to store their data.  Their main advantage is that you don't have to think about the schema of the data.    The Azure entry into this market is called  DocumentDB .  It's highly scalable, geo-replicated, and allows you to define business logic in JavaScript to be run on the server (as stored procedures and triggers).  NoSQL databases tend to be prevalent in gaming, social data and IoT applications.  SQL tends to be more prevalent in web and enterprise applications.  Both types of data store have their place in the development world.  It really depends on what sort of work load you are considering with your app.  For assistance with the choice, check out the  NoSQL vs. SQL  article.", 
            "title": "DocumentDB"
        }, 
        {
            "location": "/chapter7/services/#media-services", 
            "text": "One of the major pieces of functionality that I see in a lot of mobile applications is video.  It may be an advertisement, a training video, or a major video platform like Netflix or Hulu.  Video is a complex subject and can take up  a book  just by itself.  Fortunately,  Azure Media Services  simplifies the process by providing a scalable platform for video encoding and streaming to your mobile clients.  It also integrates media analytics, the ability to use cognitive services, and content protection in the platform.  We are going to go  more in-depth  in Media Services later on.", 
            "title": "Media services"
        }, 
        {
            "location": "/chapter7/services/#search", 
            "text": "If you have some sort of catalog (for example, a shopping cart or video library), then it's likely you will want to search within the catalog or library.  This is where  Azure Search  comes in.  It is a platform service that implements an easily consumed API for searching your catalog.  This goes beyond basic SQL queries and gets into natural language processing, fuzzy search, proximity search, term boosting and regular expressions.  It supports 56 languages and provides feedback to your user in terms of search suggestions, highlighting and faceted navigation.  We are going to do a more  in-depth study  of Azure Search later in the chapter.", 
            "title": "Search"
        }, 
        {
            "location": "/chapter7/services/#service-fabric", 
            "text": "Not all mobile applications are straight-forward view-based applications.  Some are games, for example, or highly scalable interactive platforms.  When you need custom microservices on the backend, then  Azure Service Fabric  will provide an architectural framework for developing the most complex, low-latency, data-intensive scenarios.  In these cases, a simple CRUD model is never going to be enough architecturally for the job of powering your backend.  However, such complexity comes at a cost.  In this case, the cost is the developer time necessry to invest in a highly scalable custom backend for your data flows.  If you think that you need more than basic CRUD style APIs because of latency concerns, then Service Fabric is the appropriate service.", 
            "title": "Service Fabric"
        }, 
        {
            "location": "/chapter7/services/#non-azure-services", 
            "text": "Please note that I do not receive compensation and do not endorse any of the companies or products listed in this section.  Please do your research and ensure that the product will assist you.", 
            "title": "Non-Azure Services"
        }, 
        {
            "location": "/chapter7/services/#customer-feedback", 
            "text": "You can get some information from your customers without asking.  Crash analytics and page-use analytics can be driven by code within your application and submitted in real-time or batched to an application analytics package like  Mobile Center  or  App Insights .  You may, however, need to ask your customers to leave feedback.  It could be as simple as a button click (like the Visual Studio happy face / sad face), or it may be more extensive, providing support capabilities, star ratings, and free-form text.  You can use these insights to drive UX improvements or features of your mobile app.  Some companies that provide mobile-based feedback systems include  OpinionLab  and  Apptentive .", 
            "title": "Customer Feedback"
        }, 
        {
            "location": "/chapter7/services/#in-app-purchases-and-mobile-advertising", 
            "text": "One of the big topics that mobile application developers face is how to monetize (i.e. get paid) their app.  Mobile app buyers do not like paying a lot for an app, so supplementary income comes from in-app purchases and mobile advertising.  In-app purchases (or In-app billing, as Android folks call it) are driven via an SDK provided by the mobile platform -  iOS  or  Android .  You will need to integrate these SDKs into the platform-specific project, and Xamarin has some tools to assist for  iOS  and  Android .  Mobile Advertising is another good area for investigation.  There are a number of considerations when choosing a mobile advertising partner (which is the partner who will feed your app advertisements to display and pay you for their display).  The good news for most mobile app developers is that you can use the social authentication provider as a hint as to which mobile ad provider is going to be appropriate:   Facebook uses  Facebook App Ads  Google uses  AdMob   There are also other mobile ad providers, like  OGMobi ,  Smaato  and  AdIquity , should you not be using one of these social auth providers.  It is not uncommon to see top-20 lists of mobile ad networks, so do some research - in particular, pay attention to the \"fill rate\" for customers in your area and the performance of the network (which is generally referred to as eCPM or effective cost per mille - one thousand impressions).  Your mobile ad performance will be less if the advertisements being offered by the network are not appropriate for your customers.  In general, you will need to integrate the ad network SDK into your application and add view elements to your pages where the mobile ads will be displayed.", 
            "title": "In App Purchases and Mobile Advertising"
        }, 
        {
            "location": "/chapter7/services/#real-time-communications", 
            "text": "You might want to get an instant alert of a pending change to a table, or perhaps produce the next great chat app.  Real-time Communications keeps a TCP or UDP channel open to the mobile client for communications, allowing you to send instant alerts to the connected mobile clients.  This enables quite a few collaborative use cases. There\nare quite a few \"standards\" here:   WebRTC  WebSockets  SignalR  socket.io   WebRTC uses UDP underneath, so it's better for lossy communication with low latency - for example, audio or video.  WebSockets (and the frameworks that depend on it, like socket.io and SignalR) use TCP.  This means higher latency, but lossless communication.  Since we are developing an ASP.NET mobile backend and using C# for the mobile application, SignalR would be more appropriate to use. You can find  a full example of integrating SignalR with Xamarin Forms on GitHub .", 
            "title": "Real-time Communications"
        }, 
        {
            "location": "/chapter7/search/", 
            "text": "Integrating Mobile Search\n\n\nSimple search capabilities can be handled by a little light LINQ usage and a small\nsearch bar.  However, most catalog apps or video apps have search that seems almost\nmagical in what it produces.  It can handle multiple languages, bad spelling, the\ndifference between singular and plural spellings.  Once it has a set of matches, the\norder of the results is based on relevance to your search query and the app can\nhighlight and provide additional search queries.\n\n\nAll of this is provided by external search services.  The most commonly used services\nare based on \nLucene\n which is an open-source text search engine library from the\nApache Project.  Azure Search is no exception here.  It provides a nice REST interface\nthat can be used by your app to provide search results.\n\n\nIn this chapters example, we are going to build something new and it doesn't use Azure\nMobile Apps much at all.  We are going to build a video search engine.  We will have a\nnumber of videos that we will upload.  Those videos will be processed by the backend and\nthe audio and video content will be analyzed for searchable content.  Our app will be\nable to search that content and get some matches back.\n\n\nTo start, I've created a simple Xamarin Forms app with a single view (called Search).\nWe'll update this app later on as we develop the code.  For now, the code for the app\nis on the \nGitHub repository\n.\n\n\nConfiguring Azure Search\n\n\nMy project does not depend on Azure Mobile Apps this time (yet).  Create a new Resource\nGroup, then click on the \n+ ADD\n button at the top of your resource group and add a\nnew \nAzure Search\n resource.  You will have to give it a name which becomes a part of\nthe URL.  All Azure Search resources are accessed through a https://\nname\n.search.windows.net\nURL.\n\n\n\n\nTip\n\n\nSince your users will not be typing the name in, this is a great time to use a GUID\nas the name - it fits the naming convention and is guaranteed to be unique.\n\n\n\n\nThe only other decision of note is the \nPricing Tier\n.  You will be given the Standard\ntier, which covers 15 million documents per partition and up to 12 partitions, with up\nto 36 search units for scaling.  It's an awesome production service.  We are not exactly\nat that level of need yet.  Fortunately, there is a free tier that covers a single scale\nunit, 50MB of storage and 10,000 documents - plenty for testing the service out.\n\n\n\n\nClick on the \nF Free\n option, then \nSelect\n, followed by \nCreate\n to create the\nresource.  Creation of search resources is very quick - usually less than 15 seconds.\n\n\nCreating a Search Index\n\n\nJust like Azure Mobile Apps, there is no data in the service yet, so it's fairly useless.  We\nneed to create an index that can be searched.  For right now, I've got a collection of videos.\nThese documents are JSON objects that include the following fields:\n\n\n\n\nId\n\n\nTitle\n\n\nImage\n\n\nRating\n\n\nRelease Year\n\n\nGenre\n\n\n\n\nIn Azure Search, the model for the objects going into the store need to have a type and you need\nto decide on some attributes.  Exactly one field must be a \"key\" (we'll use the Id for this), and\nfields need to be marked Retrievable, Filterable, Sortable, Facetable and/or Searchable.\n\n\n\n\nRetrievable - the app can retrieve the field\n\n\nSortable - it can be used to sort search results\n\n\nFilterable - it can be used in filter queries\n\n\nSearchable - it is a full-text search field\n\n\n\n\nThe only one I've left out here is \"Facet-able\".  This allows a field to be used in faceted\nnavigation.  This is a drill-down mechanism.  If you have been on a web store like Amazon, you\nwill have seen this feature.  It's generally depicted as a \"Refine by\" field.  For example, you\nmay search for cars, but then want to limit the search to only convertibles, then only by red\ncars.  Each of these refinements is a facet.  If I added a \"genre\" to my fields, I could use\nfaceted navigation.\n\n\nBack to my model, here it is:\n\n\n\n\n\n\n\n\nField Name\n\n\nType\n\n\nAttributes\n\n\n\n\n\n\n\n\n\n\nvideoId\n\n\nEdm.String\n\n\nKey, Retrievable\n\n\n\n\n\n\ntitle\n\n\nEdm.String\n\n\nRetrievable, Sortable, Filterable, Searchable\n\n\n\n\n\n\nimage\n\n\nEdm.String\n\n\nRetrievable\n\n\n\n\n\n\nrating\n\n\nEdm.Double\n\n\nRetrievable, Sortable, Filterable\n\n\n\n\n\n\nreleaseYear\n\n\nEdm.Int32\n\n\nRetrievable, Sortable, Filterable\n\n\n\n\n\n\ngenre\n\n\nCollection(Edm.String)\n\n\nRetrievable, Filterable, Searchable, Facetable\n\n\n\n\n\n\n\n\nThe types are based on \nOData v4 types\n.  There are a large number of primitive types.  Unfortunately, Azure\nSearch only supports \na subset of these\n.\n\n\n\n\nWarn\n\n\nYou can't sort by the genre field because it is a collection.  Collections cannot be marked sortable.\n\n\n\n\nYou can create an index via the Azure Portal or with the REST interface.  I'm going to define my\n\"videos\" index with JSON.  Here is the JSON file:\n\n\n{\n    \nname\n: \nvideos\n,\n    \nfields\n: [\n        {\n            \nname\n: \nvideoId\n,\n            \ntype\n: \nEdm.String\n,\n            \nkey\n: true,\n            \nfilterable\n: false,\n            \nsearchable\n: false,\n            \nsortable\n: false,\n            \nfacetable\n: false\n        },\n        {\n            \nname\n: \ntitle\n,\n            \ntype\n: \nEdm.String\n,\n            \nfilterable\n: true,\n            \nsortable\n: true,\n            \nfacetable\n: false\n        },\n        {\n            \nname\n: \nimage\n,\n            \ntype\n: \nEdm.String\n,\n            \nfilterable\n: false,\n            \nsearchable\n: false,\n            \nsortable\n: false,\n            \nfacetable\n: false\n        },\n        {\n            \nname\n: \nrating\n,\n            \ntype\n: \nEdm.Double\n,\n            \nfilterable\n: true,\n            \nsearchable\n: false,\n            \nsortable\n: true,\n            \nfacetable\n: false\n        },\n        {\n            \nname\n: \nreleaseYear\n,\n            \ntype\n: \nEdm.Int32\n,\n            \nfilterable\n: true,\n            \nsearchable\n: false,\n            \nsortable\n: true,\n            \nfacetable\n: false\n        },\n        {\n            \nname\n: \ngenre\n,\n            \ntype\n: \nCollection(Edm.String)\n,\n            \nfilterable\n: true,\n            \nsortable\n: false,\n            \nfacetable\n: true\n        }\n    ]\n}\n\n\n\n\n\n\nTip\n\n\nYou can skip default values.  Searchable and Retrievable default to true, so you only have to specify\nthem if you want to turn that off.  Similarly, key defaults to false, so you only have to specify the\nkey on the field that needs it.\n\n\n\n\nTo install this index, you will need the URI of the search service (which you already have) and the API Key.\nIn the \nAzure Portal\n, open your Azure Search resource and click on \nKeys\n.  You will see the primary\nand secondary \nADMIN KEY\n.  You can use either one.  Open up Postman and issue a POST to \nURI/indexes?api-version=2016-09-01\n.\nSet the Content-Type to application/json and add an \napi-key\n header set to the admin key.  The content\nof the POST should be your JSON object.\n\n\n\n\nInfo\n\n\nYou can also install an index through the Azure Portal as a one time activity.  I like using REST\nbecause it allows me to treat \"configuration as code\" and check my index definition into my source\nrepository.  This also opens up automated deployment options via PowerShell, for example.\n\n\n\n\n\n\nClick on the \nSEND\n button and you will see the return status of \n201 Created\n and an OData v4 document\nin the response body giving the full definition of the index.  You will receive a 400 series response with\nan error message if something goes wrong.  It's likely to be either a duplicate index or a malformed JSON\nobject.\n\n\n\n\nFree Limits\n\n\nYou can create 1 free Azure Search instance and that supports 3 indices (or collections of documents).\n\n\n\n\nPopulating a Search Index (the easy way)\n\n\nThere are many ways of populating a search index.  You can do what we are going to do - push data into an\nindex.  You can, however, also define an indexer that will crawl a data source periodically.  Indexers are\nprovided for Azure Blob and Table Storage, DocumentDB and SQL Server instances (through Azure SQL or SQL\nServer on a VM).\n\n\nI'm going to use a file from \nthe Internet\n to populate the search index. I had to adjust it as the format\nfor uploading is specific:\n\n\n{\n    \nvalue\n: [\n        {\n            \ntitle\n: \nDawn of the Planet of the Apes\n,\n            \nimage\n: \nhttp://api.androidhive.info/json/movies/1.jpg\n,\n            \nrating\n: 8.3,\n            \nreleaseYear\n: 2014,\n            \ngenre\n: [\n                \nAction\n,\n                \nDrama\n,\n                \nSci-Fi\n\n            ],\n            \n@search.action\n: \nupload\n,\n            \nvideoId\n: \n98ebe557-894c-48de-b61c-718f78b2adbb\n\n        },\n        {\n            \ntitle\n: \nDistrict 9\n,\n            \nimage\n: \nhttp://api.androidhive.info/json/movies/2.jpg\n,\n            \nrating\n: 8,\n            \nreleaseYear\n: 2009,\n            \ngenre\n: [\n                \nAction\n,\n                \nSci-Fi\n,\n                \nThriller\n\n            ],\n            \n@search.action\n: \nupload\n,\n            \nvideoId\n: \ne9d89038-386b-4aaa-a36c-530e2f3587c9\n\n        },\n        ...\n    ]\n}\n\n\n\n\nI can upload this document just like the creation of the index.  The only difference is that I am POSTing\nto /indexes/videos/docs/index:\n\n\n\n\nIf you see a \n200 OK\n, then all the documents were uploaded and accepted.  If you see a \n207\n message,\nthen some of the documents were not uploaded.  In that case, look at the response - each document will be\nreferenced by the key field (in our case, the videoId field), the status field will be false and there will\nbe an errorMessage field which contains the problem.  You can correct the problem and upload just that\ndocument.\n\n\nThere are four values for the \n@search.action\n field - normally, we will want to use \"upload\" for new\ndocuments and \"mergeOrUpload\" for subsequent updates.\n\n\nTesting Azure Search\n\n\nNow that we have a few documents indexed, we can test the search facility.  Go to the Overview page of your\nsearch service.  The indices are listed on the overview page and you can click on the videos index.\n\n\n\n\nYou can then click on the Search Explorer to get into the test facility.  Let's start with a fairly basic\nsearch for the word \"of\" across all searchable fields:\n\n\n\n\nWe can also do boolean searches.  For example, let's do the same search, but finding only comedies:\n\n\n\n\nAzure Search can accept a simple search string (as we have done here), an \nOData Filter\n using a restricted\nset of search criteria, or \nLucene Search Syntax\n.  The search explorer allows you to explore the various\nsearch mechanisms and their (sometimes peculiar) syntax.\n\n\nOther Service Considerations\n\n\nYou will note the use of an API key for Azure Search.  This can (and should) be regenerated at a regular\ninterval.  As a result, you will want a custom API that retrieves the current API key, perhaps only giving\nthe API key to authenticated users.  We covered custom APIs in an earlier chapter, so I won't cover that\nfunctionality here.  Instead, the demonstration code will use a \nSettings.cs\n class in the client that\ncontains the URI and API key for searching.\n\n\nUsing Azure Search\n\n\nBefore you can use Azure Search, you should generate a Query-Only API key for your Azure Search service.  When\nwe uploaded the documents to the search service for indexing (and if you intend to do any other administrative\ntasks through PowerShell or the REST API), you will use the Administrative API key.  This key is found under\nthe \nKeys\n menu item in the Azure Search resource in the Azure Portal.  In the same place is a menu item called\n\nManage query keys\n.\n\n\n\n\nThe service creates one of these keys for you with an empty name.  I like to create a query key for each version\nof the mobile software I release.  I can thus retire keys that are no longer in use.  To create a key:\n\n\n\n\nClick the \n+ Add\n button.\n\n\nEnter a descriptive name (like \"iOS v1.0\", for example)\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYou can now copy and paste the key into your settings file.  I have created a \nSettings.cs\n file in my shared\nproject:\n\n\nusing System;\n\nnamespace VideoSearch\n{\n    public static class Settings\n    {\n        public static string AzureSearchUri = \nhttps://zumbook.search.windows.net\n;\n\n        /// \nsummary\n\n        /// Replace this with your API key from the Azure Search.  You should\n        /// never check in code with an API key in it - read the key from an\n        /// Azure App Service App Setting and then provide it to your mobile\n        /// clients via a custom API.\n        /// \n/summary\n\n        public static string AzureSearchApiKey = \n88E95AB69AAAAB6FC5579E1CC40E7FC4\n;\n    }\n}\n\n\n\n\nAs we saw while we were testing the service, the search API is going to return a number of JSON objects.  We\ncan represent each return value with a model.  Here is my \nModels/Movie.cs\n model:\n\n\nusing System;\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class Movie : SearchResult\n    {\n        [JsonProperty(PropertyName = \nvideoId\n)]\n        public string Id { get; set; }\n\n\n        public string Title { get; set; }\n\n        public Uri Image { get; set; }\n\n        public double Rating { get; set; }\n\n        public int ReleaseYear { get; set; }\n\n        [JsonProperty(PropertyName = \ngenre\n)]\n        public List\nstring\n Genres { get; set; }\n    }\n}\n\n\n\n\nThe \nModels/SearchResult.cs\n model adds the \n@search.score\n value that is returned in the search results:\n\n\nusing Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class SearchResult\n    {\n        [JsonProperty(PropertyName = \n@search.score\n)]\n        public double SearchScore { get; set; }\n    }\n}\n\n\n\n\nFinally, the \nModels/MovieResults.cs\n class can be used to deserialize the entire JSON object that is returned by the\nserver:\n\n\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class MovieResults\n    {\n        [JsonProperty(PropertyName = \n@odata.context\n)]\n        public string Context { get; set; }\n\n        [JsonProperty(PropertyName = \nvalue\n)]\n        public List\nMovie\n Movies { get; set; }\n    }\n}\n\n\n\n\nI also provide a class called \nServices/SearchService.cs\n for handling search results.  In this case, it will\ndo the HTTP request to the specified server, sending the provided search string, and decode the response.  It\nwill throw an exception if the server produces an error:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json;\nusing VideoSearch.Models;\n\nnamespace VideoSearch.Services\n{\n    public class SearchService\n    {\n        private HttpClient _client;\n        private string _apiVersion = \n2016-09-01\n;\n\n        public SearchService()\n        {\n            this._client = new HttpClient();\n        }\n\n        public async Task\nList\nMovie\n SearchMoviesAsync(string searchTerms)\n        {\n            var content = await SearchAsync(\nvideos\n, searchTerms);\n            var movieResults = JsonConvert.DeserializeObject\nMovieResults\n(content);\n            return movieResults.Movies;\n        }\n\n        private async Task\nstring\n SearchAsync(string index, string searchTerms)\n        {\n            var uri = new UriBuilder($\n{Settings.AzureSearchUri}/indexes/{index}/docs\n);\n            uri.Query = $\napi-version={_apiVersion}\nsearch={Uri.EscapeDataString(searchTerms)}\n;\n\n            var request = new HttpRequestMessage\n            {\n                RequestUri = uri.Uri,\n                Method = HttpMethod.Get\n            };\n\n            request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(\napplication/json\n));\n            request.Headers.Add(\napi-key\n, Settings.AzureSearchApiKey);\n\n            var response = await _client.SendAsync(request);\n            return await response.Content.ReadAsStringAsync();\n        }\n    }\n}\n\n\n\n\nThe \nSearchAsync()\n method is a basic HTTP GET method that returns a string.  We add the appropriate\nheaders and ensure the URI is the correct format.  \n\n\nThis service class can now be used to search for movies when we type something into the search box and\ninitiate a search.  This is done in the \nViewModels\\Search.cs\n class in the shared project:\n\n\n    public Command SearchCommand =\n _cmdSearch ?? (_cmdSearch = new Command(async () =\n await ExecuteSearchCommand()));\n\n    private async Task ExecuteSearchCommand()\n    {\n        if (IsBusy)\n            return;\n        IsBusy = true;\n\n        try\n        {\n            var results = await _service.SearchMoviesAsync(SearchString);\n            SearchResults.ReplaceRange(results);\n        }\n        catch (Exception ex)\n        {\n            SearchResults.Clear();\n            await Application.Current.MainPage.DisplayAlert(\nSearch Failed\n, ex.Message, \nOK\n);\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }\n\n\n\n\nThe \nSearchString\n is a bindable string property that is bound to the Text field of the search \nbar.  The \nSearchResults\n property is an \nObservableRangeCollection\n that is bound to a list of\nitems.\n\n\nThis code creates a reference to our search service, then uses it to populate the search results PropertyName\nwith the list of movies when the search is complete.  We also needed a little bit of error handling for the\nedge case when the user types something in that isn't understood by the search service.  This is likely to\nbe rare, but we want to handle failures gracefully when they do happen.\n\n\nThe associated view must also be adjusted so that the movies are displayed.  This is done within a \nListView\n\nobject in a similar way to the way we did our task list examples:\n\n\nListView\n    CachingStrategy=\nRecycleElement\n\n    IsPullToRefreshEnabled=\nFalse\n\n    ItemsSource=\n{Binding SearchResults}\n\n    RowHeight=\n50\n\n    SelectedItem=\n{Binding SelectedItem, Mode=TwoWay}\n\n\n    \nListView.BackgroundColor\n\n        \nOnPlatform\n            x:TypeArguments=\nColor\n\n            Android=\n#2E2F30\n\n            WinPhone=\n#F0F0F0\n\n            iOS=\n#F0F0F0\n /\n\n    \n/ListView.BackgroundColor\n\n\n    \nListView.ItemTemplate\n\n        \nDataTemplate\n\n            \nViewCell\n\n                \nStackLayout\n                    Padding=\n10\n\n                    HorizontalOptions=\nFillAndExpand\n\n                    Orientation=\nHorizontal\n\n                    VerticalOptions=\nCenterAndExpand\n\n                    \nStackLayout.BackgroundColor\n\n                        \nOnPlatform\n                            x:TypeArguments=\nColor\n\n                            Android=\nBlack\n\n                            WinPhone=\nWhite\n\n                            iOS=\nWhite\n /\n\n                    \n/StackLayout.BackgroundColor\n\n                    \nLabel\n                        HorizontalOptions=\nFillAndExpand\n\n                        Text=\n{Binding Title}\n\n                        TextColor=\n#272832\n\n                        \nLabel.TextColor\n\n                            \nOnPlatform\n                                x:TypeArguments=\nColor\n\n                                Android=\n#F3F3F3\n\n                                WinPhone=\n#272832\n\n                                iOS=\n#272832\n /\n\n                        \n/Label.TextColor\n\n                    \n/Label\n\n                \n/StackLayout\n\n            \n/ViewCell\n\n        \n/DataTemplate\n\n    \n/ListView.ItemTemplate\n\n\n/ListView\n\n\n\n\n\nWe can now run this application and start using the search service!  In the \nexample code\n, I've also included\na Details panel.  This should show you all the information within a movie title from the search results by clicking\non the movie title.  It's similar in construction to the task details pane from our task list example.\n\n\nSQL vs. DocumentDB vs. Azure Search\n\n\nYou may be wondering at this point why you should use Azure Search as an addition to SQL databases or DocumentDB.\nAfter all, all three options store data, allow you to update that data, and provide access to JSON documents.  So\nwhy would I not just place all my data in a searchable index and use that instead?\n\n\nThe Azure Search facility is designed for full-text searching of data, not rapid updates from multiple sources.  As\na result of this focus, it has no ability to do offline synchronization or conflict resolution, and it is lazy about\ninserts (data inserted may not be available straight away).  It is not a good repository if your common practice is \nto work with the full data set and it is most definitely not the right source if you have more than one client with\nthe potential to update records at the same time.  It's primarily driven by a need to provide fast search results.  \n\n\nDocumentDB and SQL Azure (via Azure Mobile Apps) are are the inverse of this.  They are designed for concurrent and\nrapid updates to the data, with built-in conflict handling, incremental offline sync and guaranteed atomic writes.\nThese are features that your mobile clients need when you are writing data.  Both DocumentDB and Azure Mobile Apps\nalso have security features for limiting data retrieval - something Azure Search does not have.\n\n\nUse Azure Search when you want to do \"shopping cart\" or \"reference data search\" type functionality.  These use\ncases have slow changing data sets that are changed behind the scenes by a single client.  Use Azure Mobile Apps\nor DocumentDB when you want multiple clients to update the data set, or your mobile client updates have security\nconcerns.", 
            "title": "Azure Search"
        }, 
        {
            "location": "/chapter7/search/#integrating-mobile-search", 
            "text": "Simple search capabilities can be handled by a little light LINQ usage and a small\nsearch bar.  However, most catalog apps or video apps have search that seems almost\nmagical in what it produces.  It can handle multiple languages, bad spelling, the\ndifference between singular and plural spellings.  Once it has a set of matches, the\norder of the results is based on relevance to your search query and the app can\nhighlight and provide additional search queries.  All of this is provided by external search services.  The most commonly used services\nare based on  Lucene  which is an open-source text search engine library from the\nApache Project.  Azure Search is no exception here.  It provides a nice REST interface\nthat can be used by your app to provide search results.  In this chapters example, we are going to build something new and it doesn't use Azure\nMobile Apps much at all.  We are going to build a video search engine.  We will have a\nnumber of videos that we will upload.  Those videos will be processed by the backend and\nthe audio and video content will be analyzed for searchable content.  Our app will be\nable to search that content and get some matches back.  To start, I've created a simple Xamarin Forms app with a single view (called Search).\nWe'll update this app later on as we develop the code.  For now, the code for the app\nis on the  GitHub repository .", 
            "title": "Integrating Mobile Search"
        }, 
        {
            "location": "/chapter7/search/#configuring-azure-search", 
            "text": "My project does not depend on Azure Mobile Apps this time (yet).  Create a new Resource\nGroup, then click on the  + ADD  button at the top of your resource group and add a\nnew  Azure Search  resource.  You will have to give it a name which becomes a part of\nthe URL.  All Azure Search resources are accessed through a https:// name .search.windows.net\nURL.   Tip  Since your users will not be typing the name in, this is a great time to use a GUID\nas the name - it fits the naming convention and is guaranteed to be unique.   The only other decision of note is the  Pricing Tier .  You will be given the Standard\ntier, which covers 15 million documents per partition and up to 12 partitions, with up\nto 36 search units for scaling.  It's an awesome production service.  We are not exactly\nat that level of need yet.  Fortunately, there is a free tier that covers a single scale\nunit, 50MB of storage and 10,000 documents - plenty for testing the service out.   Click on the  F Free  option, then  Select , followed by  Create  to create the\nresource.  Creation of search resources is very quick - usually less than 15 seconds.", 
            "title": "Configuring Azure Search"
        }, 
        {
            "location": "/chapter7/search/#creating-a-search-index", 
            "text": "Just like Azure Mobile Apps, there is no data in the service yet, so it's fairly useless.  We\nneed to create an index that can be searched.  For right now, I've got a collection of videos.\nThese documents are JSON objects that include the following fields:   Id  Title  Image  Rating  Release Year  Genre   In Azure Search, the model for the objects going into the store need to have a type and you need\nto decide on some attributes.  Exactly one field must be a \"key\" (we'll use the Id for this), and\nfields need to be marked Retrievable, Filterable, Sortable, Facetable and/or Searchable.   Retrievable - the app can retrieve the field  Sortable - it can be used to sort search results  Filterable - it can be used in filter queries  Searchable - it is a full-text search field   The only one I've left out here is \"Facet-able\".  This allows a field to be used in faceted\nnavigation.  This is a drill-down mechanism.  If you have been on a web store like Amazon, you\nwill have seen this feature.  It's generally depicted as a \"Refine by\" field.  For example, you\nmay search for cars, but then want to limit the search to only convertibles, then only by red\ncars.  Each of these refinements is a facet.  If I added a \"genre\" to my fields, I could use\nfaceted navigation.  Back to my model, here it is:     Field Name  Type  Attributes      videoId  Edm.String  Key, Retrievable    title  Edm.String  Retrievable, Sortable, Filterable, Searchable    image  Edm.String  Retrievable    rating  Edm.Double  Retrievable, Sortable, Filterable    releaseYear  Edm.Int32  Retrievable, Sortable, Filterable    genre  Collection(Edm.String)  Retrievable, Filterable, Searchable, Facetable     The types are based on  OData v4 types .  There are a large number of primitive types.  Unfortunately, Azure\nSearch only supports  a subset of these .   Warn  You can't sort by the genre field because it is a collection.  Collections cannot be marked sortable.   You can create an index via the Azure Portal or with the REST interface.  I'm going to define my\n\"videos\" index with JSON.  Here is the JSON file:  {\n     name :  videos ,\n     fields : [\n        {\n             name :  videoId ,\n             type :  Edm.String ,\n             key : true,\n             filterable : false,\n             searchable : false,\n             sortable : false,\n             facetable : false\n        },\n        {\n             name :  title ,\n             type :  Edm.String ,\n             filterable : true,\n             sortable : true,\n             facetable : false\n        },\n        {\n             name :  image ,\n             type :  Edm.String ,\n             filterable : false,\n             searchable : false,\n             sortable : false,\n             facetable : false\n        },\n        {\n             name :  rating ,\n             type :  Edm.Double ,\n             filterable : true,\n             searchable : false,\n             sortable : true,\n             facetable : false\n        },\n        {\n             name :  releaseYear ,\n             type :  Edm.Int32 ,\n             filterable : true,\n             searchable : false,\n             sortable : true,\n             facetable : false\n        },\n        {\n             name :  genre ,\n             type :  Collection(Edm.String) ,\n             filterable : true,\n             sortable : false,\n             facetable : true\n        }\n    ]\n}   Tip  You can skip default values.  Searchable and Retrievable default to true, so you only have to specify\nthem if you want to turn that off.  Similarly, key defaults to false, so you only have to specify the\nkey on the field that needs it.   To install this index, you will need the URI of the search service (which you already have) and the API Key.\nIn the  Azure Portal , open your Azure Search resource and click on  Keys .  You will see the primary\nand secondary  ADMIN KEY .  You can use either one.  Open up Postman and issue a POST to  URI/indexes?api-version=2016-09-01 .\nSet the Content-Type to application/json and add an  api-key  header set to the admin key.  The content\nof the POST should be your JSON object.   Info  You can also install an index through the Azure Portal as a one time activity.  I like using REST\nbecause it allows me to treat \"configuration as code\" and check my index definition into my source\nrepository.  This also opens up automated deployment options via PowerShell, for example.    Click on the  SEND  button and you will see the return status of  201 Created  and an OData v4 document\nin the response body giving the full definition of the index.  You will receive a 400 series response with\nan error message if something goes wrong.  It's likely to be either a duplicate index or a malformed JSON\nobject.   Free Limits  You can create 1 free Azure Search instance and that supports 3 indices (or collections of documents).", 
            "title": "Creating a Search Index"
        }, 
        {
            "location": "/chapter7/search/#populating-a-search-index-the-easy-way", 
            "text": "There are many ways of populating a search index.  You can do what we are going to do - push data into an\nindex.  You can, however, also define an indexer that will crawl a data source periodically.  Indexers are\nprovided for Azure Blob and Table Storage, DocumentDB and SQL Server instances (through Azure SQL or SQL\nServer on a VM).  I'm going to use a file from  the Internet  to populate the search index. I had to adjust it as the format\nfor uploading is specific:  {\n     value : [\n        {\n             title :  Dawn of the Planet of the Apes ,\n             image :  http://api.androidhive.info/json/movies/1.jpg ,\n             rating : 8.3,\n             releaseYear : 2014,\n             genre : [\n                 Action ,\n                 Drama ,\n                 Sci-Fi \n            ],\n             @search.action :  upload ,\n             videoId :  98ebe557-894c-48de-b61c-718f78b2adbb \n        },\n        {\n             title :  District 9 ,\n             image :  http://api.androidhive.info/json/movies/2.jpg ,\n             rating : 8,\n             releaseYear : 2009,\n             genre : [\n                 Action ,\n                 Sci-Fi ,\n                 Thriller \n            ],\n             @search.action :  upload ,\n             videoId :  e9d89038-386b-4aaa-a36c-530e2f3587c9 \n        },\n        ...\n    ]\n}  I can upload this document just like the creation of the index.  The only difference is that I am POSTing\nto /indexes/videos/docs/index:   If you see a  200 OK , then all the documents were uploaded and accepted.  If you see a  207  message,\nthen some of the documents were not uploaded.  In that case, look at the response - each document will be\nreferenced by the key field (in our case, the videoId field), the status field will be false and there will\nbe an errorMessage field which contains the problem.  You can correct the problem and upload just that\ndocument.  There are four values for the  @search.action  field - normally, we will want to use \"upload\" for new\ndocuments and \"mergeOrUpload\" for subsequent updates.", 
            "title": "Populating a Search Index (the easy way)"
        }, 
        {
            "location": "/chapter7/search/#testing-azure-search", 
            "text": "Now that we have a few documents indexed, we can test the search facility.  Go to the Overview page of your\nsearch service.  The indices are listed on the overview page and you can click on the videos index.   You can then click on the Search Explorer to get into the test facility.  Let's start with a fairly basic\nsearch for the word \"of\" across all searchable fields:   We can also do boolean searches.  For example, let's do the same search, but finding only comedies:   Azure Search can accept a simple search string (as we have done here), an  OData Filter  using a restricted\nset of search criteria, or  Lucene Search Syntax .  The search explorer allows you to explore the various\nsearch mechanisms and their (sometimes peculiar) syntax.", 
            "title": "Testing Azure Search"
        }, 
        {
            "location": "/chapter7/search/#other-service-considerations", 
            "text": "You will note the use of an API key for Azure Search.  This can (and should) be regenerated at a regular\ninterval.  As a result, you will want a custom API that retrieves the current API key, perhaps only giving\nthe API key to authenticated users.  We covered custom APIs in an earlier chapter, so I won't cover that\nfunctionality here.  Instead, the demonstration code will use a  Settings.cs  class in the client that\ncontains the URI and API key for searching.", 
            "title": "Other Service Considerations"
        }, 
        {
            "location": "/chapter7/search/#using-azure-search", 
            "text": "Before you can use Azure Search, you should generate a Query-Only API key for your Azure Search service.  When\nwe uploaded the documents to the search service for indexing (and if you intend to do any other administrative\ntasks through PowerShell or the REST API), you will use the Administrative API key.  This key is found under\nthe  Keys  menu item in the Azure Search resource in the Azure Portal.  In the same place is a menu item called Manage query keys .   The service creates one of these keys for you with an empty name.  I like to create a query key for each version\nof the mobile software I release.  I can thus retire keys that are no longer in use.  To create a key:   Click the  + Add  button.  Enter a descriptive name (like \"iOS v1.0\", for example)  Click  Create .    You can now copy and paste the key into your settings file.  I have created a  Settings.cs  file in my shared\nproject:  using System;\n\nnamespace VideoSearch\n{\n    public static class Settings\n    {\n        public static string AzureSearchUri =  https://zumbook.search.windows.net ;\n\n        ///  summary \n        /// Replace this with your API key from the Azure Search.  You should\n        /// never check in code with an API key in it - read the key from an\n        /// Azure App Service App Setting and then provide it to your mobile\n        /// clients via a custom API.\n        ///  /summary \n        public static string AzureSearchApiKey =  88E95AB69AAAAB6FC5579E1CC40E7FC4 ;\n    }\n}  As we saw while we were testing the service, the search API is going to return a number of JSON objects.  We\ncan represent each return value with a model.  Here is my  Models/Movie.cs  model:  using System;\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class Movie : SearchResult\n    {\n        [JsonProperty(PropertyName =  videoId )]\n        public string Id { get; set; }\n\n\n        public string Title { get; set; }\n\n        public Uri Image { get; set; }\n\n        public double Rating { get; set; }\n\n        public int ReleaseYear { get; set; }\n\n        [JsonProperty(PropertyName =  genre )]\n        public List string  Genres { get; set; }\n    }\n}  The  Models/SearchResult.cs  model adds the  @search.score  value that is returned in the search results:  using Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class SearchResult\n    {\n        [JsonProperty(PropertyName =  @search.score )]\n        public double SearchScore { get; set; }\n    }\n}  Finally, the  Models/MovieResults.cs  class can be used to deserialize the entire JSON object that is returned by the\nserver:  using System.Collections.Generic;\nusing Newtonsoft.Json;\n\nnamespace VideoSearch.Models\n{\n    public class MovieResults\n    {\n        [JsonProperty(PropertyName =  @odata.context )]\n        public string Context { get; set; }\n\n        [JsonProperty(PropertyName =  value )]\n        public List Movie  Movies { get; set; }\n    }\n}  I also provide a class called  Services/SearchService.cs  for handling search results.  In this case, it will\ndo the HTTP request to the specified server, sending the provided search string, and decode the response.  It\nwill throw an exception if the server produces an error:  using System;\nusing System.Collections.Generic;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json;\nusing VideoSearch.Models;\n\nnamespace VideoSearch.Services\n{\n    public class SearchService\n    {\n        private HttpClient _client;\n        private string _apiVersion =  2016-09-01 ;\n\n        public SearchService()\n        {\n            this._client = new HttpClient();\n        }\n\n        public async Task List Movie  SearchMoviesAsync(string searchTerms)\n        {\n            var content = await SearchAsync( videos , searchTerms);\n            var movieResults = JsonConvert.DeserializeObject MovieResults (content);\n            return movieResults.Movies;\n        }\n\n        private async Task string  SearchAsync(string index, string searchTerms)\n        {\n            var uri = new UriBuilder($ {Settings.AzureSearchUri}/indexes/{index}/docs );\n            uri.Query = $ api-version={_apiVersion} search={Uri.EscapeDataString(searchTerms)} ;\n\n            var request = new HttpRequestMessage\n            {\n                RequestUri = uri.Uri,\n                Method = HttpMethod.Get\n            };\n\n            request.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue( application/json ));\n            request.Headers.Add( api-key , Settings.AzureSearchApiKey);\n\n            var response = await _client.SendAsync(request);\n            return await response.Content.ReadAsStringAsync();\n        }\n    }\n}  The  SearchAsync()  method is a basic HTTP GET method that returns a string.  We add the appropriate\nheaders and ensure the URI is the correct format.    This service class can now be used to search for movies when we type something into the search box and\ninitiate a search.  This is done in the  ViewModels\\Search.cs  class in the shared project:      public Command SearchCommand =  _cmdSearch ?? (_cmdSearch = new Command(async () =  await ExecuteSearchCommand()));\n\n    private async Task ExecuteSearchCommand()\n    {\n        if (IsBusy)\n            return;\n        IsBusy = true;\n\n        try\n        {\n            var results = await _service.SearchMoviesAsync(SearchString);\n            SearchResults.ReplaceRange(results);\n        }\n        catch (Exception ex)\n        {\n            SearchResults.Clear();\n            await Application.Current.MainPage.DisplayAlert( Search Failed , ex.Message,  OK );\n        }\n        finally\n        {\n            IsBusy = false;\n        }\n    }  The  SearchString  is a bindable string property that is bound to the Text field of the search \nbar.  The  SearchResults  property is an  ObservableRangeCollection  that is bound to a list of\nitems.  This code creates a reference to our search service, then uses it to populate the search results PropertyName\nwith the list of movies when the search is complete.  We also needed a little bit of error handling for the\nedge case when the user types something in that isn't understood by the search service.  This is likely to\nbe rare, but we want to handle failures gracefully when they do happen.  The associated view must also be adjusted so that the movies are displayed.  This is done within a  ListView \nobject in a similar way to the way we did our task list examples:  ListView\n    CachingStrategy= RecycleElement \n    IsPullToRefreshEnabled= False \n    ItemsSource= {Binding SearchResults} \n    RowHeight= 50 \n    SelectedItem= {Binding SelectedItem, Mode=TwoWay} \n\n     ListView.BackgroundColor \n         OnPlatform\n            x:TypeArguments= Color \n            Android= #2E2F30 \n            WinPhone= #F0F0F0 \n            iOS= #F0F0F0  / \n     /ListView.BackgroundColor \n\n     ListView.ItemTemplate \n         DataTemplate \n             ViewCell \n                 StackLayout\n                    Padding= 10 \n                    HorizontalOptions= FillAndExpand \n                    Orientation= Horizontal \n                    VerticalOptions= CenterAndExpand \n                     StackLayout.BackgroundColor \n                         OnPlatform\n                            x:TypeArguments= Color \n                            Android= Black \n                            WinPhone= White \n                            iOS= White  / \n                     /StackLayout.BackgroundColor \n                     Label\n                        HorizontalOptions= FillAndExpand \n                        Text= {Binding Title} \n                        TextColor= #272832 \n                         Label.TextColor \n                             OnPlatform\n                                x:TypeArguments= Color \n                                Android= #F3F3F3 \n                                WinPhone= #272832 \n                                iOS= #272832  / \n                         /Label.TextColor \n                     /Label \n                 /StackLayout \n             /ViewCell \n         /DataTemplate \n     /ListView.ItemTemplate  /ListView   We can now run this application and start using the search service!  In the  example code , I've also included\na Details panel.  This should show you all the information within a movie title from the search results by clicking\non the movie title.  It's similar in construction to the task details pane from our task list example.", 
            "title": "Using Azure Search"
        }, 
        {
            "location": "/chapter7/search/#sql-vs-documentdb-vs-azure-search", 
            "text": "You may be wondering at this point why you should use Azure Search as an addition to SQL databases or DocumentDB.\nAfter all, all three options store data, allow you to update that data, and provide access to JSON documents.  So\nwhy would I not just place all my data in a searchable index and use that instead?  The Azure Search facility is designed for full-text searching of data, not rapid updates from multiple sources.  As\na result of this focus, it has no ability to do offline synchronization or conflict resolution, and it is lazy about\ninserts (data inserted may not be available straight away).  It is not a good repository if your common practice is \nto work with the full data set and it is most definitely not the right source if you have more than one client with\nthe potential to update records at the same time.  It's primarily driven by a need to provide fast search results.    DocumentDB and SQL Azure (via Azure Mobile Apps) are are the inverse of this.  They are designed for concurrent and\nrapid updates to the data, with built-in conflict handling, incremental offline sync and guaranteed atomic writes.\nThese are features that your mobile clients need when you are writing data.  Both DocumentDB and Azure Mobile Apps\nalso have security features for limiting data retrieval - something Azure Search does not have.  Use Azure Search when you want to do \"shopping cart\" or \"reference data search\" type functionality.  These use\ncases have slow changing data sets that are changed behind the scenes by a single client.  Use Azure Mobile Apps\nor DocumentDB when you want multiple clients to update the data set, or your mobile client updates have security\nconcerns.", 
            "title": "SQL vs. DocumentDB vs. Azure Search"
        }, 
        {
            "location": "/chapter7/media/", 
            "text": "Azure Media Services\n\n\nOne of the common use cases for mobile applications involves video streaming.  In the consumer space, this can include applications like Hulu or Netflix along with video reviews and new segments in apps like CNN and CNET.  In the enterprise space, we see video learning and employee broadcast solutions.\n\n\nWhatever their source, they have some basic functionality in common:\n\n\n\n\nThe video asset is uploaded and converted (also known as encoding) to a streaming format.\n\n\nA live video channel can be provided for multiplex streaming to a large audience.\n\n\nThe encoded video is provided to clients for download with a suitable web endpoint.\n\n\nAdditional services extract information from the video for search capabilities.\n\n\n\n\nMany enterprises wrap such functionality in a combined web and mobile site to provide streaming video for eLearning.  We are going to look at what it takes to produce the mobile side of such a site in this section.\n\n\nThe Video Search Application with Media Services\n\n\nWe are going to produce a media services mobile application for this section, based on our last example for Azure Search.  In the new example, this is the approximate flow of the application:\n\n\n\n\nThe administrator will upload an MP4 video via the Visual Studio Storage Explorer.  This will be automatically picked up by an Azure Function that encode the video, placing the encoded video into a download area.  The next Azure Function will pick up that video and use Cognitive Services on it to extract information that can be searched and insert that information into the Azure Search instance.  Finally, a third Azure Function will insert the data about the video into a database so that it can be picked up by Azure Mobile Apps.  We are going to use three distinct operations here because encoding and cognitive services are asynchronous - we want to kick them off and let them complete in their own time.\n\n\nOn the client side, we will use the Azure Search instance to find apps, display the information held within Azure Mobile Apps, and allow the user to stream the video using the video player.\n\n\nAs you can see, there are many more services in use in this example than our previous examples:\n\n\n\n\nAzure Media Services\n is used for video encoding and streaming endpoints.\n\n\nAzure Logic Apps\n are used for workflow automation.\n\n\nAzure Functions\n are used for automation.\n\n\nCognitive Services\n are used to extract information from the videos.\n\n\nAzure App Service\n is used to act as a coordinator for the mobile app.\n\n\nAzure Storage\n is used to store the individual video assets and for some queuing capabilities.\n\n\nSQL Azure\n is used as the backing store for the Azure Mobile Apps data store.\n\n\n\n\nThis is now a fairly typical mobile application.  We are using 8 different Azure services in a composite manner to provide the facilities needed by our application.\n\n\nCreating the Mobile Encoding flow\n\n\nWhen I look at the architecture for our mobile backend, I see two distinct parts.  The first is the backend flow that processes the incoming videos.  As videos are uploaded, they need to be injected into a queue.  From there, a series of processes are kicked off to process the incoming video.  First, the video is encoded; then data is extracted from the video for search purposes; finally, the video is added to the SQL database so it can be searched.\n\n\nThe other flow is from the mobile client - it connects to the App Service and makes requests based on what it needs to do.  In this case, we have a set of data tables for providing data about the video and a few custom APIs for handling search and video streaming.\n\n\nLet's take each of these in turn.   The configuration of most of the services  have already been discussed, so I will not go over them and only provide the options I used.  This includes Storage, Search, SQL Azure, and Functions.\n\n\nCreating pre-requisite services\n\n\nBefore I start with the new services, I need an \nAzure Storage\n account, an [Azure Search] instance and an \nAzure Function App\n.  I've covered all these items in previous sections, so I won't go into them here. The configuration is as follows:\n\n\n\n\nMy \nAzure Storage\n account called \nzumomediach7.core.windows.net\n as \nGeneral Purpose\n storage with \nLRS\n replication.\n\n\nMy \nAzure Functions\n app called \nzumomediach7-functions.azurewebsites.net\n in the \nConsumption Plan\n.  I'm\n   using my \nzumomediach7\n storage account.\n\n\nMy \nSQL Azure\n service is called \nzumomediach7.database.windows.net\n.\n\n\nMy \nSQL Azure\n database is called \nvideosearch\n in the \nB Basic\n pricing plan.\n\n\nMy \nAzure App Service\n is created via the \nMobile App\n template and called \nzumomediach7.azurewebsites.net\n.  It\n    has an \nB1 Basic\n app service plan associated with it.\n\n\n\n\nIn addition, I've linked the SQL Azure database and storage accounts to the App Service via the Data Connections menu option.\n\n\nOur resource group looks quite extensive now:\n\n\n\n\nConfiguration for the services is as follows:\n\n\nAzure Storage\n has a container for incoming videos called \nincoming\n.\n\n\nAzure App Service\n has a basic TableController which is based on the following DTO model:\n\n\nusing Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class Video : EntityData\n    {\n        public string Filename { get; set; }\n\n        public string VideoUri { get; set; }\n    }\n}\n\n\n\n\nThis encompasses information from the majority of the book thus far.  If you are uncertain on how to perform any of this configuration, review the appropriate sections of the book:\n\n\n\n\nChapter 1\n covers creating a Mobile App.\n\n\nChapter 4\n covers Azure Functions.\n\n\n\n\n\n\nAzure Search is there\n\n\nOne of the things I've added into this project that I don't describe is the integration with Azure Search.  I use this to integrate Cognitive Services with the solution so that I can search for videos based on their content (audio, video or metadata).  You can use this as a research project.\n\n\n\n\nCreating an Azure Media Services account\n\n\nSo far, we've done a lot of infrastructure work.  We've generated an Azure Mobile App that our mobile app can use to retrieve information about the videos, generated an Azure Search instance with a suitable index, and a storage account for processing the videos.  We now want to move onto the meat of this section - working with video.  In order to do that, we will need an Azure Media Services account.\n\n\nCreating an Azure Media Services account is very similar to other Azure resources.  Log in to the [Azure Portal] and open the resource group you are using to hold all the resources for this application.\n\n\n\n\nClick \n+ ADD\n to add a resource to the resource group.\n\n\nSearch for \nMedia Services\n, select it, then click on \nCreate\n.\n\n\nFill in the form:\n\n\nSelect a name for your service.  It needs to be unique within the service.\n\n\nSelect your existing storage account (note the limitations on the replication policy if you use your own).\n\n\nEnsure the region matches your storage account and other resources.\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nThe Media Services accounts may take a couple of minutes to create.  Do not continue until the deployment is complete.\n\n\n\n\nTesting tools for Media Services\n\n\nIf you intend to do any development in Azure Media Services, you should download and become familiar with the \nAzure Media Services Explorer\n. this is a test tool for Windows that allows you to upload, download, process, encode and package assets ith Azure Media Services.  You should also grow a collection of test videos.  A great starting point are \nthese videos from TechSlides\n.\n\n\n\n\nYou could stop here and do all the work manually.  If you wish to check out the full set of tutorials, follow the official documentation:\n\n\n\n\nUploading Assets\n\n\nEncoding Assets\n\n\nPublish Assets\n\n\n\n\nThe Encoding Pipeline\n\n\nThere is an \nexcellent sample\n that uses Azure Functions and Azure Logic Apps as a media workflow.  The Azure Functions do the actual processing, using the Azure Media Services SDK to communicate with the Media Services resource, and the Logic App (below) is used to control the workflow and ensure it works properly.\n\n\n\n\nTo create this flow, first create the Azure Functions required by the flow in the Function App.  There are five Functions that are required:\n\n\n\n\ncheck-job-status\n\n\ncreate-empty-asset\n\n\npublish-asset\n\n\nsubmit-job\n\n\nsync-asset\n\n\n\n\nStart by creating the \nshared\n and \npresets\n folders.  You can do this using the \nApp Service Editor\n, which is located in the \nFunction app settings\n.  Just create each file and then copy-and-paste the contents into the file.\n\n\nThe source code for each function is in \nthe referenced project\n.  Create the Function from the \nGenericWebhook-CSharp\n template.  Then add the \nproject.json\n file, which is needed to load the Media Services SDK from NuGet.  Once you save the \nproject.json\n file, let the NuGet restore happen before continuing.  You can check the Log window to ensure it is complete.  Finally, copy-and-paste the code for the \nrun.csx\n file.\n\n\nNext, create a \nLogic App\n:\n\n\n\n\nClose the Function App to return to your Resource Group.\n\n\nClick \nAdd\n at the top of the blade.\n\n\nSearch for and select \nLogic App\n.\n\n\nClick \nCreate\n.\n\n\nGive it a name (like \nzumobook-logicapp\n) and ensure the location is the same as all your other resources, then click \nCreate\n.\n\n\n\n\nAfter deployment, we can set up the logic app.  Click the newly created logic app to open the \nLogic Apps Designer\n.   The first thing you want to add is a \nTrigger\n - something that triggers the execution of the workflow.  You can upload a file to OneDrive or Dropbox, for example.   In this example, I'm going to use my OneDrive \nIncomingVideos\n folder:\n\n\n\n\nFind and click \nOneDrive\n.  You may have to click \nSEE MORE\n to find it.\n\n\nSign in to create a connection to OneDrive.  You will also have to authorize Logic Apps to access your information.\n\n\nClick the folder icon to Select a Folder.  Click \n next to Root, then \nIncomingVideos\n.  It will be listed as \n/IncomingVideos\n.\n\n\n\n\n\n\nUse the folder picker\n\n\nSome of the triggers will encode the arguments.  Use the folder picker rather than typing in the box if your trigger doesn't seem to fire successfully.\n\n\n\n\nNow that we have the trigger, we need to continue building the Logic App based on the diagram above.   There are a few types of steps - a Function step (where an Azure Function is called via a Webhook):\n\n\n\n\nClick \n+ New step\n, then \nAdd an action\n.\n\n\nFind and click \nAzure Functions\n (you may have to click \nSEE MORE\n).\n\n\nClick \nAzure Functions - Choose an Azure function\n.  If you only have one Function App, it will be added automatically, otherwise, select the required Function App name.\n\n\nClick \nAzure Functions - {your Function App}\n.  It will load the list of functions.\n\n\nClick the name of the function you wish to add as the step.  The first one is \ncreate-empty-asset\n.\n\n\n\n\nEnter the Request Body based on the comment at the top of each Azure Function.  For instance, the \ncreate-empty-asset\n should look like this:\n\n\n\n\nThe Name can be added by clicking \nAdd dynamic content\n, then finding the appropriate field.\n\n\n\n\n\n\nAfter \ncreate-empty-asset\n has been complete, you may want to click \nSave\n to save your work.  Then continue by clicking \n+ New step\n, then \nAdd an action\n.  The next step is a \nCreate blob\n step.  You can use the search box to find actions to perform.  The Create blob step should look like this when you are finished.\n\n\n\n\nWhen you do configure this step in the Logic Apps Designer, you will note that the \ncontainerPath\n is not available from the dynamic content.  When the Webhook returns, it provides a JSON response.  The JSON response is documented at the top of the code of each Azure Function.  To enter this value, switch to the \nCode view\n, find the \nCreate_blob\n action, look for the \nqueries\n section, then insert the following value:\n\n\n\n\nOnce you have entered the value, click \nSave\n, then \nDesigner\n to switch back to the designer view.  Continue to add \nsync-asset\n as an action.  The request body will have to be set within the code view as it relies on the output of \ncreate-empty-asset\n.  Set the \nbody\n section to:\n\n\n    \nbody\n: {\n        \nassetId\n: \n@{body('create-empty-asset')['assetId']}\n\n    }\n\n\n\n\n\n\nUse the template to create everything for you!\n\n\nThe \nsample\n has a \"Deploy to Azure\" button that allows you to create all the functions and the logic app in one swoop.  It's great to understand how Logic Apps are put together, but if you would rather get on with it, just use the shortcut.\n\n\n\n\nLinking the \nsubmit-job\n next, set the \nbody\n in the Code view as follows:\n\n\n    \nbody\n: {\n        \nassetId\n: \n@{body('create-empty-asset')['assetId']}\n,\n        \nmesPreset\n: \nAdaptive Streaming\n\n    }\n\n\n\n\nThe next step is an \"Until\" step.  You are not limited to just a straight step-flow with Logic Apps.  You can do loops and conditional execution as well.  In this case, the \nsubmit-job\n Azure Function kicks off an encoding job for the incoming video.  However, the process to encode that video can take some time.  Even a small video can take upwards of 15 minutes to encode because of queuing and process limitations.  The \nAdd a do until\n step is in the \"More\" section after you click \n+ New step\n.  Start by clicking on \nAdd an action\n within the Until loop.  Add the \ncheck-job-status\n Azure Function with a request body:\n\n\n    \nbody\n: {\n        \njobId\n: \n@{body('submit-job')['jobId']}\n\n    }\n\n\n\n\nWhile you are in the code view, set the \"expression\" field for the Until loop to the following:\n\n\n    \nexpression\n: \n@equals(body('check-job-status')['isRunning'], 'False')\n,\n\n\n\n\n\n\nCheck the template if you get lost!\n\n\nYou can configure everything within the \nCode view\n, so if you get lost, just use copy-and-paste to configure each step within the logic app.\n\n\n\n\nAfter the Until loop, we can add a Condition to check if the \nisSuccessful\n field returned by the latest invocation of \ncheck-job-status\n was true.  Click on \nEdit in advanced mode\n and enter the condition \n@equals(body('check-job-status')['isSuccessful'], 'True')\n.  You now have two sections - a YES and a NO section.  My NO section uses \"Outlook.com - Send an email\" to send me an email.  I use the \nFile name\n field in the body to indicate what file was problematic.\n\n\nOn the YES side, I am going to add multiple steps.  Firstly, I will add an Azure Function for \npublish-asset\n with a body:\n\n\n    \nbody\n: {\n        \nassetId\n: \n@{body('submit-job')['mes']['assetId']}\n\n    }\n\n\n\n\nTechnically, this is now a complete encoding pipeline.  However, I also want to put the asset into the database so that my client can download it.  In the canonical example, the URL of the encoded video is published at \n@{body('publish-asset')['playerUrl']}\n.  I can pass that into a new Azure Function that inserts it into the database.  I can create a new function from directly within the Logic App.  However, there are a number of problems with that.  Firstly, it creates a Node.js function and I like C#.  Secondly, the code editor leaves a lot to be desired.  It's a small text box with no Intellisense.  Use the \nSave\n button to save your Logic App, then close the Logic Apps Designer and switch over to your Azure Function App.\n\n\n\n\nAdditional Resources Created\n\n\nIf you have created your Logic App correctly, you will note additional resources have been created for the connections to the Azure Blob storage, OneDrive and potentially Outlook.  These are part of your Logic App and should not be configured separately.\n\n\n\n\nUse the \nGenericWebHook-CSharp\n template to create a Function called \ninsert-into-database\n.  The code for the Webhook is as follows:\n\n\n/*\nThis function check a job status.\n\nInput:\n{\n    \nfileName\n: \nsome-name\n,\n    \nurl\n: \nsome-url\n\n }\n\nOutput:\n{\n    \ndbId\n: \nsome-guid\n         // The new object reference\n }\n*/\n#r \nNewtonsoft.Json\n\n#r \nSystem.Data\n\n\nusing System;\nusing System.Configuration;\nusing System.Data.SqlClient;\nusing System.Net;\nusing Newtonsoft.Json;\n\npublic static async Task\nobject\n Run(HttpRequestMessage req, TraceWriter log)\n{\n    log.Info($\nWebhook was triggered!\n);\n\n    string jsonContent = await req.Content.ReadAsStringAsync();\n    dynamic data = JsonConvert.DeserializeObject(jsonContent);\n\n    if (data.url == null || data.fileName == null) {\n        return req.CreateResponse(HttpStatusCode.BadRequest, new {\n            error = \nPlease pass all properties in the input object\n\n        });\n    }\n\n    var connectionString = ConfigurationManager.ConnectionStrings[\nMS_TableConnectionString\n].ConnectionString;\n    log.Info($\nUsing Connection String {connectionString}\n);\n\n    var dbId = Guid.NewGuid().ToString(\nN\n);\n    try\n    {\n        using (var sqlConnection = new SqlConnection(connectionString))\n        {\n            using (var sqlCommand = sqlConnection.CreateCommand())\n            {\n                log.Info(\nInitiating SQL Connection\n);\n                sqlConnection.Open();\n\n                log.Info(\nExecuting SQL Statement\n);\n                sqlCommand.CommandText = $\nINSERT INTO [dbo].[Videos] ([Id], [Deleted], [Filename], [VideoUri]) VALUES ('{dbId}', 0, '{data.fileName}', '{data.url}')\n;\n                var rowsAffected = sqlCommand.ExecuteNonQuery();\n                log.Info($\n{rowsAffected} rows inserted.\n);\n\n                sqlConnection.Close();\n            }\n        }\n    }\n    catch (Exception ex)\n    {\n        return req.CreateResponse(HttpStatusCode.BadRequest, new {\n            error = ex.Message\n        });\n    }\n\n    return req.CreateResponse(HttpStatusCode.OK, new {\n        greeting = $\n{dbId}\n\n    });\n}\n\n\n\n\nThis inserts a record into the Videos table with the filename and URI specified.  I can now add this function to the YES column in my Logic App by specifying the following body in the Code view:\n\n\n    \nbody\n: {\n        \nfileName\n: \n@{triggerOutputs()['headers']['x-ms-file-name']}\n,\n        \nurl\n: \n@{body('publish-asset')['pathUrl']}\n\n    }\n\n\n\n\nThere are three URLs that are returned by the previous step:\n\n\n\n\npathUrl\n is the path to the assets (but not the filename).\n\n\nsmoothUrl\n is the real-time streaming endpoint.\n\n\nplayerUrl\n is a web-page with an embedded player.\n\n\n\n\nYou need to use the corresponding URL for your implementation.  If you are unsure, then store both the pathUrl and smoothUrl in the database (which will require a modification of the model).  The \nplayerUrl\n can be computed on the client if you need it.\n\n\nBefore we can try this pipeline out, the other resources must be specified as Application Settings inside the Function App:\n\n\n\n\nAMSAccount\n is the name of your Media Services resource.\n\n\nAMSKey\n is the primary key for your Media Services resource.\n\n\nMediaServicesStorageAccountName\n is the name of your Azure Storage resource.\n\n\nMediaServicesStorageAccountKey\n is the primary key for your Azure Storage resource.\n\n\nMS_TableConnectionString\n is the connection string to your video database (from your App Service).\n\n\n\n\nOnce these are set, you are ready to test your logic app.  Go to the Logic Apps Designer and click \nRun\n.  This allows you to monitor the progress of the workflow live.  Then drop a video file in the /IncomingVideos folder of your OneDrive connection and watch the process.  It's likely that something will go wrong the first time.  In the case of Azure Functions, the error will be displayed:\n\n\n\n\nIf the error is in a Logic App provided trigger, then consult the Diagnostics and Log search menu items under Monitoring.  For the Azure Functions triggers, it is more informative to consult the error logs in the Function App.  Open the \nMonitor\n tab to check the logs for the latest run.  Also, you can create a test run with the appropriate input object and/or place more logging in the Azure Function.  I faked this error by removing the \nAMSAccount\n application setting.  If you have copied the source code directly, it's likely that any errors will be in the app settings.\n\n\n\n\nInsert-to-Database Failures\n\n\nThe \ninsert-to-database\n function and the logic app will fail because the database is not created until the first request by a client.  You can either pre-create the database or use the client that is developed before you try out the encoding pipeline.\n\n\n\n\nThe Video Mobile App\n\n\nNow that the backend has been brought online and we can populate it with videos, it's time to turn our attention to the client app.  I've started with an app very similar to the Task List.  The models are slightly different (since the data set is different), but ultimately, the app provides a list of videos to play.  You can find \nthe starting project on GitHub\n.\n\n\n\n\nUse the starting point to create the database\n\n\nIn the last section, I mentioned that one of the functions would not work because the database was not created until the first client request.  You can use the starting point for the project to create the necessary database.  Create all the backend resources, then run the client to create the database, then test out the encoding pipeline.\n\n\n\n\nYou can integrate any video player that supports a streaming endpoint, and there are several to choose from - each with their own complexities for integration.  For simplicity, I am going to integrate the Azure Media Services Player - a web-based streaming media player which I will integrate into a \nWebView\n within the page.  Let's start by hooking up a new view in the shared project.  The new view is called \nPages/VideoDetail.xaml\n:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage\n    x:Class=\nVideoApp.Pages.VideoDetail\n\n    xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n    xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n    \nStackLayout\n\n        \nWebView\n            x:Name=\nbrowser\n\n            HorizontalOptions=\nFillAndExpand\n\n            VerticalOptions=\nFillAndExpand\n /\n\n    \n/StackLayout\n\n\n/ContentPage\n\n\n\n\n\nThis page creates a \nWebView\n that occupies the entire page.  There is a backing C# source file as well that implements the viewer:\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace VideoApp.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class VideoDetail : ContentPage\n    {\n        public VideoDetail (Models.Video video)\n        {\n            InitializeComponent ();\n\n            var htmlSource = new HtmlWebViewSource();\n            var sourceInfo = @\n\n\nhtml\n\n    \nhead\n\n        \ntitle\nTest\n/title\n\n        \nlink href=\nhttps://amp.azure.net/libs/amp/1.8.3/skins/amp-default/azuremediaplayer.min.css\n rel=\nstylesheet\n\n        \nscript src=\nhttps://amp.azure.net/libs/amp/1.8.3/azuremediaplayer.min.js\n/script\n\n     \n/head\n\n    \nbody\n\n        \nvideo id=\nazuremediaplayer\n class=\nazuremediaplayer amp-default-skin amp-big-play-centered\n tabindex=\n0\n/video\n\n        \nscript\n\nvar myOptions = {\n    \nnativeControlsForTouch\n: false,\n    controls: true,\n    autoplay: true,\n    width: \n640\n,\n    height: \n400\n,\n};\nmyPlayer = amp(\nazuremediaplayer\n, myOptions);\nmyPlayer.src([\n    {\n    src: \n{Binding Source}\n,\n    type: \napplication/vnd.ms-sstr+xml\n\n    }\n]);\n        \n/script\n\n    \n/body\n\n\n/html\n\n\n;\n            htmlSource.Html = sourceInfo.Replace(\n{Binding Source}\n, video.VideoUri);\n            browser.Source = htmlSource;\n        }\n    }\n}\n\n\n\n\nThe HTML and Javascript libraries that I use here are provided by Azure Media Services.  You can find them as follows:\n\n\n\n\nStart \nAzure Media Services Explorer\n and connect to your Media Services account.\n\n\nRight-click a published video, then select \nPlayback\n -\n \nwith Azure Media Player\n.\n\n\nA web-page will open.  Click \nCode\n -\n \nGet Player Code\n (under the video).\n\n\n\n\nThe code will be displayed:\n\n\n\n\nI replaced the \nsrc\n object with something I can string-replace later on.  The player is completely cross-platform.  You do, however, have to specify the height and width.  One of the advantages of using a native control is that it will set the height and width for you.\n\n\nWrap Up\n\n\nObviously, this isn't the prettiest app that has been produced.  However, it is functional and it demonstrates the basic capabilities.  We don't have to stop where we did, however.\n\n\n\n\nWe could use \nCognitive Services\n to extract the audio track and submit to an \nAzure Search\n facility.  We could then allow searching of the Azure Search facility to come up with a list of videos that match based on the audio track.  This is, quite frankly, something that still amazes me and something that we could not do without the Azure Cloud.  The \nCognitive Services integration\n is available as the advanced option in the sample for the media processing workflow.\n\n\nWe could adjust the \ninsert-into-database\n Function to extract metadata from the MP4 file.  The MP4 file contains a whole host of information.  This can be inserted into the database so you can display it.  You could also add a JSON file in the upload to provide additional content that is inserted into the database.\n\n\nWe could provide ratings and other controls on the list.  This can be stored on the mobile backend as well to provide information to other users.\n\n\n\n\nVideo media is one of those areas of development that is complex to understand and implement, but has so much potential in the mobile space.", 
            "title": "Media Services"
        }, 
        {
            "location": "/chapter7/media/#azure-media-services", 
            "text": "One of the common use cases for mobile applications involves video streaming.  In the consumer space, this can include applications like Hulu or Netflix along with video reviews and new segments in apps like CNN and CNET.  In the enterprise space, we see video learning and employee broadcast solutions.  Whatever their source, they have some basic functionality in common:   The video asset is uploaded and converted (also known as encoding) to a streaming format.  A live video channel can be provided for multiplex streaming to a large audience.  The encoded video is provided to clients for download with a suitable web endpoint.  Additional services extract information from the video for search capabilities.   Many enterprises wrap such functionality in a combined web and mobile site to provide streaming video for eLearning.  We are going to look at what it takes to produce the mobile side of such a site in this section.", 
            "title": "Azure Media Services"
        }, 
        {
            "location": "/chapter7/media/#the-video-search-application-with-media-services", 
            "text": "We are going to produce a media services mobile application for this section, based on our last example for Azure Search.  In the new example, this is the approximate flow of the application:   The administrator will upload an MP4 video via the Visual Studio Storage Explorer.  This will be automatically picked up by an Azure Function that encode the video, placing the encoded video into a download area.  The next Azure Function will pick up that video and use Cognitive Services on it to extract information that can be searched and insert that information into the Azure Search instance.  Finally, a third Azure Function will insert the data about the video into a database so that it can be picked up by Azure Mobile Apps.  We are going to use three distinct operations here because encoding and cognitive services are asynchronous - we want to kick them off and let them complete in their own time.  On the client side, we will use the Azure Search instance to find apps, display the information held within Azure Mobile Apps, and allow the user to stream the video using the video player.  As you can see, there are many more services in use in this example than our previous examples:   Azure Media Services  is used for video encoding and streaming endpoints.  Azure Logic Apps  are used for workflow automation.  Azure Functions  are used for automation.  Cognitive Services  are used to extract information from the videos.  Azure App Service  is used to act as a coordinator for the mobile app.  Azure Storage  is used to store the individual video assets and for some queuing capabilities.  SQL Azure  is used as the backing store for the Azure Mobile Apps data store.   This is now a fairly typical mobile application.  We are using 8 different Azure services in a composite manner to provide the facilities needed by our application.", 
            "title": "The Video Search Application with Media Services"
        }, 
        {
            "location": "/chapter7/media/#creating-the-mobile-encoding-flow", 
            "text": "When I look at the architecture for our mobile backend, I see two distinct parts.  The first is the backend flow that processes the incoming videos.  As videos are uploaded, they need to be injected into a queue.  From there, a series of processes are kicked off to process the incoming video.  First, the video is encoded; then data is extracted from the video for search purposes; finally, the video is added to the SQL database so it can be searched.  The other flow is from the mobile client - it connects to the App Service and makes requests based on what it needs to do.  In this case, we have a set of data tables for providing data about the video and a few custom APIs for handling search and video streaming.  Let's take each of these in turn.   The configuration of most of the services  have already been discussed, so I will not go over them and only provide the options I used.  This includes Storage, Search, SQL Azure, and Functions.", 
            "title": "Creating the Mobile Encoding flow"
        }, 
        {
            "location": "/chapter7/media/#creating-pre-requisite-services", 
            "text": "Before I start with the new services, I need an  Azure Storage  account, an [Azure Search] instance and an  Azure Function App .  I've covered all these items in previous sections, so I won't go into them here. The configuration is as follows:   My  Azure Storage  account called  zumomediach7.core.windows.net  as  General Purpose  storage with  LRS  replication.  My  Azure Functions  app called  zumomediach7-functions.azurewebsites.net  in the  Consumption Plan .  I'm\n   using my  zumomediach7  storage account.  My  SQL Azure  service is called  zumomediach7.database.windows.net .  My  SQL Azure  database is called  videosearch  in the  B Basic  pricing plan.  My  Azure App Service  is created via the  Mobile App  template and called  zumomediach7.azurewebsites.net .  It\n    has an  B1 Basic  app service plan associated with it.   In addition, I've linked the SQL Azure database and storage accounts to the App Service via the Data Connections menu option.  Our resource group looks quite extensive now:   Configuration for the services is as follows:  Azure Storage  has a container for incoming videos called  incoming .  Azure App Service  has a basic TableController which is based on the following DTO model:  using Microsoft.Azure.Mobile.Server;\n\nnamespace Backend.DataObjects\n{\n    public class Video : EntityData\n    {\n        public string Filename { get; set; }\n\n        public string VideoUri { get; set; }\n    }\n}  This encompasses information from the majority of the book thus far.  If you are uncertain on how to perform any of this configuration, review the appropriate sections of the book:   Chapter 1  covers creating a Mobile App.  Chapter 4  covers Azure Functions.    Azure Search is there  One of the things I've added into this project that I don't describe is the integration with Azure Search.  I use this to integrate Cognitive Services with the solution so that I can search for videos based on their content (audio, video or metadata).  You can use this as a research project.", 
            "title": "Creating pre-requisite services"
        }, 
        {
            "location": "/chapter7/media/#creating-an-azure-media-services-account", 
            "text": "So far, we've done a lot of infrastructure work.  We've generated an Azure Mobile App that our mobile app can use to retrieve information about the videos, generated an Azure Search instance with a suitable index, and a storage account for processing the videos.  We now want to move onto the meat of this section - working with video.  In order to do that, we will need an Azure Media Services account.  Creating an Azure Media Services account is very similar to other Azure resources.  Log in to the [Azure Portal] and open the resource group you are using to hold all the resources for this application.   Click  + ADD  to add a resource to the resource group.  Search for  Media Services , select it, then click on  Create .  Fill in the form:  Select a name for your service.  It needs to be unique within the service.  Select your existing storage account (note the limitations on the replication policy if you use your own).  Ensure the region matches your storage account and other resources.    Click  Create .    The Media Services accounts may take a couple of minutes to create.  Do not continue until the deployment is complete.   Testing tools for Media Services  If you intend to do any development in Azure Media Services, you should download and become familiar with the  Azure Media Services Explorer . this is a test tool for Windows that allows you to upload, download, process, encode and package assets ith Azure Media Services.  You should also grow a collection of test videos.  A great starting point are  these videos from TechSlides .   You could stop here and do all the work manually.  If you wish to check out the full set of tutorials, follow the official documentation:   Uploading Assets  Encoding Assets  Publish Assets", 
            "title": "Creating an Azure Media Services account"
        }, 
        {
            "location": "/chapter7/media/#the-encoding-pipeline", 
            "text": "There is an  excellent sample  that uses Azure Functions and Azure Logic Apps as a media workflow.  The Azure Functions do the actual processing, using the Azure Media Services SDK to communicate with the Media Services resource, and the Logic App (below) is used to control the workflow and ensure it works properly.   To create this flow, first create the Azure Functions required by the flow in the Function App.  There are five Functions that are required:   check-job-status  create-empty-asset  publish-asset  submit-job  sync-asset   Start by creating the  shared  and  presets  folders.  You can do this using the  App Service Editor , which is located in the  Function app settings .  Just create each file and then copy-and-paste the contents into the file.  The source code for each function is in  the referenced project .  Create the Function from the  GenericWebhook-CSharp  template.  Then add the  project.json  file, which is needed to load the Media Services SDK from NuGet.  Once you save the  project.json  file, let the NuGet restore happen before continuing.  You can check the Log window to ensure it is complete.  Finally, copy-and-paste the code for the  run.csx  file.  Next, create a  Logic App :   Close the Function App to return to your Resource Group.  Click  Add  at the top of the blade.  Search for and select  Logic App .  Click  Create .  Give it a name (like  zumobook-logicapp ) and ensure the location is the same as all your other resources, then click  Create .   After deployment, we can set up the logic app.  Click the newly created logic app to open the  Logic Apps Designer .   The first thing you want to add is a  Trigger  - something that triggers the execution of the workflow.  You can upload a file to OneDrive or Dropbox, for example.   In this example, I'm going to use my OneDrive  IncomingVideos  folder:   Find and click  OneDrive .  You may have to click  SEE MORE  to find it.  Sign in to create a connection to OneDrive.  You will also have to authorize Logic Apps to access your information.  Click the folder icon to Select a Folder.  Click   next to Root, then  IncomingVideos .  It will be listed as  /IncomingVideos .    Use the folder picker  Some of the triggers will encode the arguments.  Use the folder picker rather than typing in the box if your trigger doesn't seem to fire successfully.   Now that we have the trigger, we need to continue building the Logic App based on the diagram above.   There are a few types of steps - a Function step (where an Azure Function is called via a Webhook):   Click  + New step , then  Add an action .  Find and click  Azure Functions  (you may have to click  SEE MORE ).  Click  Azure Functions - Choose an Azure function .  If you only have one Function App, it will be added automatically, otherwise, select the required Function App name.  Click  Azure Functions - {your Function App} .  It will load the list of functions.  Click the name of the function you wish to add as the step.  The first one is  create-empty-asset .   Enter the Request Body based on the comment at the top of each Azure Function.  For instance, the  create-empty-asset  should look like this:   The Name can be added by clicking  Add dynamic content , then finding the appropriate field.    After  create-empty-asset  has been complete, you may want to click  Save  to save your work.  Then continue by clicking  + New step , then  Add an action .  The next step is a  Create blob  step.  You can use the search box to find actions to perform.  The Create blob step should look like this when you are finished.   When you do configure this step in the Logic Apps Designer, you will note that the  containerPath  is not available from the dynamic content.  When the Webhook returns, it provides a JSON response.  The JSON response is documented at the top of the code of each Azure Function.  To enter this value, switch to the  Code view , find the  Create_blob  action, look for the  queries  section, then insert the following value:   Once you have entered the value, click  Save , then  Designer  to switch back to the designer view.  Continue to add  sync-asset  as an action.  The request body will have to be set within the code view as it relies on the output of  create-empty-asset .  Set the  body  section to:       body : {\n         assetId :  @{body('create-empty-asset')['assetId']} \n    }   Use the template to create everything for you!  The  sample  has a \"Deploy to Azure\" button that allows you to create all the functions and the logic app in one swoop.  It's great to understand how Logic Apps are put together, but if you would rather get on with it, just use the shortcut.   Linking the  submit-job  next, set the  body  in the Code view as follows:       body : {\n         assetId :  @{body('create-empty-asset')['assetId']} ,\n         mesPreset :  Adaptive Streaming \n    }  The next step is an \"Until\" step.  You are not limited to just a straight step-flow with Logic Apps.  You can do loops and conditional execution as well.  In this case, the  submit-job  Azure Function kicks off an encoding job for the incoming video.  However, the process to encode that video can take some time.  Even a small video can take upwards of 15 minutes to encode because of queuing and process limitations.  The  Add a do until  step is in the \"More\" section after you click  + New step .  Start by clicking on  Add an action  within the Until loop.  Add the  check-job-status  Azure Function with a request body:       body : {\n         jobId :  @{body('submit-job')['jobId']} \n    }  While you are in the code view, set the \"expression\" field for the Until loop to the following:       expression :  @equals(body('check-job-status')['isRunning'], 'False') ,   Check the template if you get lost!  You can configure everything within the  Code view , so if you get lost, just use copy-and-paste to configure each step within the logic app.   After the Until loop, we can add a Condition to check if the  isSuccessful  field returned by the latest invocation of  check-job-status  was true.  Click on  Edit in advanced mode  and enter the condition  @equals(body('check-job-status')['isSuccessful'], 'True') .  You now have two sections - a YES and a NO section.  My NO section uses \"Outlook.com - Send an email\" to send me an email.  I use the  File name  field in the body to indicate what file was problematic.  On the YES side, I am going to add multiple steps.  Firstly, I will add an Azure Function for  publish-asset  with a body:       body : {\n         assetId :  @{body('submit-job')['mes']['assetId']} \n    }  Technically, this is now a complete encoding pipeline.  However, I also want to put the asset into the database so that my client can download it.  In the canonical example, the URL of the encoded video is published at  @{body('publish-asset')['playerUrl']} .  I can pass that into a new Azure Function that inserts it into the database.  I can create a new function from directly within the Logic App.  However, there are a number of problems with that.  Firstly, it creates a Node.js function and I like C#.  Secondly, the code editor leaves a lot to be desired.  It's a small text box with no Intellisense.  Use the  Save  button to save your Logic App, then close the Logic Apps Designer and switch over to your Azure Function App.   Additional Resources Created  If you have created your Logic App correctly, you will note additional resources have been created for the connections to the Azure Blob storage, OneDrive and potentially Outlook.  These are part of your Logic App and should not be configured separately.   Use the  GenericWebHook-CSharp  template to create a Function called  insert-into-database .  The code for the Webhook is as follows:  /*\nThis function check a job status.\n\nInput:\n{\n     fileName :  some-name ,\n     url :  some-url \n }\n\nOutput:\n{\n     dbId :  some-guid          // The new object reference\n }\n*/\n#r  Newtonsoft.Json \n#r  System.Data \n\nusing System;\nusing System.Configuration;\nusing System.Data.SqlClient;\nusing System.Net;\nusing Newtonsoft.Json;\n\npublic static async Task object  Run(HttpRequestMessage req, TraceWriter log)\n{\n    log.Info($ Webhook was triggered! );\n\n    string jsonContent = await req.Content.ReadAsStringAsync();\n    dynamic data = JsonConvert.DeserializeObject(jsonContent);\n\n    if (data.url == null || data.fileName == null) {\n        return req.CreateResponse(HttpStatusCode.BadRequest, new {\n            error =  Please pass all properties in the input object \n        });\n    }\n\n    var connectionString = ConfigurationManager.ConnectionStrings[ MS_TableConnectionString ].ConnectionString;\n    log.Info($ Using Connection String {connectionString} );\n\n    var dbId = Guid.NewGuid().ToString( N );\n    try\n    {\n        using (var sqlConnection = new SqlConnection(connectionString))\n        {\n            using (var sqlCommand = sqlConnection.CreateCommand())\n            {\n                log.Info( Initiating SQL Connection );\n                sqlConnection.Open();\n\n                log.Info( Executing SQL Statement );\n                sqlCommand.CommandText = $ INSERT INTO [dbo].[Videos] ([Id], [Deleted], [Filename], [VideoUri]) VALUES ('{dbId}', 0, '{data.fileName}', '{data.url}') ;\n                var rowsAffected = sqlCommand.ExecuteNonQuery();\n                log.Info($ {rowsAffected} rows inserted. );\n\n                sqlConnection.Close();\n            }\n        }\n    }\n    catch (Exception ex)\n    {\n        return req.CreateResponse(HttpStatusCode.BadRequest, new {\n            error = ex.Message\n        });\n    }\n\n    return req.CreateResponse(HttpStatusCode.OK, new {\n        greeting = $ {dbId} \n    });\n}  This inserts a record into the Videos table with the filename and URI specified.  I can now add this function to the YES column in my Logic App by specifying the following body in the Code view:       body : {\n         fileName :  @{triggerOutputs()['headers']['x-ms-file-name']} ,\n         url :  @{body('publish-asset')['pathUrl']} \n    }  There are three URLs that are returned by the previous step:   pathUrl  is the path to the assets (but not the filename).  smoothUrl  is the real-time streaming endpoint.  playerUrl  is a web-page with an embedded player.   You need to use the corresponding URL for your implementation.  If you are unsure, then store both the pathUrl and smoothUrl in the database (which will require a modification of the model).  The  playerUrl  can be computed on the client if you need it.  Before we can try this pipeline out, the other resources must be specified as Application Settings inside the Function App:   AMSAccount  is the name of your Media Services resource.  AMSKey  is the primary key for your Media Services resource.  MediaServicesStorageAccountName  is the name of your Azure Storage resource.  MediaServicesStorageAccountKey  is the primary key for your Azure Storage resource.  MS_TableConnectionString  is the connection string to your video database (from your App Service).   Once these are set, you are ready to test your logic app.  Go to the Logic Apps Designer and click  Run .  This allows you to monitor the progress of the workflow live.  Then drop a video file in the /IncomingVideos folder of your OneDrive connection and watch the process.  It's likely that something will go wrong the first time.  In the case of Azure Functions, the error will be displayed:   If the error is in a Logic App provided trigger, then consult the Diagnostics and Log search menu items under Monitoring.  For the Azure Functions triggers, it is more informative to consult the error logs in the Function App.  Open the  Monitor  tab to check the logs for the latest run.  Also, you can create a test run with the appropriate input object and/or place more logging in the Azure Function.  I faked this error by removing the  AMSAccount  application setting.  If you have copied the source code directly, it's likely that any errors will be in the app settings.   Insert-to-Database Failures  The  insert-to-database  function and the logic app will fail because the database is not created until the first request by a client.  You can either pre-create the database or use the client that is developed before you try out the encoding pipeline.", 
            "title": "The Encoding Pipeline"
        }, 
        {
            "location": "/chapter7/media/#the-video-mobile-app", 
            "text": "Now that the backend has been brought online and we can populate it with videos, it's time to turn our attention to the client app.  I've started with an app very similar to the Task List.  The models are slightly different (since the data set is different), but ultimately, the app provides a list of videos to play.  You can find  the starting project on GitHub .   Use the starting point to create the database  In the last section, I mentioned that one of the functions would not work because the database was not created until the first client request.  You can use the starting point for the project to create the necessary database.  Create all the backend resources, then run the client to create the database, then test out the encoding pipeline.   You can integrate any video player that supports a streaming endpoint, and there are several to choose from - each with their own complexities for integration.  For simplicity, I am going to integrate the Azure Media Services Player - a web-based streaming media player which I will integrate into a  WebView  within the page.  Let's start by hooking up a new view in the shared project.  The new view is called  Pages/VideoDetail.xaml :  ?xml version= 1.0  encoding= utf-8  ?  ContentPage\n    x:Class= VideoApp.Pages.VideoDetail \n    xmlns= http://xamarin.com/schemas/2014/forms \n    xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n     StackLayout \n         WebView\n            x:Name= browser \n            HorizontalOptions= FillAndExpand \n            VerticalOptions= FillAndExpand  / \n     /StackLayout  /ContentPage   This page creates a  WebView  that occupies the entire page.  There is a backing C# source file as well that implements the viewer:  using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing Xamarin.Forms;\nusing Xamarin.Forms.Xaml;\n\nnamespace VideoApp.Pages\n{\n    [XamlCompilation(XamlCompilationOptions.Compile)]\n    public partial class VideoDetail : ContentPage\n    {\n        public VideoDetail (Models.Video video)\n        {\n            InitializeComponent ();\n\n            var htmlSource = new HtmlWebViewSource();\n            var sourceInfo = @  html \n     head \n         title Test /title \n         link href= https://amp.azure.net/libs/amp/1.8.3/skins/amp-default/azuremediaplayer.min.css  rel= stylesheet \n         script src= https://amp.azure.net/libs/amp/1.8.3/azuremediaplayer.min.js /script \n      /head \n     body \n         video id= azuremediaplayer  class= azuremediaplayer amp-default-skin amp-big-play-centered  tabindex= 0 /video \n         script \nvar myOptions = {\n     nativeControlsForTouch : false,\n    controls: true,\n    autoplay: true,\n    width:  640 ,\n    height:  400 ,\n};\nmyPlayer = amp( azuremediaplayer , myOptions);\nmyPlayer.src([\n    {\n    src:  {Binding Source} ,\n    type:  application/vnd.ms-sstr+xml \n    }\n]);\n         /script \n     /body  /html  ;\n            htmlSource.Html = sourceInfo.Replace( {Binding Source} , video.VideoUri);\n            browser.Source = htmlSource;\n        }\n    }\n}  The HTML and Javascript libraries that I use here are provided by Azure Media Services.  You can find them as follows:   Start  Azure Media Services Explorer  and connect to your Media Services account.  Right-click a published video, then select  Playback  -   with Azure Media Player .  A web-page will open.  Click  Code  -   Get Player Code  (under the video).   The code will be displayed:   I replaced the  src  object with something I can string-replace later on.  The player is completely cross-platform.  You do, however, have to specify the height and width.  One of the advantages of using a native control is that it will set the height and width for you.", 
            "title": "The Video Mobile App"
        }, 
        {
            "location": "/chapter7/media/#wrap-up", 
            "text": "Obviously, this isn't the prettiest app that has been produced.  However, it is functional and it demonstrates the basic capabilities.  We don't have to stop where we did, however.   We could use  Cognitive Services  to extract the audio track and submit to an  Azure Search  facility.  We could then allow searching of the Azure Search facility to come up with a list of videos that match based on the audio track.  This is, quite frankly, something that still amazes me and something that we could not do without the Azure Cloud.  The  Cognitive Services integration  is available as the advanced option in the sample for the media processing workflow.  We could adjust the  insert-into-database  Function to extract metadata from the MP4 file.  The MP4 file contains a whole host of information.  This can be inserted into the database so you can display it.  You could also add a JSON file in the upload to provide additional content that is inserted into the database.  We could provide ratings and other controls on the list.  This can be stored on the mobile backend as well to provide information to other users.   Video media is one of those areas of development that is complex to understand and implement, but has so much potential in the mobile space.", 
            "title": "Wrap Up"
        }, 
        {
            "location": "/chapter8/developing/", 
            "text": "The Development Environment\n\n\nIn these last two chapters, I want to go over some of the complexities of developing mobile applications when there is a\ncloud-enabled backend.  Working with cloud services presents its own special challenges, especially when you use the\nfeatures of the provider.  In the case of Azure App Service, this means that dealing with App Service Authentication\nand App Service Push requires some special configuration.\n\n\nWorking with Azure Mobile Apps Locally\n\n\nIn general, you can use Azure Mobile Apps locally by running inside a local IIS container.  If you are using\nVisual Studio, then this is set up for you.  However, you will need to handle the SQL database connection.\nYou have two choices for this.  Firstly, you can use a SQL Azure instance and just point your server to that\ninstance.  Secondly, you can install SQL Express and use that instance instead.  Both are perfectly viable\noptions.  I like to use SQL Express in early development, then switch over to a SQL Azure instance as I get\ncloser to deployment.  Switching over when you are close to deployment enables to you detect any problems\nin upgrades or the use of encrypted channels.\n\n\nConfiguring SQL Express for Local Development\n\n\nStart by downloading and installing the \nMicrosoft SQL Server Express\n edition.  Azure Mobile Apps will work\nwith just about any recent SQL Server Express edition.  I personally recommend the\n\nSQL Server 2016 SP1 Express\n edition.\n\n\nThe process by which SQL Server Express is installed varies by edition and version, so keep these tips in mind.\n\n\n\n\nAlways elect the custom installation option.\n\n\nYou need the Database Engine and the \nManagement Tools\n (possibly via separate download)\n\n\nYou do not need reporting or integration services.\n\n\nUse \nMixed mode\n for authentication and set an \nsa\n password.\n\n\nIf possible, place the data directories on a different disk.\n\n\n\n\nOnce you have installed the database engine and management tools, you will need to create a user that has\npermissions to create databases:\n\n\n\n\nRun the SQL Server Management Studio and connect to your local SQL Express instance.\n\n\nEnsure the \nSQL Server and Windows Authentication mode\n is checked in the \nProperties\n \n \nSecurity\n page.\n\n\nExpand \nSecurity\n \n \nLogins\n in the \nObject Explorer\n.\n\n\nRight-click the \nLogins\n node and select \nNew login...\n.\n    a.  Enter a unique login name\n    b.  Select \nSQL Server authentication\n.\n    c.  Enter a password, then enter the same password in \nConfirm password\n.\n    d.  Click \nOK\n.\n\n\nRight-click on your new login and select \nProperties\n.\n    a.  Click \nServer Roles\n under \nSelect a page\n.\n    b.  Check the box next to the \ndbcreator\n role.\n    c. Click \nOK\n.\n\n\nClose the SQL Server Management Studio.\n\n\n\n\nEnsure you record the username and password you selected.  You may need to assign additional server roles\nor permissions depending on your specific database requirements.  Your connection string will need to look\nlike this:\n\n\nServer=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word;\n\n\n\n\nReplace the user ID and password with the user ID and password you just created.  The database will be created\nfor you, so ensure it is rememberable.  To set the connection string, you will need to edit the \nWeb.config\n\nfile.  At around line 12, you will see the default connection string.  Simply replace it with your SQL Express\nconnection string:\n\n\n  \nconnectionStrings\n\n    \nadd name=\nMS_TableConnectionString\n\n        connectionString=\nData Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word;\n\n        providerName=\nSystem.Data.SqlClient\n /\n\n  \n/connectionStrings\n\n\n\n\n\nYou should now be able to press F5 on your server and run it locally.\n\n\nConfiguring SQL Azure for Local Development\n\n\nI have already discussed creating a SQL Azure instance.  By default, however, the SQL Azure instance\ncan only be used by other Azure resources as a security measure.  There is a firewall that limits\nconnectivity from the Internet to your database.  To run your server locally while connecting to the\nSQL Azure instance, you need to do two things:\n\n\n\n\nOpen the firewall for connections from your IP address.\n\n\nUpdate the connection string in your servers \nWeb.config\n file.\n\n\n\n\nFrom your development system (the workstation that you will be using to run the service locally),\nopen a browser and log into the \nAzure portal\n.  You will note that there are two resources for your\nSQL database - a server and a database.  Open the resource for the SQL server, then:\n\n\n\n\nClick the \nFirewall\n menu option.\n\n\nClick \n+ Add client IP\n.\n\n\nClick \nSave\n, then \nOK\n.\n\n\n\n\nIf you are using a different workstation to run the service, then you can enter an explicit IP\naddress.  To get the connection strings:\n\n\n\n\nFrom the SQL Server \nOverview\n page, click the SQL database.\n\n\nFrom the SQL database \nOverview\n page, click \nShow database connection strings\n.\n\n\nCopy the ADO.NET (SQL authentication) connection string.\n\n\n\n\nThis will need to be copied into the \nWeb.config\n in the same way as the SQL Express version (above).\nYou will need to replace the \n{your_username}\n and \n{your_password}\n strings with the username and\npassword of your SQL server.  If you can't remember them, look in your App Service - they are available\nin the configured connection string under \nApplication Settings\n.\n\n\nOnce this done, you will be able to press F5 on your server and run it locally.\n\n\nHandling Cloud Services while Developing Locally\n\n\nAt this point, you have done plenty of test runs in the cloud and you have also run both your client\nand the server locally.  However,t here are certain problems when you are running the server locally\nbut using the Azure App Service resources for pieces of your application.  The two main resources that\nmobile developers use in Azure App Service are authentication and registration for push notifications.\nThe basics for these facilities were covered in \nChapter 2\n and \nChapter 5\n respectively.  There\nare some additional code requirements when you want to run your main server locally for debugging.\n\n\nHandling Authentication\n\n\nWhen dealing with authentication, you want the majority of your requests to go to your main server, but you\nwant your authentication requests to go to the Azure App Service you are using for development.  To set\nthis scenario up, you need to configure your mobile client to connect to your Azure App Service and receive\na token, then continue using the local server.  You also need to configure your local server so that it\ncan decode the token.\n\n\nThe Azure Mobile Apps Client SDK has a setting called \nAlternateLoginUrl\n.  You can specify the URL of the\nAzure App Server in this if you are operating against a local server.  For example:\n\n\nnamespace TaskList.Helpers\n{\n    public static class Locations\n    {\n//#if DEBUG\n//        public static readonly string AppServiceUrl = \nhttp://localhost:17568/\n;\n//        public static readonly string AlternateLoginHost = \nhttps://the-book.azurewebsites.net\n;\n//#else\n        public static readonly string AppServiceUrl = \nhttps://the-book.azurewebsites.net\n;\n        public static readonly string AlternateLoginHost = null;\n//#endif\n    }\n}\n\n\n\n\nIn this snippet (from the Chapter 2 test project), I get the \nAlternateLoginHost\n set only if I define the\nDEBUG setting in my build properties.  When I create the \nMobileServiceClient\n, I can use this logic to set\nthe \nAlternateLoginUrl\n:\n\n\n    public AzureCloudService()\n    {\n        client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler());\n\n        if (Locations.AlternateLoginHost != null)\n            client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n    }\n\n\n\n\nTo set the DEBUG setting, I can right-click on the project and select \nProperties\n, then select \nBuild\n in\nthe resulting window.  The  \nDefine DEBUG constant\n is available in this window:\n\n\n\n\nSince my location is only set when the DEBUG constant is set, this allows me to switch between the two modes.\nThere are other ways to switch between the two modes if you also want to support debug mode against an Azure\nApp Service based server.  One of my favorites is to have an application setting that enabled \"local mode\".\nWhen I check that, I can specify a local URL.  At that point, the \nAppServerUrl\n is set to what I enter and\nthe \nAlternateLoginUrl\n becomes the original value of the \nAppServiceUrl\n - effectively swapping to the local\nserver but preserving the auth configuration.\n\n\nNow that I've enabled the mobile client for local development, I also need to enable the local server.  The\nlocal server should have the following code already within the \nApp_Start\\Startup.MobileApps.cs\n file:\n\n\n    MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n    if (string.IsNullOrEmpty(settings.HostName))\n    {\n        app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n        {\n            // This middleware is intended to be used locally for debugging. By default, HostName will\n            // only have a value when running in an App Service application.\n            SigningKey = ConfigurationManager.AppSettings[\nSigningKey\n],\n            ValidAudiences = new[] { ConfigurationManager.AppSettings[\nValidAudience\n] },\n            ValidIssuers = new[] { ConfigurationManager.AppSettings[\nValidIssuer\n] },\n            TokenHandler = config.GetAppServiceTokenHandler()\n        });\n    }\n\n\n\n\nThis code says \"if the service is not running within Azure App Service, then validate the authentication\ntokens using the following information\".  We need the information provided from the Azure App Service that\nis producing the tokens.  To obtain this information, we need to peek into \nKudu\n - a backend service\nfor examining the information that Azure App Service uses to run the service.\n\n\n\n\nLog on to the \nAzure portal\n.\n\n\nSelect your Azure App Service.\n\n\nIn the left-hand menu for the server, type \nAdvanced Tools\n in the search box.\n\n\nClick \nAdvanced Tools\n, then click \nGo\n.\n\n\nIn the new window, click \nEnvironment\n, then \nEnvironment variables\n.\n\n\nScroll down (or use search) to find \nWEBSITE_AUTH_SIGNING_KEY\n.\n\n\n\n\nYour required values are:\n\n\n\n\nSigningKey\n is the \nWEBSITE_AUTH_SIGNING_KEY\n above.\n\n\nValidAudience\n and \nValidIssuer\n are both of the form \nhttps://{your-sitename}/\n.\n\n\n\n\nOnce you have these two values, you can insert them into the \nWeb.config\n file for local development:\n\n\n  \nappSettings\n\n    \nadd key=\nPreserveLoginUrl\n value=\ntrue\n /\n\n    \nadd key=\nMS_SigningKey\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nEMA_RuntimeUrl\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nMS_NotificationHubName\n value=\nOverridden by portal settings\n /\n\n    \nadd key=\nSigningKey\n value=\n4CFD0AA0148455E90F076F58D401377DA9D2443CC5C2E64E36C2D5FC96A71E9C\n /\n\n    \nadd key=\nValidAudience\n value=\nhttps://the-book.azurewebsites.net/\n /\n\n    \nadd key=\nValidIssuer\n value=\nhttps://the-book.azurewebsites.net/\n /\n\n  \n/appSettings\n\n\n\n\n\nThese values will be overridden by the portal settings when operating within the Azure App Service.  You\ncan now run the local server and the mobile client locally while authenticating via the Azure-based App\nService.\n\n\nHandling Push Notifications\n\n\nIn chapter 5, I espoused using \nInvokeApiAsync\n()\n to register for push notifications, mostly because it\nenabled me to register with a Notification Hubs \nInstallation\n object rather than just my registration ID.\nThis enabled me, for example, to register with tags for better audience selection.  If you look at how the\n\nInvokeApiAsync\n method is implemented:\n\n\n    private async Task\nstring\n InternalInvokeApiAsync(string apiName, string content, HttpMethod method, IDictionary\nstring, string\n parameters, MobileServiceFeatures features, CancellationToken cancellationToken = default(CancellationToken))\n    {\n        method = method ?? defaultHttpMethod;\n        if (parameters != null \n parameters.Count \n 0)\n        {\n            features |= MobileServiceFeatures.AdditionalQueryParameters;\n        }\n\n        MobileServiceHttpResponse response = await this.HttpClient.RequestAsync(method, CreateAPIUriString(apiName, parameters), this.CurrentUser, content, false, features: features, cancellationToken: cancellationToken);\n        return response.Content;\n    }\n\n\n\n\nYou will note it's just a HTTP request with some serialization and deserialization around it.  I can\nreplicate this by creatign a new \nHttpClient\n object and publishing the same serialized content.  I'm\nnot concerned with the actual response, so this is fairly easy.  The adjusted code might look something\nlike this:\n\n\n    try\n    {\n        var registrationId = GcmClient.GetRegistrationId(RootView);\n        var client = new HttpClient(new Uri(Locations.AlternateLoginHost ?? Locations.AppServiceUrl));\n\n        var installation = new DeviceInstallation\n        {\n            InstallationId = client.InstallationId,\n            Platform = \ngcm\n,\n            PushChannel = registrationId\n        };\n        installation.Tags.Add(\ntopic:Sports\n);\n        installation.Templates.Add(\ngenericTemplate\n, new PustTemplate\n        {\n            Body = \n{\\\ndata\\\n:{\\\nmessage\\\n:\\\n$(messageParam)\\\n}}\n\n        });\n\n        // Register with NH\n        var json = JsonConvert.SerializeObject(installation).ToString();\n        var response = await client.PutAsync(\n            $\n/push/installations/{client.InstallationId}\n,\n            new StringContent(json, Encoding.UTF8, \napplication/json\n));\n        if (!response.IsSuccessStatusCode)\n        {\n            throw new MobileServiceInvalidOperationException(\nInvalid Response\n, null, response);\n        }\n    }\n    catch (Exception ex)\n    {\n        Log.Error(\nDroidPlatformProvider\n, $\nCould not register with NH: {ex.Message}\n);\n    }\n\n\n\n\nThis uses a new HttpClient that is created for the purposes of registering with Notification Hubs.  I have\nto handle all the serialization of the objects myself because I'm not operating in the confines of the\nAzure Mobile Apps SDK any more.  The code differences here are minimal.\n\n\nAnother way to deal with this is to stub the \n/push/installations\n endpoint in your server during local\ndebugging with a standard WebAPI controller.  This allows you to verify that the data being sent to the\nserver is correct (i.e. your client code is correct) without actually registering for push notifications.\nYou will not receive push notifications until you publish your service to Azure App Service and properly\nconfigure the App Service Push feature.  Your controller will be over-ridden by App Service Push when you\ndo publish to Azure App Service.\n\n\nDebugging your Cloud Mobile Backend\n\n\nWhen you are running the server locally, you can easily set breakpoints, view the state of the service,\nand emit log events that are captured in the Output window of your Visual Studio IDE.  When you are\nrunning the server within Azure App Service, you can still do all these things.  However, they are slower\nbecause you are accessing the service over the Internet rather than on your local machine.  They also\ntake a little bit more to set up.\n\n\nDiagnostic Logging\n\n\nDiagnostic logging is my preferred way of isolating problems.  This is mostly because you can see historical\ncontext - what happened in what order - and you don't need to have a debugger attached to the process to\nuse it.  This means that diagnostic logging should be provided even after your service has move to\nproduction.  However, diagnostic logging does require you to decide what information is important and\nto emit events associated with that information.\n\n\nYou can configure diagnostic logging within the \nAzure portal\n or within Visual Studio.  To configure\ndiagnostic logging in the \nAzure portal\n:\n\n\n\n\nOpen the \nAzure portal\n and select your Azure App Service.\n\n\nIn the left-hand menu search box, type \nDiagnostic logs\n.\n\n\nClick the \nDiagnostic Logs\n menu item.\n\n\nIn the Diagnostic logs page, turn on the following items:\n    a. \nApplication Logging (Filesystem)\n (also set the \nLevel\n to \nVerbose\n)\n    b. \nWeb server logging\n (select \nFile System\n)\n    c. \nDetailed error messages\n\n    d. \nFailed request tracing\n\n\nClick \nSave\n\n\n\n\nYou can view the stream of logs from the \nAzure portal\n by selecting your Azure App Service and then using\nthe \nLog Stream\n menu option in the left-hand menu.  The logs are also available as text files via Kudu\n(\nAdvanced Tools\n in the left-hand menu) or via FTP (the location and login ID are provided on the\n\nDiagnostic Logs\n screen).\n\n\nTo configure diagnostic logging within Visual Studio:\n\n\n\n\nUse \nView\n \n \nServer Explorer\n to show the Server Explorer.\n\n\nExpand the \nAzure\n node, then \nApp Service\n, then your resource group.\n\n\nRight-click your Azure App Service and select \nView Settings\n.\n\n\nIn the Configuration panel, select the following:\n    a.  \nWeb Server Logging\n = On\n    b.  \nDetailed Error Messages\n = On\n    c.  \nFailed Request Tracing\n = On\n    d.  \nApplication Logging (File System)\n = Verbose\n\n\nClick \nSave\n.\n\n\n\n\nYou can view the stream of logs from Visual Studio by right-clicking your Azure App Service within the\n\nServer Explorer\n, then selecting \nView Streaming Logs\n.  The logs appear in a section of the \nOutput\n\nwindow called \nMicrosoft Azure Logs - {your-site}\n.  The first message received will indicate that\nvisual Studio is connected to the log-streaming service.\n\n\nUsing the Visual Studio Debugger\n\n\nEven though diagnostic logging will probably be your go-to troubleshooting tool in the long run, there are\ntimes when you need to set a breakpoint and step through the code during development.  In my experience, this\nhappens quite a lot, especially when there are differences between how the server reacts when running locally\nvs. when running within Azure.\n\n\nBefore you attach a debugger to an App Service running within Azure, you need to turn on remote debugging.\nThis should \nNOT\n be done on a production service.  You can turn on remote debugging within Visual Studio\nor in the Azure portal.  For Visual Studio:\n\n\n\n\nUse \nView\n \n \nServer Explorer\n to show the Server Explorer.\n\n\nExpand the \nAzure\n node, then \nApp Service\n, then your resource group.\n\n\nRight-click your Azure App Service and select \nView Settings\n.\n\n\nIn the Configuration panel, set \nRemote Debugging\n = On.\n\n\nClick \nSave\n.\n\n\n\n\nIn the \nAzure portal\n, the remote debugging option is set through the \nApplication settings\n menu option.\nYou will need to set the Remote Visual Studio version to be the same version as your copy of Visual Studio.\n\n\nTo debug your service, deploy a Debug version of your server.  Then right-click the App Service within the\n\nServer Explorer\n and select \nAttach Debugger\n.  You can now set breakpoints and step through the code.\n\n\n\n\nServer Hangups\n\n\nUsing the debugger is also a great way to ensure that your mobile client handles slow links and service\ninterruptions gracefully.  As developers, we tend to have pretty good cellular coverage and fast connections\nto the Internet, but your users probably won't be in that situation.  Set a breakpoint on the \nGetAll()\n\nmethod of your controller, trigger a synchronization, and then let the service just sit there at the\nbreakpoint.  Your mobile client will time out and you can see the effect of a bad connection.\n\n\n\n\nOne of the other significant things you can do with your server when remote debugging is turned on is to profile\nthe server.  Profiling is a good way of determining performance bottlenecks in your code.  Azure App Service\nsupports \nremote profiling within Visual Studio\n for the server component.  Use the \nXamarin Profiler\n to\nprofile your mobile application.", 
            "title": "The Development Environment"
        }, 
        {
            "location": "/chapter8/developing/#the-development-environment", 
            "text": "In these last two chapters, I want to go over some of the complexities of developing mobile applications when there is a\ncloud-enabled backend.  Working with cloud services presents its own special challenges, especially when you use the\nfeatures of the provider.  In the case of Azure App Service, this means that dealing with App Service Authentication\nand App Service Push requires some special configuration.", 
            "title": "The Development Environment"
        }, 
        {
            "location": "/chapter8/developing/#working-with-azure-mobile-apps-locally", 
            "text": "In general, you can use Azure Mobile Apps locally by running inside a local IIS container.  If you are using\nVisual Studio, then this is set up for you.  However, you will need to handle the SQL database connection.\nYou have two choices for this.  Firstly, you can use a SQL Azure instance and just point your server to that\ninstance.  Secondly, you can install SQL Express and use that instance instead.  Both are perfectly viable\noptions.  I like to use SQL Express in early development, then switch over to a SQL Azure instance as I get\ncloser to deployment.  Switching over when you are close to deployment enables to you detect any problems\nin upgrades or the use of encrypted channels.", 
            "title": "Working with Azure Mobile Apps Locally"
        }, 
        {
            "location": "/chapter8/developing/#configuring-sql-express-for-local-development", 
            "text": "Start by downloading and installing the  Microsoft SQL Server Express  edition.  Azure Mobile Apps will work\nwith just about any recent SQL Server Express edition.  I personally recommend the SQL Server 2016 SP1 Express  edition.  The process by which SQL Server Express is installed varies by edition and version, so keep these tips in mind.   Always elect the custom installation option.  You need the Database Engine and the  Management Tools  (possibly via separate download)  You do not need reporting or integration services.  Use  Mixed mode  for authentication and set an  sa  password.  If possible, place the data directories on a different disk.   Once you have installed the database engine and management tools, you will need to create a user that has\npermissions to create databases:   Run the SQL Server Management Studio and connect to your local SQL Express instance.  Ensure the  SQL Server and Windows Authentication mode  is checked in the  Properties     Security  page.  Expand  Security     Logins  in the  Object Explorer .  Right-click the  Logins  node and select  New login... .\n    a.  Enter a unique login name\n    b.  Select  SQL Server authentication .\n    c.  Enter a password, then enter the same password in  Confirm password .\n    d.  Click  OK .  Right-click on your new login and select  Properties .\n    a.  Click  Server Roles  under  Select a page .\n    b.  Check the box next to the  dbcreator  role.\n    c. Click  OK .  Close the SQL Server Management Studio.   Ensure you record the username and password you selected.  You may need to assign additional server roles\nor permissions depending on your specific database requirements.  Your connection string will need to look\nlike this:  Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word;  Replace the user ID and password with the user ID and password you just created.  The database will be created\nfor you, so ensure it is rememberable.  To set the connection string, you will need to edit the  Web.config \nfile.  At around line 12, you will see the default connection string.  Simply replace it with your SQL Express\nconnection string:     connectionStrings \n     add name= MS_TableConnectionString \n        connectionString= Data Server=127.0.0.1; Database=mytestdatabase; User Id=azuremobile; Password=T3stPa55word; \n        providerName= System.Data.SqlClient  / \n   /connectionStrings   You should now be able to press F5 on your server and run it locally.", 
            "title": "Configuring SQL Express for Local Development"
        }, 
        {
            "location": "/chapter8/developing/#configuring-sql-azure-for-local-development", 
            "text": "I have already discussed creating a SQL Azure instance.  By default, however, the SQL Azure instance\ncan only be used by other Azure resources as a security measure.  There is a firewall that limits\nconnectivity from the Internet to your database.  To run your server locally while connecting to the\nSQL Azure instance, you need to do two things:   Open the firewall for connections from your IP address.  Update the connection string in your servers  Web.config  file.   From your development system (the workstation that you will be using to run the service locally),\nopen a browser and log into the  Azure portal .  You will note that there are two resources for your\nSQL database - a server and a database.  Open the resource for the SQL server, then:   Click the  Firewall  menu option.  Click  + Add client IP .  Click  Save , then  OK .   If you are using a different workstation to run the service, then you can enter an explicit IP\naddress.  To get the connection strings:   From the SQL Server  Overview  page, click the SQL database.  From the SQL database  Overview  page, click  Show database connection strings .  Copy the ADO.NET (SQL authentication) connection string.   This will need to be copied into the  Web.config  in the same way as the SQL Express version (above).\nYou will need to replace the  {your_username}  and  {your_password}  strings with the username and\npassword of your SQL server.  If you can't remember them, look in your App Service - they are available\nin the configured connection string under  Application Settings .  Once this done, you will be able to press F5 on your server and run it locally.", 
            "title": "Configuring SQL Azure for Local Development"
        }, 
        {
            "location": "/chapter8/developing/#handling-cloud-services-while-developing-locally", 
            "text": "At this point, you have done plenty of test runs in the cloud and you have also run both your client\nand the server locally.  However,t here are certain problems when you are running the server locally\nbut using the Azure App Service resources for pieces of your application.  The two main resources that\nmobile developers use in Azure App Service are authentication and registration for push notifications.\nThe basics for these facilities were covered in  Chapter 2  and  Chapter 5  respectively.  There\nare some additional code requirements when you want to run your main server locally for debugging.", 
            "title": "Handling Cloud Services while Developing Locally"
        }, 
        {
            "location": "/chapter8/developing/#handling-authentication", 
            "text": "When dealing with authentication, you want the majority of your requests to go to your main server, but you\nwant your authentication requests to go to the Azure App Service you are using for development.  To set\nthis scenario up, you need to configure your mobile client to connect to your Azure App Service and receive\na token, then continue using the local server.  You also need to configure your local server so that it\ncan decode the token.  The Azure Mobile Apps Client SDK has a setting called  AlternateLoginUrl .  You can specify the URL of the\nAzure App Server in this if you are operating against a local server.  For example:  namespace TaskList.Helpers\n{\n    public static class Locations\n    {\n//#if DEBUG\n//        public static readonly string AppServiceUrl =  http://localhost:17568/ ;\n//        public static readonly string AlternateLoginHost =  https://the-book.azurewebsites.net ;\n//#else\n        public static readonly string AppServiceUrl =  https://the-book.azurewebsites.net ;\n        public static readonly string AlternateLoginHost = null;\n//#endif\n    }\n}  In this snippet (from the Chapter 2 test project), I get the  AlternateLoginHost  set only if I define the\nDEBUG setting in my build properties.  When I create the  MobileServiceClient , I can use this logic to set\nthe  AlternateLoginUrl :      public AzureCloudService()\n    {\n        client = new MobileServiceClient(Locations.AppServiceUrl, new AuthenticationDelegatingHandler());\n\n        if (Locations.AlternateLoginHost != null)\n            client.AlternateLoginHost = new Uri(Locations.AlternateLoginHost);\n    }  To set the DEBUG setting, I can right-click on the project and select  Properties , then select  Build  in\nthe resulting window.  The   Define DEBUG constant  is available in this window:   Since my location is only set when the DEBUG constant is set, this allows me to switch between the two modes.\nThere are other ways to switch between the two modes if you also want to support debug mode against an Azure\nApp Service based server.  One of my favorites is to have an application setting that enabled \"local mode\".\nWhen I check that, I can specify a local URL.  At that point, the  AppServerUrl  is set to what I enter and\nthe  AlternateLoginUrl  becomes the original value of the  AppServiceUrl  - effectively swapping to the local\nserver but preserving the auth configuration.  Now that I've enabled the mobile client for local development, I also need to enable the local server.  The\nlocal server should have the following code already within the  App_Start\\Startup.MobileApps.cs  file:      MobileAppSettingsDictionary settings = config.GetMobileAppSettingsProvider().GetMobileAppSettings();\n\n    if (string.IsNullOrEmpty(settings.HostName))\n    {\n        app.UseAppServiceAuthentication(new AppServiceAuthenticationOptions\n        {\n            // This middleware is intended to be used locally for debugging. By default, HostName will\n            // only have a value when running in an App Service application.\n            SigningKey = ConfigurationManager.AppSettings[ SigningKey ],\n            ValidAudiences = new[] { ConfigurationManager.AppSettings[ ValidAudience ] },\n            ValidIssuers = new[] { ConfigurationManager.AppSettings[ ValidIssuer ] },\n            TokenHandler = config.GetAppServiceTokenHandler()\n        });\n    }  This code says \"if the service is not running within Azure App Service, then validate the authentication\ntokens using the following information\".  We need the information provided from the Azure App Service that\nis producing the tokens.  To obtain this information, we need to peek into  Kudu  - a backend service\nfor examining the information that Azure App Service uses to run the service.   Log on to the  Azure portal .  Select your Azure App Service.  In the left-hand menu for the server, type  Advanced Tools  in the search box.  Click  Advanced Tools , then click  Go .  In the new window, click  Environment , then  Environment variables .  Scroll down (or use search) to find  WEBSITE_AUTH_SIGNING_KEY .   Your required values are:   SigningKey  is the  WEBSITE_AUTH_SIGNING_KEY  above.  ValidAudience  and  ValidIssuer  are both of the form  https://{your-sitename}/ .   Once you have these two values, you can insert them into the  Web.config  file for local development:     appSettings \n     add key= PreserveLoginUrl  value= true  / \n     add key= MS_SigningKey  value= Overridden by portal settings  / \n     add key= EMA_RuntimeUrl  value= Overridden by portal settings  / \n     add key= MS_NotificationHubName  value= Overridden by portal settings  / \n     add key= SigningKey  value= 4CFD0AA0148455E90F076F58D401377DA9D2443CC5C2E64E36C2D5FC96A71E9C  / \n     add key= ValidAudience  value= https://the-book.azurewebsites.net/  / \n     add key= ValidIssuer  value= https://the-book.azurewebsites.net/  / \n   /appSettings   These values will be overridden by the portal settings when operating within the Azure App Service.  You\ncan now run the local server and the mobile client locally while authenticating via the Azure-based App\nService.", 
            "title": "Handling Authentication"
        }, 
        {
            "location": "/chapter8/developing/#handling-push-notifications", 
            "text": "In chapter 5, I espoused using  InvokeApiAsync ()  to register for push notifications, mostly because it\nenabled me to register with a Notification Hubs  Installation  object rather than just my registration ID.\nThis enabled me, for example, to register with tags for better audience selection.  If you look at how the InvokeApiAsync  method is implemented:      private async Task string  InternalInvokeApiAsync(string apiName, string content, HttpMethod method, IDictionary string, string  parameters, MobileServiceFeatures features, CancellationToken cancellationToken = default(CancellationToken))\n    {\n        method = method ?? defaultHttpMethod;\n        if (parameters != null   parameters.Count   0)\n        {\n            features |= MobileServiceFeatures.AdditionalQueryParameters;\n        }\n\n        MobileServiceHttpResponse response = await this.HttpClient.RequestAsync(method, CreateAPIUriString(apiName, parameters), this.CurrentUser, content, false, features: features, cancellationToken: cancellationToken);\n        return response.Content;\n    }  You will note it's just a HTTP request with some serialization and deserialization around it.  I can\nreplicate this by creatign a new  HttpClient  object and publishing the same serialized content.  I'm\nnot concerned with the actual response, so this is fairly easy.  The adjusted code might look something\nlike this:      try\n    {\n        var registrationId = GcmClient.GetRegistrationId(RootView);\n        var client = new HttpClient(new Uri(Locations.AlternateLoginHost ?? Locations.AppServiceUrl));\n\n        var installation = new DeviceInstallation\n        {\n            InstallationId = client.InstallationId,\n            Platform =  gcm ,\n            PushChannel = registrationId\n        };\n        installation.Tags.Add( topic:Sports );\n        installation.Templates.Add( genericTemplate , new PustTemplate\n        {\n            Body =  {\\ data\\ :{\\ message\\ :\\ $(messageParam)\\ }} \n        });\n\n        // Register with NH\n        var json = JsonConvert.SerializeObject(installation).ToString();\n        var response = await client.PutAsync(\n            $ /push/installations/{client.InstallationId} ,\n            new StringContent(json, Encoding.UTF8,  application/json ));\n        if (!response.IsSuccessStatusCode)\n        {\n            throw new MobileServiceInvalidOperationException( Invalid Response , null, response);\n        }\n    }\n    catch (Exception ex)\n    {\n        Log.Error( DroidPlatformProvider , $ Could not register with NH: {ex.Message} );\n    }  This uses a new HttpClient that is created for the purposes of registering with Notification Hubs.  I have\nto handle all the serialization of the objects myself because I'm not operating in the confines of the\nAzure Mobile Apps SDK any more.  The code differences here are minimal.  Another way to deal with this is to stub the  /push/installations  endpoint in your server during local\ndebugging with a standard WebAPI controller.  This allows you to verify that the data being sent to the\nserver is correct (i.e. your client code is correct) without actually registering for push notifications.\nYou will not receive push notifications until you publish your service to Azure App Service and properly\nconfigure the App Service Push feature.  Your controller will be over-ridden by App Service Push when you\ndo publish to Azure App Service.", 
            "title": "Handling Push Notifications"
        }, 
        {
            "location": "/chapter8/developing/#debugging-your-cloud-mobile-backend", 
            "text": "When you are running the server locally, you can easily set breakpoints, view the state of the service,\nand emit log events that are captured in the Output window of your Visual Studio IDE.  When you are\nrunning the server within Azure App Service, you can still do all these things.  However, they are slower\nbecause you are accessing the service over the Internet rather than on your local machine.  They also\ntake a little bit more to set up.", 
            "title": "Debugging your Cloud Mobile Backend"
        }, 
        {
            "location": "/chapter8/developing/#diagnostic-logging", 
            "text": "Diagnostic logging is my preferred way of isolating problems.  This is mostly because you can see historical\ncontext - what happened in what order - and you don't need to have a debugger attached to the process to\nuse it.  This means that diagnostic logging should be provided even after your service has move to\nproduction.  However, diagnostic logging does require you to decide what information is important and\nto emit events associated with that information.  You can configure diagnostic logging within the  Azure portal  or within Visual Studio.  To configure\ndiagnostic logging in the  Azure portal :   Open the  Azure portal  and select your Azure App Service.  In the left-hand menu search box, type  Diagnostic logs .  Click the  Diagnostic Logs  menu item.  In the Diagnostic logs page, turn on the following items:\n    a.  Application Logging (Filesystem)  (also set the  Level  to  Verbose )\n    b.  Web server logging  (select  File System )\n    c.  Detailed error messages \n    d.  Failed request tracing  Click  Save   You can view the stream of logs from the  Azure portal  by selecting your Azure App Service and then using\nthe  Log Stream  menu option in the left-hand menu.  The logs are also available as text files via Kudu\n( Advanced Tools  in the left-hand menu) or via FTP (the location and login ID are provided on the Diagnostic Logs  screen).  To configure diagnostic logging within Visual Studio:   Use  View     Server Explorer  to show the Server Explorer.  Expand the  Azure  node, then  App Service , then your resource group.  Right-click your Azure App Service and select  View Settings .  In the Configuration panel, select the following:\n    a.   Web Server Logging  = On\n    b.   Detailed Error Messages  = On\n    c.   Failed Request Tracing  = On\n    d.   Application Logging (File System)  = Verbose  Click  Save .   You can view the stream of logs from Visual Studio by right-clicking your Azure App Service within the Server Explorer , then selecting  View Streaming Logs .  The logs appear in a section of the  Output \nwindow called  Microsoft Azure Logs - {your-site} .  The first message received will indicate that\nvisual Studio is connected to the log-streaming service.", 
            "title": "Diagnostic Logging"
        }, 
        {
            "location": "/chapter8/developing/#using-the-visual-studio-debugger", 
            "text": "Even though diagnostic logging will probably be your go-to troubleshooting tool in the long run, there are\ntimes when you need to set a breakpoint and step through the code during development.  In my experience, this\nhappens quite a lot, especially when there are differences between how the server reacts when running locally\nvs. when running within Azure.  Before you attach a debugger to an App Service running within Azure, you need to turn on remote debugging.\nThis should  NOT  be done on a production service.  You can turn on remote debugging within Visual Studio\nor in the Azure portal.  For Visual Studio:   Use  View     Server Explorer  to show the Server Explorer.  Expand the  Azure  node, then  App Service , then your resource group.  Right-click your Azure App Service and select  View Settings .  In the Configuration panel, set  Remote Debugging  = On.  Click  Save .   In the  Azure portal , the remote debugging option is set through the  Application settings  menu option.\nYou will need to set the Remote Visual Studio version to be the same version as your copy of Visual Studio.  To debug your service, deploy a Debug version of your server.  Then right-click the App Service within the Server Explorer  and select  Attach Debugger .  You can now set breakpoints and step through the code.   Server Hangups  Using the debugger is also a great way to ensure that your mobile client handles slow links and service\ninterruptions gracefully.  As developers, we tend to have pretty good cellular coverage and fast connections\nto the Internet, but your users probably won't be in that situation.  Set a breakpoint on the  GetAll() \nmethod of your controller, trigger a synchronization, and then let the service just sit there at the\nbreakpoint.  Your mobile client will time out and you can see the effect of a bad connection.   One of the other significant things you can do with your server when remote debugging is turned on is to profile\nthe server.  Profiling is a good way of determining performance bottlenecks in your code.  Azure App Service\nsupports  remote profiling within Visual Studio  for the server component.  Use the  Xamarin Profiler  to\nprofile your mobile application.", 
            "title": "Using the Visual Studio Debugger"
        }, 
        {
            "location": "/chapter8/testing/", 
            "text": "Testing your Mobile Application\n\n\nThere is nothing that causes more problems than when a developer works on testing.  Testing a cross-platform client-server application across all the permutations that are possible is hard work.  You will spend more time on developing tests than on writing code.  Much of what is asked, however, is not required.  That is primarily because most people want to test the entire stack.  There are generally minimal custom code in the backend, so that can significantly reduce the amount of tests you write.\n\n\nIn this section, we will look at what it takes to do unit tests for your mobile backend and the mobile app, together with an end-to-end testing capability that allows you to test your application on many devices at once.\n\n\nTesting your Mobile Backend\n\n\nMost of the code within the mobile backend is pulled from libraries - ASP.NET, Entity Framework and Azure Mobile Apps.  These libraries are already tested before release and there is not much you can do about bugs other than reporting them (although Azure Mobile Apps does accept fixes as well).  As a result, you should concentrate your testing on the following areas:\n\n\n\n\nFilters, Transforms and Actions associated with your table controllers.\n\n\nCustom APIs.\n\n\n\n\nYou should also do \"end-to-end\" testing.  This is where you use UI testing to test both the client and the server at the same time.   End to end testing is a much better test of the overall functionality of your server.\n\n\nIn addition, your mobile backend will come under a lot of strain after you go to production.  You should plan on a load test prior to each major release in a staging environment that is identical to your production environment.  We'll cover this later in the book.\n\n\nUnit Testing\n\n\nLet's take a simple example of an app that we developed back in Chapter 3.  We used the data connections to develop a personal todo store - one in which the users ID is associated with each submitted record and the user could only see their own records.  The table controller looked like the following:\n\n\nnamespace Backend.Controllers\n{\n    public class TodoItemController : TableController\nTodoItem\n\n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager\nTodoItem\n(context, Request, enableSoftDelete: true);\n        }\n\n        public string UserId =\n ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n        public void ValidateOwner(string id)\n        {\n            var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault\nTodoItem\n();\n            if (result == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.NotFound);\n            }\n        }\n\n        // GET tables/TodoItem\n        public IQueryable\nTodoItem\n GetAllTodoItems()\n        {\n            return Query().PerUserFilter(UserId);\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult\nTodoItem\n GetTodoItem(string id)\n        {\n            return new SingleResult\nTodoItem\n(Lookup(id).Queryable.PerUserFilter(UserId));\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task\nTodoItem\n PatchTodoItem(string id, Delta\nTodoItem\n patch)\n        {\n            ValidateOwner(id);\n            return UpdateAsync(id, patch);\n        }\n\n        // POST tables/TodoItem\n        public async Task\nIHttpActionResult\n PostTodoItem(TodoItem item)\n        {\n            item.UserId = UserId;\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute(\nTables\n, new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id)\n        {\n            ValidateOwner(id);\n            return DeleteAsync(id);\n        }\n    }\n}\n\n\n\n\nIn addition, we have a LINQ extension method for handling the \nPerUserFilter\n:\n\n\nusing Backend.DataObjects;\nusing System.Linq;\n\nnamespace Backend.Extensions\n{\n    public static class PerUserFilterExtension\n    {\n        public static IQueryable\nTodoItem\n PerUserFilter(this IQueryable\nTodoItem\n query, string userid)\n        {\n            return query.Where(item =\n item.UserId.Equals(userid));\n\n        }\n    }\n}\n\n\n\n\nIn my minimalist testing suggestion, I would test the following:\n\n\n\n\nThe LINQ Extension \nPerUserFilter\n.\n\n\nThe \nUserId\n property.\n\n\nThe \nValidateOwner\n method.\n\n\n\n\nThe other methods are straight out of the standard table controller.  I would defer unit testing of these until the end-to-end tests.  Unit tests should be short and should be idempotent.  The test should be able to be run multiple times and always return the same result.  Since our service is defined to be run out of a stateful SQL database, it cannot be defined to be idempotent.  However, the individual parts we are operating can be idempotent.\n\n\nUnit tests are generally defined to be a separate project within the Visual Studio solution.  By convention, they are named by appending \n.Tests\n to the project they are testing.  My project is called \nBackend\n, so the test project is called \nBackend.Tests\n.  To create the test project:\n\n\n\n\nOpen the solution in Visual Studio.\n\n\nRight-click the solution, choose \nAdd\n -\n \nNew Project...\n.\n\n\nSelect \nInstalled\n \n \nVisual C#\n \n \nTest\n in the project type tree.\n\n\nSelect \nxUnit Test Project\n as the project type.\n\n\nEnter \nBackend.Tests\n as the name, then click \nOK\n. \n\n\n\n\n\n\nxUnit vs. MSTest vs. Others\n\n\nMost version of Visual Studio support a specific type of test called \nMSTest\n.  However, Visual Studio 2017 has integrated \nxUnit\n testing as well.  xUnit is cross-platform whereas MSTest is PC only.  I will be using xUnit for this project.  If you are using a version of Visual Studio earlier than VS2017, you will not have the xUnit Test Project available.  However, you can \nsimulate the same project type manually\n.  In addition, there are \nother test frameworks\n available.  We will only be covering xUnit here.\n\n\n\n\nGenerally, copy the folder format from the main project to the test project.  For example, the \nPerUserFilterExtension.cs\n file is in an \nExtensions\n folder within the main project.  I'm going to create an \nExtensions\n folder within the test project and create a \nPerUserFilterExtensionTests.cs\n file with the tests in it.  To create the tests:\n\n\n\n\nRight-click the \nExtensions\n folder, and select \nAdd\n -\n \nNew Item...\n.\n\n\nSelect \nInstalled\n \n \nVisual C# Items\n \n \nTest\n in the project type tree.\n\n\nSelect \nxUnit Test\n, and enter \nPerUserFilterExtensionTests.cs\n as the name.\n\n\nClick \nAdd\n.\n\n\n\n\n\n\nAdd your Project under Test as a Reference\n\n\nYou will need to add your project under test (in this case, the \nBackend\n project) as a reference to the test project.\n\n\n\n\nYou will get this code generated:\n\n\nusing System;\nusing System.Linq;\nusing Xunit;\n\nnamespace Backend.Tests.Extensions\n{\n    public class PerUserFilterExtensionTests\n    {\n        [Fact]\n        public void TestMethod1()\n        {\n        }\n    }\n}\n\n\n\n\nWe are going to replace the \nTestMethod1()\n method with our unit tests.  XUnit tests are designated with the \n[Fact]\n attribute.  You do some work on the class to test specific conditions, then assert that the results are valid.  In the case of this class, for instance, we want to test that the result is correct under the following conditions:\n\n\n\n\nA valid string is provided.\n\n\nA zero-length string is provided.\n\n\nNull is provided.\n\n\n\n\nUnder no conditions should the extension method throw an exception.  That means three tests, coded thusly:\n\n\nusing Backend.DataObjects;\nusing Backend.Extensions;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing Xunit;\n\nnamespace Backend.Tests.Extensions\n{\n    public class PerUserFilterExtensionTests\n    {\n        [Fact]\n        public void UserId_Is_Valid()\n        {\n            List\nTodoItem\n items = new List\nTodoItem\n\n            {\n                new TodoItem { UserId = \ntest\n, Text = \nTask 1\n, Complete = false },\n                new TodoItem { UserId = \ntest2\n, Text = \nTask 2\n, Complete = true },\n                new TodoItem { UserId = \ntest\n, Text = \nTask 3\n, Complete = false }\n            };\n\n            var result = items.AsQueryable\nTodoItem\n().PerUserFilter(\ntest\n);\n\n            Assert.NotNull(result);\n            Assert.Equal(2, result.Count());\n        }\n\n        [Fact]\n        public void UserId_Is_Empty()\n        {\n            List\nTodoItem\n items = new List\nTodoItem\n\n            {\n                new TodoItem { UserId = \ntest\n, Text = \nTask 1\n, Complete = false },\n                new TodoItem { UserId = \ntest2\n, Text = \nTask 2\n, Complete = true },\n                new TodoItem { UserId = \ntest\n, Text = \nTask 3\n, Complete = false }\n            };\n\n            var result = items.AsQueryable\nTodoItem\n().PerUserFilter(String.Empty);\n\n            Assert.NotNull(result);\n            Assert.Equal(0, result.Count());\n        }\n\n        [Fact]\n        public void UserId_Is_Null()\n        {\n            List\nTodoItem\n items = new List\nTodoItem\n\n            {\n                new TodoItem { UserId = \ntest\n, Text = \nTask 1\n, Complete = false },\n                new TodoItem { UserId = \ntest2\n, Text = \nTask 2\n, Complete = true },\n                new TodoItem { UserId = \ntest\n, Text = \nTask 3\n, Complete = false }\n            };\n\n            var result = items.AsQueryable\nTodoItem\n().PerUserFilter(null);\n\n            Assert.NotNull(result);\n            Assert.Equal(0, result.Count());\n        }\n    }\n}\n\n\n\n\n\n\nUse the same .NET Framework Version\n\n\nYou will note that your tests will not compile at this point.  That is because the server is dependent on .NET Framework 4.6 and the test project is created with .NET Framework 4.5.  Both test and main project must be configured to use the same version of the .NET Framework.  Right-click the test project, select \nProperties\n, then change the version of the .NET Framework to match your main project.  Save and re-build your test project.\n\n\n\n\nVisual Studio has a couple of methods of running the tests.  Visual Studio 2017 has in-built support for the xUnit test runner.  You may have to download an extension or run them manually in earlier versions of Visual Studio.  My favorite way of running the tests is to open the Test Explorer using \nTest\n -\n \nWindows\n -\n \nTest Explorer\n, then click \nRun All\n.  You can then right-click the Test Explorer tab and select \nFloat\n to float it as a window.  This allows you to enlarge the window so you can see the tests after they have run:\n\n\n\n\nAs you can see, my tests all passed.  I can run these tests as many times as necessary as they do not depend on external requirements.  This is not generally the case with table controllers.  The table controller takes a dependency on a domain manager (most normally, the \nEntityDomainManager\n).  The \nEntityDomainManager\n is configured to use a database via a connection string.  Thus, we need to do things differently for testing table controllers even if we only test the unique functionality.  \n\n\nLet's take a look at the tests for the \nUserId\n property.  The \nUserId\n property contains the contents of the \nNameIdentifier\n claim.  My tests for this are:\n\n\n\n\nA correct set of claims are provided.\n\n\nAn incomplete set of claims (without a NameIdentifier) are provided.\n\n\nNo claims are provided.\n\n\n\n\nThe first and last are the typical authenticated and anonymous access tests.  The first should provide the username in the NameIdentifier, and the latter should throw an error.  The middle test is an important one for us.  What do you want to happen if the user is authenticated, but the NameIdentifier claim was not provided?  It's bad form for us to return a 500 Internal Server Error, even though that would be appropriate here.  Instead I want to assume that the user id is blank so that everything keeps on working.  (One can argue that this is not correct either!)\n\n\n\n\nInstall the same NuGet packages\n\n\nUnlike the scaffolded project for Azure Mobile Apps or ASP.NET MVC, no additional packages are added to the test project, which means you will need to figure out which packages you need to simulate the requirements for the test.  Don't guess.  Look at the packages that are in your project under test and duplicate them.  Right-click the solution and select \nManage NuGet Packages\n to get a good idea of what your test package is missing.  Under the \nInstalled\n list, you can tell what packages are required and which projects have them installed.\n\n\n\n\nMock the \nClaimsIdentity\n to test authentication:\n\n\nusing System.Security.Claims;\n\nnamespace Backend.Tests.Utilities\n{\n    public class TestPrincipal : ClaimsPrincipal\n    {\n        public TestPrincipal(params Claim[] claims) : base(new TestIdentity(claims))\n        {\n        }\n    }\n\n    public class TestIdentity : ClaimsIdentity\n    {\n        public TestIdentity(params Claim[] claims) : base(claims)\n        {\n        }\n    }\n}\n\n\n\n\nMy (incorrect - deliberately) test looks like the following:\n\n\nusing Backend.Controllers;\nusing Backend.Tests.Utilities;\nusing System.Security.Claims;\nusing System.Threading;\nusing Xunit;\n\nnamespace Backend.Tests.Controllers\n{\n    public class TodoItemControllerTests\n    {\n        [Fact]\n        public void UserId_With_Correct_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = new TestPrincipal(\n                new Claim(\nname\n, \ntestuser\n),\n                new Claim(\nsub\n, \nfoo\n)\n            );\n            var result = controller.UserId;\n\n            Assert.NotNull(result);\n            Assert.Equal(\ntestuser\n, result);\n        }\n\n        [Fact]\n        public void UserId_With_Incomplete_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = new TestPrincipal(\n                new Claim(\nsub\n, \nfoo\n)\n            );\n            var result = controller.UserId;\n\n            Assert.Null(result);\n        }\n\n        [Fact]\n        public void UserId_With_Null_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = null;\n            var ex = Assert.Throws\nHttpResponseException\n(() =\n { var result = controller.UserId; });\n            Assert.Equal(HttpStatusCode.Unauthorized, ex.Response.StatusCode);\n        }\n    }\n}\n\n\n\n\nThe \nUserId_With_Null_Claims\n test is an interesting recipe for testing that the right exception is thrown.  In this case, I expect the methods to return a 401 Unauthorized response to the client.  Of course, the \n[Authorize]\n tag will do this for my well before my code is hit, but it's good to be accurate.\n\n\nIf I run the tests, I get the following:\n\n\n\n\nWhat I want to do is run that test again, but attach a debugger.  To do this, set a breakpoint on the property in the \nTodoItemController\n.  Then right-click the failing test and select \nDebug Selected Tests\n.  This runs the test with a debugger connected.  The breakpoint you set will be hit and you will be able to inspect the code state while it is running.  The first test is failing because \nClaimTypes.NameIdentifier\n is not \"name\".  I re-wrote the test as follows:\n\n\n    [Fact]\n    public void UserId_With_Correct_Claims()\n    {\n        var controller = new TodoItemController();\n        controller.User = new TestPrincipal(\n            new Claim(ClaimTypes.NameIdentifier, \ntestuser\n),\n            new Claim(\nsub\n, \nfoo\n)\n        );\n        var result = controller.UserId;\n\n        Assert.NotNull(result);\n        Assert.Equal(\ntestuser\n, result);\n    }\n\n\n\n\nThis test will now pass.  The other two tests are actually the result of incorrect code.  I've adjusted the code accordingly:\n\n\n    public string UserId\n    {\n        get\n        {\n            if (User == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.Unauthorized);\n            }\n            var principal = User as ClaimsPrincipal;\n            Claim cl = principal.Claims.FirstOrDefault(c =\n c.Type == ClaimTypes.NameIdentifier);\n            return cl?.Value;\n        }\n    }\n\n\n\n\nThis is a little longer than the original one-liner, but it's more accurate.  This means that when I've forgotten what this particular method does in six months time, it will still do the right thing in all conditions.\n\n\n\n\nUse Test-Driven Development\n\n\nThere is a whole school of thought on how to develop using testing as the driver known as \nTest Driven Development\n or TDD.  In this school of thought, you write the tests first, ensuring you have 100% of the cases covered.  Your code is correct when the tests pass.  This method provides for very rapid development, but you do spend most of your time developing tests rather than code. \n\n\n\n\nThe other big class of testing to do is against custom APIs.   You can test these the same way.  For example, the standard scaffolding for an Azure Mobile Apps server contains a \nValuesController.cs\n, which I have modified:\n\n\nusing System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\n\nnamespace Backend.Controllers\n{\n    // Use the MobileAppController attribute for each ApiController you want to use\n    // from your mobile clients\n    [MobileAppController]\n    public class ValuesController : ApiController\n    {\n        // GET api/values\n        public string Get(string user)\n        {\n            return $\nHello {user}!\n;\n        }\n    }\n}\n\n\n\n\nI can test this with the following:\n\n\nusing Backend.Controllers;\nusing Xunit;\n\nnamespace Backend.Tests.Controllers\n{\n    public class ValuesControllerTests\n    {\n        [Fact]\n        public void Get_Name_Works()\n        {\n            var controller = new ValuesController();\n            var result = controller.Get(\nadrian\n);\n            Assert.Equal(\nHello adrian!\n, result);\n        }\n    }\n}\n\n\n\n\nAs with all other testing, ensure you think about all the things that could happen here, and test them all.  Ensure that the appropriate response is always returned and that you are never leave your server or a connected client in a bad state.  A big example of this in the case of mobile apps:  If a response is meant to be a JSON encoded version of an object on your client, ensure it can be deserialized to that object under all conditions.\n\n\nTesting your Mobile Client\n\n\nTesting your mobile client will generally be a multi-part affair:\n\n\n\n\nImplement mock data services and test the UI in isolation.\n\n\nImplement unit tests for the non-UI components.\n\n\nDo end-to-end tests to ensure both client and server work together.\n\n\n\n\nUnit tests for non-UI code is the same as the server-side code.  You need to write the tests in a unit test framework like \nxUnit\n or \nMSTest\n.  Use \nTest-driven development\n to ensure that your code is living up to its contract.\n\n\nUsing Mock Data Services\n\n\nUnfortunately, setting up repeatable unit tests becomes increasingly difficult in a client-server application such as a cloud-enabled mobile app.  For these aspects, you want to mock the data services.  If you have followed along from the beginning, we've actually done a lot of the hard work for this.\n\n\n\n\nCreate an Interface that represents the interface to the data service.\n\n\nCreate a concrete implementation of that interface.\n\n\nUse a dependency injection service to inject the concrete implementation.\n\n\n\n\nThe whole idea here is that changing just one line of code will enable you to update from the mock implementation to the cloud implementation.  This allows you to develop the UI independently of the backend communication code, and allows you to do repeatable UI tests later on.\n\n\nLet's take a look at an example.  In \nmy Chapter8 project\n, I've got the Xamarin Forms application from \nthe very first chapter\n.  In the shared \nTaskList\n project, there is an \nAbstractions\n folder that contains the definitions for \nICloudService\n:\n\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable\nT\n GetTable\nT\n() where T : TableData;\n    }\n}\n\n\n\n\nThere is also a definition for \nICloudTable\n:\n\n\nusing System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable\nT\n where T : TableData\n    {\n        Task\nT\n CreateItemAsync(T item);\n        Task\nT\n ReadItemAsync(string id);\n        Task\nT\n UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n        Task\nICollection\nT\n ReadAllItemsAsync();\n    }\n}\n\n\n\n\nThe important part of this is this.  The only place where the concrete edition, \nAzureCloudService()\n, is mentioned is in the \nApp.cs\n file:\n\n\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}\n\n\n\n\nEverywhere else uses the \nICloudService\n interface and does not mention the concrete version.  The application sets up the cloud service and every other class uses it.  This allows us to set up a mock cloud service as follows.  First, let's define the \nMockCloudService\n:\n\n\nusing System.Collections.Generic;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class MockCloudService : ICloudService\n    {\n        public Dictionary\nstring, object\n tables = new Dictionary\nstring, object\n();\n\n        public ICloudTable\nT\n GetTable\nT\n() where T : TableData\n        {\n            var tableName = typeof(T).Name;\n            if (!tables.ContainsKey(tableName))\n            {\n                var table = new MockCloudTable\nT\n();\n                tables[tableName] = table;\n            }\n            return (ICloudTable\nT\n)tables[tableName];\n        }\n    }\n}\n\n\n\n\n\nIt's very similar to the \nAzureCloudService\n class, but there is no \nMobileServiceClient\n.  Instead, we store the cloud table instances in a dictionary to ensure successive calls to \nGetTable\n()\n return the same singleton reference.  We aren't using the backend service.  Similarly, we use a \nDictionary\n to hold the items instead of the backend service in the \nMockCloudTable\n class:\n\n\nusing Microsoft.WindowsAzure.MobileServices;\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\n\n#pragma warning disable CS1998 // Async method lacks 'await' operators and will run synchronously\n\nnamespace TaskList.Services\n{\n    public class MockCloudTable\nT\n : ICloudTable\nT\n where T : TableData\n    {\n        private Dictionary\nstring, T\n items = new Dictionary\nstring, T\n();\n        private int currentVersion = 1;\n\n        public async Task\nT\n CreateItemAsync(T item)\n        {\n            item.Id = Guid.NewGuid().ToString(\nN\n);\n            item.CreatedAt = DateTimeOffset.Now;\n            item.UpdatedAt = DateTimeOffset.Now;\n            item.Version = ToVersionString(currentVersion++);\n            items.Add(item.Id, item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            if (item.Id == null)\n            {\n                throw new NullReferenceException();\n            }\n            if (items.ContainsKey(item.Id))\n            {\n                items.Remove(item.Id);\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException(\nNot Found\n, null, null);\n            }\n        }\n\n        public async Task\nICollection\nT\n ReadAllItemsAsync()\n        {\n            List\nT\n allItems = new List\nT\n(items.Values);\n            return allItems;\n        }\n\n        public async Task\nT\n ReadItemAsync(string id)\n        {\n            if (items.ContainsKey(id))\n            {\n                return items[id];\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException(\nNot Found\n, null, null);\n            }\n        }\n\n        public async Task\nT\n UpdateItemAsync(T item)\n        {\n            if (item.Id == null)\n            {\n                throw new NullReferenceException();\n            }\n            if (items.ContainsKey(item.Id))\n            {\n                item.UpdatedAt = DateTimeOffset.Now;\n                item.Version = ToVersionString(currentVersion++);\n                items[item.Id] = item;\n                return item;\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException(\nNot Found\n, null, null);\n            }\n        }\n\n        private byte[] ToVersionString(int i)\n        {\n            byte[] b = BitConverter.GetBytes(i);\n            string str = Convert.ToBase64String(b);\n            return Encoding.ASCII.GetBytes(str);\n        }\n    }\n}\n\n\n\n\nThe mock service is instantiated within the \nApp.cs\n file:\n\n\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}\n\n\n\n\nFinally, I need to actually define \nUSE_MOCK_SERVICES\n somewhere.  Right-click the project and select \nProperties\n.  Click \nBuild\n.  Add the \nUSE_MOCK_SERVICES\n to the \nConditional compilation symbols\n (which is a semi-colon separated list).  Save the properties then rebuild the project you modified.  You can run this version without any backend at all.  It will not persist the data, but that's the point of mock data services.\n\n\n\n\nUse a new Configuration\n\n\nAnother, more advanced, way of accomplishing this is to set up a new configuration.  You can see the configuration is \"Active (Debug)\".  You can add another configuration to this list called \"Mock Services\" by using the \nBuild\n \n \nConfiguration Manager...\n.  When you select that configuration, the mock services will automatically be brought in.\n\n\n\n\nUI Testing\n\n\nThe mock services are a tool to enable UI unit testing.  UI testing is unit testing for your UI.  These are small tests that are executed on a real device and check to see if your main UI flows work as expected.  There are actually a few ways of creating tests. I'm going to produce a simple test.  In the test, I will simulate clicking on the entry button and ensuring that the task list page is produced.  This test can then be run against one or more devices.  Let's start by creating a \nTaskList.Tests\n project:\n\n\n\n\nRight-click the solution and select \nAdd\n -\n \nNew Project...\n\n\nIn the \nInstalled\n \n \nVisual C#\n \n \nCross-Platform\n node of the tree, select \nUI Test App (Xamarin.UITest | Cross-Platform)\n.\n\n\nGive it a snappy name, like \nTaskList.Tests\n, then click \nOK\n.\n\n\nWait for the project to be created.\n\n\nRight-click the \nReferences\n node in the newly created project and select \nAdd Reference...\n.\n\n\nClick \nProjects\n in the left hand side-bar.\n\n\nClick \nTaskList.Android\n.  Ensure there is a checked box next to the TaskList.Android project.\n\n\nClick \nOK\n.\n\n\n\n\nWe are only going to test the Android edition of the project in this walkthrough, mostly because I do most of my work on a PC.  The same methodology can be used for iOS, however.\n\n\n\n\nUse NUnit v2.x\n\n\nXamarin.UITest does not support NUnit 3.x - make sure you do not upgrade the NUnit framework beyond the latest v2.x release when updating your NuGet packages.\n\n\n\n\nThe project contains two source files - \nAppInitializer.cs\n and \nTests.cs\n.  This latter file is where we are going to write the tests.  The most efficient way of writing tests is to use the \nXamarin Test Recorder\n.  The workflow for using Xamarin Test Recorder is:\n\n\n\n\nIf needed, export the mobile client so that it can be used on a device.\n\n\nStart Xamarin Test Recorder\n\n\nSpecify the application to be tested and the device to run the aplication on.\n\n\nInteract with the application.  The Test Recorder will create a C# test method.\n\n\nIncorporate the test into a Xamarin UITest project.\n\n\n\n\nWe already have the UITest project, so let's walk through the process of creating a simple test case for the process of creating a new task.  Open the \nTests.cs\n file.  Note the \"lightning\" icon next to each \n[TestFixture]\n attribute:\n\n\n\n\nSwitch to a Release build in the configuration manager (or on the toolbar), then right-click the \nTaskList.Android\n project and select \nBuild\n to build the project.  Now that you have a working app, click the lightning icon next to the \nTestFixture\n for the Android platform, then select \nRecord new test\n -\n \nBuild TaskList.Android project\n.  \n\n\n\n\nThis will start your project in the selected device - normally an emulator.  It will also create a \nNewTest()\n method.  In the emulator, click on the Login button, followed by Add New Item, followed by Save.  When you are done, switch back to the Visual Studio instance, click on the spanner next at the bottom of the file and select \nStop Recording\n. The following code will be generated:\n\n\n    [Test]\n    public void NewTest()\n    {\n        app.Tap(x =\n x.Text(\nLogin\n));\n        app.Tap(x =\n x.Text(\nAdd New Item\n));\n        app.Tap(x =\n x.Text(\nSave\n));\n    }\n\n\n\n\nYou can take screen shots, by adding \napp.Screenshot(\"description\");\n in between each step:\n\n\n    [Test]\n    public void NewTest()\n    {\n        app.Tap(x =\n x.Text(\nLogin\n));\n        app.Screenshot(\nLogged in - initial list of items\n);\n        app.Tap(x =\n x.Text(\nAdd New Item\n));\n        app.Screenshot(\nEmpty detail record\n);\n        app.Tap(x =\n x.Text(\nSave\n));\n        app.Screenshot(\nBack at list of items\n);\n    }\n\n\n\n\n\n\nPlatform Support\n\n\nXamarin Test Recorder supports iOS and Android.  You cannot record an iOS UITest on the PC.  If you want to use one platform for recording tests, use a Mac.\n\n\n\n\nYou can run this as long as you adjust the \nAppInitializer.cs\n file as follows:\n\n\n    if (platform == Platform.Android)\n    {\n        return ConfigureApp\n            .Android\n            .InstalledApp(\ntasklist.tasklist\n)\n            .StartApp();\n    }\n\n\n\n\nReplace the \ntasklist.tasklist\n with the package name of your app.  You can retrieve this in the \nProperties\n \n \nAndroid Manifest\n page for the \nTaskList.Android\n app.  Use the \nTest Explorer\n to run the test.  You will see that the clicks are performed as expected.\n\n\nLet's take this a little further.  Let's say that rather than just clicking a few times, we wanted to ensure that the text box was filled with the text that is expected via an assertion.  To uniquely identify a view, we need to add an \nAutomationId\n to the view.  Adjust the \nTaskDetail.xaml\n file in the shared project as follows:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage\n    x:Class=\nTaskList.Pages.TaskDetail\n\n    xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n    xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n    Title=\n{Binding Title}\n\n    \nContentPage.Content\n\n        \nStackLayout Padding=\n10\n Spacing=\n10\n\n            \nLabel Text=\nWhat should I be doing?\n /\n\n            \nEntry AutomationId=\nentrytext\n Text=\n{Binding Item.Text}\n /\n\n            \nLabel Text=\nCompleted?\n /\n\n            \nSwitch IsToggled=\n{Binding Item.Complete}\n /\n\n            \nStackLayout VerticalOptions=\nCenterAndExpand\n /\n\n            \nStackLayout Orientation=\nVertical\n VerticalOptions=\nEnd\n\n                \nStackLayout HorizontalOptions=\nCenter\n Orientation=\nHorizontal\n\n                    \nButton\n                        BackgroundColor=\n#A6E55E\n\n                        Command=\n{Binding SaveCommand}\n\n                        Text=\nSave\n\n                        TextColor=\nWhite\n /\n\n                    \nButton\n                        BackgroundColor=\nRed\n\n                        Command=\n{Binding DeleteCommand}\n\n                        Text=\nDelete\n\n                        TextColor=\nWhite\n /\n\n                \n/StackLayout\n\n            \n/StackLayout\n\n        \n/StackLayout\n\n    \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nNote line 10.  I've explicitly added an \nAutomationId\n to the entry text.  I can now adjust the test to use this:\n\n\n    [Test]\n    public void NewTest()\n    {\n        app.Tap(x =\n x.Text(\nLogin\n));\n        app.Screenshot(\nLogged in - initial list of items\n);\n        app.Tap(x =\n x.Text(\nAdd New Item\n));\n        app.Screenshot(\nEmpty detail record\n);\n\n        AppResult[] results = app.Query(\nentrytext\n);\n        Assert.AreEqual(1, results.Length);\n        Assert.AreEqual(\nNew Item\n, results[0].Text);\n\n        app.Tap(x =\n x.Text(\nSave\n));\n        app.Screenshot(\nBack at list of items\n);\n    }\n\n\n\n\n\n\nRun tests on Visual Studio Mobile Center\n\n\nAlthough this process is good, it requires that you own a large number of devices and you manually test.  Visual Studio Mobile Center (introduced below) has a test facility that allows you to run the same tests on hundreds of real devices and get reports of failures.  Combine this with crash and analytics reporting and you will have a robust release testing mechanism for your product.\n\n\n\n\nYou can now create a complete set of tests:\n\n\n\n\nUnit tests for the custom code in the mobile backend.\n\n\nUnit tests with a mocked backend for the mobile client.\n\n\nUI tests with a mocked backend for the mobile client.\n\n\nEnd to End UI tests with a test backend and known dataset.\n\n\n\n\nWhen you are in the normal development cycle, you should be doing the first three on a very regular basis throughout the day.  You should perform end to end tests with the known test backend across a wide variety of devices before every release to the public app store.\n\n\nDistributing your Mobile Client to Beta Users\n\n\nThere are a number of services that will distribute your client applications to your beta users.  Since I am working primarily within Azure and the Microsoft family of products, I'm going to use \nVisual Studio Mobile Center\n.  As of writing, Visual Studio Mobile Center is in preview, but already supports a large number of highly desirable features.  One of these features is beta distribution.  To create an application:\n\n\n\n\nSign in to \nVisual Studio Mobile Center\n.  You will need to create an account if you have not already done so.\n\n\nClick \nAdd new\n -\n \nAdd new app\n.\n\n\nGive your app a name and select the OS (iOS or Android) and Platform (Xamarin). \n\n\nClick \nAdd new app\n at the bottom of the page.\n\n\n\n\n\n\nCreate two apps if you distribute both iOS and Android apps\n\n\nIf you are distributing both an iOS and an Android app, you will need to create two apps - one for the iOS edition and one for the Android edition.\n\n\n\n\nAt this point, the cloud infrastructure necessary to handle the mobile client app is created.  We can move on to the distribution phase.  We need to create a group and invite some beta users to our group before doing a distribution:\n\n\n\n\nClick the \nDistribute\n icon (it looks like three arrow pointing up if your menu is collapsed).\n\n\nClick \nNew Group\n.\n\n\nName your group something like \nBeta Testers\n.\n\n\nEnter a comma-delimited list of email addresses to invite users to join the group. \n\n\nClick \nCreate Group\n.\n\n\n\n\n\n\nCollect and register UDID for iOS Distribution\n\n\nAnother quirk of the iOS ecosystem.  You will need to \nregister the UDID\n for each iOS beta tester.  Apple does not allow software to be installed on non-registered devices except via the App Store.\n\n\n\n\nA group called \nCollaborators\n is created and maintained by default.  This contains a list of people whom you are collaborating with via Visual Studio Mobile Center.  The APK or IPA file must be generated first.\n\n\n\n\nOpen your solution in Visual Studio 2017.\n\n\nSelect the \nRelease\n, \nAny CPU\n, \nTaskList.Android\n configuration in the menu bar (for Android).\n\n\nRight-click the TaskList.Android project and select \nRebuild\n.\n\n\nOnce the build is complete, click \nBuild\n -\n \nArchive...\n.\n\n\nOnce the archive is created, click \nOpen Folder\n.\n\n\n\n\nThe process is similar for iOS.  You must build for a device and will only be able to distribute to registered devices.   Now that we have an APK file:\n\n\n\n\nReturn to the \nVisual Studio Mobile Center\n and select your app.\n\n\nClick \nDistribute\n in the left-hand sidebar.\n\n\nClick the group you created.\n\n\nClick \nDistribute new release\n.\n\n\nDrag and drop the APK file from the open folder to the landing area (or click upload and find your APK file).\n\n\nWait for the APK file to be uploaded.  A thin blue bar above the landing area is used to indicate progress.\n\n\nWhile you wait, type in some release note into the box provided.\n\n\nClick \nNext\n.\n\n\nClick \nNext\n again, as the group is already selected. \n\n\nClick \nDistribute\n.\n\n\n\n\nYour beta testers will now get an email telling them there is a new app available and how to download it.", 
            "title": "Testing your Application"
        }, 
        {
            "location": "/chapter8/testing/#testing-your-mobile-application", 
            "text": "There is nothing that causes more problems than when a developer works on testing.  Testing a cross-platform client-server application across all the permutations that are possible is hard work.  You will spend more time on developing tests than on writing code.  Much of what is asked, however, is not required.  That is primarily because most people want to test the entire stack.  There are generally minimal custom code in the backend, so that can significantly reduce the amount of tests you write.  In this section, we will look at what it takes to do unit tests for your mobile backend and the mobile app, together with an end-to-end testing capability that allows you to test your application on many devices at once.", 
            "title": "Testing your Mobile Application"
        }, 
        {
            "location": "/chapter8/testing/#testing-your-mobile-backend", 
            "text": "Most of the code within the mobile backend is pulled from libraries - ASP.NET, Entity Framework and Azure Mobile Apps.  These libraries are already tested before release and there is not much you can do about bugs other than reporting them (although Azure Mobile Apps does accept fixes as well).  As a result, you should concentrate your testing on the following areas:   Filters, Transforms and Actions associated with your table controllers.  Custom APIs.   You should also do \"end-to-end\" testing.  This is where you use UI testing to test both the client and the server at the same time.   End to end testing is a much better test of the overall functionality of your server.  In addition, your mobile backend will come under a lot of strain after you go to production.  You should plan on a load test prior to each major release in a staging environment that is identical to your production environment.  We'll cover this later in the book.", 
            "title": "Testing your Mobile Backend"
        }, 
        {
            "location": "/chapter8/testing/#unit-testing", 
            "text": "Let's take a simple example of an app that we developed back in Chapter 3.  We used the data connections to develop a personal todo store - one in which the users ID is associated with each submitted record and the user could only see their own records.  The table controller looked like the following:  namespace Backend.Controllers\n{\n    public class TodoItemController : TableController TodoItem \n    {\n        protected override void Initialize(HttpControllerContext controllerContext)\n        {\n            base.Initialize(controllerContext);\n            MobileServiceContext context = new MobileServiceContext();\n            DomainManager = new EntityDomainManager TodoItem (context, Request, enableSoftDelete: true);\n        }\n\n        public string UserId =  ((ClaimsPrincipal)User).FindFirst(ClaimTypes.NameIdentifier).Value;\n\n        public void ValidateOwner(string id)\n        {\n            var result = Lookup(id).Queryable.PerUserFilter(UserId).FirstOrDefault TodoItem ();\n            if (result == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.NotFound);\n            }\n        }\n\n        // GET tables/TodoItem\n        public IQueryable TodoItem  GetAllTodoItems()\n        {\n            return Query().PerUserFilter(UserId);\n        }\n\n        // GET tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public SingleResult TodoItem  GetTodoItem(string id)\n        {\n            return new SingleResult TodoItem (Lookup(id).Queryable.PerUserFilter(UserId));\n        }\n\n        // PATCH tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task TodoItem  PatchTodoItem(string id, Delta TodoItem  patch)\n        {\n            ValidateOwner(id);\n            return UpdateAsync(id, patch);\n        }\n\n        // POST tables/TodoItem\n        public async Task IHttpActionResult  PostTodoItem(TodoItem item)\n        {\n            item.UserId = UserId;\n            TodoItem current = await InsertAsync(item);\n            return CreatedAtRoute( Tables , new { id = current.Id }, current);\n        }\n\n        // DELETE tables/TodoItem/48D68C86-6EA6-4C25-AA33-223FC9A27959\n        public Task DeleteTodoItem(string id)\n        {\n            ValidateOwner(id);\n            return DeleteAsync(id);\n        }\n    }\n}  In addition, we have a LINQ extension method for handling the  PerUserFilter :  using Backend.DataObjects;\nusing System.Linq;\n\nnamespace Backend.Extensions\n{\n    public static class PerUserFilterExtension\n    {\n        public static IQueryable TodoItem  PerUserFilter(this IQueryable TodoItem  query, string userid)\n        {\n            return query.Where(item =  item.UserId.Equals(userid));\n\n        }\n    }\n}  In my minimalist testing suggestion, I would test the following:   The LINQ Extension  PerUserFilter .  The  UserId  property.  The  ValidateOwner  method.   The other methods are straight out of the standard table controller.  I would defer unit testing of these until the end-to-end tests.  Unit tests should be short and should be idempotent.  The test should be able to be run multiple times and always return the same result.  Since our service is defined to be run out of a stateful SQL database, it cannot be defined to be idempotent.  However, the individual parts we are operating can be idempotent.  Unit tests are generally defined to be a separate project within the Visual Studio solution.  By convention, they are named by appending  .Tests  to the project they are testing.  My project is called  Backend , so the test project is called  Backend.Tests .  To create the test project:   Open the solution in Visual Studio.  Right-click the solution, choose  Add  -   New Project... .  Select  Installed     Visual C#     Test  in the project type tree.  Select  xUnit Test Project  as the project type.  Enter  Backend.Tests  as the name, then click  OK .     xUnit vs. MSTest vs. Others  Most version of Visual Studio support a specific type of test called  MSTest .  However, Visual Studio 2017 has integrated  xUnit  testing as well.  xUnit is cross-platform whereas MSTest is PC only.  I will be using xUnit for this project.  If you are using a version of Visual Studio earlier than VS2017, you will not have the xUnit Test Project available.  However, you can  simulate the same project type manually .  In addition, there are  other test frameworks  available.  We will only be covering xUnit here.   Generally, copy the folder format from the main project to the test project.  For example, the  PerUserFilterExtension.cs  file is in an  Extensions  folder within the main project.  I'm going to create an  Extensions  folder within the test project and create a  PerUserFilterExtensionTests.cs  file with the tests in it.  To create the tests:   Right-click the  Extensions  folder, and select  Add  -   New Item... .  Select  Installed     Visual C# Items     Test  in the project type tree.  Select  xUnit Test , and enter  PerUserFilterExtensionTests.cs  as the name.  Click  Add .    Add your Project under Test as a Reference  You will need to add your project under test (in this case, the  Backend  project) as a reference to the test project.   You will get this code generated:  using System;\nusing System.Linq;\nusing Xunit;\n\nnamespace Backend.Tests.Extensions\n{\n    public class PerUserFilterExtensionTests\n    {\n        [Fact]\n        public void TestMethod1()\n        {\n        }\n    }\n}  We are going to replace the  TestMethod1()  method with our unit tests.  XUnit tests are designated with the  [Fact]  attribute.  You do some work on the class to test specific conditions, then assert that the results are valid.  In the case of this class, for instance, we want to test that the result is correct under the following conditions:   A valid string is provided.  A zero-length string is provided.  Null is provided.   Under no conditions should the extension method throw an exception.  That means three tests, coded thusly:  using Backend.DataObjects;\nusing Backend.Extensions;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing Xunit;\n\nnamespace Backend.Tests.Extensions\n{\n    public class PerUserFilterExtensionTests\n    {\n        [Fact]\n        public void UserId_Is_Valid()\n        {\n            List TodoItem  items = new List TodoItem \n            {\n                new TodoItem { UserId =  test , Text =  Task 1 , Complete = false },\n                new TodoItem { UserId =  test2 , Text =  Task 2 , Complete = true },\n                new TodoItem { UserId =  test , Text =  Task 3 , Complete = false }\n            };\n\n            var result = items.AsQueryable TodoItem ().PerUserFilter( test );\n\n            Assert.NotNull(result);\n            Assert.Equal(2, result.Count());\n        }\n\n        [Fact]\n        public void UserId_Is_Empty()\n        {\n            List TodoItem  items = new List TodoItem \n            {\n                new TodoItem { UserId =  test , Text =  Task 1 , Complete = false },\n                new TodoItem { UserId =  test2 , Text =  Task 2 , Complete = true },\n                new TodoItem { UserId =  test , Text =  Task 3 , Complete = false }\n            };\n\n            var result = items.AsQueryable TodoItem ().PerUserFilter(String.Empty);\n\n            Assert.NotNull(result);\n            Assert.Equal(0, result.Count());\n        }\n\n        [Fact]\n        public void UserId_Is_Null()\n        {\n            List TodoItem  items = new List TodoItem \n            {\n                new TodoItem { UserId =  test , Text =  Task 1 , Complete = false },\n                new TodoItem { UserId =  test2 , Text =  Task 2 , Complete = true },\n                new TodoItem { UserId =  test , Text =  Task 3 , Complete = false }\n            };\n\n            var result = items.AsQueryable TodoItem ().PerUserFilter(null);\n\n            Assert.NotNull(result);\n            Assert.Equal(0, result.Count());\n        }\n    }\n}   Use the same .NET Framework Version  You will note that your tests will not compile at this point.  That is because the server is dependent on .NET Framework 4.6 and the test project is created with .NET Framework 4.5.  Both test and main project must be configured to use the same version of the .NET Framework.  Right-click the test project, select  Properties , then change the version of the .NET Framework to match your main project.  Save and re-build your test project.   Visual Studio has a couple of methods of running the tests.  Visual Studio 2017 has in-built support for the xUnit test runner.  You may have to download an extension or run them manually in earlier versions of Visual Studio.  My favorite way of running the tests is to open the Test Explorer using  Test  -   Windows  -   Test Explorer , then click  Run All .  You can then right-click the Test Explorer tab and select  Float  to float it as a window.  This allows you to enlarge the window so you can see the tests after they have run:   As you can see, my tests all passed.  I can run these tests as many times as necessary as they do not depend on external requirements.  This is not generally the case with table controllers.  The table controller takes a dependency on a domain manager (most normally, the  EntityDomainManager ).  The  EntityDomainManager  is configured to use a database via a connection string.  Thus, we need to do things differently for testing table controllers even if we only test the unique functionality.    Let's take a look at the tests for the  UserId  property.  The  UserId  property contains the contents of the  NameIdentifier  claim.  My tests for this are:   A correct set of claims are provided.  An incomplete set of claims (without a NameIdentifier) are provided.  No claims are provided.   The first and last are the typical authenticated and anonymous access tests.  The first should provide the username in the NameIdentifier, and the latter should throw an error.  The middle test is an important one for us.  What do you want to happen if the user is authenticated, but the NameIdentifier claim was not provided?  It's bad form for us to return a 500 Internal Server Error, even though that would be appropriate here.  Instead I want to assume that the user id is blank so that everything keeps on working.  (One can argue that this is not correct either!)   Install the same NuGet packages  Unlike the scaffolded project for Azure Mobile Apps or ASP.NET MVC, no additional packages are added to the test project, which means you will need to figure out which packages you need to simulate the requirements for the test.  Don't guess.  Look at the packages that are in your project under test and duplicate them.  Right-click the solution and select  Manage NuGet Packages  to get a good idea of what your test package is missing.  Under the  Installed  list, you can tell what packages are required and which projects have them installed.   Mock the  ClaimsIdentity  to test authentication:  using System.Security.Claims;\n\nnamespace Backend.Tests.Utilities\n{\n    public class TestPrincipal : ClaimsPrincipal\n    {\n        public TestPrincipal(params Claim[] claims) : base(new TestIdentity(claims))\n        {\n        }\n    }\n\n    public class TestIdentity : ClaimsIdentity\n    {\n        public TestIdentity(params Claim[] claims) : base(claims)\n        {\n        }\n    }\n}  My (incorrect - deliberately) test looks like the following:  using Backend.Controllers;\nusing Backend.Tests.Utilities;\nusing System.Security.Claims;\nusing System.Threading;\nusing Xunit;\n\nnamespace Backend.Tests.Controllers\n{\n    public class TodoItemControllerTests\n    {\n        [Fact]\n        public void UserId_With_Correct_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = new TestPrincipal(\n                new Claim( name ,  testuser ),\n                new Claim( sub ,  foo )\n            );\n            var result = controller.UserId;\n\n            Assert.NotNull(result);\n            Assert.Equal( testuser , result);\n        }\n\n        [Fact]\n        public void UserId_With_Incomplete_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = new TestPrincipal(\n                new Claim( sub ,  foo )\n            );\n            var result = controller.UserId;\n\n            Assert.Null(result);\n        }\n\n        [Fact]\n        public void UserId_With_Null_Claims()\n        {\n            var controller = new TodoItemController();\n            controller.User = null;\n            var ex = Assert.Throws HttpResponseException (() =  { var result = controller.UserId; });\n            Assert.Equal(HttpStatusCode.Unauthorized, ex.Response.StatusCode);\n        }\n    }\n}  The  UserId_With_Null_Claims  test is an interesting recipe for testing that the right exception is thrown.  In this case, I expect the methods to return a 401 Unauthorized response to the client.  Of course, the  [Authorize]  tag will do this for my well before my code is hit, but it's good to be accurate.  If I run the tests, I get the following:   What I want to do is run that test again, but attach a debugger.  To do this, set a breakpoint on the property in the  TodoItemController .  Then right-click the failing test and select  Debug Selected Tests .  This runs the test with a debugger connected.  The breakpoint you set will be hit and you will be able to inspect the code state while it is running.  The first test is failing because  ClaimTypes.NameIdentifier  is not \"name\".  I re-wrote the test as follows:      [Fact]\n    public void UserId_With_Correct_Claims()\n    {\n        var controller = new TodoItemController();\n        controller.User = new TestPrincipal(\n            new Claim(ClaimTypes.NameIdentifier,  testuser ),\n            new Claim( sub ,  foo )\n        );\n        var result = controller.UserId;\n\n        Assert.NotNull(result);\n        Assert.Equal( testuser , result);\n    }  This test will now pass.  The other two tests are actually the result of incorrect code.  I've adjusted the code accordingly:      public string UserId\n    {\n        get\n        {\n            if (User == null)\n            {\n                throw new HttpResponseException(HttpStatusCode.Unauthorized);\n            }\n            var principal = User as ClaimsPrincipal;\n            Claim cl = principal.Claims.FirstOrDefault(c =  c.Type == ClaimTypes.NameIdentifier);\n            return cl?.Value;\n        }\n    }  This is a little longer than the original one-liner, but it's more accurate.  This means that when I've forgotten what this particular method does in six months time, it will still do the right thing in all conditions.   Use Test-Driven Development  There is a whole school of thought on how to develop using testing as the driver known as  Test Driven Development  or TDD.  In this school of thought, you write the tests first, ensuring you have 100% of the cases covered.  Your code is correct when the tests pass.  This method provides for very rapid development, but you do spend most of your time developing tests rather than code.    The other big class of testing to do is against custom APIs.   You can test these the same way.  For example, the standard scaffolding for an Azure Mobile Apps server contains a  ValuesController.cs , which I have modified:  using System.Web.Http;\nusing Microsoft.Azure.Mobile.Server.Config;\n\nnamespace Backend.Controllers\n{\n    // Use the MobileAppController attribute for each ApiController you want to use\n    // from your mobile clients\n    [MobileAppController]\n    public class ValuesController : ApiController\n    {\n        // GET api/values\n        public string Get(string user)\n        {\n            return $ Hello {user}! ;\n        }\n    }\n}  I can test this with the following:  using Backend.Controllers;\nusing Xunit;\n\nnamespace Backend.Tests.Controllers\n{\n    public class ValuesControllerTests\n    {\n        [Fact]\n        public void Get_Name_Works()\n        {\n            var controller = new ValuesController();\n            var result = controller.Get( adrian );\n            Assert.Equal( Hello adrian! , result);\n        }\n    }\n}  As with all other testing, ensure you think about all the things that could happen here, and test them all.  Ensure that the appropriate response is always returned and that you are never leave your server or a connected client in a bad state.  A big example of this in the case of mobile apps:  If a response is meant to be a JSON encoded version of an object on your client, ensure it can be deserialized to that object under all conditions.", 
            "title": "Unit Testing"
        }, 
        {
            "location": "/chapter8/testing/#testing-your-mobile-client", 
            "text": "Testing your mobile client will generally be a multi-part affair:   Implement mock data services and test the UI in isolation.  Implement unit tests for the non-UI components.  Do end-to-end tests to ensure both client and server work together.   Unit tests for non-UI code is the same as the server-side code.  You need to write the tests in a unit test framework like  xUnit  or  MSTest .  Use  Test-driven development  to ensure that your code is living up to its contract.", 
            "title": "Testing your Mobile Client"
        }, 
        {
            "location": "/chapter8/testing/#using-mock-data-services", 
            "text": "Unfortunately, setting up repeatable unit tests becomes increasingly difficult in a client-server application such as a cloud-enabled mobile app.  For these aspects, you want to mock the data services.  If you have followed along from the beginning, we've actually done a lot of the hard work for this.   Create an Interface that represents the interface to the data service.  Create a concrete implementation of that interface.  Use a dependency injection service to inject the concrete implementation.   The whole idea here is that changing just one line of code will enable you to update from the mock implementation to the cloud implementation.  This allows you to develop the UI independently of the backend communication code, and allows you to do repeatable UI tests later on.  Let's take a look at an example.  In  my Chapter8 project , I've got the Xamarin Forms application from  the very first chapter .  In the shared  TaskList  project, there is an  Abstractions  folder that contains the definitions for  ICloudService :  namespace TaskList.Abstractions\n{\n    public interface ICloudService\n    {\n        ICloudTable T  GetTable T () where T : TableData;\n    }\n}  There is also a definition for  ICloudTable :  using System.Collections.Generic;\nusing System.Threading.Tasks;\n\nnamespace TaskList.Abstractions\n{\n    public interface ICloudTable T  where T : TableData\n    {\n        Task T  CreateItemAsync(T item);\n        Task T  ReadItemAsync(string id);\n        Task T  UpdateItemAsync(T item);\n        Task DeleteItemAsync(T item);\n        Task ICollection T  ReadAllItemsAsync();\n    }\n}  The important part of this is this.  The only place where the concrete edition,  AzureCloudService() , is mentioned is in the  App.cs  file:  using TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            CloudService = new AzureCloudService();\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}  Everywhere else uses the  ICloudService  interface and does not mention the concrete version.  The application sets up the cloud service and every other class uses it.  This allows us to set up a mock cloud service as follows.  First, let's define the  MockCloudService :  using System.Collections.Generic;\nusing TaskList.Abstractions;\n\nnamespace TaskList.Services\n{\n    public class MockCloudService : ICloudService\n    {\n        public Dictionary string, object  tables = new Dictionary string, object ();\n\n        public ICloudTable T  GetTable T () where T : TableData\n        {\n            var tableName = typeof(T).Name;\n            if (!tables.ContainsKey(tableName))\n            {\n                var table = new MockCloudTable T ();\n                tables[tableName] = table;\n            }\n            return (ICloudTable T )tables[tableName];\n        }\n    }\n}  It's very similar to the  AzureCloudService  class, but there is no  MobileServiceClient .  Instead, we store the cloud table instances in a dictionary to ensure successive calls to  GetTable ()  return the same singleton reference.  We aren't using the backend service.  Similarly, we use a  Dictionary  to hold the items instead of the backend service in the  MockCloudTable  class:  using Microsoft.WindowsAzure.MobileServices;\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.Threading.Tasks;\nusing TaskList.Abstractions;\n\n#pragma warning disable CS1998 // Async method lacks 'await' operators and will run synchronously\n\nnamespace TaskList.Services\n{\n    public class MockCloudTable T  : ICloudTable T  where T : TableData\n    {\n        private Dictionary string, T  items = new Dictionary string, T ();\n        private int currentVersion = 1;\n\n        public async Task T  CreateItemAsync(T item)\n        {\n            item.Id = Guid.NewGuid().ToString( N );\n            item.CreatedAt = DateTimeOffset.Now;\n            item.UpdatedAt = DateTimeOffset.Now;\n            item.Version = ToVersionString(currentVersion++);\n            items.Add(item.Id, item);\n            return item;\n        }\n\n        public async Task DeleteItemAsync(T item)\n        {\n            if (item.Id == null)\n            {\n                throw new NullReferenceException();\n            }\n            if (items.ContainsKey(item.Id))\n            {\n                items.Remove(item.Id);\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException( Not Found , null, null);\n            }\n        }\n\n        public async Task ICollection T  ReadAllItemsAsync()\n        {\n            List T  allItems = new List T (items.Values);\n            return allItems;\n        }\n\n        public async Task T  ReadItemAsync(string id)\n        {\n            if (items.ContainsKey(id))\n            {\n                return items[id];\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException( Not Found , null, null);\n            }\n        }\n\n        public async Task T  UpdateItemAsync(T item)\n        {\n            if (item.Id == null)\n            {\n                throw new NullReferenceException();\n            }\n            if (items.ContainsKey(item.Id))\n            {\n                item.UpdatedAt = DateTimeOffset.Now;\n                item.Version = ToVersionString(currentVersion++);\n                items[item.Id] = item;\n                return item;\n            }\n            else\n            {\n                throw new MobileServiceInvalidOperationException( Not Found , null, null);\n            }\n        }\n\n        private byte[] ToVersionString(int i)\n        {\n            byte[] b = BitConverter.GetBytes(i);\n            string str = Convert.ToBase64String(b);\n            return Encoding.ASCII.GetBytes(str);\n        }\n    }\n}  The mock service is instantiated within the  App.cs  file:  using TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}  Finally, I need to actually define  USE_MOCK_SERVICES  somewhere.  Right-click the project and select  Properties .  Click  Build .  Add the  USE_MOCK_SERVICES  to the  Conditional compilation symbols  (which is a semi-colon separated list).  Save the properties then rebuild the project you modified.  You can run this version without any backend at all.  It will not persist the data, but that's the point of mock data services.   Use a new Configuration  Another, more advanced, way of accomplishing this is to set up a new configuration.  You can see the configuration is \"Active (Debug)\".  You can add another configuration to this list called \"Mock Services\" by using the  Build     Configuration Manager... .  When you select that configuration, the mock services will automatically be brought in.", 
            "title": "Using Mock Data Services"
        }, 
        {
            "location": "/chapter8/testing/#ui-testing", 
            "text": "The mock services are a tool to enable UI unit testing.  UI testing is unit testing for your UI.  These are small tests that are executed on a real device and check to see if your main UI flows work as expected.  There are actually a few ways of creating tests. I'm going to produce a simple test.  In the test, I will simulate clicking on the entry button and ensuring that the task list page is produced.  This test can then be run against one or more devices.  Let's start by creating a  TaskList.Tests  project:   Right-click the solution and select  Add  -   New Project...  In the  Installed     Visual C#     Cross-Platform  node of the tree, select  UI Test App (Xamarin.UITest | Cross-Platform) .  Give it a snappy name, like  TaskList.Tests , then click  OK .  Wait for the project to be created.  Right-click the  References  node in the newly created project and select  Add Reference... .  Click  Projects  in the left hand side-bar.  Click  TaskList.Android .  Ensure there is a checked box next to the TaskList.Android project.  Click  OK .   We are only going to test the Android edition of the project in this walkthrough, mostly because I do most of my work on a PC.  The same methodology can be used for iOS, however.   Use NUnit v2.x  Xamarin.UITest does not support NUnit 3.x - make sure you do not upgrade the NUnit framework beyond the latest v2.x release when updating your NuGet packages.   The project contains two source files -  AppInitializer.cs  and  Tests.cs .  This latter file is where we are going to write the tests.  The most efficient way of writing tests is to use the  Xamarin Test Recorder .  The workflow for using Xamarin Test Recorder is:   If needed, export the mobile client so that it can be used on a device.  Start Xamarin Test Recorder  Specify the application to be tested and the device to run the aplication on.  Interact with the application.  The Test Recorder will create a C# test method.  Incorporate the test into a Xamarin UITest project.   We already have the UITest project, so let's walk through the process of creating a simple test case for the process of creating a new task.  Open the  Tests.cs  file.  Note the \"lightning\" icon next to each  [TestFixture]  attribute:   Switch to a Release build in the configuration manager (or on the toolbar), then right-click the  TaskList.Android  project and select  Build  to build the project.  Now that you have a working app, click the lightning icon next to the  TestFixture  for the Android platform, then select  Record new test  -   Build TaskList.Android project .     This will start your project in the selected device - normally an emulator.  It will also create a  NewTest()  method.  In the emulator, click on the Login button, followed by Add New Item, followed by Save.  When you are done, switch back to the Visual Studio instance, click on the spanner next at the bottom of the file and select  Stop Recording . The following code will be generated:      [Test]\n    public void NewTest()\n    {\n        app.Tap(x =  x.Text( Login ));\n        app.Tap(x =  x.Text( Add New Item ));\n        app.Tap(x =  x.Text( Save ));\n    }  You can take screen shots, by adding  app.Screenshot(\"description\");  in between each step:      [Test]\n    public void NewTest()\n    {\n        app.Tap(x =  x.Text( Login ));\n        app.Screenshot( Logged in - initial list of items );\n        app.Tap(x =  x.Text( Add New Item ));\n        app.Screenshot( Empty detail record );\n        app.Tap(x =  x.Text( Save ));\n        app.Screenshot( Back at list of items );\n    }   Platform Support  Xamarin Test Recorder supports iOS and Android.  You cannot record an iOS UITest on the PC.  If you want to use one platform for recording tests, use a Mac.   You can run this as long as you adjust the  AppInitializer.cs  file as follows:      if (platform == Platform.Android)\n    {\n        return ConfigureApp\n            .Android\n            .InstalledApp( tasklist.tasklist )\n            .StartApp();\n    }  Replace the  tasklist.tasklist  with the package name of your app.  You can retrieve this in the  Properties     Android Manifest  page for the  TaskList.Android  app.  Use the  Test Explorer  to run the test.  You will see that the clicks are performed as expected.  Let's take this a little further.  Let's say that rather than just clicking a few times, we wanted to ensure that the text box was filled with the text that is expected via an assertion.  To uniquely identify a view, we need to add an  AutomationId  to the view.  Adjust the  TaskDetail.xaml  file in the shared project as follows:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage\n    x:Class= TaskList.Pages.TaskDetail \n    xmlns= http://xamarin.com/schemas/2014/forms \n    xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n    Title= {Binding Title} \n     ContentPage.Content \n         StackLayout Padding= 10  Spacing= 10 \n             Label Text= What should I be doing?  / \n             Entry AutomationId= entrytext  Text= {Binding Item.Text}  / \n             Label Text= Completed?  / \n             Switch IsToggled= {Binding Item.Complete}  / \n             StackLayout VerticalOptions= CenterAndExpand  / \n             StackLayout Orientation= Vertical  VerticalOptions= End \n                 StackLayout HorizontalOptions= Center  Orientation= Horizontal \n                     Button\n                        BackgroundColor= #A6E55E \n                        Command= {Binding SaveCommand} \n                        Text= Save \n                        TextColor= White  / \n                     Button\n                        BackgroundColor= Red \n                        Command= {Binding DeleteCommand} \n                        Text= Delete \n                        TextColor= White  / \n                 /StackLayout \n             /StackLayout \n         /StackLayout \n     /ContentPage.Content  /ContentPage   Note line 10.  I've explicitly added an  AutomationId  to the entry text.  I can now adjust the test to use this:      [Test]\n    public void NewTest()\n    {\n        app.Tap(x =  x.Text( Login ));\n        app.Screenshot( Logged in - initial list of items );\n        app.Tap(x =  x.Text( Add New Item ));\n        app.Screenshot( Empty detail record );\n\n        AppResult[] results = app.Query( entrytext );\n        Assert.AreEqual(1, results.Length);\n        Assert.AreEqual( New Item , results[0].Text);\n\n        app.Tap(x =  x.Text( Save ));\n        app.Screenshot( Back at list of items );\n    }   Run tests on Visual Studio Mobile Center  Although this process is good, it requires that you own a large number of devices and you manually test.  Visual Studio Mobile Center (introduced below) has a test facility that allows you to run the same tests on hundreds of real devices and get reports of failures.  Combine this with crash and analytics reporting and you will have a robust release testing mechanism for your product.   You can now create a complete set of tests:   Unit tests for the custom code in the mobile backend.  Unit tests with a mocked backend for the mobile client.  UI tests with a mocked backend for the mobile client.  End to End UI tests with a test backend and known dataset.   When you are in the normal development cycle, you should be doing the first three on a very regular basis throughout the day.  You should perform end to end tests with the known test backend across a wide variety of devices before every release to the public app store.", 
            "title": "UI Testing"
        }, 
        {
            "location": "/chapter8/testing/#distributing-your-mobile-client-to-beta-users", 
            "text": "There are a number of services that will distribute your client applications to your beta users.  Since I am working primarily within Azure and the Microsoft family of products, I'm going to use  Visual Studio Mobile Center .  As of writing, Visual Studio Mobile Center is in preview, but already supports a large number of highly desirable features.  One of these features is beta distribution.  To create an application:   Sign in to  Visual Studio Mobile Center .  You will need to create an account if you have not already done so.  Click  Add new  -   Add new app .  Give your app a name and select the OS (iOS or Android) and Platform (Xamarin).   Click  Add new app  at the bottom of the page.    Create two apps if you distribute both iOS and Android apps  If you are distributing both an iOS and an Android app, you will need to create two apps - one for the iOS edition and one for the Android edition.   At this point, the cloud infrastructure necessary to handle the mobile client app is created.  We can move on to the distribution phase.  We need to create a group and invite some beta users to our group before doing a distribution:   Click the  Distribute  icon (it looks like three arrow pointing up if your menu is collapsed).  Click  New Group .  Name your group something like  Beta Testers .  Enter a comma-delimited list of email addresses to invite users to join the group.   Click  Create Group .    Collect and register UDID for iOS Distribution  Another quirk of the iOS ecosystem.  You will need to  register the UDID  for each iOS beta tester.  Apple does not allow software to be installed on non-registered devices except via the App Store.   A group called  Collaborators  is created and maintained by default.  This contains a list of people whom you are collaborating with via Visual Studio Mobile Center.  The APK or IPA file must be generated first.   Open your solution in Visual Studio 2017.  Select the  Release ,  Any CPU ,  TaskList.Android  configuration in the menu bar (for Android).  Right-click the TaskList.Android project and select  Rebuild .  Once the build is complete, click  Build  -   Archive... .  Once the archive is created, click  Open Folder .   The process is similar for iOS.  You must build for a device and will only be able to distribute to registered devices.   Now that we have an APK file:   Return to the  Visual Studio Mobile Center  and select your app.  Click  Distribute  in the left-hand sidebar.  Click the group you created.  Click  Distribute new release .  Drag and drop the APK file from the open folder to the landing area (or click upload and find your APK file).  Wait for the APK file to be uploaded.  A thin blue bar above the landing area is used to indicate progress.  While you wait, type in some release note into the box provided.  Click  Next .  Click  Next  again, as the group is already selected.   Click  Distribute .   Your beta testers will now get an email telling them there is a new app available and how to download it.", 
            "title": "Distributing your Mobile Client to Beta Users"
        }, 
        {
            "location": "/chapter9/arm/", 
            "text": "Repeatable Deployments\n\n\nYou've finally got to the point where you want to release your app to the general public.  Congratulations!  I'm not going to cover releasing to the App Store in this section.  Each App Store has their own rules and requirements for apps and they change all the time.  You will need to follow their instructions.  If you haven't already, make sure you have a paid developer account for this activity.\n\n\nThat doesn't mean that you can't deal with the backend.  Now that your app is in production, you want your backend in production as well.  That means being as hands-off as possible and ensuring that human error does not cause a significant downtime.  We've heard about various outages as a result of human error.  Make sure that doesn't happen to you.  Fortunately, Azure has a number of facilities to ensure that you can automate the process.\n\n\nOptions for Deployment\n\n\nWhen deploying the backend, you want a repeatable process.  Ideally, you want a set of scripts that you check into source code that describes the deployment of Azure resources.  Once you have set up your environment, you can use continuous deployment (in the next section) to actually perform the deployments.  However, you still want to automate the deployment of a new set of resources.\n\n\nFortunately, Azure (in common with any reasonable cloud service) has tools to do this.  In essence, the tools consist of a file - the \nAzure Resource Manager\n Template, and a set of PowerShell or command-line tools to deploy a new service.  Every time you create a mobile app in Azure App Service, Azure Resource Manager handles the actual provisioning behind the scenes.  It is much faster to use an ARM template rather than the point-and-click mechanism we have been using thus far.\n\n\nCreating an ARM Template\n\n\nLet's start by examining a typical application - our Task List backend.  We've set this up a number of times within this book, so you should be familiar with it.  It has these resources within a resource group:\n\n\n\n\nAzure App Service with type=mobile\n\n\nAzure App Service Plan\n\n\nSQL Azure Logical Server\n\n\nSQL Azure Database\n\n\n\n\nARM templates have three distinct sections:\n\n\n\n\nParameters provide information about the deployment set in a single place.\n\n\nVariables are computed parameters.\n\n\nResources list the resources necessary to be deployed.\n\n\n\n\nA typical (empty) ARM template looks like this:\n\n\n{\n    \n$schema\n: \nhttp://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\n,\n    \ncontentVersion\n: \n1.0.0.0\n,\n    \nparameters\n: {\n    },\n    \nvariables\n: {\n    },\n    \nresources\n: [\n    ]\n}\n\n\n\n\nThere is also an \noutputs\n section, but I find most templates do not require this section.\n\n\nParameters and Variables\n\n\nIf I am creating a deployment called \nzumobook\n, then I always name the resources like this:\n\n\n\n\nResource Group = zumobook\n\n\nAzure App Service = zumobook\n\n\nApp Service Plan = zumobook-plan\n\n\nSQL Azure Logical Server = zumobook-sql\n\n\nSQL Azure Database = zumobook-db\n\n\n\n\nAll the resources are based on the root name of the deployment.  Ideally, the deployment name would be a parameter (something I specify) and the other names would be computed.  In addition, I always make the administrator login for the SQL Azure Logical Server be \nazure\n and I specify the password.  Let's take a look at what that would look like:\n\n\n{\n    \n$schema\n: \nhttp://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\n,\n    \ncontentVersion\n: \n1.0.0.0\n,\n    \nparameters\n: {\n        \nserviceName\n: {\n            \ntype\n: \nstring\n\n        },\n        \nsqlServerAdminLogin\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nazure\n\n        },\n        \nsqlServerAdminPassword\n: {\n            \ntype\n: \nsecurestring\n\n        }\n    },\n    \nvariables\n: {\n        \nappServiceName\n: \nparameters('serviceName')\n,\n        \nappServicePlanName\n: \n[concat(parameters('serviceName), '-plan')]\n,\n        \nsqlServerName\n: \n[concat(parameters('serviceName), '-sql')]\n,\n        \nsqlDbName\n: \n[concat(parameters('serviceName), '-db')]\n\n    },\n    \nresources\n: [\n    ]\n}\n\n\n\n\n\n\nVariable Functions\n\n\nNote the \nconcat()\n is a template function.  You can find the large list of template functions in the \nAzure Resource Manager documentation\n.\n\n\n\n\nAs a best practice, I abstract anything I would normally enter in creating the resource and make it either a parameter or a variable, depending on the circumstance.  I try to reduce the number of parameters and make as much as I can either a default value or a variable.  For example, one of the things that we need to set when configuring a SQL Azure database is the database size.  I could just specify this in bytes, but specifying the size in Mb is a better experience:\n\n\n    \nparameters\n: {\n        \nsqlDbSizeMb\n {\n            \ntype\n: \nint\n,\n            \nminValue\n: 1,\n            \nmaxValue\n: 20,\n            \ndescription\n: \nSize of the database in Mb\n,\n            \ndefaultValue\n: 5\n        }\n    },\n    \nvariables\n: {\n        \nsqlDbSize\n: \n[mul(parameters('sqlDbSizeMb'), 1048576)]\n\n    }\n\n\n\n\nResources\n\n\nResources describe the deployment of individual resources and their dependency.  There are, inevitably, more resources than you expect.   Each resource has an \napiVersion\n, formatted as a date, that describes the format of the resource.  Each resource also has a \nname\n and a \ntype\n plus a number of other elements that describe configuration elements.  Finally, there is an optional \ndependsOn\n that ensures that resources are not configured until their dependent resources are deployed.  If, for example, I look at my deployment, I can represent this as a tree:\n\n\n\n\nResource Group\n\n\nApp Service Plan\n\n\nApp Service\n\n\nApp Settings\n\n\nConnection Strings\n\n\nContinuous Deployment Source\n\n\n\n\n\n\n\n\n\n\nSQL Azure Logical Server\n\n\nSQL Database\n\n\nSQL Server Firewall Rules\n\n\n\n\n\n\n\n\n\n\n\n\nLet's look at each resource in turn.\n\n\nThe SQL Database\n\n\nThere are actually three resources for the SQL Database - the server, the database and the firewall rule to allow other resources to connect to it:\n\n\n{\n    \n$schema\n: \nhttp://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\n,\n    \ncontentVersion\n: \n1.0.0.0\n,\n    \nparameters\n: {\n        \nserviceName\n: {\n            \ntype\n: \nstring\n\n        },\n        \nlocation\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nSouth Central US\n\n        },\n        \nsqlServerAdminLogin\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nazure\n\n        },\n        \nsqlServerAdminPassword\n: {\n            \ntype\n: \nsecurestring\n\n        },\n        \nsqlDbSizeMb\n {\n            \ntype\n: \nint\n,\n            \nminValue\n: 1,\n            \nmaxValue\n: 20,\n            \ndescription\n: \nSize of the database in Mb\n,\n            \ndefaultValue\n: 2\n        },\n        \nsqlDbCollation\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nSQL_Latin1_General_CP1_CI_AS\n\n        },\n        \nsqlDbEdition\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nBasic\n\n        },\n        \nsqlDbServiceObjectiveId\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \ndd6d99bb-f193-4ec1-86f2-43d3bccbc49c\n\n        },\n    },\n    \nvariables\n: {\n        \nsqlDbSize\n: \n[mul(parameters('sqlDbSizeMb'), 1048576)]\n,\n        \nsqlServerName\n: \n[concat(parameters('serviceName), '-sql')]\n,\n        \nsqlDbName\n: \n[concat(parameters('serviceName), '-db')]\n\n    },\n    \nresources\n: [\n        {\n            \napiVersion\n: \n2014-04-01-preview\n,\n            \nname\n: \n[variables('sqlServerName')]\n,\n            \ntype\n: \nMicrosoft.Sql/servers\n,\n            \nlocation\n: \n[parameters('location')]\n,\n            \nproperties\n: {\n                \nadministratorLogin\n: \n[parameters('sqlServerAdminLogin')]\n,\n                \nadministratorPassword\n: \n[parameters('sqlServerAdminPassword')]\n\n            },\n            \nresources\n: [\n                {\n                    \napiVersion\n: \n2015-05-01-preview\n,\n                    \nname\n: \n[variables('sqlDbName')]\n,\n                    \ntype\n: \ndatabases\n,\n                    \nlocation\n: \n[parameters('location')]\n,\n                    \ndependsOn\n: [\n                        \n[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\n\n                    ],\n                    \nproperties\n: {\n                        \nedition\n: \n[parameters('sqlDbEdition')]\n,\n                        \ncollation\n: \n[parameters('sqlDbCollation')]\n,\n                        \nmaxSizeBytes\n: \n[variables('sqlDbSize')]\n,\n                        \nrequestedServiceObjectiveId\n: \n[parameters('sqlDbServiceObjectiveId')]\n\n                    }\n                },\n                {\n                    \napiVersion\n: \n2014-04-01-preview\n,\n                    \nname\n: \nSQLServerFirewallRules\n,\n                    \ntype\n: \nfirewallrules\n,\n                    \nlocation\n: \n[parameters('location')]\n,\n                    \ndependsOn\n: [\n                        \n[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\n\n                    ],\n                    \nproperties\n: {\n                        \nendIpAddress\n: \n0.0.0.0\n,\n                        \nstartIpAddress\n: \n0.0.0.0\n\n                    }\n                }\n            ]\n        },\n    ]\n}\n\n\n\n\nThis is a fairly lengthy example, but it incorporates a few things.  Firstly, note that I use the \n[variables()]\n and \n[parameters()]\n to bring in the variables and parameters respectively.  I also have embedded resources here.  The resource heirarchy is a tree-structure.  Each embedded resource has a \ndependsOn\n array that has the parent as a dependent resource.\n\n\nYou are probably wondering where you can get this information for any resource.  Microsoft Azure has a site called \nresources.azure.com\n.  This contains the resources that you currently have configured on your subscription.  You can set the deployment up by hand and then look at the resources to get the details:\n\n\n\n\nIn this example, I'm showing the resource for the database.  I can extract the various pieces based on what I have set to get to this point - everything else is defaulted.  In addition, note that I can see the \napiVersion\n at the top of the resource manager within the URL for the resource definition.\n\n\nThe Azure App Service Plan\n\n\nMy next resource is the App Service Plan.  Unlike the database (which has a definite heirarchy), the App Service Plan is separate because many App Services can use the same App Service Plan.  You can think of the App Service Plan as the set of virtual machines that belong in a single scale set, and the App Services as the web sites running on top of that scale set.  The App Service Plan looks like this:\n\n\n{\n    \n$schema\n: \nhttp://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\n,\n    \ncontentVersion\n: \n1.0.0.0\n,\n    \nparameters\n: {\n        \nserviceName\n: {\n            \ntype\n: \nstring\n\n        },\n        \nlocation\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nSouth Central US\n\n        },\n        \nsku\n: {\n            \ntype\n: \nstring\n,\n            \nallowedValues\n: [\n                \nFree\n,\n                \nShared\n,\n                \nBasic\n,\n                \nStandard\n\n            ],\n            \ndefaultValue\n: \nBasic\n\n        },\n        \nworkerSize\n: {\n            \ntype\n: \nstring\n,\n            \nallowedValues\n: [\n                \n0\n, \n1\n, \n2\n\n            ],\n            \ndefaultValue\n: \n0\n\n        }\n    },\n     \nvariables\n: {\n        \nappServicePlanName\n: \n[concat(parameters('serviceName), '-plan')]\n,\n    },\n    \nresources\n: [\n        {\n            \napiVersion\n: \n2015-08-01\n,\n            \nname\n: \n[variables('appServicePlanName')]\n,\n            \ntype\n: \nMicrosoft.Web/serverFarms\n,\n            \nlocation\n: \n[parameters('location')]\n,\n            \nproperties\n: {\n                \nname\n: \n[parameters('appServicePlanName')]\n,\n                \nsku\n: \n[parameters('sku')]\n,\n                \nworkerSize\n: \n[parameters('workerSize')]\n,\n                \nnumberOfWorkers\n: 1\n            }\n        }\n    ]\n}\n\n\n\n\nThe \nhostingPlanName\n is the name of the hosting plan.  This is normally something like F1, B2, or P3.  The first letter indicates the SKU and the number indicates the size of the worker.  The size of the worker is always one less than the number that is displayed within the Azure portal.\n\n\nThe Azure App Service\n\n\nThe Azure App Service is\n\n\n{\n    \n$schema\n: \nhttp://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json\n,\n    \ncontentVersion\n: \n1.0.0.0\n,\n    \nparameters\n: {\n        \nserviceName\n: {\n            \ntype\n: \nstring\n\n        },\n        \nlocation\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nSouth Central US\n\n        },\n        \nsqlServerAdminLogin\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nazure\n\n        },\n        \nsqlServerAdminPassword\n: {\n            \ntype\n: \nsecurestring\n\n        },\n        \ndeploymentRepoUrl\n: {\n            \ntype\n: \nstring\n\n        },\n        \ndeploymentRepoBranch\n: {\n            \ntype\n: \nstring\n,\n            \ndefaultValue\n: \nmaster\n\n        }\n    },\n    \nvariables\n: {\n        \nappServiceName\n: \nparameters('serviceName')\n,\n        \nappServicePlanName\n: \n[concat(parameters('serviceName), '-plan')]\n,\n        \nsqlServerName\n: \n[concat(parameters('serviceName), '-sql')]\n,\n        \nsqlDbName\n: \n[concat(parameters('serviceName), '-db')]\n\n    },\n    \nresources\n: [\n        {\n            \napiVersion\n: \n2015-04-01\n,\n            \nname\n: \n[variables('appServiceName')]\n,\n            \ntype\n: \nMicrosoft.Web/sites\n,\n            \nlocation\n: \n[parameters('location')]\n,\n            \ndependsOn\n: [\n                \n[resourceId('Microsoft.Web/serverFarms', parameters('appServicePlanName'))]\n\n            ],\n            \nproperties\n: {\n                \nserverFarmId\n: \n[parameters('appServicePlanName')]\n\n            },\n            \nresources\n: [\n                {\n                    \napiVersion\n: \n2015-04-01\n,\n                    \nname\n: \nconnectionstrings\n,\n                    \ntype\n: \nconfig\n,\n                    \ndependsOn\n: [\n                        \n[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\n,\n                        \n[resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))]\n\n                    ],\n                    \nproperties\n: {\n                        \nMS_TableConnectionString\n: { \nvalue\n: \n[concat('Data Source=tcp:', reference(concat('Microsoft.Sql/servers/', variables('sqlServerName'))).fullyQualifiedDomainName, ',1433;Initial Catalog=', variables('sqlDbName'), ';User Id=', parameters('sqlServerAdminLogin'), '@', parameters('sqlServerName'), ';Password=', parameters('sqlServerAdminPassword'), ';')]\n, \ntype\n: \nSQLAzure\n }\n                    }\n                },\n                {\n                    \napiVersion\n: \n2015-04-01\n,\n                    \nname\n: \nweb\n,\n                    \ntype\n: \nsourcecontrols\n,\n                    \ndependsOn\n: [\n                        \n[resourceId('Microsoft.Web/Sites', variables('appServiceName'))]\n,\n                        \n[resourceId('Microsoft.Web/Sites/config', variables('appServiceName'), 'connectionstrings')]\n\n                    ],\n                    \nproperties\n: {\n                        \nRepoUrl\n: \n[parameters('deploymentRepoUrl')]\n,\n                        \nbranch\n: \n[parameters('deploymentRepoBranch')]\n\n                    }\n                }\n            ]\n        },\n    ]\n}\n\n\n\n\nIn this case, we've got two dependent resources - one to set up the connection string to the database and the other to set up continuous deployment.  Both of those dependent resources have the normal \ndependsOn\n terms to ensure that the resources linked exist.\n\n\nYou can combine all three sections - the SQL Server, App Service Plan and App Service together.  There are overlapping parameters and variables.  You just need one copy.\n\n\nDeploying an ARM Template on Windows\n\n\nCreating an Azure Resource Manager template on Windows can be done several ways, but my favorite is to use Visual Studio.\n\n\n\n\nStart up Visual Studio 2017 and open your solution.\n\n\nRight-click your solution and select \nAdd\n -\n \nNew Project...\n.\n\n\nSelect \nInstalled\n \n \nVisual C#\n \n \nCloud\n \n \nAzure Resource Group\n.\n\n\nGive the project a name, then click \nOK\n\n\nSelect a template (I use \nWeb app + SQL\n normally as a starting point), then click \nOK\n.\n\n\n\n\nThree files are created within the project:\n\n\n\n\nDeploy-AzureResourceGroup.ps1\n\n\nWebSiteSQLDatabase.json (or similar)\n\n\nWebSiteSQLDatbase.parameters.json (or similar)\n\n\n\n\nThe \nWebSiteSQLDatabase.json\n is the ARM template.  The \n.parameters.json\n file contains the parameters for an unattended deployment.  Since the parameters contains the SQL server password, we need to be careful to ensure that the database password does not end up checked into source code.  Finally, the \nDeploy-AzureResourceGroup.ps1\n file is a handy PowerShell script for deploying to Azure.  Running it will be the equivalent of doing a deployment via Visual Studio.\n\n\nYou can now edit the ARM template to include any additional resources you need.  You can also edit the \n.parameters.json\n file to set standard values for any parameters you wish, if they don't change very often, although you won't need to do this as there is an easier way of setting parameters as we shall see.\n\n\nThere are two steps to deploying this ARM template: Validation and Deployment.  To validate the template, right-click the project and select \nValidate\n.\n\n\n\n\nClick into the \nResource group\n and select \nCreate new\n.  This allows you to select a resource group name and a location.  Once you have done that, click \nCreate\n.\n\n\nThe next step is to click \nEdit Parameters...\n.  This edit the \n.parameters.json\n file:\n\n\n\n\nNote that we need to set a \nhostingPlanName\n, as indicated by the red exclamation error mark.  We can also set any other values we want.  Anything that has a null value should be entered.  Click \nSave\n when complete.\n\n\n\n\nDo not save password as plain text\n\n\nIt's likely that these files will end up in source control.  You should never save passwords as plain text in any file that is going to end up in a source control repository.\n\n\n\n\nFinally, click \nValidate\n.  You may get validation errors - fix the validation errors before continuing.  Once you have validated successfully, right-click the deployment project and select \nDeploy\n, then select the resource group you entered during validation.  You may have to type the SQL administrator password multiple times.\n\n\nWatch the output window for a progress report on your deployment.  Once complete, log into the \nAzure portal\n to check the resources.  Also check that the resources match what you were expecting within the \nAzure Resource Explorer\n.\n\n\nDeploying an ARM Template on MacOS\n\n\nThe Mac does not come with the tools necessary to deploy ARM templates, so you will need to download and install them first.  In a Terminal window:\n\n\nadrian$ curl -L https://aka.ms/InstallAzureCli | bash\nadrian$ exec -l $SHELL\n\n\n\n\nThe first command does the actual installation.  It will ask you questions about where to install and confirm that it can write to files.  In general, pressing Enter will do the right thing.  The second command restarts your shell.  After the installation is complete, run \naz\n in your Terminal window to ensure the installation was successful.\n\n\nThe next step is to login to Azure with the CLI.  Use the \naz login\n command from the Terminal window.  This command will output a code to use in the next step.\n\n\n\n\nNote the code that is produced.  Now go to \nhttp://aka.ms/devicelogin\n and enter the code in the box provided.\n\n\n\n\nEnter the code, press enter, then click \nContinue\n.  You will be asked to authenticate to Azure.  Once that is complete, the terminal window will continue and you will be logged into Azure.  The \naz login\n command will provide a little piece of JSON to show you the subscription information to which you connected.\n\n\nDeploying a set of resources is a two-step process on the command-line.  Firstly, you need to create a resource group.  Then you need to deploy the resources within that resource group.  To create a resource group, use:\n\n\naz group create -l westus -n MyResourceGroup\n\n\n\n\nYou can get a list of locations that are available to you using:\n\n\naz account list-locations | grep name\n\n\n\n\nTake a look at the response from the \naz group create\n command.  Ensure that the \nprovisioningState\n property is something that indicates success.\n\n\nNow that we have a resource group, we can deploy our resources.  Edit the \n.parameters.json\n file to contain the values for your parameters.  For example:\n\n\n{\n  \n$schema\n: \nhttps://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\n,\n  \ncontentVersion\n: \n1.0.0.0\n,\n  \nparameters\n: {\n    \nhostingPlanName\n: {\n      \nvalue\n: \nChapter8-plan\n\n    },\n    \nskuName\n: {\n      \nvalue\n: \nB1\n\n    },\n    \nskuCapacity\n: {\n      \nvalue\n: 1\n    },\n    \nadministratorLogin\n: {\n      \nvalue\n: \nazure\n\n    },\n    \nadministratorLoginPassword\n: {\n      \nvalue\n: \nmyPassword\n\n    },\n    \ndatabaseName\n: {\n      \nvalue\n: \nZUMOAPPDB\n\n    },\n    \nedition\n: {\n      \nvalue\n: \nBasic\n\n    }\n  }\n}\n\n\n\n\n\n\nDo not check in passwords!\n\n\nIf your parameters file contains password, do not check it in to source code control.\n\n\n\n\nYou can now deploy the service with the following:\n\n\naz group deployment create --resource-group MyResourceGroup\n    --mode Complete\n    --parameters @WebSiteSQLDatabase.parameters.json\n    --template-file WebSiteSQLDatabase.json\n    --verbose\n\n\n\n\nAlthough I have split this command over multiple lines, you should enter this command on one line.\n\n\n\n\nUse --mode to maintain deployments\n\n\nThere are two modes for ARM deployments.  \nComplete\n will remove any resources that are not listed in your ARM template.  \nIncremental\n will only add resources to the resource group.  You can use these two modes to effectively maintain the resources in the resource group using only the command line.\n\n\n\n\nAt this point, the deployment will kick off.  It's a long running process, so expect a delay of a few minutes before it returns information.  This deployment is (deliberately) going to fail so we can see what happens when it goes wrong and how to fix it.\n\n\nThe response after this first run is this:\n\n\nThe deployment operation failed, because it was unable to cleanup. Please see https://aka.ms/arm-debug for usage details.  Correlation ID: aed1850d-22ca-4e28-b7fa-83a2ac587867\n\n\n\n\nStart by logging into the \nAzure portal\n, then go to the resource group was just created, then click \nDeployments\n.  Select the latest deployment.\n\n\n\n\nClick on the failed request:\n\n\n\n\nThe failed request is a lot clearer than the original failure message.  In this case, the password we entered for the SQL database did not meet the complexity requirements.  This is easily updated within the parameters file.  Then re-run the deployment.\n\n\nCreating the ARM Template\n\n\nOne of the issues with ARM templates is their actual creation.  There are three methods of creating ARM templates:\n\n\n\n\nCreate a new Resource Group within Visual Studio (covered above)\n\n\nExport a template from a Resource Group.\n\n\nLog in to the \nAzure portal\n.\n\n\nSelect your resource group.\n\n\nIn the menu, click \nAutomation Scripts\n.\n\n\nClick \nDownload\n.\n\n\n\n\n\n\nDownload \na starter template from GitHub\n.\n\n\n\n\nWhich ever mechanism you choose to create the template, you will still need to edit the result by hand for your individual requirements.", 
            "title": "Repeatable Deployments"
        }, 
        {
            "location": "/chapter9/arm/#repeatable-deployments", 
            "text": "You've finally got to the point where you want to release your app to the general public.  Congratulations!  I'm not going to cover releasing to the App Store in this section.  Each App Store has their own rules and requirements for apps and they change all the time.  You will need to follow their instructions.  If you haven't already, make sure you have a paid developer account for this activity.  That doesn't mean that you can't deal with the backend.  Now that your app is in production, you want your backend in production as well.  That means being as hands-off as possible and ensuring that human error does not cause a significant downtime.  We've heard about various outages as a result of human error.  Make sure that doesn't happen to you.  Fortunately, Azure has a number of facilities to ensure that you can automate the process.", 
            "title": "Repeatable Deployments"
        }, 
        {
            "location": "/chapter9/arm/#options-for-deployment", 
            "text": "When deploying the backend, you want a repeatable process.  Ideally, you want a set of scripts that you check into source code that describes the deployment of Azure resources.  Once you have set up your environment, you can use continuous deployment (in the next section) to actually perform the deployments.  However, you still want to automate the deployment of a new set of resources.  Fortunately, Azure (in common with any reasonable cloud service) has tools to do this.  In essence, the tools consist of a file - the  Azure Resource Manager  Template, and a set of PowerShell or command-line tools to deploy a new service.  Every time you create a mobile app in Azure App Service, Azure Resource Manager handles the actual provisioning behind the scenes.  It is much faster to use an ARM template rather than the point-and-click mechanism we have been using thus far.", 
            "title": "Options for Deployment"
        }, 
        {
            "location": "/chapter9/arm/#creating-an-arm-template", 
            "text": "Let's start by examining a typical application - our Task List backend.  We've set this up a number of times within this book, so you should be familiar with it.  It has these resources within a resource group:   Azure App Service with type=mobile  Azure App Service Plan  SQL Azure Logical Server  SQL Azure Database   ARM templates have three distinct sections:   Parameters provide information about the deployment set in a single place.  Variables are computed parameters.  Resources list the resources necessary to be deployed.   A typical (empty) ARM template looks like this:  {\n     $schema :  http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json ,\n     contentVersion :  1.0.0.0 ,\n     parameters : {\n    },\n     variables : {\n    },\n     resources : [\n    ]\n}  There is also an  outputs  section, but I find most templates do not require this section.", 
            "title": "Creating an ARM Template"
        }, 
        {
            "location": "/chapter9/arm/#parameters-and-variables", 
            "text": "If I am creating a deployment called  zumobook , then I always name the resources like this:   Resource Group = zumobook  Azure App Service = zumobook  App Service Plan = zumobook-plan  SQL Azure Logical Server = zumobook-sql  SQL Azure Database = zumobook-db   All the resources are based on the root name of the deployment.  Ideally, the deployment name would be a parameter (something I specify) and the other names would be computed.  In addition, I always make the administrator login for the SQL Azure Logical Server be  azure  and I specify the password.  Let's take a look at what that would look like:  {\n     $schema :  http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json ,\n     contentVersion :  1.0.0.0 ,\n     parameters : {\n         serviceName : {\n             type :  string \n        },\n         sqlServerAdminLogin : {\n             type :  string ,\n             defaultValue :  azure \n        },\n         sqlServerAdminPassword : {\n             type :  securestring \n        }\n    },\n     variables : {\n         appServiceName :  parameters('serviceName') ,\n         appServicePlanName :  [concat(parameters('serviceName), '-plan')] ,\n         sqlServerName :  [concat(parameters('serviceName), '-sql')] ,\n         sqlDbName :  [concat(parameters('serviceName), '-db')] \n    },\n     resources : [\n    ]\n}   Variable Functions  Note the  concat()  is a template function.  You can find the large list of template functions in the  Azure Resource Manager documentation .   As a best practice, I abstract anything I would normally enter in creating the resource and make it either a parameter or a variable, depending on the circumstance.  I try to reduce the number of parameters and make as much as I can either a default value or a variable.  For example, one of the things that we need to set when configuring a SQL Azure database is the database size.  I could just specify this in bytes, but specifying the size in Mb is a better experience:       parameters : {\n         sqlDbSizeMb  {\n             type :  int ,\n             minValue : 1,\n             maxValue : 20,\n             description :  Size of the database in Mb ,\n             defaultValue : 5\n        }\n    },\n     variables : {\n         sqlDbSize :  [mul(parameters('sqlDbSizeMb'), 1048576)] \n    }", 
            "title": "Parameters and Variables"
        }, 
        {
            "location": "/chapter9/arm/#resources", 
            "text": "Resources describe the deployment of individual resources and their dependency.  There are, inevitably, more resources than you expect.   Each resource has an  apiVersion , formatted as a date, that describes the format of the resource.  Each resource also has a  name  and a  type  plus a number of other elements that describe configuration elements.  Finally, there is an optional  dependsOn  that ensures that resources are not configured until their dependent resources are deployed.  If, for example, I look at my deployment, I can represent this as a tree:   Resource Group  App Service Plan  App Service  App Settings  Connection Strings  Continuous Deployment Source      SQL Azure Logical Server  SQL Database  SQL Server Firewall Rules       Let's look at each resource in turn.", 
            "title": "Resources"
        }, 
        {
            "location": "/chapter9/arm/#the-sql-database", 
            "text": "There are actually three resources for the SQL Database - the server, the database and the firewall rule to allow other resources to connect to it:  {\n     $schema :  http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json ,\n     contentVersion :  1.0.0.0 ,\n     parameters : {\n         serviceName : {\n             type :  string \n        },\n         location : {\n             type :  string ,\n             defaultValue :  South Central US \n        },\n         sqlServerAdminLogin : {\n             type :  string ,\n             defaultValue :  azure \n        },\n         sqlServerAdminPassword : {\n             type :  securestring \n        },\n         sqlDbSizeMb  {\n             type :  int ,\n             minValue : 1,\n             maxValue : 20,\n             description :  Size of the database in Mb ,\n             defaultValue : 2\n        },\n         sqlDbCollation : {\n             type :  string ,\n             defaultValue :  SQL_Latin1_General_CP1_CI_AS \n        },\n         sqlDbEdition : {\n             type :  string ,\n             defaultValue :  Basic \n        },\n         sqlDbServiceObjectiveId : {\n             type :  string ,\n             defaultValue :  dd6d99bb-f193-4ec1-86f2-43d3bccbc49c \n        },\n    },\n     variables : {\n         sqlDbSize :  [mul(parameters('sqlDbSizeMb'), 1048576)] ,\n         sqlServerName :  [concat(parameters('serviceName), '-sql')] ,\n         sqlDbName :  [concat(parameters('serviceName), '-db')] \n    },\n     resources : [\n        {\n             apiVersion :  2014-04-01-preview ,\n             name :  [variables('sqlServerName')] ,\n             type :  Microsoft.Sql/servers ,\n             location :  [parameters('location')] ,\n             properties : {\n                 administratorLogin :  [parameters('sqlServerAdminLogin')] ,\n                 administratorPassword :  [parameters('sqlServerAdminPassword')] \n            },\n             resources : [\n                {\n                     apiVersion :  2015-05-01-preview ,\n                     name :  [variables('sqlDbName')] ,\n                     type :  databases ,\n                     location :  [parameters('location')] ,\n                     dependsOn : [\n                         [resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))] \n                    ],\n                     properties : {\n                         edition :  [parameters('sqlDbEdition')] ,\n                         collation :  [parameters('sqlDbCollation')] ,\n                         maxSizeBytes :  [variables('sqlDbSize')] ,\n                         requestedServiceObjectiveId :  [parameters('sqlDbServiceObjectiveId')] \n                    }\n                },\n                {\n                     apiVersion :  2014-04-01-preview ,\n                     name :  SQLServerFirewallRules ,\n                     type :  firewallrules ,\n                     location :  [parameters('location')] ,\n                     dependsOn : [\n                         [resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))] \n                    ],\n                     properties : {\n                         endIpAddress :  0.0.0.0 ,\n                         startIpAddress :  0.0.0.0 \n                    }\n                }\n            ]\n        },\n    ]\n}  This is a fairly lengthy example, but it incorporates a few things.  Firstly, note that I use the  [variables()]  and  [parameters()]  to bring in the variables and parameters respectively.  I also have embedded resources here.  The resource heirarchy is a tree-structure.  Each embedded resource has a  dependsOn  array that has the parent as a dependent resource.  You are probably wondering where you can get this information for any resource.  Microsoft Azure has a site called  resources.azure.com .  This contains the resources that you currently have configured on your subscription.  You can set the deployment up by hand and then look at the resources to get the details:   In this example, I'm showing the resource for the database.  I can extract the various pieces based on what I have set to get to this point - everything else is defaulted.  In addition, note that I can see the  apiVersion  at the top of the resource manager within the URL for the resource definition.", 
            "title": "The SQL Database"
        }, 
        {
            "location": "/chapter9/arm/#the-azure-app-service-plan", 
            "text": "My next resource is the App Service Plan.  Unlike the database (which has a definite heirarchy), the App Service Plan is separate because many App Services can use the same App Service Plan.  You can think of the App Service Plan as the set of virtual machines that belong in a single scale set, and the App Services as the web sites running on top of that scale set.  The App Service Plan looks like this:  {\n     $schema :  http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json ,\n     contentVersion :  1.0.0.0 ,\n     parameters : {\n         serviceName : {\n             type :  string \n        },\n         location : {\n             type :  string ,\n             defaultValue :  South Central US \n        },\n         sku : {\n             type :  string ,\n             allowedValues : [\n                 Free ,\n                 Shared ,\n                 Basic ,\n                 Standard \n            ],\n             defaultValue :  Basic \n        },\n         workerSize : {\n             type :  string ,\n             allowedValues : [\n                 0 ,  1 ,  2 \n            ],\n             defaultValue :  0 \n        }\n    },\n      variables : {\n         appServicePlanName :  [concat(parameters('serviceName), '-plan')] ,\n    },\n     resources : [\n        {\n             apiVersion :  2015-08-01 ,\n             name :  [variables('appServicePlanName')] ,\n             type :  Microsoft.Web/serverFarms ,\n             location :  [parameters('location')] ,\n             properties : {\n                 name :  [parameters('appServicePlanName')] ,\n                 sku :  [parameters('sku')] ,\n                 workerSize :  [parameters('workerSize')] ,\n                 numberOfWorkers : 1\n            }\n        }\n    ]\n}  The  hostingPlanName  is the name of the hosting plan.  This is normally something like F1, B2, or P3.  The first letter indicates the SKU and the number indicates the size of the worker.  The size of the worker is always one less than the number that is displayed within the Azure portal.", 
            "title": "The Azure App Service Plan"
        }, 
        {
            "location": "/chapter9/arm/#the-azure-app-service", 
            "text": "The Azure App Service is  {\n     $schema :  http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json ,\n     contentVersion :  1.0.0.0 ,\n     parameters : {\n         serviceName : {\n             type :  string \n        },\n         location : {\n             type :  string ,\n             defaultValue :  South Central US \n        },\n         sqlServerAdminLogin : {\n             type :  string ,\n             defaultValue :  azure \n        },\n         sqlServerAdminPassword : {\n             type :  securestring \n        },\n         deploymentRepoUrl : {\n             type :  string \n        },\n         deploymentRepoBranch : {\n             type :  string ,\n             defaultValue :  master \n        }\n    },\n     variables : {\n         appServiceName :  parameters('serviceName') ,\n         appServicePlanName :  [concat(parameters('serviceName), '-plan')] ,\n         sqlServerName :  [concat(parameters('serviceName), '-sql')] ,\n         sqlDbName :  [concat(parameters('serviceName), '-db')] \n    },\n     resources : [\n        {\n             apiVersion :  2015-04-01 ,\n             name :  [variables('appServiceName')] ,\n             type :  Microsoft.Web/sites ,\n             location :  [parameters('location')] ,\n             dependsOn : [\n                 [resourceId('Microsoft.Web/serverFarms', parameters('appServicePlanName'))] \n            ],\n             properties : {\n                 serverFarmId :  [parameters('appServicePlanName')] \n            },\n             resources : [\n                {\n                     apiVersion :  2015-04-01 ,\n                     name :  connectionstrings ,\n                     type :  config ,\n                     dependsOn : [\n                         [resourceId('Microsoft.Web/Sites', variables('appServiceName'))] ,\n                         [resourceId('Microsoft.Sql/servers', parameters('sqlServerName'))] \n                    ],\n                     properties : {\n                         MS_TableConnectionString : {  value :  [concat('Data Source=tcp:', reference(concat('Microsoft.Sql/servers/', variables('sqlServerName'))).fullyQualifiedDomainName, ',1433;Initial Catalog=', variables('sqlDbName'), ';User Id=', parameters('sqlServerAdminLogin'), '@', parameters('sqlServerName'), ';Password=', parameters('sqlServerAdminPassword'), ';')] ,  type :  SQLAzure  }\n                    }\n                },\n                {\n                     apiVersion :  2015-04-01 ,\n                     name :  web ,\n                     type :  sourcecontrols ,\n                     dependsOn : [\n                         [resourceId('Microsoft.Web/Sites', variables('appServiceName'))] ,\n                         [resourceId('Microsoft.Web/Sites/config', variables('appServiceName'), 'connectionstrings')] \n                    ],\n                     properties : {\n                         RepoUrl :  [parameters('deploymentRepoUrl')] ,\n                         branch :  [parameters('deploymentRepoBranch')] \n                    }\n                }\n            ]\n        },\n    ]\n}  In this case, we've got two dependent resources - one to set up the connection string to the database and the other to set up continuous deployment.  Both of those dependent resources have the normal  dependsOn  terms to ensure that the resources linked exist.  You can combine all three sections - the SQL Server, App Service Plan and App Service together.  There are overlapping parameters and variables.  You just need one copy.", 
            "title": "The Azure App Service"
        }, 
        {
            "location": "/chapter9/arm/#deploying-an-arm-template-on-windows", 
            "text": "Creating an Azure Resource Manager template on Windows can be done several ways, but my favorite is to use Visual Studio.   Start up Visual Studio 2017 and open your solution.  Right-click your solution and select  Add  -   New Project... .  Select  Installed     Visual C#     Cloud     Azure Resource Group .  Give the project a name, then click  OK  Select a template (I use  Web app + SQL  normally as a starting point), then click  OK .   Three files are created within the project:   Deploy-AzureResourceGroup.ps1  WebSiteSQLDatabase.json (or similar)  WebSiteSQLDatbase.parameters.json (or similar)   The  WebSiteSQLDatabase.json  is the ARM template.  The  .parameters.json  file contains the parameters for an unattended deployment.  Since the parameters contains the SQL server password, we need to be careful to ensure that the database password does not end up checked into source code.  Finally, the  Deploy-AzureResourceGroup.ps1  file is a handy PowerShell script for deploying to Azure.  Running it will be the equivalent of doing a deployment via Visual Studio.  You can now edit the ARM template to include any additional resources you need.  You can also edit the  .parameters.json  file to set standard values for any parameters you wish, if they don't change very often, although you won't need to do this as there is an easier way of setting parameters as we shall see.  There are two steps to deploying this ARM template: Validation and Deployment.  To validate the template, right-click the project and select  Validate .   Click into the  Resource group  and select  Create new .  This allows you to select a resource group name and a location.  Once you have done that, click  Create .  The next step is to click  Edit Parameters... .  This edit the  .parameters.json  file:   Note that we need to set a  hostingPlanName , as indicated by the red exclamation error mark.  We can also set any other values we want.  Anything that has a null value should be entered.  Click  Save  when complete.   Do not save password as plain text  It's likely that these files will end up in source control.  You should never save passwords as plain text in any file that is going to end up in a source control repository.   Finally, click  Validate .  You may get validation errors - fix the validation errors before continuing.  Once you have validated successfully, right-click the deployment project and select  Deploy , then select the resource group you entered during validation.  You may have to type the SQL administrator password multiple times.  Watch the output window for a progress report on your deployment.  Once complete, log into the  Azure portal  to check the resources.  Also check that the resources match what you were expecting within the  Azure Resource Explorer .", 
            "title": "Deploying an ARM Template on Windows"
        }, 
        {
            "location": "/chapter9/arm/#deploying-an-arm-template-on-macos", 
            "text": "The Mac does not come with the tools necessary to deploy ARM templates, so you will need to download and install them first.  In a Terminal window:  adrian$ curl -L https://aka.ms/InstallAzureCli | bash\nadrian$ exec -l $SHELL  The first command does the actual installation.  It will ask you questions about where to install and confirm that it can write to files.  In general, pressing Enter will do the right thing.  The second command restarts your shell.  After the installation is complete, run  az  in your Terminal window to ensure the installation was successful.  The next step is to login to Azure with the CLI.  Use the  az login  command from the Terminal window.  This command will output a code to use in the next step.   Note the code that is produced.  Now go to  http://aka.ms/devicelogin  and enter the code in the box provided.   Enter the code, press enter, then click  Continue .  You will be asked to authenticate to Azure.  Once that is complete, the terminal window will continue and you will be logged into Azure.  The  az login  command will provide a little piece of JSON to show you the subscription information to which you connected.  Deploying a set of resources is a two-step process on the command-line.  Firstly, you need to create a resource group.  Then you need to deploy the resources within that resource group.  To create a resource group, use:  az group create -l westus -n MyResourceGroup  You can get a list of locations that are available to you using:  az account list-locations | grep name  Take a look at the response from the  az group create  command.  Ensure that the  provisioningState  property is something that indicates success.  Now that we have a resource group, we can deploy our resources.  Edit the  .parameters.json  file to contain the values for your parameters.  For example:  {\n   $schema :  https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json# ,\n   contentVersion :  1.0.0.0 ,\n   parameters : {\n     hostingPlanName : {\n       value :  Chapter8-plan \n    },\n     skuName : {\n       value :  B1 \n    },\n     skuCapacity : {\n       value : 1\n    },\n     administratorLogin : {\n       value :  azure \n    },\n     administratorLoginPassword : {\n       value :  myPassword \n    },\n     databaseName : {\n       value :  ZUMOAPPDB \n    },\n     edition : {\n       value :  Basic \n    }\n  }\n}   Do not check in passwords!  If your parameters file contains password, do not check it in to source code control.   You can now deploy the service with the following:  az group deployment create --resource-group MyResourceGroup\n    --mode Complete\n    --parameters @WebSiteSQLDatabase.parameters.json\n    --template-file WebSiteSQLDatabase.json\n    --verbose  Although I have split this command over multiple lines, you should enter this command on one line.   Use --mode to maintain deployments  There are two modes for ARM deployments.   Complete  will remove any resources that are not listed in your ARM template.   Incremental  will only add resources to the resource group.  You can use these two modes to effectively maintain the resources in the resource group using only the command line.   At this point, the deployment will kick off.  It's a long running process, so expect a delay of a few minutes before it returns information.  This deployment is (deliberately) going to fail so we can see what happens when it goes wrong and how to fix it.  The response after this first run is this:  The deployment operation failed, because it was unable to cleanup. Please see https://aka.ms/arm-debug for usage details.  Correlation ID: aed1850d-22ca-4e28-b7fa-83a2ac587867  Start by logging into the  Azure portal , then go to the resource group was just created, then click  Deployments .  Select the latest deployment.   Click on the failed request:   The failed request is a lot clearer than the original failure message.  In this case, the password we entered for the SQL database did not meet the complexity requirements.  This is easily updated within the parameters file.  Then re-run the deployment.", 
            "title": "Deploying an ARM Template on MacOS"
        }, 
        {
            "location": "/chapter9/arm/#creating-the-arm-template", 
            "text": "One of the issues with ARM templates is their actual creation.  There are three methods of creating ARM templates:   Create a new Resource Group within Visual Studio (covered above)  Export a template from a Resource Group.  Log in to the  Azure portal .  Select your resource group.  In the menu, click  Automation Scripts .  Click  Download .    Download  a starter template from GitHub .   Which ever mechanism you choose to create the template, you will still need to edit the result by hand for your individual requirements.", 
            "title": "Creating the ARM Template"
        }, 
        {
            "location": "/chapter9/appsvc/", 
            "text": "Safe Deployments\n\n\nAt some point, you will have a production app that needs updating.  Hopefully, you will have many thousands of users by that point and they are clamouring for an update.  If this is you, congratulations.  You have a successful mobile app.  Now, how do you update your app without your users noticing?\n\n\nA big part of the success of any update process is to ensure your Entity Framework code is appropriately updating the database and that the database can handle both the old schema and new schema.  That means being able to cope with optional fields, and using default values.  It also means ensuring that you are using automatic code-first migrations, which I covered way back in \nChapter 3\n. \n\n\nIn this section, we will cover two topics:\n\n\n\n\nHow do I upgrade my mobile backend without my users noticing?\n\n\nHow do I scale my site to elastically manage load?\n\n\n\n\nFortunately, Azure App Service has built-in features for both these questions.\n\n\nUsing Slots\n\n\nLet's start with upgrading the mobile backend.  The feature that we are going to use is \nDeployment Slots\n.  A slot is a second (or third or fourth) copy of your mobile backend that shares all (or most) of the settings of the original, but runs different code.  Slots is available in Standard and above App Service Plans.  A Standard App Service has 5 slots and a Premium App Service has 20 slots. If your site is not running on a Standard service, use \nScale Up\n to change the App Service Plan to a Standard or better plan.\n\n\n\n\nCheck your App Service Plan\n\n\nEach App Service Plan has a number of slots.  This is the number of web sites or mobile backends you can run on the same App Service Plan.  Remember an App Service Plan is a set of virtual machines that run the same set of web sites.   The slot is also a web site.\n\n\n\n\nLet's say you want to deploy your site with zero downtime.  Establish a slot called \"staging\":\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick \nDeployment slots\n.\n\n\nClick \nAdd Slot\n.\n\n\nEnter \nstaging\n in the \nName\n field.\n\n\nClick \nOK\n.\n\n\n\n\nThis is a one-time activity.  If the main site is called \nmywebsite.azurewebsites.net\n, then the new slot will be called \nmywebsite-staging.azurewebsites.net\n.  We can deploy the server to the new slot in almost exactly the same way as before.  Instead of selecting your website, expand the website, select \nDeployment Slots\n then the name of the slot that was created:\n\n\n![Select the slot][img]\n\n\n\n\nUpdate your client to use -staging\n\n\nYou can produce a test version of your mobile client that uses the staging slot as the URL.  This allows you to do ad-hoc testing prior to doing the update to production.\n\n\n\n\nOnce the mobile backend is deployed to the staging slot, you can do any testing required before deploying to production.  The next step is to turn staging into production:\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick \nDeployment slots\n.\n\n\nClick \nSwap\n.\n\n\nSelect the Source and Destination slots to swap.\n\n\nClick \nOK\n.\n\n\n\n\nAt this point, the internal routing will swap the two services.  Existing requests that are already being handled by the old server will be completed on the old server, but new requests will be handled by the new server.  Once the swap is complete, there will be no more requests being dealt with on the old server.  \n\n\nIf your test client (configured to use the -staging URL) is run, it will be using the old server.  The old server is in the -staging slot now.\n\n\nBest Practices: 3 Slots\n\n\nI recommend using three slots:\n\n\n\n\nProduction\n\n\nStaging\n\n\nLastKnownGood\n\n\n\n\nWhen you swap the slot, do two swaps:\n\n\n\n\nFirst, deploy your new server to \nStaging\n.\n\n\nThen, swap \nProduction\n with \nStaging\n.  Your new server is now in production.\n\n\nFinally, swap \nStaging\n with \nLastKnownGood\n.  This saves the (working) production server.\n\n\n\n\nIf your new server runs into problems and you have to switch back to the old server, it is stored away in the \nLastKnownGood\n slot.  You can restore the old (working) server by swapping \nLastKnownGood\n with \nProduction\n.\n\n\nBest Practices: Continuous Deployment\n\n\nI also recommend linking a Continuous Deployment stream to the staging slot, rather than deploying via Visual Studio.  Continuous Deployment automatically deploys code from a source code repository like GitHub or Visual Studio Team Services.  To link staging to a GitHub repository:\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick \nDeployment slots\n.\n\n\nClick the staging slot name.\n\n\nClick \nDeployment options\n.\n\n\nClick \nChoose Source\n, then select GitHub.\n\n\nIf necessary, click \nAuthorization\n to log in to GitHub.\n\n\nIf necessary, click \nChoose your organization\n to select a GitHub organization.\n\n\nClick \nChoose project\n and select your GitHub repository.\n\n\nClick \nChoose branch\n and select the branch you wish to deploy.\n\n\nClick \nOK\n.\n\n\n\n\nThe service will download the latest source code from the specified branch, build the server and deploy it.  You can check the status of the deployment through \nDeployment options\n on the staging slot as well.\n\n\nAs a best practice, create a new branch (I call my branch \"azure\") and deploy that branch.  When you wish to update the deployed server, merge from the master branch to the azure branch and then push to the remote repository.  This will then be picked up by your App Service and the new service will be deployed.\n\n\nBy combining continuous deployment with slots, you can have control over the deployment to staging and control the production deployment easily.  This is a powerful \"DevOps\" combination of features.\n\n\nScaling your Site\n\n\nI hope that your mobile app is wildly successful.  If it is, one copy of the server will not be enough to handle the connections.  You will need to scale both the mobile backend and the database.  How will you know when to scale?\n\n\n\n\nFor the App Service, keep an eye on the \nMetrics per Instance (App Service plan)\n.  If your production service is getting close to 80% CPU or close to memory limits, it's probably time to scale the site.\n\n\nFor the database, check the \nQuery Performance Insight\n.  Your database is limited to a certain number of DTUs (database transaction units - a blend of CPU and I/O operations).\n\n\n\n\nIn a production app, you want to scale your mobile backend according to demand.  This means that the service spins up additional copies of the App Service as needed.  To set up automatic scaling for an App Service:\n\n\n\n\nOpen your App Service in the \nAzure portal\n.\n\n\nClick **Scale out (App Service plan).\n\n\nSelect \nCPU Percentage\n in the \nScale by\n drop-down.\n\n\nSelect a minimum and maximum number of instances (for example, 1-10 for a standard App Service Plan).\n\n\nSelect a target range for CPU (for example, 50-75%)\n\n\nOptionally, fill in the \nNotifications\n section.\n\n\nClick \nSave\n.\n\n\n\n\nThe App Service Plan will try and keep an appropriate number of instances such that the CPU is kept in the specified range.  If the average CPU drops below the threshold, the App Service Plan will take one of the instances out of server.  No new connections will be sent to the instance, the existing requests will be allowed to complete.  Once all connections are complete, the instance will be turned off.   You only pay for the instances that are actually running.\n\n\nYou can choose to send an email when a scale event happens.  You can also configure a Webhook so that your operations group is notified more broadly.  For example, you could use this facility to run an Azure Function that sends a notification to a Slack channel when an instance is added, or when more the last instance is added (indicating extreme load where your users are going to be impacted).\n\n\nBest Practices for Production Apps\n\n\nWe've mentioned a few best practices here, so here is a wrap-up:\n\n\n\n\nRun production apps on Standard or Premium App Services.\n\n\nSet up automatic scaling based on CPU percentage.\n\n\nSet up alerting so that you are informed when the last instance is added.\n\n\nUse three slots (production, staging and lastknowngood) to ensure no downtime.\n\n\nUse continuous deployment to the staging slot rather than deploy from Visual Studio.\n\n\n\n\nFollow these best practices and you will ensure your users are not limited by mobile backend deployments or load.", 
            "title": "Safe Deployments"
        }, 
        {
            "location": "/chapter9/appsvc/#safe-deployments", 
            "text": "At some point, you will have a production app that needs updating.  Hopefully, you will have many thousands of users by that point and they are clamouring for an update.  If this is you, congratulations.  You have a successful mobile app.  Now, how do you update your app without your users noticing?  A big part of the success of any update process is to ensure your Entity Framework code is appropriately updating the database and that the database can handle both the old schema and new schema.  That means being able to cope with optional fields, and using default values.  It also means ensuring that you are using automatic code-first migrations, which I covered way back in  Chapter 3 .   In this section, we will cover two topics:   How do I upgrade my mobile backend without my users noticing?  How do I scale my site to elastically manage load?   Fortunately, Azure App Service has built-in features for both these questions.", 
            "title": "Safe Deployments"
        }, 
        {
            "location": "/chapter9/appsvc/#using-slots", 
            "text": "Let's start with upgrading the mobile backend.  The feature that we are going to use is  Deployment Slots .  A slot is a second (or third or fourth) copy of your mobile backend that shares all (or most) of the settings of the original, but runs different code.  Slots is available in Standard and above App Service Plans.  A Standard App Service has 5 slots and a Premium App Service has 20 slots. If your site is not running on a Standard service, use  Scale Up  to change the App Service Plan to a Standard or better plan.   Check your App Service Plan  Each App Service Plan has a number of slots.  This is the number of web sites or mobile backends you can run on the same App Service Plan.  Remember an App Service Plan is a set of virtual machines that run the same set of web sites.   The slot is also a web site.   Let's say you want to deploy your site with zero downtime.  Establish a slot called \"staging\":   Open your App Service in the  Azure portal .  Click  Deployment slots .  Click  Add Slot .  Enter  staging  in the  Name  field.  Click  OK .   This is a one-time activity.  If the main site is called  mywebsite.azurewebsites.net , then the new slot will be called  mywebsite-staging.azurewebsites.net .  We can deploy the server to the new slot in almost exactly the same way as before.  Instead of selecting your website, expand the website, select  Deployment Slots  then the name of the slot that was created:  ![Select the slot][img]   Update your client to use -staging  You can produce a test version of your mobile client that uses the staging slot as the URL.  This allows you to do ad-hoc testing prior to doing the update to production.   Once the mobile backend is deployed to the staging slot, you can do any testing required before deploying to production.  The next step is to turn staging into production:   Open your App Service in the  Azure portal .  Click  Deployment slots .  Click  Swap .  Select the Source and Destination slots to swap.  Click  OK .   At this point, the internal routing will swap the two services.  Existing requests that are already being handled by the old server will be completed on the old server, but new requests will be handled by the new server.  Once the swap is complete, there will be no more requests being dealt with on the old server.    If your test client (configured to use the -staging URL) is run, it will be using the old server.  The old server is in the -staging slot now.", 
            "title": "Using Slots"
        }, 
        {
            "location": "/chapter9/appsvc/#best-practices-3-slots", 
            "text": "I recommend using three slots:   Production  Staging  LastKnownGood   When you swap the slot, do two swaps:   First, deploy your new server to  Staging .  Then, swap  Production  with  Staging .  Your new server is now in production.  Finally, swap  Staging  with  LastKnownGood .  This saves the (working) production server.   If your new server runs into problems and you have to switch back to the old server, it is stored away in the  LastKnownGood  slot.  You can restore the old (working) server by swapping  LastKnownGood  with  Production .", 
            "title": "Best Practices: 3 Slots"
        }, 
        {
            "location": "/chapter9/appsvc/#best-practices-continuous-deployment", 
            "text": "I also recommend linking a Continuous Deployment stream to the staging slot, rather than deploying via Visual Studio.  Continuous Deployment automatically deploys code from a source code repository like GitHub or Visual Studio Team Services.  To link staging to a GitHub repository:   Open your App Service in the  Azure portal .  Click  Deployment slots .  Click the staging slot name.  Click  Deployment options .  Click  Choose Source , then select GitHub.  If necessary, click  Authorization  to log in to GitHub.  If necessary, click  Choose your organization  to select a GitHub organization.  Click  Choose project  and select your GitHub repository.  Click  Choose branch  and select the branch you wish to deploy.  Click  OK .   The service will download the latest source code from the specified branch, build the server and deploy it.  You can check the status of the deployment through  Deployment options  on the staging slot as well.  As a best practice, create a new branch (I call my branch \"azure\") and deploy that branch.  When you wish to update the deployed server, merge from the master branch to the azure branch and then push to the remote repository.  This will then be picked up by your App Service and the new service will be deployed.  By combining continuous deployment with slots, you can have control over the deployment to staging and control the production deployment easily.  This is a powerful \"DevOps\" combination of features.", 
            "title": "Best Practices: Continuous Deployment"
        }, 
        {
            "location": "/chapter9/appsvc/#scaling-your-site", 
            "text": "I hope that your mobile app is wildly successful.  If it is, one copy of the server will not be enough to handle the connections.  You will need to scale both the mobile backend and the database.  How will you know when to scale?   For the App Service, keep an eye on the  Metrics per Instance (App Service plan) .  If your production service is getting close to 80% CPU or close to memory limits, it's probably time to scale the site.  For the database, check the  Query Performance Insight .  Your database is limited to a certain number of DTUs (database transaction units - a blend of CPU and I/O operations).   In a production app, you want to scale your mobile backend according to demand.  This means that the service spins up additional copies of the App Service as needed.  To set up automatic scaling for an App Service:   Open your App Service in the  Azure portal .  Click **Scale out (App Service plan).  Select  CPU Percentage  in the  Scale by  drop-down.  Select a minimum and maximum number of instances (for example, 1-10 for a standard App Service Plan).  Select a target range for CPU (for example, 50-75%)  Optionally, fill in the  Notifications  section.  Click  Save .   The App Service Plan will try and keep an appropriate number of instances such that the CPU is kept in the specified range.  If the average CPU drops below the threshold, the App Service Plan will take one of the instances out of server.  No new connections will be sent to the instance, the existing requests will be allowed to complete.  Once all connections are complete, the instance will be turned off.   You only pay for the instances that are actually running.  You can choose to send an email when a scale event happens.  You can also configure a Webhook so that your operations group is notified more broadly.  For example, you could use this facility to run an Azure Function that sends a notification to a Slack channel when an instance is added, or when more the last instance is added (indicating extreme load where your users are going to be impacted).", 
            "title": "Scaling your Site"
        }, 
        {
            "location": "/chapter9/appsvc/#best-practices-for-production-apps", 
            "text": "We've mentioned a few best practices here, so here is a wrap-up:   Run production apps on Standard or Premium App Services.  Set up automatic scaling based on CPU percentage.  Set up alerting so that you are informed when the last instance is added.  Use three slots (production, staging and lastknowngood) to ensure no downtime.  Use continuous deployment to the staging slot rather than deploy from Visual Studio.   Follow these best practices and you will ensure your users are not limited by mobile backend deployments or load.", 
            "title": "Best Practices for Production Apps"
        }, 
        {
            "location": "/chapter9/monitoring/", 
            "text": "Monitoring and Troubleshooting\n\n\nThe last two topics are joined together - monitoring and troubleshooting.  There are many reasons for monitoring:\n\n\n\n\nTo gain marketing information into your userbase.\n\n\nTo detect potential security threats.\n\n\nTo detect problems in your code.\n\n\n\n\nThis latter reason is the reason most developers want to monitor their mobile app.  However, there is much to be gained from monitoring your mobile app for marketing purposes, and security should be a top-of-mind issue for everyone with an app that others use.  Security is generally monitored at the mobile backend.  Marketing information is gleaned from monitoring the usage of the mobile client, and problems can occur anywhere in your code base.\n\n\nYou should always monitor as close to your mobile backend as possible.  If you monitor your mobile backend from another provider (for example), you will be paying for network charges needlessly.  Except in rare cases (such as the recently advertised Amazon outages), the cloud provider will keep the cloud operating without any involvement from you.  Even in those cases where there is an outage at the cloud provider, you will not be able to prevent the outage simply by having monitoring elsewhere.\n\n\nSince the preferred hosting platform for Azure Mobile Apps is Azure App Service, this means using the analytics platform provided by Azure.  This consists of two parts:\n\n\n\n\nApplication Insights\n for monitoring and troubleshooting the mobile backend.\n\n\nVisual Studio Mobile Center\n for monitoring and troubleshooting the mobile client.\n\n\n\n\nMonitoring your Backend\n\n\nThere is a lot of \nmonitoring built into Azure App Service\n.  For these tools, you don't have to do anything.  They are always available.  However, there are certain pieces of information that are not going to be available.  For instance, you will not be able to correlate uploads with users since Azure App Service really doesn't know anything about users.  For this reason, it's a good idea to integrate \nApplication Insights\n into your mobile backend.  This is a two-step process:\n\n\n\n\nCreate an Application Insights instance in your Azure resource group.\n\n\nAdd Application Insights to your mobile backend project.\n\n\n\n\nThere is more to Application Insights than the basics we are going to cover here, including \ncustom events\n, \nfiltering\n, and \nintegration with other platforms\n.  Application Insights is a powerful service and we will only be able to scratch the surface of it.\n\n\n\n\nAutomate Application Insights creation with ARM\n\n\nJust like all other Azure resources, you can create an Application Insights with an automated deployment via an ARM template.\n\n\n\n\nCreate an Application Insights Instance\n\n\nStart by opening the \nAzure portal\n, then:\n\n\n\n\nOpen your resource group.\n\n\nClick \nAdd\n.\n\n\nSearch for then click \nApplication Insights\n.\n\n\nClick \nCreate\n.\n\n\nEnter a name for the resource.  Pick \nASP.NET web application\n for the application type.\n\n\nSelect the nearest location to your mobile backend.\n\n\n\n\n\n\nApplication Insights isn't everywhere\n\n\nApplication Insights can only be placed in larger regions where storage and compute capacity is large.  That means your Application Insights will probably not be in the same location as your mobile backend.\n\n\n\n\n\n\nOnce you have set the required configuration, click \nCreate\n.\n\n\n\n\nThere won't be any data until you have connected your mobile backend to the Application Insights resource.\n\n\nAdd Application Insights to your Mobile Backend\n\n\nLet's continue with our cloud backend that we developed for \nChapter 8\n.  Open your solution in Visual Studio, right-click the project and select \nAdd\n -\n \nApplication Insights Telemetry...\n.\n\n\n\n\nOnce added, the configuration process will start.  Click \nStart Free\n.\n\n\n\n\nIf you need to, select and authenticate to the right Azure account and select your subscription.  In the \nResource\n drop-down, select the Application Insights resource you just created.  Then click \nRegister\n.  This will add the appropriate NuGet packages to your project and configure most of what is required in the source code.  Once the process is complete, click \nFinish\n.\n\n\nYou can optionally enable trace collection.  This allows you to search the output from \nSystem.Diagnostics\n, enabling you to capture even more information.  Diagnostic data is generally verbose and slows the service down.  The free tier of Application Insights is limited to 1Gb/month, so this step is optional.\n\n\nYour last remaining step is to publish your site to Azure App Service, which we have done many times during the course of the book.\n\n\nViewing Application Insights Data\n\n\nThe Application Insights data is available through the \nAzure portal\n or within Visual Studio.  Before we go any further, generate some Application Insights data by using your mobile app.  Once done, you can review the Application Insights data by right-clicking on your backend project and choosing \nApplication Insights\n -\n \nOpen Application Insights Portal\n.  The same page is available by clicking the Application Insights resource in your resource group.\n\n\nApplication Insights is constantly changing, so take some time to learn about the metrics and reports you can view using the portal.\n\n\nMonitoring your Mobile Clients\n\n\nMonitoring your mobile client is important to gather usage and marketing data about your users.  You can also find out how your users are actually using your mobile app.  This allows you to target additional improvements in areas that people find useful and perhaps remove or improve areas that are less used.  To integrate monitoring, first create a \nVisual Studio Mobile Center\n app as we described in the \nTesting\n section.\n\n\n\n\nReturn to the \nVisual Studio Mobile Center\n and select your app.\n\n\nIf you were at another beacon, click \nGetting Started\n.\n\n\nClick \nManage app\n.\n\n\nCopy the App secret somewhere accessible - we will need it in a few moments.\n\n\nOpen the mobile app project in Visual Studio 2017 or Visual Studio for Mac.\n\n\nRight-click the solution and select \nManage NuGet Packages for Solution...\n.\n\n\nClick \nBrowse\n, then search for \nMicrosoft.Azure.Mobile.Analytics\n.\n\n\nInstall the \nMicrosoft.Azure.Mobile.Analytics\n package into the Android and iOS projects.\n\n\n\n\nOnce the package(s) are installed, open \nApp.cs\n in the shared project.  Add the \nMobileCenter.Start\n line at the start of the constructor:\n\n\nusing Microsoft.Azure.Mobile;\nusing Microsoft.Azure.Mobile.Analytics;\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            MobileCenter.Start(\nandroid=609b2734-0353-4e71-a654-fedd9df1632a\n, typeof(Analytics));\n\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}\n\n\n\n\nWhen you start your app, the analytics service will send a session start message to Visual Studio Mobile Center, allowing you to get basic demographics about your mobile app usage, such as how many distinct users you have, where they are located, and what device(s) they are using.  To get this level of information, visit the Analytics page in the Visual Studio Mobile Center portal.\n\n\nYou can get better analytics by defining custom events.  An event is a custom interaction with the content in your app that allows you to better understand your user's behavior.  For example, you may want to understand what sort of things your users are uploading - are they uploading from the camera or picking a picture?  You will not be able to get this information from the server.  The server will only be able to tell that the user uploaded a picture.  To use custom events, you can use the \ntrackEvent()\n method, like this:\n\n\n// at the top of the file:\n// using Microsoft.Azure.Mobile.Analytics\n\nAnalytics.TrackEvent(\nFILE_UPLOAD\n, new Dictionary\nstring, string\n {\n    { \nSource\n, \nCamera\n },\n    { \nFileName\n, \npic1.jpg\n }\n});  \n\n\n\n\nTo analyze custom events, go to the Events page in the Visual Studio Mobile Center portal.  The events are attached to a session so you can report on the number of distinct users using a camera for upload, for example.  Custom Events are limited by your imagination, but they do have a cost associated with them - each event does cost a small amount of bandwidth to the user.\n\n\n\n\nOther Analytics Limits\n\n\nYou can only produce 200 custom event types per app.  The \"FILE_UPLOAD\" is an event type.  There is a total maximum of 256 characters per event type.  Each property name and value must be less than 64 characters.\n\n\n\n\nCrash Reporting\n\n\nOnce your application is out in the wild, whether it be with your beta testers, your employees or the general public via a public app store, you want to ensure it is working.  You can do all the testing available to you and there will still be some combination of factors that may cause your application to crash.  Your users will most likely just delete your application at that point, so it's a good idea to collect the cause of the crash before they do that.  \n\n\nTo integrate crash reporting, add the Mobile Center SDK to your project, then add a single line of code to your application.  We've already \ncreated a Visual Studio Mobile Center app\n for beta distribution (which we covered above), so we'll use the same application.\n\n\n\n\nReturn to the \nVisual Studio Mobile Center\n and select your app.\n\n\nIf you were at another beacon, click \nGetting Started\n.\n\n\nClick \nManage app\n.\n\n\nCopy the App secret somewhere accessible - we will need it in a few moments.\n\n\nOpen the mobile app project in Visual Studio 2017 or Visual Studio for Mac.\n\n\nRight-click the solution and select \nManage NuGet Packages for Solution...\n.\n\n\nClick \nBrowse\n, then search for \nMicrosoft.Azure.Mobile.Crashes\n.\n\n\nInstall the \nMicrosoft.Azure.Mobile.Crashes\n package into the Android and iOS projects.\n\n\n\n\nIf you have both an iOS and Android app, make note of BOTH app secrets - one from each app within Mobile Center.\n\n\nOnce the package(s) are installed, open \nApp.cs\n in the shared project.  Add the \nMobileCenter.Start\n line at the start of the constructor:\n\n\nusing Microsoft.Azure.Mobile;\nusing Microsoft.Azure.Mobile.Analytics;\nusing Microsoft.Azure.Mobile.Crashes;\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            MobileCenter.Start(\nandroid=609b2734-0353-4e71-a654-fedd9df1632a\n, \n                typeof(Analytics), typeof(Crashes));\n\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}\n\n\n\n\nI am combining analytics with crash reporting here.  You can choose to integrate analytics, crash reporting or both by adjusting the \nMobileCenter.Start()\n call.  Replace the app secret placeholders with your app secrets.\n\n\nEventually, your app is going to crash.  When this happens, the crash will appear in the \nCrashes\n service within Mobile Center, allowing you to see what type of mobile device was being used, the version of the app being run and the stack trace of the app at that point.  Visual Studio Mobile Center also groups like crashes together so that you can see commonality between crashes and better target your bug fixing.  \n\n\nBest Practices\n\n\nThere is, thankfully, a short list of best practices to close out the book:\n\n\n\n\nIntegrate Analytics and Crash Reporting in every mobile app.\n\n\nIntegrate Application Insights into your mobile backend.\n\n\nInvestigate every single crash reported by your mobile app.\n\n\nUse UI Testing to re-create the crash.\n\n\nUse a bug tracking system such as the one integrated into GitHub or Visual Studio Team Services.\n\n\nInvestigate every \n500 Internal Server Error\n produced by your mobile backend.  It means your server crashed.\n\n\n\n\nAnd in closing\n\n\nThank you for taking the time to read this book.  I hope you found it informative.  \n\n\nNow go and make awesome mobile apps!", 
            "title": "Monitoring and Troubleshooting"
        }, 
        {
            "location": "/chapter9/monitoring/#monitoring-and-troubleshooting", 
            "text": "The last two topics are joined together - monitoring and troubleshooting.  There are many reasons for monitoring:   To gain marketing information into your userbase.  To detect potential security threats.  To detect problems in your code.   This latter reason is the reason most developers want to monitor their mobile app.  However, there is much to be gained from monitoring your mobile app for marketing purposes, and security should be a top-of-mind issue for everyone with an app that others use.  Security is generally monitored at the mobile backend.  Marketing information is gleaned from monitoring the usage of the mobile client, and problems can occur anywhere in your code base.  You should always monitor as close to your mobile backend as possible.  If you monitor your mobile backend from another provider (for example), you will be paying for network charges needlessly.  Except in rare cases (such as the recently advertised Amazon outages), the cloud provider will keep the cloud operating without any involvement from you.  Even in those cases where there is an outage at the cloud provider, you will not be able to prevent the outage simply by having monitoring elsewhere.  Since the preferred hosting platform for Azure Mobile Apps is Azure App Service, this means using the analytics platform provided by Azure.  This consists of two parts:   Application Insights  for monitoring and troubleshooting the mobile backend.  Visual Studio Mobile Center  for monitoring and troubleshooting the mobile client.", 
            "title": "Monitoring and Troubleshooting"
        }, 
        {
            "location": "/chapter9/monitoring/#monitoring-your-backend", 
            "text": "There is a lot of  monitoring built into Azure App Service .  For these tools, you don't have to do anything.  They are always available.  However, there are certain pieces of information that are not going to be available.  For instance, you will not be able to correlate uploads with users since Azure App Service really doesn't know anything about users.  For this reason, it's a good idea to integrate  Application Insights  into your mobile backend.  This is a two-step process:   Create an Application Insights instance in your Azure resource group.  Add Application Insights to your mobile backend project.   There is more to Application Insights than the basics we are going to cover here, including  custom events ,  filtering , and  integration with other platforms .  Application Insights is a powerful service and we will only be able to scratch the surface of it.   Automate Application Insights creation with ARM  Just like all other Azure resources, you can create an Application Insights with an automated deployment via an ARM template.", 
            "title": "Monitoring your Backend"
        }, 
        {
            "location": "/chapter9/monitoring/#create-an-application-insights-instance", 
            "text": "Start by opening the  Azure portal , then:   Open your resource group.  Click  Add .  Search for then click  Application Insights .  Click  Create .  Enter a name for the resource.  Pick  ASP.NET web application  for the application type.  Select the nearest location to your mobile backend.    Application Insights isn't everywhere  Application Insights can only be placed in larger regions where storage and compute capacity is large.  That means your Application Insights will probably not be in the same location as your mobile backend.    Once you have set the required configuration, click  Create .   There won't be any data until you have connected your mobile backend to the Application Insights resource.", 
            "title": "Create an Application Insights Instance"
        }, 
        {
            "location": "/chapter9/monitoring/#add-application-insights-to-your-mobile-backend", 
            "text": "Let's continue with our cloud backend that we developed for  Chapter 8 .  Open your solution in Visual Studio, right-click the project and select  Add  -   Application Insights Telemetry... .   Once added, the configuration process will start.  Click  Start Free .   If you need to, select and authenticate to the right Azure account and select your subscription.  In the  Resource  drop-down, select the Application Insights resource you just created.  Then click  Register .  This will add the appropriate NuGet packages to your project and configure most of what is required in the source code.  Once the process is complete, click  Finish .  You can optionally enable trace collection.  This allows you to search the output from  System.Diagnostics , enabling you to capture even more information.  Diagnostic data is generally verbose and slows the service down.  The free tier of Application Insights is limited to 1Gb/month, so this step is optional.  Your last remaining step is to publish your site to Azure App Service, which we have done many times during the course of the book.", 
            "title": "Add Application Insights to your Mobile Backend"
        }, 
        {
            "location": "/chapter9/monitoring/#viewing-application-insights-data", 
            "text": "The Application Insights data is available through the  Azure portal  or within Visual Studio.  Before we go any further, generate some Application Insights data by using your mobile app.  Once done, you can review the Application Insights data by right-clicking on your backend project and choosing  Application Insights  -   Open Application Insights Portal .  The same page is available by clicking the Application Insights resource in your resource group.  Application Insights is constantly changing, so take some time to learn about the metrics and reports you can view using the portal.", 
            "title": "Viewing Application Insights Data"
        }, 
        {
            "location": "/chapter9/monitoring/#monitoring-your-mobile-clients", 
            "text": "Monitoring your mobile client is important to gather usage and marketing data about your users.  You can also find out how your users are actually using your mobile app.  This allows you to target additional improvements in areas that people find useful and perhaps remove or improve areas that are less used.  To integrate monitoring, first create a  Visual Studio Mobile Center  app as we described in the  Testing  section.   Return to the  Visual Studio Mobile Center  and select your app.  If you were at another beacon, click  Getting Started .  Click  Manage app .  Copy the App secret somewhere accessible - we will need it in a few moments.  Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac.  Right-click the solution and select  Manage NuGet Packages for Solution... .  Click  Browse , then search for  Microsoft.Azure.Mobile.Analytics .  Install the  Microsoft.Azure.Mobile.Analytics  package into the Android and iOS projects.   Once the package(s) are installed, open  App.cs  in the shared project.  Add the  MobileCenter.Start  line at the start of the constructor:  using Microsoft.Azure.Mobile;\nusing Microsoft.Azure.Mobile.Analytics;\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            MobileCenter.Start( android=609b2734-0353-4e71-a654-fedd9df1632a , typeof(Analytics));\n\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}  When you start your app, the analytics service will send a session start message to Visual Studio Mobile Center, allowing you to get basic demographics about your mobile app usage, such as how many distinct users you have, where they are located, and what device(s) they are using.  To get this level of information, visit the Analytics page in the Visual Studio Mobile Center portal.  You can get better analytics by defining custom events.  An event is a custom interaction with the content in your app that allows you to better understand your user's behavior.  For example, you may want to understand what sort of things your users are uploading - are they uploading from the camera or picking a picture?  You will not be able to get this information from the server.  The server will only be able to tell that the user uploaded a picture.  To use custom events, you can use the  trackEvent()  method, like this:  // at the top of the file:\n// using Microsoft.Azure.Mobile.Analytics\n\nAnalytics.TrackEvent( FILE_UPLOAD , new Dictionary string, string  {\n    {  Source ,  Camera  },\n    {  FileName ,  pic1.jpg  }\n});    To analyze custom events, go to the Events page in the Visual Studio Mobile Center portal.  The events are attached to a session so you can report on the number of distinct users using a camera for upload, for example.  Custom Events are limited by your imagination, but they do have a cost associated with them - each event does cost a small amount of bandwidth to the user.   Other Analytics Limits  You can only produce 200 custom event types per app.  The \"FILE_UPLOAD\" is an event type.  There is a total maximum of 256 characters per event type.  Each property name and value must be less than 64 characters.", 
            "title": "Monitoring your Mobile Clients"
        }, 
        {
            "location": "/chapter9/monitoring/#crash-reporting", 
            "text": "Once your application is out in the wild, whether it be with your beta testers, your employees or the general public via a public app store, you want to ensure it is working.  You can do all the testing available to you and there will still be some combination of factors that may cause your application to crash.  Your users will most likely just delete your application at that point, so it's a good idea to collect the cause of the crash before they do that.    To integrate crash reporting, add the Mobile Center SDK to your project, then add a single line of code to your application.  We've already  created a Visual Studio Mobile Center app  for beta distribution (which we covered above), so we'll use the same application.   Return to the  Visual Studio Mobile Center  and select your app.  If you were at another beacon, click  Getting Started .  Click  Manage app .  Copy the App secret somewhere accessible - we will need it in a few moments.  Open the mobile app project in Visual Studio 2017 or Visual Studio for Mac.  Right-click the solution and select  Manage NuGet Packages for Solution... .  Click  Browse , then search for  Microsoft.Azure.Mobile.Crashes .  Install the  Microsoft.Azure.Mobile.Crashes  package into the Android and iOS projects.   If you have both an iOS and Android app, make note of BOTH app secrets - one from each app within Mobile Center.  Once the package(s) are installed, open  App.cs  in the shared project.  Add the  MobileCenter.Start  line at the start of the constructor:  using Microsoft.Azure.Mobile;\nusing Microsoft.Azure.Mobile.Analytics;\nusing Microsoft.Azure.Mobile.Crashes;\nusing TaskList.Abstractions;\nusing TaskList.Services;\nusing Xamarin.Forms;\n\nnamespace TaskList\n{\n    public class App : Application\n    {\n        public static ICloudService CloudService { get; set; }\n\n        public App()\n        {\n            MobileCenter.Start( android=609b2734-0353-4e71-a654-fedd9df1632a , \n                typeof(Analytics), typeof(Crashes));\n\n#if USE_MOCK_SERVICES\n            CloudService = new MockCloudService();\n#else\n            CloudService = new AzureCloudService();\n#endif\n            MainPage = new NavigationPage(new Pages.EntryPage());\n        }\n    }\n}  I am combining analytics with crash reporting here.  You can choose to integrate analytics, crash reporting or both by adjusting the  MobileCenter.Start()  call.  Replace the app secret placeholders with your app secrets.  Eventually, your app is going to crash.  When this happens, the crash will appear in the  Crashes  service within Mobile Center, allowing you to see what type of mobile device was being used, the version of the app being run and the stack trace of the app at that point.  Visual Studio Mobile Center also groups like crashes together so that you can see commonality between crashes and better target your bug fixing.", 
            "title": "Crash Reporting"
        }, 
        {
            "location": "/chapter9/monitoring/#best-practices", 
            "text": "There is, thankfully, a short list of best practices to close out the book:   Integrate Analytics and Crash Reporting in every mobile app.  Integrate Application Insights into your mobile backend.  Investigate every single crash reported by your mobile app.  Use UI Testing to re-create the crash.  Use a bug tracking system such as the one integrated into GitHub or Visual Studio Team Services.  Investigate every  500 Internal Server Error  produced by your mobile backend.  It means your server crashed.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/chapter9/monitoring/#and-in-closing", 
            "text": "Thank you for taking the time to read this book.  I hope you found it informative.    Now go and make awesome mobile apps!", 
            "title": "And in closing"
        }, 
        {
            "location": "/xamarin_tips/", 
            "text": "Xamarin Forms Tips\n\n\nOver the many months I have spent coding Xamarin Forms, a number of people from the community and the Xamarin team have given me little tips to improve my Xamarin Forms code.  I hope you find them as useful as I did.\n\n\nImprove your ListView performance\n\n\nIt should be no surprise that performance matters.  Little things like smooth scrolling and fast load times are a must.  A lot of improvement can be gained by the techniques we have shown in this book since most of the perceived delays are caused by the back end loading times.\n\n\nEventually, a lot of apps generate a list.  The normal way for implementing this is with a \nListView\n that has an \nObservableCollection\n  You load your data into the ObservableCollection and then update that whenever the data changes.  That, in turn, updates the ListView.\n\n\nThere are two problems that are inherent here.  The first is in the ObservableCollection and the second is in the ListView.  Let's tackle the ObservableCollection first.\n\n\nThe problem with the ObservableCollection is that it's very hard to update.  What normally ends up happening is code like this:\n\n\n// Earlier in the code\nvar listContents = new ObservableCollection\nModel\n();\n\n// When updating the code\nvar items = await table.ReadAllItemsAsync();\nlistContents.Clear();\nfor (var item in items) {\n    listContents.Add(item);\n}\n\n\n\n\nThe point of the ObservableCollection is that it emits an event whenever the list changes.  In the case where the table has thousands of entries, thousands of events will cause thousands of redraws, causing a major slow down in your code that you probably won't know until you have a large enough data set to note the problem.\n\n\nFortunately, one of the top Xamarin Evangelists, \nJames Montemagno\n, has created a set of helper classes that assist with this sort of problem.  The solution to this problem is to use the \nObservableRangeCollection\n, like this:\n\n\n// Earlier in the code\nvar listContents = new ObservableRangeCollection\nModel\n();\n\n// When updating the code\nvar items = await table.ReadAllItemsAsync();\nlistContents.ReplaceRange(items);\n\n\n\n\nWith this code, Xamarin Forms gets notified once instead of thousands of times.  There are actually several versions of this same behavior in the NuGet repository.\n\n\nAs to the second problem.  A \nListView\n with thousands of items will not be showing all the items at once.  A ListView will update all the items that have been updated, irrespective of whether they are visible or not.  The answer is to use a caching strategy.  There are two potential caching strategies.  With \nRetainElement\n, the ListView will generate a cell for each item in the list. This is the default, but it's really only good for certain situations (most notably when the cell has a large number of bindings).  For almost all situations, the \nRecycleElement\n caching strategy should be used.  In this caching strategy, the ListView will minimize the memory foot print and execution speed by only updating cells when they are in the viewable area.  This is good pretty much all the time, but explicitly when the cell has a small number of bindings or when each cell in the list has the same template.  All data about the cell must come from the binding context when using the \nRecycleElement\n caching strategy.\n\n\nYou can set the caching strategy right in the XAML:\n\n\nListView CachingStrategy=\nRecycleElement\n ...\n\n\n\n\n\nAlternatively, if you are creating a ListView in code, you can specify the caching strategy in the constructor:\n\n\nvar listView = new ListView(ListViewCachingStrategy.RecycleElement);\n\n\n\n\nThere are more techniques for improving ListView performance in the \nXamarin documentation\n\n\nBuild a Floating Action Button\n\n\nOne of the things that I wanted to do to my apps was to give them a little more in the way of normal UI design.  My original design (which I introduced back in [Chapter 1][ch1]) had teal buttons at the bottom of the page.  These buttons scrolled off the page when there were more items on the page than could reasonably be fit on the page.  To fix this, I wanted to create a button that was always relative to the viewport.\n\n\nThere are two steps to this.  Firstly, you need to convert the layout to a \nRelativeLayout\n.  For instance, my new \nListView.xaml\n file:\n\n\n?xml version=\n1.0\n encoding=\nutf-8\n ?\n\n\nContentPage x:Class=\nTaskList.Pages.TaskList\n\n             xmlns=\nhttp://xamarin.com/schemas/2014/forms\n\n             xmlns:x=\nhttp://schemas.microsoft.com/winfx/2009/xaml\n\n             Title=\n{Binding Title}\n\n    \nContentPage.Content\n\n        \nRelativeLayout\n\n            \nStackLayout RelativeLayout.HeightConstraint=\n{ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1}\n\n                         RelativeLayout.WidthConstraint=\n{ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1}\n\n\n\n\n\nThe original \nStackLayout\n layout renderer is placed inside the newly added \nRelativeLayout\n. The height and width constraints tell the StackLayout to consume the whole screen.\n\n\nAt the bottom of the page, I can add my button:\n\n\n            \n/StackLayout\n\n\n            \n!--  The Floating Button  --\n\n            \nButton BackgroundColor=\nTeal\n\n                    Command=\n{Binding AddNewItemCommand}\n\n                    RelativeLayout.XConstraint=\n{ConstraintExpression Type=RelativeToParent,\n                                                                      Property=Width,\n                                                                      Factor=1,\n                                                                      Constant=-60}\n\n                    RelativeLayout.YConstraint=\n{ConstraintExpression Type=RelativeToParent,\n                                                                      Property=Height,\n                                                                      Factor=1,\n                                                                      Constant=-60}\n\n                    Text=\n+\n\n                    TextColor=\nWhite\n /\n\n        \n/RelativeLayout\n\n    \n/ContentPage.Content\n\n\n/ContentPage\n\n\n\n\n\nThe \nStackLayout\n is the end of the StackLayout I introduced in the previous listing.  Now I can add a button (or any other View type control, including a custom control that I may have downloaded from NuGet or the Xamarin Plugins site) as well.  The button will float above the other content since it is added later.  The constraints in this case provide the location of the button.\n\n\nThere is more work to do.  For instance, you cannot click on the thing behind the button - the button always receives the click, which will (in this case) initiate the addition of a new item.  A custom control will allow you to provide iconography for the button, handle situations where you want to scroll behind and provide for the circular styling of the button which seems to be in-vogue right now.\n\n\nIf you want to place commands that are not normally used, you may want to consider the ToolbarItem area of the ContentPage.  Here is a snippet:\n\n\n    \nContentPage.ToolbarItems\n\n        \nToolbarItem Name=\nRefresh\n\n                     Command=\n{Binding RefreshCommand}\n\n                     Icon=\nrefresh.png\n\n                     Order=\nPrimary\n\n                     Priority=\n0\n /\n\n    \n/ContentPage.ToolbarItems\n\n\n\n\n\nOn Universal Windows (where this is actually important due to a lack of \"pull-to-refresh\" logic), you will see the familiar triple-dot on the app bar - clicking on the triple dot gives you access to the refresh command.\n\n\nWork with a Picker in a ViewModel\n\n\nThere are some controls that do not work well with a \nBindingContext\n.  One of these is the \nPicker\n. It has a number of Items that indicate the elements in the drop-down.   However, the Items are not bindable, which means that they cannot be accessed via a ViewModel.\n\n\nLet's say you wish to use the drop-down Picker from a view model and use an async command to update the elements in the Picker.  In the XAML file, give your picker a name:\n\n\nPicker x:Name=\nmyPicker\n /\n\n\n\n\n\nThis makes \nmyPicker\n a private variable in the generated class.  You can use this in the XAML code-behind file to pass the picker to your view-model by overriding the \nOnBindingContextChanged\n method:\n\n\nprotected override void OnBindingContextChanged()\n{\n    base.OnBindingContextChanged();\n\n    MyViewModel vm = BindingContext as MyViewModel;\n    if (vm != null)\n    {\n        vm.MyPicker = myPicker;\n        vm.RefreshPickerCommand.Execute(null);\n    }\n}\n\n\n\n\nIn your view-model, you can create a standard async command:\n\n\nasync Task RefreshPickerAsync()\n{\n    if (MyPicker.Items.Count == 0)\n    {\n        /* This section gets the list of items in the picker */\n        var table = await CloudService.GetTableAsync\nTag\n();\n        var tags = await tagTable.ReadAllItemsAsync();\n\n        /* This section updates MyPicker with the items */\n        foreach (var item in tags)\n        {\n            MyPicker.Items.Add(item.TagName);\n            // You may want to do a test for something in your item\n            // here and set SelectedIndex to the index if it matches\n        }\n    }\n}\n\n\n\n\nThe \nRefreshPickerAsync()\n command gets executed when the binding context is updated, which means the XAML has been executed and bound (thus the myPicker variable is set to the picker).  \n\n\nAnother way to accomplish this would be to encapsulate a picker in a custom control that does have a bindable Items element.  This has been discussed on the \nXamarin Forums\n.\n\n\nDisplay a Forms Element on only one platform\n\n\nOne of the issues I ran into was with the \nActivityIndicator\n.  The ListView has a built in activity indicator which is displayed on any platform that has \"pull to refresh\" logic enabled.  This is most notably iOS and Android, but not Windows.  The \nActivityIndicator\n object needs to be displayed on windows, but not the others.  The answer to this conundrum is to wrap the element in a \nContentView\n like this:\n\n\n    \nContentView\n\n        \nOnPlatform x:TypeArguments=\nView\n\n            \nOnPlatform.WinPhone\n\n                \nActivityIndicator\n                    HorizontalOptions=\nEnd\n\n                    IsRunning=\n{Binding IsBusy, Mode=OneWay}\n\n                    VerticalOptions=\nCenter\n\n                    Color=\nBlack\n /\n\n            \n/OnPlatform.WinPhone\n\n        \n/OnPlatform\n\n    \n/ContentView\n\n\n\n\n\nthe \nActivityIndicator\n will be displayed (at the appropriate time) on Windows, but not other platforms.\n\n\nInstall NuGet Packages in Multiple Projects\n\n\nOne of the common requirements we have in Xamarin Forms is to install NuGet packages\nin all the Xamarin Forms projects.  To do this, right-click on the solution and select\n\nManage NuGet Packages for Solution...\n.  When you install a package here, you can\nselect which projects it should be applied to, allowing you to install a package once\nacross all the dependent projects.\n\n\nAuto-Deploy Universal Windows Apps\n\n\nOne of the more annoying things is that you have to \nDeploy\n the Universal Windows app.\nThis gets in the way of the build process.  I like to build then run.  Having the extra\nDeploy step in there might not seem like much until you find yourself deploying several\ntimes an hour.\n\n\nFortunately, there is a simple fix for this.  Set up the \nConfiguration Manager\n to\nautomatically deploy the right libraries on every successful build.  To do this:\n\n\n\n\nIn Visual Studio, select \nBuild\n -\n \nConfiguration Manager...\n\n\nCheck the boxes you can under \nDeploy\n\n\n\n\n\n\n\n\nClick on \nClose\n\n\n\n\nThis setting is saved within the solution, so you only need to do it once per project.", 
            "title": "Xamarin Forms Tips"
        }, 
        {
            "location": "/xamarin_tips/#xamarin-forms-tips", 
            "text": "Over the many months I have spent coding Xamarin Forms, a number of people from the community and the Xamarin team have given me little tips to improve my Xamarin Forms code.  I hope you find them as useful as I did.", 
            "title": "Xamarin Forms Tips"
        }, 
        {
            "location": "/xamarin_tips/#improve-your-listview-performance", 
            "text": "It should be no surprise that performance matters.  Little things like smooth scrolling and fast load times are a must.  A lot of improvement can be gained by the techniques we have shown in this book since most of the perceived delays are caused by the back end loading times.  Eventually, a lot of apps generate a list.  The normal way for implementing this is with a  ListView  that has an  ObservableCollection   You load your data into the ObservableCollection and then update that whenever the data changes.  That, in turn, updates the ListView.  There are two problems that are inherent here.  The first is in the ObservableCollection and the second is in the ListView.  Let's tackle the ObservableCollection first.  The problem with the ObservableCollection is that it's very hard to update.  What normally ends up happening is code like this:  // Earlier in the code\nvar listContents = new ObservableCollection Model ();\n\n// When updating the code\nvar items = await table.ReadAllItemsAsync();\nlistContents.Clear();\nfor (var item in items) {\n    listContents.Add(item);\n}  The point of the ObservableCollection is that it emits an event whenever the list changes.  In the case where the table has thousands of entries, thousands of events will cause thousands of redraws, causing a major slow down in your code that you probably won't know until you have a large enough data set to note the problem.  Fortunately, one of the top Xamarin Evangelists,  James Montemagno , has created a set of helper classes that assist with this sort of problem.  The solution to this problem is to use the  ObservableRangeCollection , like this:  // Earlier in the code\nvar listContents = new ObservableRangeCollection Model ();\n\n// When updating the code\nvar items = await table.ReadAllItemsAsync();\nlistContents.ReplaceRange(items);  With this code, Xamarin Forms gets notified once instead of thousands of times.  There are actually several versions of this same behavior in the NuGet repository.  As to the second problem.  A  ListView  with thousands of items will not be showing all the items at once.  A ListView will update all the items that have been updated, irrespective of whether they are visible or not.  The answer is to use a caching strategy.  There are two potential caching strategies.  With  RetainElement , the ListView will generate a cell for each item in the list. This is the default, but it's really only good for certain situations (most notably when the cell has a large number of bindings).  For almost all situations, the  RecycleElement  caching strategy should be used.  In this caching strategy, the ListView will minimize the memory foot print and execution speed by only updating cells when they are in the viewable area.  This is good pretty much all the time, but explicitly when the cell has a small number of bindings or when each cell in the list has the same template.  All data about the cell must come from the binding context when using the  RecycleElement  caching strategy.  You can set the caching strategy right in the XAML:  ListView CachingStrategy= RecycleElement  ...   Alternatively, if you are creating a ListView in code, you can specify the caching strategy in the constructor:  var listView = new ListView(ListViewCachingStrategy.RecycleElement);  There are more techniques for improving ListView performance in the  Xamarin documentation", 
            "title": "Improve your ListView performance"
        }, 
        {
            "location": "/xamarin_tips/#build-a-floating-action-button", 
            "text": "One of the things that I wanted to do to my apps was to give them a little more in the way of normal UI design.  My original design (which I introduced back in [Chapter 1][ch1]) had teal buttons at the bottom of the page.  These buttons scrolled off the page when there were more items on the page than could reasonably be fit on the page.  To fix this, I wanted to create a button that was always relative to the viewport.  There are two steps to this.  Firstly, you need to convert the layout to a  RelativeLayout .  For instance, my new  ListView.xaml  file:  ?xml version= 1.0  encoding= utf-8  ?  ContentPage x:Class= TaskList.Pages.TaskList \n             xmlns= http://xamarin.com/schemas/2014/forms \n             xmlns:x= http://schemas.microsoft.com/winfx/2009/xaml \n             Title= {Binding Title} \n     ContentPage.Content \n         RelativeLayout \n             StackLayout RelativeLayout.HeightConstraint= {ConstraintExpression Type=RelativeToParent, Property=Height, Factor=1} \n                         RelativeLayout.WidthConstraint= {ConstraintExpression Type=RelativeToParent, Property=Width, Factor=1}   The original  StackLayout  layout renderer is placed inside the newly added  RelativeLayout . The height and width constraints tell the StackLayout to consume the whole screen.  At the bottom of the page, I can add my button:               /StackLayout \n\n             !--  The Floating Button  -- \n             Button BackgroundColor= Teal \n                    Command= {Binding AddNewItemCommand} \n                    RelativeLayout.XConstraint= {ConstraintExpression Type=RelativeToParent,\n                                                                      Property=Width,\n                                                                      Factor=1,\n                                                                      Constant=-60} \n                    RelativeLayout.YConstraint= {ConstraintExpression Type=RelativeToParent,\n                                                                      Property=Height,\n                                                                      Factor=1,\n                                                                      Constant=-60} \n                    Text= + \n                    TextColor= White  / \n         /RelativeLayout \n     /ContentPage.Content  /ContentPage   The  StackLayout  is the end of the StackLayout I introduced in the previous listing.  Now I can add a button (or any other View type control, including a custom control that I may have downloaded from NuGet or the Xamarin Plugins site) as well.  The button will float above the other content since it is added later.  The constraints in this case provide the location of the button.  There is more work to do.  For instance, you cannot click on the thing behind the button - the button always receives the click, which will (in this case) initiate the addition of a new item.  A custom control will allow you to provide iconography for the button, handle situations where you want to scroll behind and provide for the circular styling of the button which seems to be in-vogue right now.  If you want to place commands that are not normally used, you may want to consider the ToolbarItem area of the ContentPage.  Here is a snippet:       ContentPage.ToolbarItems \n         ToolbarItem Name= Refresh \n                     Command= {Binding RefreshCommand} \n                     Icon= refresh.png \n                     Order= Primary \n                     Priority= 0  / \n     /ContentPage.ToolbarItems   On Universal Windows (where this is actually important due to a lack of \"pull-to-refresh\" logic), you will see the familiar triple-dot on the app bar - clicking on the triple dot gives you access to the refresh command.", 
            "title": "Build a Floating Action Button"
        }, 
        {
            "location": "/xamarin_tips/#work-with-a-picker-in-a-viewmodel", 
            "text": "There are some controls that do not work well with a  BindingContext .  One of these is the  Picker . It has a number of Items that indicate the elements in the drop-down.   However, the Items are not bindable, which means that they cannot be accessed via a ViewModel.  Let's say you wish to use the drop-down Picker from a view model and use an async command to update the elements in the Picker.  In the XAML file, give your picker a name:  Picker x:Name= myPicker  /   This makes  myPicker  a private variable in the generated class.  You can use this in the XAML code-behind file to pass the picker to your view-model by overriding the  OnBindingContextChanged  method:  protected override void OnBindingContextChanged()\n{\n    base.OnBindingContextChanged();\n\n    MyViewModel vm = BindingContext as MyViewModel;\n    if (vm != null)\n    {\n        vm.MyPicker = myPicker;\n        vm.RefreshPickerCommand.Execute(null);\n    }\n}  In your view-model, you can create a standard async command:  async Task RefreshPickerAsync()\n{\n    if (MyPicker.Items.Count == 0)\n    {\n        /* This section gets the list of items in the picker */\n        var table = await CloudService.GetTableAsync Tag ();\n        var tags = await tagTable.ReadAllItemsAsync();\n\n        /* This section updates MyPicker with the items */\n        foreach (var item in tags)\n        {\n            MyPicker.Items.Add(item.TagName);\n            // You may want to do a test for something in your item\n            // here and set SelectedIndex to the index if it matches\n        }\n    }\n}  The  RefreshPickerAsync()  command gets executed when the binding context is updated, which means the XAML has been executed and bound (thus the myPicker variable is set to the picker).    Another way to accomplish this would be to encapsulate a picker in a custom control that does have a bindable Items element.  This has been discussed on the  Xamarin Forums .", 
            "title": "Work with a Picker in a ViewModel"
        }, 
        {
            "location": "/xamarin_tips/#display-a-forms-element-on-only-one-platform", 
            "text": "One of the issues I ran into was with the  ActivityIndicator .  The ListView has a built in activity indicator which is displayed on any platform that has \"pull to refresh\" logic enabled.  This is most notably iOS and Android, but not Windows.  The  ActivityIndicator  object needs to be displayed on windows, but not the others.  The answer to this conundrum is to wrap the element in a  ContentView  like this:       ContentView \n         OnPlatform x:TypeArguments= View \n             OnPlatform.WinPhone \n                 ActivityIndicator\n                    HorizontalOptions= End \n                    IsRunning= {Binding IsBusy, Mode=OneWay} \n                    VerticalOptions= Center \n                    Color= Black  / \n             /OnPlatform.WinPhone \n         /OnPlatform \n     /ContentView   the  ActivityIndicator  will be displayed (at the appropriate time) on Windows, but not other platforms.", 
            "title": "Display a Forms Element on only one platform"
        }, 
        {
            "location": "/xamarin_tips/#install-nuget-packages-in-multiple-projects", 
            "text": "One of the common requirements we have in Xamarin Forms is to install NuGet packages\nin all the Xamarin Forms projects.  To do this, right-click on the solution and select Manage NuGet Packages for Solution... .  When you install a package here, you can\nselect which projects it should be applied to, allowing you to install a package once\nacross all the dependent projects.", 
            "title": "Install NuGet Packages in Multiple Projects"
        }, 
        {
            "location": "/xamarin_tips/#auto-deploy-universal-windows-apps", 
            "text": "One of the more annoying things is that you have to  Deploy  the Universal Windows app.\nThis gets in the way of the build process.  I like to build then run.  Having the extra\nDeploy step in there might not seem like much until you find yourself deploying several\ntimes an hour.  Fortunately, there is a simple fix for this.  Set up the  Configuration Manager  to\nautomatically deploy the right libraries on every successful build.  To do this:   In Visual Studio, select  Build  -   Configuration Manager...  Check the boxes you can under  Deploy     Click on  Close   This setting is saved within the solution, so you only need to do it once per project.", 
            "title": "Auto-Deploy Universal Windows Apps"
        }, 
        {
            "location": "/android_appendix/", 
            "text": "Android Developer Notes\n\n\nThis chapter contains random notes that I discovered while developing mobile apps\nwith Xamarin Forms on the Android platform.  I hope they are useful to you.\n\n\nUnfortunately, your app has stopped\n\n\nSometimes, when operating with Android emulators, things will just break.  Some of the ideas here may assist with getting things going again:\n\n\n\n\n\n\nDon't use a Shared Mono Runtime\n\n\nIn Visual Studio, right-click the Android project, then select \nProperties\n or \nOptions\n.  In the \nAndroid Build\n section, select the \nGeneral\n tab.  Uncheck the checkbox next to \nUse Shared Mono Runtime\n.\n\n\n\n\n\n\nDisable non-required ABIs\n\n\nAlso in Visual Studio, right-click the Android project, then select \nProperties\n or \nOptions\n.  In the \nAndroid Build\n section, select the \nAdvanced\n tab.  Select only the ABIs that you are using.  For example, if you are running on the x86 emulator, you will want to disable the \narmeabi\n ABI.\n\n\n\n\n\n\nRebuild the entire project and then try running again.\n\n\nMissing libaot-mscorlib.dll.so\n\n\nWhen running an application is debug mode, I sometimes saw the following deployment issue:\n\n\nD/Mono    ( 1366): AOT module 'mscorlib.dll.so' not found: dlopen failed: library \n/data/app-lib/TaskList.Droid-2/libaot-mscorlib.dll.so\n not found\n\n\n\n\nTo fix this:\n\n\n\n\nRight-click on the \nDroid\n project and select \nProperties\n.\n\n\nSelect the \nAndroid Options\n tab.\n\n\nUncheck the \nUse Fast Deployment\n option.\n\n\nSave the properties sheet.\n\n\nRedeploy the application.\n\n\n\n\nFixing Errors with the Visual Studio Emulator for Android\n\n\nOne of the issues I found while running on the Visual Studio Emulator for Android involved debugging.  The Android app\nstarts, then immediately closes and debugging stops.  In the output window, you see \nCould not connect to the debugger\n.\nTo fix this:\n\n\n\n\nClose the Android Emulator window.\n\n\nOpen the \nHyper-V Manager\n.\n\n\nRight-click the emulator you are trying to use and select \nSettings...\n.\n\n\nExpand the \nProcessor\n node and select \nCompatibility\n.\n\n\nCheck the \nMigrate to a physical computer with a different processor version\n box.\n\n\nClick on \nOK\n.\n\n\n\n\nIt's a good idea to do this on all the emulators.  When you start the emulator, this error should be gone.\n\n\nDisabling Visual Studio Emulator for Android\n\n\nThere are two emulators for Android.  One is supplied by Microsoft (the Visual Studio Emulator for Android) and\none is supplied by Google (the Google Android Emulator).  You can only use one of them.  To enable the usage of\nthe Google Android Emulator, you have to disable the Visual Studio Emulator for Android.\n\n\nTo disable the Visual Studio Emulator for Android, disable Hyper-V with this command:\n\n\nbcdedit /set hypervisorlaunchtype off\n\n\n\n\nYou should then reboot.  If you wish to switch back to using the Visual Studio Emulator for Android, set the\nhypervisorlaunchtype to auto and reboot again.", 
            "title": "Android Developer Notes"
        }, 
        {
            "location": "/android_appendix/#android-developer-notes", 
            "text": "This chapter contains random notes that I discovered while developing mobile apps\nwith Xamarin Forms on the Android platform.  I hope they are useful to you.", 
            "title": "Android Developer Notes"
        }, 
        {
            "location": "/android_appendix/#unfortunately-your-app-has-stopped", 
            "text": "Sometimes, when operating with Android emulators, things will just break.  Some of the ideas here may assist with getting things going again:    Don't use a Shared Mono Runtime  In Visual Studio, right-click the Android project, then select  Properties  or  Options .  In the  Android Build  section, select the  General  tab.  Uncheck the checkbox next to  Use Shared Mono Runtime .    Disable non-required ABIs  Also in Visual Studio, right-click the Android project, then select  Properties  or  Options .  In the  Android Build  section, select the  Advanced  tab.  Select only the ABIs that you are using.  For example, if you are running on the x86 emulator, you will want to disable the  armeabi  ABI.    Rebuild the entire project and then try running again.", 
            "title": "Unfortunately, your app has stopped"
        }, 
        {
            "location": "/android_appendix/#missing-libaot-mscorlibdllso", 
            "text": "When running an application is debug mode, I sometimes saw the following deployment issue:  D/Mono    ( 1366): AOT module 'mscorlib.dll.so' not found: dlopen failed: library  /data/app-lib/TaskList.Droid-2/libaot-mscorlib.dll.so  not found  To fix this:   Right-click on the  Droid  project and select  Properties .  Select the  Android Options  tab.  Uncheck the  Use Fast Deployment  option.  Save the properties sheet.  Redeploy the application.", 
            "title": "Missing libaot-mscorlib.dll.so"
        }, 
        {
            "location": "/android_appendix/#fixing-errors-with-the-visual-studio-emulator-for-android", 
            "text": "One of the issues I found while running on the Visual Studio Emulator for Android involved debugging.  The Android app\nstarts, then immediately closes and debugging stops.  In the output window, you see  Could not connect to the debugger .\nTo fix this:   Close the Android Emulator window.  Open the  Hyper-V Manager .  Right-click the emulator you are trying to use and select  Settings... .  Expand the  Processor  node and select  Compatibility .  Check the  Migrate to a physical computer with a different processor version  box.  Click on  OK .   It's a good idea to do this on all the emulators.  When you start the emulator, this error should be gone.", 
            "title": "Fixing Errors with the Visual Studio Emulator for Android"
        }, 
        {
            "location": "/android_appendix/#disabling-visual-studio-emulator-for-android", 
            "text": "There are two emulators for Android.  One is supplied by Microsoft (the Visual Studio Emulator for Android) and\none is supplied by Google (the Google Android Emulator).  You can only use one of them.  To enable the usage of\nthe Google Android Emulator, you have to disable the Visual Studio Emulator for Android.  To disable the Visual Studio Emulator for Android, disable Hyper-V with this command:  bcdedit /set hypervisorlaunchtype off  You should then reboot.  If you wish to switch back to using the Visual Studio Emulator for Android, set the\nhypervisorlaunchtype to auto and reboot again.", 
            "title": "Disabling Visual Studio Emulator for Android"
        }, 
        {
            "location": "/vs_extensions/", 
            "text": "Visual Studio Extensions\n\n\nThis is the list of Visual Studio Extensions I use when developing for a combined web and mobile application:\n\n\n.ignore\n\n\nThere are many files that get added to projects that don't really need an editor.  They get set once and then\nforgotten about.  One of those is \n.gitignore\n.  There are others that follow this same pattern.  So there is\nan extension for adding them and dealing with them.\n\n\nMore Information\n\n\nAdd New File\n\n\nNot all files that you need to create have an extension.  A couple that I tend to have to deal with are README\nand LICENSE.  They have no extension.  Others include the \n.babelrc\n file for configuring BabelJS.  With this\nextension, you can create any file with any extension you need.\n\n\nMore Information\n\n\nEditor Enhancements\n\n\nI love small, single function extensions like this.  This one provides HTML and URL encoding - something I have\nto do a lot of when I am developing for the web.\n\n\nMore Information\n\n\nEditorConfig\n\n\nThere is a specification for a file called \n.editorconfig\n that will configure your editor for the requirements\nof a project in terms of character set, line ending, what a tab means and so on.  This extension reads that\nfile and configured the settings for the project accordingly.\n\n\nMore Information\n\n\nFile Icons\n\n\nVisual Studio comes with a pretty good list of file icons that appear in the Solution Explorer to tell you what\nsort of file it is.  But it isn't as exhaustive as it could be.  Two of our extensions thus far have dealt with\nadding files that Visual Studio can't handle - this gives them icons.\n\n\nMore Information\n\n\nGlyphfriend 2015\n\n\nIn our web applications, we use a lot of glyphs.  These are available through web frameworks like \nBootstrap\n.\nThis extensions shows what the glyph actually looks like right in the editor, which helps me ensure what I am\ndoing is correct.\n\n\nMore Information\n\n\nIndent Guides\n\n\nI sometimes write really long blocks of code.  This extension tells me where the blocks start and end.\n\n\nMore Information\n\n\nNPM Task Runner\n\n\nIf you are doing web development, this is not only recommended; it's vital.  It allows you to run npm commands\nfrom within the Task Runner Explorer, thus relieving you of the process of dropping down to the command line.\n\n\nMore Information\n\n\nOpen Command Line\n\n\nFor those occassions when you just can't avoid dropping down to the command line, this will ensure your command\nline starts at the right place.\n\n\nPackage Installer\n\n\nIn web development, sometimes you need to grab a package from elsewhere.  Unfortunately, there are a dozen different\nways of grabbing that package.  This extension deals with all the myriad ways of getting the package.\n\n\nMore Information\n\n\nRegex Tester\n\n\nAre you a regex master?  Me neither, which is why I like to test all my regular expressions before they go in my\ncode.  This extension adds that functionality as a window.\n\n\nMore Information\n\n\nRoslynator\n\n\nRoslyn was a major step forward in the power of C#, and there are a number of ways that it makes your code cleaner.\nUnfortunately, you have to learn them - all 160+ of them.  This extension looks for common refactorings for you, so\nyou can carry on coding as before.  Over time, you will learn how to do those things in Roslyn right the first time.\n\n\nMore Information\n\n\nSQLite / SQL Server Compact Toolbox\n\n\nAzure Mobile Apps uses SQLite as the basis of its offline cache.  That means that occassionally you are going to need\nto peek inside the SQLite database, even if you are only curious.\n\n\nMore Information\n\n\nSQLite for Universal Windows Platform\n\n\nDid I mention Azure Mobile Apps uses SQLite?  If you are developing UWP applications, you will need this extension.\n\n\nMore Information\n\n\nTrailing Whitespace Visualizer\n\n\nLanguages are sometimes a little problematic when it comes to embedded white space at the end of lines, especially in\nmulti-line strings.  This extension shows them up in the editor, allowing you to easily find and destroy them.\n\n\nMore Information\n\n\nWeb Essentials\n\n\nThere are few extensions that are more needed when you switch to web development.  This provides capabilities for all\nthe common file types used for web development, including style sheets and JavaScript files.\n\n\nMore Information\n\n\nWeb Extension Package\n\n\nThis isn't one extension.  It's several.  There are several extension modules for image optimizations, bundling,\nicon handling and accessibility monitoring.  This one extension installs all the others.  Use this if you are\ngoing to be doing web + mobile development.\n\n\nMore Information\n\n\nXamarin Forms Player\n\n\nOne of the big gotchas in Xamarin Forms development is the process by which the XAML is parsed, built and reviewed.\nIf you install the Xamarin Forms Player onto a device, this extension will push your XAML to that app and the app\nwill render the page for you, allowing a much tighter development cycle.\n\n\nMore Information\n\n\nXamarin Forms Templates\n\n\nThe default Xamarin Forms templates include projects for iOS and Android.  I wanted UWP, Windows Phone 8.1 and\npotentially others as well.  This set of templates gives me those additional templates.\n\n\nMore Information\n\n\nXamarin Test Recorder\n\n\nCreating UI tests is painful.  Running them on a large number of mobile devices is also painful.  Fortunately,\nthis extension handles the former - creating UI tests.  You can run the same UI tests across thousands of devices\nby using \nVisual Studio Mobile Center\n testing facilities.\n\n\nMore Information\n\n\nXAML Styler\n\n\nI find organizing XAML tedious.  Fortunately, I can clean up my XAML and give it a consistent style by using this\nextension.  You may not agree with its opinions, but at least it will make your code consistent.\n\n\nMore Information\n\n\nTaking a look at all these extensions, a big shout-out goes to \nMads Kristensen\n.  He may work at Microsoft,\nbut he puts out the great web extensions that make Visual Studio a major player in that space.", 
            "title": "Visual Studio Extensions"
        }, 
        {
            "location": "/vs_extensions/#visual-studio-extensions", 
            "text": "This is the list of Visual Studio Extensions I use when developing for a combined web and mobile application:", 
            "title": "Visual Studio Extensions"
        }, 
        {
            "location": "/vs_extensions/#ignore", 
            "text": "There are many files that get added to projects that don't really need an editor.  They get set once and then\nforgotten about.  One of those is  .gitignore .  There are others that follow this same pattern.  So there is\nan extension for adding them and dealing with them.  More Information", 
            "title": ".ignore"
        }, 
        {
            "location": "/vs_extensions/#add-new-file", 
            "text": "Not all files that you need to create have an extension.  A couple that I tend to have to deal with are README\nand LICENSE.  They have no extension.  Others include the  .babelrc  file for configuring BabelJS.  With this\nextension, you can create any file with any extension you need.  More Information", 
            "title": "Add New File"
        }, 
        {
            "location": "/vs_extensions/#editor-enhancements", 
            "text": "I love small, single function extensions like this.  This one provides HTML and URL encoding - something I have\nto do a lot of when I am developing for the web.  More Information", 
            "title": "Editor Enhancements"
        }, 
        {
            "location": "/vs_extensions/#editorconfig", 
            "text": "There is a specification for a file called  .editorconfig  that will configure your editor for the requirements\nof a project in terms of character set, line ending, what a tab means and so on.  This extension reads that\nfile and configured the settings for the project accordingly.  More Information", 
            "title": "EditorConfig"
        }, 
        {
            "location": "/vs_extensions/#file-icons", 
            "text": "Visual Studio comes with a pretty good list of file icons that appear in the Solution Explorer to tell you what\nsort of file it is.  But it isn't as exhaustive as it could be.  Two of our extensions thus far have dealt with\nadding files that Visual Studio can't handle - this gives them icons.  More Information", 
            "title": "File Icons"
        }, 
        {
            "location": "/vs_extensions/#glyphfriend-2015", 
            "text": "In our web applications, we use a lot of glyphs.  These are available through web frameworks like  Bootstrap .\nThis extensions shows what the glyph actually looks like right in the editor, which helps me ensure what I am\ndoing is correct.  More Information", 
            "title": "Glyphfriend 2015"
        }, 
        {
            "location": "/vs_extensions/#indent-guides", 
            "text": "I sometimes write really long blocks of code.  This extension tells me where the blocks start and end.  More Information", 
            "title": "Indent Guides"
        }, 
        {
            "location": "/vs_extensions/#npm-task-runner", 
            "text": "If you are doing web development, this is not only recommended; it's vital.  It allows you to run npm commands\nfrom within the Task Runner Explorer, thus relieving you of the process of dropping down to the command line.  More Information", 
            "title": "NPM Task Runner"
        }, 
        {
            "location": "/vs_extensions/#open-command-line", 
            "text": "For those occassions when you just can't avoid dropping down to the command line, this will ensure your command\nline starts at the right place.", 
            "title": "Open Command Line"
        }, 
        {
            "location": "/vs_extensions/#package-installer", 
            "text": "In web development, sometimes you need to grab a package from elsewhere.  Unfortunately, there are a dozen different\nways of grabbing that package.  This extension deals with all the myriad ways of getting the package.  More Information", 
            "title": "Package Installer"
        }, 
        {
            "location": "/vs_extensions/#regex-tester", 
            "text": "Are you a regex master?  Me neither, which is why I like to test all my regular expressions before they go in my\ncode.  This extension adds that functionality as a window.  More Information", 
            "title": "Regex Tester"
        }, 
        {
            "location": "/vs_extensions/#roslynator", 
            "text": "Roslyn was a major step forward in the power of C#, and there are a number of ways that it makes your code cleaner.\nUnfortunately, you have to learn them - all 160+ of them.  This extension looks for common refactorings for you, so\nyou can carry on coding as before.  Over time, you will learn how to do those things in Roslyn right the first time.  More Information", 
            "title": "Roslynator"
        }, 
        {
            "location": "/vs_extensions/#sqlite-sql-server-compact-toolbox", 
            "text": "Azure Mobile Apps uses SQLite as the basis of its offline cache.  That means that occassionally you are going to need\nto peek inside the SQLite database, even if you are only curious.  More Information", 
            "title": "SQLite / SQL Server Compact Toolbox"
        }, 
        {
            "location": "/vs_extensions/#sqlite-for-universal-windows-platform", 
            "text": "Did I mention Azure Mobile Apps uses SQLite?  If you are developing UWP applications, you will need this extension.  More Information", 
            "title": "SQLite for Universal Windows Platform"
        }, 
        {
            "location": "/vs_extensions/#trailing-whitespace-visualizer", 
            "text": "Languages are sometimes a little problematic when it comes to embedded white space at the end of lines, especially in\nmulti-line strings.  This extension shows them up in the editor, allowing you to easily find and destroy them.  More Information", 
            "title": "Trailing Whitespace Visualizer"
        }, 
        {
            "location": "/vs_extensions/#web-essentials", 
            "text": "There are few extensions that are more needed when you switch to web development.  This provides capabilities for all\nthe common file types used for web development, including style sheets and JavaScript files.  More Information", 
            "title": "Web Essentials"
        }, 
        {
            "location": "/vs_extensions/#web-extension-package", 
            "text": "This isn't one extension.  It's several.  There are several extension modules for image optimizations, bundling,\nicon handling and accessibility monitoring.  This one extension installs all the others.  Use this if you are\ngoing to be doing web + mobile development.  More Information", 
            "title": "Web Extension Package"
        }, 
        {
            "location": "/vs_extensions/#xamarin-forms-player", 
            "text": "One of the big gotchas in Xamarin Forms development is the process by which the XAML is parsed, built and reviewed.\nIf you install the Xamarin Forms Player onto a device, this extension will push your XAML to that app and the app\nwill render the page for you, allowing a much tighter development cycle.  More Information", 
            "title": "Xamarin Forms Player"
        }, 
        {
            "location": "/vs_extensions/#xamarin-forms-templates", 
            "text": "The default Xamarin Forms templates include projects for iOS and Android.  I wanted UWP, Windows Phone 8.1 and\npotentially others as well.  This set of templates gives me those additional templates.  More Information", 
            "title": "Xamarin Forms Templates"
        }, 
        {
            "location": "/vs_extensions/#xamarin-test-recorder", 
            "text": "Creating UI tests is painful.  Running them on a large number of mobile devices is also painful.  Fortunately,\nthis extension handles the former - creating UI tests.  You can run the same UI tests across thousands of devices\nby using  Visual Studio Mobile Center  testing facilities.  More Information", 
            "title": "Xamarin Test Recorder"
        }, 
        {
            "location": "/vs_extensions/#xaml-styler", 
            "text": "I find organizing XAML tedious.  Fortunately, I can clean up my XAML and give it a consistent style by using this\nextension.  You may not agree with its opinions, but at least it will make your code consistent.  More Information  Taking a look at all these extensions, a big shout-out goes to  Mads Kristensen .  He may work at Microsoft,\nbut he puts out the great web extensions that make Visual Studio a major player in that space.", 
            "title": "XAML Styler"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\nFurther Reading\n\n\nAPI References\n\n\nSamples\n\n\nHow to get Help", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "", 
            "title": "References"
        }, 
        {
            "location": "/references/#further-reading", 
            "text": "", 
            "title": "Further Reading"
        }, 
        {
            "location": "/references/#api-references", 
            "text": "", 
            "title": "API References"
        }, 
        {
            "location": "/references/#samples", 
            "text": "", 
            "title": "Samples"
        }, 
        {
            "location": "/references/#how-to-get-help", 
            "text": "", 
            "title": "How to get Help"
        }, 
        {
            "location": "/credits/", 
            "text": "Credits\n\n\nI got a lot of assistance during the writing of this book.  The following people assisted (in no\nparticular order):\n\n\n\n\nAustin Emser\n\n\nChris Gillum\n\n\nChris Risner\n\n\nDonna Malayeri\n\n\nErich Brunner\n\n\nFabio Cavalcante\n\n\nJames Montemagno\n\n\nKevin Hilscher\n\n\nMarthin Freij\n\n\nMike James\n\n\nPierce Boggan\n\n\nSteve Lee\n\n\n\n\nIn addition, several people helped by correcting formatting and spelling mistakes.  A huge thank you to everyone who participated in this book.  It would not have been possible without you.", 
            "title": "Credits"
        }, 
        {
            "location": "/credits/#credits", 
            "text": "I got a lot of assistance during the writing of this book.  The following people assisted (in no\nparticular order):   Austin Emser  Chris Gillum  Chris Risner  Donna Malayeri  Erich Brunner  Fabio Cavalcante  James Montemagno  Kevin Hilscher  Marthin Freij  Mike James  Pierce Boggan  Steve Lee   In addition, several people helped by correcting formatting and spelling mistakes.  A huge thank you to everyone who participated in this book.  It would not have been possible without you.", 
            "title": "Credits"
        }
    ]
}